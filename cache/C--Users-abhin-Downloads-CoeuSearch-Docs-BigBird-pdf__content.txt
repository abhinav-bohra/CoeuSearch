Big Bird : Transformers Longer Sequences Manzil Zaheer , Guru Guruganesh , Avinava Dubey , Joshua Ainslie , Chris Alberti , Anirudh Ravula , Qifan Wang , Santiago Ontanon , Li Yang , Amr Ahmed Philip Pham ,         n  J   ] G L .  c [   v           .         : v  X r  Google Research { manzilz , gurug , avinavadubey } @ google.com Abstract Transformers-based models , BERT , one successful deep learning models NLP . Unfortunately , one core limitations quadratic dependency ( mainly terms memory ) sequence length due full attention mechanism . remedy , propose , BIGBIRD , sparse attention mechanism reduces quadratic dependency linear . show BIGBIRD universal approximator sequence functions Turing complete , thereby preserving properties quadratic , full attention model . Along way , theoretical analysis reveals beneﬁts (   ) global tokens ( CLS ) , attend entire sequence part sparse attention mechanism . proposed sparse attention handle sequences length  x previously possible using similar hardware . consequence capability handle longer context , BIGBIRD drastically improves performance various NLP tasks question answering summarization . also propose novel applications genomics data .   Introduction Models based Transformers [    ] , BERT [    ,    ] , wildly successful wide variety Natural Language Processing ( NLP ) tasks consequently mainstay modern NLP research . versatility robustness primary drivers behind wide-scale adoption Transformers . model easily adapted diverse range sequence based tasks – seq seq model translation [    ] , summarization [    ] , generation [    ] , etc . standalone encoders sentiment analysis [    ] , POS tagging [    ] , machine reading comprehension [    ] , etc . – known vastly outperform previous sequence models like LSTM [    ] . key innovation Transformers introduction self-attention mechanism , evaluated parallel token input sequence , eliminating sequential dependency recurrent neural networks , like LSTM . parallelism enables Transformers leverage full power modern SIMD hardware accelerators like GPUs/TPUs , thereby facilitating training NLP models datasets unprecedented size . ability train large scale data led surfacing models like BERT [    ] T  [    ] , pretrain transformers large general purpose corpora transfer knowledge down-stream task . pretraining led signiﬁcant improvement low data regime downstream tasks [    ] well tasks sufﬁcient data [     ] thus major force behind ubiquity transformers contemporary NLP . self-attention mechanism overcomes constraints RNNs ( namely sequential nature RNN ) allowing token input sequence attend independently every token sequence . design choice several interesting repercussions . particular , full self-attention computational memory requirement quadratic sequence length . note corpus large , sequence length , provides context many applications limited . Using commonly available current hardware model sizes , requirement   th Conference Neural Information Processing Systems ( NeurIPS      ) , Vancouver , Canada .       translates roughly able handle input sequences length     tokens . reduces direct applicability tasks require larger context , like QA [    ] , document classiﬁcation , etc . However , know self-attention Transformers useful , theoretical understanding rudimentary . aspects self-attention model necessary performance ? say expressivity Transformers similar models ? Apriori , even clear design proposed self-attention mechanism effective RNNs . example , self-attention even obey sequence order permutation equivariant . concern partially resolved , Yun et al . [     ] showed transformers expressive enough capture continuous sequence sequence functions compact domain . Meanwhile , Pérez et al . [    ] showed full transformer Turing Complete ( i.e . simulate full Turing machine ) . Two natural questions arise : achieve empirical beneﬁts fully quadratic self-attention scheme using fewer inner-products ? sparse attention mechanisms preserve expressivity ﬂexibility original network ? paper , address questions produce sparse attention mechanism improves performance multitude tasks require long contexts . systematically develop BIGBIRD , attention mechanism whose complexity linear number tokens ( Sec .   ) . take inspiration graph sparsiﬁcation methods understand proof expressiveness Transformers breaks full-attention relaxed form proposed attention pattern . understanding helped us develop BIGBIRD , theoretically expressive also empirically useful . particular , BIGBIRD consists three main part : • set g global tokens attending parts sequence . • tokens attending set w local neighboring tokens . • tokens attending set r random tokens . leads high performing attention mechanism scaling much longer sequence lengths (  x ) . summarize , main contributions :   . BIGBIRD satisﬁes known theoretical properties full transformer ( Sec .   ) . particular , show adding extra tokens allows one express continuous sequence sequence functions ( n ) -inner products . Furthermore , show standard assumptions regarding precision , BIGBIRD Turing complete .   . Empirically , show extended context modelled BIGBIRD beneﬁts variety NLP tasks . achieve state art results question answering document summarization number different datasets . Summary results presented Sec .   .   . Lastly , introduce novel application attention based models long contexts beneﬁcial : extracting contextual representations genomics sequences like DNA . longer masked LM pretraining , BIGBIRD improves performance downstream tasks promoter- region chromatin proﬁle prediction ( Sec .   ) .  .  Related Work number interesting attempts , aimed alleviating quadratic dependency Transformers , broadly categorized two directions . First line work embraces length limitation develops method around . Simplest methods category employ sliding window [    ] , general work ﬁts following general paradigm : using mechanism select smaller subset relevant contexts feed transformer optionally iterate , i.e . call transformer block multiple time different contexts time . prominently , SpanBERT [    ] , ORQA [    ] , REALM [    ] , RAG [    ] achieved strong performance different tasks . However , worth noting methods often require signiﬁcant engineering efforts ( like back prop large scale nearest neighbor search ) hard train . Second line work questions full attention essential tried come approaches require full attention , thereby reducing memory computation requirements . Prominently , Dai et al . [    ] , Sukhbaatar et al . [    ] , Rae et al . [    ] proposed auto-regresive models work well left-to-right language modeling suffer tasks require bidirectional context . Child et al . [    ] proposed sparse model reduces complexity ( n n ) , Kitaev et al . [    ] reduced complexity ( n log ( n ) ) using LSH compute nearest neighbors . √   ( ) Random attention ( b ) Window attention ( c ) Global Attention ( ) BIGBIRD Figure   : Building blocks attention mechanism used BIGBIRD . White color indicates absence attention . ( ) random attention r =   , ( b ) sliding window attention w =   ( c ) global attention g =   . ( ) combined BIGBIRD model . Ye et al . [     ] proposed binary partitions data Qiu et al . [    ] reduced complexity using block sparsity . Recently , Longformer [   ] introduced localized sliding window based mask global mask reduce computation extended BERT longer sequence based tasks . Finally , work closely related built work Extended Transformers Construction [   ] . work designed encode structure text transformers . idea global tokens used extensively achieve goals . theoretical work seen providing justiﬁcation success models well . important note aforementioned methods heuristic based empirically versatile robust original transformer , i.e . architecture attain SoTA multiple standard benchmarks . ( one exception Longformer include comparisons , see App . E.  detailed comparison ) . Moreover , approximations come theoretical guarantees .   BIGBIRD Architecture section , describe BIGBIRD model using generalised attention mechanism used layer transformer operating input sequence X = ( x  , ... , xn ) ∈ Rn×d . generalized attention mechanism described directed graph whose vertex set [ n ] = {   , . . . , n } . set arcs ( directed edges ) represent set inner products attention mechanism consider . Let N ( ) denote out-neighbors set node , ith output vector generalized attention mechanism deﬁned ATTND ( X ) = xi + H ( cid:   ) ( cid:   ) Qh ( xi ) Kh ( XN ( ) ) ( cid:   ) σ · Vh ( XN ( ) ) ( ) h=  Qh , Kh : Rd → Rm query key functions respectively , Vh : Rd → Rd value function , σ scoring function ( e.g . softmax hardmax ) H denotes number heads . Also note XN ( ) corresponds matrix formed stacking { xj : j ∈ N ( ) } inputs . complete digraph , recover full quadratic attention mechanism Vaswani et al . [    ] . simplify exposition , operate adjacency matrix graph even though underlying graph maybe sparse . elaborate , ∈ [   ,   ] n×n ( , j ) =   query attends key j zero otherwise . example , ones matrix ( BERT ) , leads quadratic complexity , since tokens attend every token . view self-attention fully connected graph allows us exploit existing graph theory help reduce complexity . problem reducing quadratic complexity self-attention seen graph sparsiﬁcation problem . well-known random graphs expanders approximate complete graphs number different contexts including spectral properties [    ,    ] . believe sparse random graph attention mechanism two desiderata : small average path length nodes notion locality , discuss . Let us consider simplest random graph construction , known Erd˝os-Rényi model , edge independently chosen ﬁxed probability . random graph ˜Θ ( n ) edges , shortest path two nodes logarithmic number nodes [    ,    ] . consequence , random graph approximates complete graph spectrally second eigenvalue ( adjacency matrix ) quite far ﬁrst eigenvalue [   ,    ,   ] . property leads rapid mixing time random walks grpah , informally suggests information ﬂow fast pair nodes . Thus , propose sparse attention query attends r random number keys i.e . ( , · ) =   r randomly chosen keys ( see Fig .  a ) .   second viewpoint inspired creation BIGBIRD contexts within NLP computational biology data displays great deal locality reference . phenomenon , great deal information token derived neighboring tokens . pertinently , Clark et al . [    ] investigated self-attention models NLP tasks concluded neighboring inner-products extremely important . concept locality , proximity tokens linguistic structure , also forms basis various linguistic theories transformational- generative grammar . terminology graph theory , clustering coefﬁcient measure locality connectivity , high graph contains many cliques near-cliques ( subgraphs almost fully interconnected ) . Simple Erd˝os-Rényi random graphs high clustering coefﬁcient [    ] , class random graphs , known small world graphs , exhibit high clustering coefﬁcient [    ] . particular model introduced Watts Strogatz [    ] high relevance us achieves good balance average shortest path notion locality . generative process model follows : Construct regular ring lattice , graph n nodes connected w neighbors , w/  side . Model MLM SQuAD MNLI words begin sliding window nodes . random subset ( k % ) connections replaced random connection . (     - k ) % local connections retained . However , deleting random edges might in- efﬁcient modern hardware , retain , affect properties . summary , capture local structures context , BIGBIRD , deﬁne sliding window attention , self attention width w , query location attends − w/  + w/  keys . notation , ( , − w/  : + w/  ) =   ( see Fig .  b ) . initial sanity check , performed basic experiments test whether intuitions sufﬁcient getting performance close BERT like models , keeping attention linear number tokens . found random blocks local window insufﬁcient capturing context necessary compete performance BERT . BERT-base Random ( R ) Window ( W ) R + W Table   : Building block comparison @       .    .    .    .    .    .    .    .    .    .    .    .  ﬁnal piece BIGBIRD inspired theoretical analysis ( Sec .   ) , critical empirical performance . speciﬁcally , theory utilizes importance “ global tokens ” ( tokens attend tokens sequence tokens attend ( see Fig .  c ) . global tokens deﬁned two ways : • BIGBIRD-ITC : internal transformer construction ( ITC ) , make existing tokens “ global ” , attend entire sequence . Concretely , choose subset G indices ( g : = |G| ) , ( , : ) =   ( : , ) =   ∈ G . • BIGBIRD-ETC : extended transformer construction ( ETC ) , include additional “ global ” tokens CLS . Concretely , add g global tokens attend existing tokens . notation , corresponds creating new matrix B ∈ [   ,   ] ( N +g ) × ( N +g ) adding g rows matrix , B ( , : ) =   , B ( : , ) =   ∈ {   ,   , . . . g } , B ( g + , g + j ) = ( , j ) ∀ , j ∈ {   , . . . , N } . adds extra location store context see experiments improves performance . ﬁnal attention mechanism BIGBIRD ( Fig .  d ) three properties : queries attend r random keys , query attends w/  tokens left location w/  right location contain g global tokens ( global tokens existing tokens extra added tokens ) . provide implementation details App . .   Theoretical Results Sparse Attention Mechanism section , show sparse attention mechanisms powerful expressive full-attention mechanisms two respects . First , show sparse attention mechanisms used standalone encoder ( BERT ) , Universal Approximators sequence sequence functions style Yun et al . [     ] . note property also explored theoretically contemporary work Yun et al . [     ] . Second , unlike [     ] , show sparse encoder-decoder transformers Turing Complete ( assuming conditions deﬁned [    ] ) . Complementing positive results , also show moving sparse-attention   mechanism incurs cost , i.e . free lunch . Sec .  .  , show lower bounds exhibiting natural task sufﬁciently sparse mechanism require polynomially layers .  .  Notation complete Transformer encoder stack nothing repeated application single-layer encoder ( independent parameters ) . denote class Transformer encoders stack , deﬁned using generalized encoder ( Sec .   ) , H , , q consists H-heads head size q hidden layer size output network , attention layer deﬁned directed graph .  key difference proposed attention mechanism Vaswani et al . [    ] , Yun et al . [     ] add special token beginning sequence assign special vector . refer x  . Therefore graph vertex set {   } ∪ [ n ] = {   ,   ,   , . . . , n } . assume extra node respective vector dropped ﬁnal output layer transformer . avoid cumbersome notation , still treat transformer mapping sequences X ∈ Rn×d Rn×d . also allow transformer append position embeddings E ∈ Rd×n matrix X input layer . Finally , need deﬁne function class distance measure proving universal approximation property . Let FCD denote set continuous functions f : [   ,   ] n×d → Rn×d continuous respect topology deﬁned ( cid:   ) p norm . Recall p ≥   , ( cid:   ) p distance dp ( f  , f  ) = ( cid:  ) ( cid:   ) ( cid:    ) f  ( X ) − f  ( X ) ( cid:    ) p pdX ( cid:  )  /p .  .  Universal Approximators Deﬁnition   . star-graph centered   graph deﬁned {   , . . . , n } . neighborhood vertices N ( ) = {   , } ∈ {   . . . n } N (   ) = {   , . . . n } . main theorem sparse attention mechanism deﬁned graph containing universal approximator : Theorem   . Given   < p < ∞ ( cid:   ) >   , f ∈ FCD , exists transformer sparse-attention , g ∈ H , , q dp ( f , g ) ≤ ( cid:   ) graph containing star graph .  prove theorem , follow standard proof structure outlined [     ] . Step   : Approximate FCD piece-wise constant functions . Since f continuous function bounded domain [   ,   ) n×d , approximate suitable piece-wise constant function . accomplished suitable partition region [   ,   ) grid granularity δ get discrete set Gδ . Therefore , assume dealing function ¯f : Gδ → Rn×d , dp ( f , ¯f ) ≤ ( cid:   )   . Step   : Approximate piece-wise constant functions modiﬁed transformers . key step proof self-attention mechanism used generate contextual-mapping input . Informally , contextual mapping unique code pair consisting matrix ( X , xi ) column . uniqueness allows Feed forward layers use code map unique output column . main technical challenge computing contextual mapping using sparse attention mechanism . done [     ] using “ selective ” shift operator shift entries speciﬁc interval . Key proof fact shift , exactly range largest entry smallest entry . Creating contextual mapping sparse attention mechanism quite challenge . particular , query attends keys , clear sufﬁcient information corralled make contextual embedding entire matrix . get around , develop sparse shift operator shifts entries matrices lie certain range . exact amount shift controlled directed sparse attention graphg D. second key ingredient use additional global token . carefully applying operator set chosen ranges , show column contain unique mapping full mapping . Therefore , augment loss inner-products self attention mechanism using multiple layers auxiliary global token .   Step   : Approximate modiﬁed transformers original Transformers : ﬁnal step ap- proximate modiﬁed transformers original transformer uses ReLU softmax . provide full details App . .  .  Turing Completeness Transformers general class . original paper Vaswani et al . [    ] , used encoder decoder . previous section outlined powerful encoders , another natural question ask additional power decoder along encoder ? Pérez et al . [    ] showed full transformer based quadratic attention mechanism Turing Complete . result makes one unrealistic assumption , model works arbitrary precision model . course , necessary otherwise , Transformers bounded ﬁnite state machines Turing Complete . natural ask full attention mechanism necessary . sparse attention mechanism also used simulate Turing Machine ? show indeed case : use sparse encoder sparse decoder simulate Turing Machine . use sparse attention mechanism transformer architecture , need deﬁne suitable modiﬁcation token reacts previous tokens . Unlike case BERT , entire attention mechanism applied , full transformers , sparse attention mechanism decoder side used token token . Secondly work Pérez et al . [    ] , uses token representation tape history uses full attention move retrieve correct tape symbol . construction Pérez et al . [    ] goes sparse attentions , except addressing scheme point back history ( Lemma B.  [    ] ) . show simulate using sparse attention mechanism defer details App . B .  .  Limitations demonstrate natural task solved full attention mechanism (   ) -layers . However , standard complexity theoretic assumptions , problem requires ˜Ω ( n ) -layers sparse attention layers ˜O ( n ) edges ( BIGBIRD ) . ( ˜O hides poly-logarthmic factors ) . Consider simple problem ﬁnding corresponding furthest vector vector given sequence length n. Formally , Task   . Given n unit vectors { u  , . . . , un } , ﬁnd f ( u  , . . . , un ) → ( u ∗ , . . . , un∗ ) ﬁxed j ∈ [ n ] , deﬁne j∗ = arg maxk ( cid:    ) uk − uj ( cid:    )     . Finding vectors furthest apart boils minimize inner product search case unit vectors . full-attention mechanism appropriate query keys , task easy evaluate pair-wise inner products . impossibility sparse-attention follows hardness results stemming Orthogonal Vector Conjecture ( OVC ) [   ,   ,   ,    ] . OVC widely used assumption ﬁne-grained complexity . Informally , states one determine minimum inner product among n boolean vectors   subquadratic time . App . C , show reduction using OVC show transformer g ∈ H=  , m= d , q=  sparse directed graph evaluate Task   , solve orthogonal vector problem . Proposition   . exists single layer full self-attention g ∈ H=  , m= d , q=  evaluate Task   , i.e . g ( u  , ... , un ) = [ u ∗ , . . . , un∗ ] , sparse-attention graph ˜O ( n ) edges ( i.e . inner product evaluations ) , would require ˜Ω ( n −o (   ) ) layers . give formal proof fact App . C .    Experiments : Natural Language Processing section goal showcase beneﬁts modeling longer input sequence NLP tasks , select three representative tasks . begin basic masked language modeling ( MLM ; Devlin et al .    ) check better contextual representations learnt utilizing longer contiguous sequences . Next , consider QA supporting evidence , capability handle longer sequence would allow us retrieve evidence using crude systems like TF-IDF/BM   .   Model RoBERTa Longformer BIGBIRD-ITC BIGBIRD-ETC HotpotQA NaturalQ TriviaQA WikiHop Ans   .    .    .    .  Sup   .    .    .    .  Joint   .    .    .    .  LA - -   .    .  SA - -   .    .  Full   .    .    .    .  MCQ   .    .    .    .  Table   : QA Dev results using Base size models . report accuracy WikiHop F  HotpotQA , Natural Questions , TriviaQA . Model HGN [    ] GSAN ReﬂectionNet [    ] RikiNet-v  [    ] Fusion-in-Decoder [    ] SpanBERT [    ] MRC-GCN [    ] MultiHop [    ] Longformer [   ] BIGBIRD-ETC HotpotQA NaturalQ TriviaQA WikiHop Ans   .    .  - - - - - -   .    .  Sup   .    .  - - - - - -   .    .  Joint   .    .  - - - - - -   .    .  LA - -   .    .  - - - - -   .  SA - -   .    .  - - - - -   .  Full Veriﬁed MCQ - - - -   .    .  - -   .    .  - - - -   .    .  - -   .    .  - - - - - -   .    .    .    .  Table   : Fine-tuning results Test set QA tasks . Test results ( F  HotpotQA , Natural Questions , TriviaQA , Accuracy WikiHop ) picked respective leaderboard . task top-  leaders picked including BIGBIRD-etc . Natural Questions Long Answer ( LA ) , TriviaQA , WikiHop , BIGBIRD-ETC new state-of-the-art . HotpotQA third leaderboard F  second Exact Match ( EM ) . Finally , tackle long document classiﬁcation discriminating information may located ﬁrst     tokens . summarize results BIGBIRD using sequence length       , defer setup details including computational resources , batch size , step size , App . E . Pretraining MLM follow [    ,    ] create base large versions BIGBIRD pretrain using MLM objective . task involves predicting random subset tokens masked . use four standard data-sets pretraining ( listed App . E.  , Tab .   ) , warm-starting public RoBERTa checkpoint  . compare performance predicting masked tokens terms bits per character , following [   ] . seen App . E.  , Tab .    , BIGBIRD Longformer perform better limited length RoBERTa , BIGBIRD-ETC performing best . note trained models reasonable   GB memory/chip batch size   -   . memory efﬁciency due efﬁcient blocking sparsity structure sparse attention mechanism described Sec .   . Question Answering ( QA ) considered following four challenging datasets :   . Natural Questions [    ] : given question , ﬁnd short span answer ( SA ) given evidences well highlight paragraph given evidences containing information correct answer ( LA ) .   . HotpotQA-distractor [     ] : Similar natural questions , requires ﬁnding answer ( Ans ) well supporting facts ( Sup ) different documents needed multi-hop reasoning given evidences .   . TriviaQA-wiki [    ] : need provide answer given question using provided Wikipedia evidence , however , answer might present given evidence .  code available http : //goo.gle/bigbird-transformer  https : //github.com/pytorch/fairseq/tree/master/examples/roberta   smaller veriﬁed subset question , given evidence guaranteed contain answer . Nevertheless , model answer span selection problem case well .   . WikiHop [    ] : Chose correct option multiple-choice questions ( MCQ ) , aggregating information spread across multiple documents given evidences . tasks competitive , multiple highly engineered systems designed speciﬁc dataset conﬁrming respective output formats . fair comparison , use additional regularization training BIGBIRD , details provided App . E.  along exact architecture description . experiment using base sized model select best conﬁguration development set dataset ( reported Tab .   ) . see BIGBIRD-ETC , expanded global tokens consistently outperforms models . Thus , chose conﬁguration train large sized model used evaluation hidden test set . Tab .   , compare BIGBIRD-ETC model top-  entries leaderboard excluding BIGBIRD . One clearly see importance using longer context Longformer BIGBIRD outperform models smaller contexts . Also , worth noting BIGBIRD submission single model , whereas top-  entries Natural Questions ensembles , might explain slightly lower accuracy exact answer phrase selection . Classiﬁcation experiment datasets different lengths contents , speciﬁcally various document classiﬁcation GLUE tasks . Following BERT , used one layer cross entropy loss top ﬁrst [ CLS ] token . see gains using BIGBIRD signiﬁcant longer documents fewer training examples . instance , using base sized model , BIGBIRD improves state-of-the-art Arxiv dataset   % points . Patents dataset , improvement using simple BERT/RoBERTa , given large size training data improvement SoTA ( BERT based ) signiﬁcant . Note performance gain seen much smaller IMDb dataset . Along experimental setup detail , present detailed results App . E.  show competitive performance .  .  Encoder-Decoder Tasks encoder-decoder setup , one easily see suffer quadratic complexity due full self attention . focus introducing sparse attention mechanism BIGBIRD encoder side . , practical generative applications , length output sequence typically small compared input . example text summarization , see realistic scenarios ( c.f . App . E.  Tab .    ) median output sequence length ∼     input Model Arxiv PubMed BigPatent R-  R-  R-L R-  R-  R-L R-  R-  R-L  r  r   r P SumBasic [    ] LexRank [    ] LSA [    ] Attn-Seq Seq [    ] Pntr-Gen-Seq Seq [    ] Long-Doc-Seq Seq [    ] Sent-CLF [    ] Sent-PTR [    ] Extr-Abst-TLM [    ] Dancer [    ] Transformer e   B + RoBERTa [    ] + Pegasus [     ] BIGBIRD-RoBERTa   .     .     .     .     .     .     .     .     .     .     .     .     .     .   e Pegasus ( Reported ) [     ]   .     .     .   Pegasus ( Re-eval ) BIGBIRD-Pegasus g r  L  .     .    .    .    .     .    .     .     .     .    .    .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .    .    .     .     .     .     .     .     .    .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .   -   .     .   -   .     .     .   -   .     .     .     .     .     .     .    .     .   -  .     .   -   .     .     .   -   .     .     .     .     .     .     .     .     .   -   .     .   -   .     .     .   -   .     .     .     .     .     .     .   Table   : Summarization ROUGE score long documents .   sequence ’ median length >      . applications , efﬁcient use sparse attention mechanism encoder full self-attention decoder . Summarization Document summarization task creating short accurate summary text document . used three long document datasets testing model details mention Tab .    . paper focus abstractive summarization long documents using longer contextual encoder improve performance . reasons two fold : First , salient content evenly distributed long document , ﬁrst     tokens , design BigPatents dataset [    ] . Second , longer documents exhibit richer discourse structure summaries considerably abstractive , thereby observing context helps . pointed recently [    ,     ] , pretraining helps generative tasks , warm start general purpose MLM pretraining base-sized models well utilizing state-of-the-art summarization speciﬁc pretraining Pegasus [     ] large-sized models . results training BIGBIRD sparse encoder along full decoder long document datasets presented Tab .   . clearly see modeling longer context brings signiﬁcant improvement . Along hyperparameters , also present results shorter widespread datasets App . E.  , show using sparse attention hamper performance either .   Experiments : Genomics recent upsurge using deep learning genomics data [    ,     ,    ] , resulted improved performance several biologically-signiﬁcant tasks promoter site prediction [    ] , methylation analysis [    ] , predicting functional effects non-coding variant [     ] , etc . approaches consume DNA sequence fragments inputs , therefore believe longer input sequence handling capability BIGBIRD would beneﬁcial many functional effects DNA highly non-local [    ] . Furthermore , taking inspiration NLP , learn powerful contextual representations DNA fragments utilizing abundant unlabeled data ( e.g . human reference genome , Saccharomyces Genome Database ) via MLM pretraining . Next , showcase long input BIGBIRD along proposed pretraining signiﬁcantly improves performances two downstream tasks . Detailed experimental setup two tasks provided App . F . Pre-training MLM explored Liang [    ] , instead oper- ating base pairs , propose ﬁrst segment DNA tokens increase context length ( App . F , Fig .   ) . particular , build byte-pair encoding [    ] table DNA sequence size   K , token representing  .   base pairs average . learn contextual representation token human reference genome ( GRCh   )   using MLM objective . report bits per character ( BPC ) held-out set Tab .   . ﬁnd attention based contextual representation DNA improve BPC , improved using longer context . SRILM [    ] BERT ( sqln .     ) Table   : MLM BPC BIGBIRD ( sqln .      )  .    .   Model BPC  .   Promoter Region Prediction Promoter DNA region typically lo- cated upstream gene , site transcription initiation . Multiple methods proposed identify promoter regions given DNA sequence [    ,    ,    ,    ,    ] , important ﬁrst step understanding gene regulation . corresponding machine learning task classify given DNA fragment promoter non-promoter sequence . use dataset compiled Oubounyt et al . [    ] built Eukaryotic Promoter Database ( EPDnew ) [    ]   . ﬁnetuned pretrained BIGBIRD model , using training data report F  test dataset . compare results previously reported best method Tab .   . see BIGBIRD achieve nearly perfect accuracy   % jump previous best reported accuracy . CNNProm [    ] DeePromoter [    ] Table   : Comparison . BIGBIRD   .    .  Model   .  F   https : //www.ncbi.nlm.nih.gov/assembly/GCF_         .  /  https : //epd.epfl.ch/human/human_database.php ? db=human   TF Model -   .    .    .  -   .  BIGBIRD HM DHS gkm-SVM [    ] DeepSea [     ] Chromatin-Proﬁle Prediction Non-coding regions DNA code proteins . Majority diseases trait associated single-nucleotide polymorphism correlated non-coding genomic variations [     ,    ] . Thus , understanding functional effects non-coding regions DNA important task . important step process , deﬁned Zhou Troyanskaya [     ] , predict large-scale chromatin-proﬁling non-coding genomic sequence . effect , DeepSea [     ] , compiled     chromatin-proﬁle  . M non-coding variants Encyclopedia DNA Elements ( ENCODE )   Roadmap Epigenomics projects  . corresponding ML task predict , given non-coding region DNA ,     chromatin-proﬁle including     transcription factors ( TF ) binding proﬁles     different TFs ,     DNase sensitivity ( DHS ) proﬁles     histone-mark ( HM ) proﬁles . jointly learn     binary classiﬁers predict functional effects sequence DNA fragments . held-out chromosomes , compare AUC baselines Tab .   see signiﬁcantly improve performance harder task HM , known longer-range correlations [    ] others . Table   : Chromatin-Proﬁle Prediction   .    .    .    Conclusion propose BIGBIRD : sparse attention mechanism linear number tokens . BIGBIRD satisﬁes number theoretical results : universal approximator sequence sequence functions also Turing complete . Theoretically , use power extra global tokens preserve expressive powers model . complement results showing moving sparse attention mechanism incur cost . Empirically , BIGBIRD gives state-of-the-art performance number NLP tasks question answering long document classiﬁcation . introduce attention based contextual language model DNA ﬁne-tune stream tasks promoter region prediction predicting effects non-coding variants . References [   ] A. Abboud , V. V. Williams , O. Weimann . Consequences faster alignment se- quences . International Colloquium Automata , Languages , Programming , pages   –   . Springer ,      . [   ] A. Abboud , A. Backurs , V. V. Williams . Tight hardness results lcs sequence similarity measures .      IEEE   th Annual Symposium Foundations Computer Science , pages   –   . IEEE ,      . [   ] J. Abreu , L. Fred , D. Macêdo , C. Zanchettin . Hierarchical attentional hybrid neural net- works document classiﬁcation . International Conference Artiﬁcial Neural Networks , pages    –    . Springer ,      . [   ] J. Ainslie , S. Ontanon , C. Alberti , P. Pham , A. Ravula , S. Sanghai . Etc : Encoding long structured data transformers . arXiv preprint arXiv:    .      ,      . [   ] C. Alberti , K. Lee , M. Collins . bert baseline natural questions . arXiv preprint arXiv:    .      ,      . [   ] J. Alt , R. Ducatez , A. Knowles . Extremal eigenvalues critical erd\h { } sr\ ’ enyi graphs . arXiv preprint arXiv:    .      ,      . [   ] A. Backurs P. Indyk . Edit distance computed strongly subquadratic time ( unless seth false ) . Proceedings forty-seventh annual ACM symposium Theory computing , pages   –   ,      . [   ] I. Beltagy , M. E. Peters , A. Cohan . Longformer : long-document transformer . arXiv preprint arXiv:    .      ,      .  https : //www.encodeproject.org/  http : //www.roadmapepigenomics.org/    [   ] F. Benaych-Georges , C. Bordenave , A. Knowles , et al . Largest eigenvalues sparse inhomo- geneous erd˝os–rényi graphs . Annals Probability ,    (   ) :    –     ,      . [    ] F. Benaych-Georges , C. Bordenave , A. Knowles , et al . Spectral radii sparse random matrices . Annales de l ’ Institut Henri Poincaré , Probabilités et Statistiques , volume    , pages     –     . Institut Henri Poincaré ,      . [    ] R. Bharanikumar , K. A. R. Premkumar , A. Palaniappan . Promoterpredict : sequence-based modelling escherichia coli σ   promoter strength yields logarithmic dependence promoter strength sequence . PeerJ ,   : e     ,      . [    ] S. Buldyrev , A. Goldberger , S. Havlin , R. Mantegna , M. Matsa , C.-K. Peng , M. Simons , H. Stanley . Long-range correlation properties coding noncoding dna sequences : Genbank analysis . Physical Review E ,    (   ) :     ,      . [    ] A. Busia , G. E. Dahl , C. Fannjiang , D. H. Alexander , E. Dorfman , R. Poplin , C. Y. McLean , P.-C. Chang , M. DePristo . deep learning approach pattern recognition short dna sequences . BioRxiv , page        ,      . [    ] J. Chen , S.-t. Lin , G. Durrett . Multi-hop question answering via reasoning chains . arXiv preprint arXiv:    .      ,      . [    ] Y.-C. Chen , Z. Gan , Y. Cheng , J. Liu , J. Liu . Distilling knowledge bert text generation . arXiv preprint arXiv:    .      ,      . [    ] R. Child , S. Gray , A. Radford , I. Sutskever . Generating long sequences sparse transformers . arXiv preprint arXiv:    .      ,      . [    ] F. Chung L. Lu . average distances random graphs given expected degrees . Proceedings National Academy Sciences ,    (    ) :     –      ,      . [    ] C. Clark M. Gardner . Simple effective multi-paragraph reading comprehension . arXiv preprint arXiv:    .      ,      . [    ] K. Clark , U. Khandelwal , . Levy , C. D. Manning . bert look ? analysis bert ’ attention . arXiv preprint arXiv:    .      ,      . [    ] A. Cohan , F. Dernoncourt , D. S. Kim , T. Bui , S. Kim , W. Chang , N. Goharian . discourse-aware attention model abstractive summarization long documents . arXiv preprint arXiv:    .      ,      . [    ] Z. Dai , Z. Yang , Y. Yang , J. Carbonell , Q. V. Le , R. Salakhutdinov . Transformer-xl : Attentive language models beyond ﬁxed-length context . arXiv:    .      ,      . [    ] J. Devlin , M.-W. Chang , K. Lee , K. Toutanova . Bert : Pre-training deep bidirectional transformers language understanding . arXiv preprint arXiv:    .      ,      . [    ] L. Dong , N. Yang , W. Wang , F. Wei , X. Liu , Y. Wang , J. Gao , M. Zhou , H.-W. Hon . Uniﬁed language model pre-training natural language understanding generation . Advances Neural Information Processing Systems , pages      –      ,      . [    ] R. Dreos , G. Ambrosini , R. Cavin Périer , P. Bucher . Epd epdnew , high-quality promoter resources next-generation sequencing era . Nucleic acids research ,    ( D  ) : D   –D    ,      . [    ] G. Erkan D. R. Radev . Lexrank : Graph-based lexical centrality salience text summarization . Journal artiﬁcial intelligence research ,   :   –    ,      . [    ] Y. Fang , S. Sun , Z. Gan , R. Pillai , S. Wang , J. Liu . Hierarchical graph network multi-hop question answering . arXiv preprint arXiv:    .      ,      . [    ] L. . Gates , C. E. Foulds , B. W. ’ Malley . Histone marks ‘ driver ’ seat ’ : functional roles steering transcription cycle . Trends biochemical sciences ,    (    ) :   –    ,      .    [    ] J. Gehring , M. Auli , D. Grangier , D. Yarats , Y. N. Dauphin . Convolutional sequence Proceedings   th International Conference Machine sequence learning . Learning-Volume    , pages     –     . JMLR . org ,      . [    ] S. Gehrmann , Y. Deng , A. M. Rush . Bottom-up abstractive summarization . arXiv preprint arXiv:    .      ,      . [    ] M. Ghandi , D. Lee , M. Mohammad-Noori , M. . Beer . Enhanced regulatory sequence prediction using gapped k-mer features . PLoS computational biology ,    (   ) ,      . [    ] A. Gidiotis G. Tsoumakas . divide-and-conquer approach summarization academic articles . arXiv preprint arXiv:    .      ,      . [    ] M. Gong . ReﬂectionNet ,      ( accessed June   ,      ) . URL https : //www.microsoft . com/en-us/research/people/migon/ . [    ] S. Gray , A. Radford , D. P. Kingma . Gpu kernels block-sparse weights . arXiv preprint arXiv:    .      ,   ,      . [    ] K. Guu , K. Lee , Z. Tung , P. Pasupat , M.-W. Chang . Realm : Retrieval-augmented language model pre-training . arXiv preprint arXiv:    .      ,      . [    ] J . , L. Wang , L. Liu , J. Feng , H. Wu . Long document classiﬁcation local word glimpses via recurrent attention learning . IEEE Access ,  :     –      ,      . [    ] K. M. Hermann , T. Kocisky , E. Grefenstette , L. Espeholt , W. Kay , M. Suleyman , P. Blun- Advances neural information som . Teaching machines read comprehend . processing systems , pages     –     ,      . [    ] S. Hochreiter J. Schmidhuber . Long short-term memory . Neural computation ,   (   ) :     –     ,      . [    ] S. Hoory , N. Linial , A. Wigderson . Expander graphs applications . Bulletin American Mathematical Society ,    (   ) :   –    ,      . [    ] G. Izacard E. Grave . Leveraging passage retrieval generative models open domain question answering . arXiv preprint arXiv:    .      ,      . [    ] Y. Jiang , J. Petrak , X . Song , K. Bontcheva , D. Maynard . Team bertha von suttner semeval-     task   : Hyperpartisan news detection using elmo sentence representation Proceedings   th International Workshop Semantic convolutional network . Evaluation , pages    –    ,      . [    ] M. Joshi , E. Choi , D. S. Weld , L. Zettlemoyer . Triviaqa : large scale distantly supervised challenge dataset reading comprehension . Proceedings   th Annual Meeting Association Computational Linguistics , Vancouver , Canada , July      . Association Computational Linguistics . [    ] M. Joshi , D. Chen , Y. Liu , D. S. Weld , L. Zettlemoyer , . Levy . Spanbert : Improv- ing pre-training representing predicting spans . Transactions Association Computational Linguistics ,  :  –   ,      . [    ] E. Katzav , O. Biham , A. K. Hartmann . Distribution shortest path lengths subcritical erd˝os-rényi networks . Physical Review E ,    (   ) :       ,      . [    ] W. J. Kent , C. W. Sugnet , T. S. Furey , K. M. Roskin , T. H. Pringle , A. M. Zahler , D. Haussler . human genome browser ucsc . Genome research ,    (   ) :   –     ,      . [    ] U. Khandelwal , K. Clark , D. Jurafsky , L. Kaiser . Sample efﬁcient text summarization using single pre-trained transformer . arXiv preprint arXiv:    .      ,      . [    ] E. Khurana , Y. Fu , D. Chakravarty , F. Demichelis , M. A. Rubin , M. Gerstein . Role non-coding sequence variants cancer . Nature Reviews Genetics ,    (   ) :   ,      .    [    ] J. Kiesel , M. Mestre , R. Shukla , E. Vincent , P. Adineh , D. Corney , B. Stein , M. Potthast . Semeval-     task   : Hyperpartisan news detection . Proceedings   th International Workshop Semantic Evaluation , pages    –    ,      . [    ] B. Kim , H. Kim , G. Kim . Abstractive summarization reddit posts multi-level memory networks . arXiv preprint arXiv:    .      ,      . [    ] N. Kitaev , L. Kaiser , A. Levskaya . Reformer : efﬁcient transformer . International Conference Learning Representations ,      . [    ] T. Kudo J. Richardson . Sentencepiece : simple language independent subword tokenizer detokenizer neural text processing . arXiv preprint arXiv:    .      ,      . [    ] V. Kumar , A. Choudhary , E. Cho . Data augmentation using pre-trained transformer models . arXiv preprint arXiv:    .      ,      . [    ] T. Kwiatkowski , J. Palomaki , O. Redﬁeld , M. Collins , A. Parikh , C. Alberti , D. Epstein , I. Polosukhin , J. Devlin , K. Lee , et al . Natural questions : benchmark question answering research . Transactions Association Computational Linguistics ,  :   –    ,      . [    ] J.-S. Lee J. Hsiang . Patent classiﬁcation ﬁne-tuning bert language model . World Patent Information ,   :       ,      . [    ] K. Lee , M.-W. Chang , K. Toutanova . Latent retrieval weakly supervised open domain question answering . arXiv preprint arXiv:    .      ,      . [    ] J. J . Levy , A. J. Titus , C. L. Petersen , Y. Chen , L. A. Salas , B. C. Christensen . Methylnet : automated modular deep learning approach dna methylation analysis . BMC bioinformatics ,    (   ) : –   ,      . [    ] M. Lewis , Y. Liu , N. Goyal , M. Ghazvininejad , A. Mohamed , . Levy , V. Stoyanov , L. Zettlemoyer . Bart : Denoising sequence-to-sequence pre-training natural language generation , translation , comprehension . arXiv preprint arXiv:    .      ,      . [    ] P. Lewis , E. Perez , A. Piktus , F. Petroni , V. Karpukhin , N. Goyal , H. Küttler , M. Lewis , W.-t . Yih , T. Rocktäschel , et al . Retrieval-augmented generation knowledge-intensive nlp tasks . arXiv preprint arXiv:    .      ,      . [    ] W. Liang . Segmenting dna sequence words based statistical language model . Nature Precedings , pages  –  ,      . [    ] H. Lin , Z.-Y . Liang , H. Tang , W. Chen . Identifying sigma   promoters novel pseudo nucleotide composition . IEEE/ACM transactions computational biology bioinformatics ,      . [    ] J. Lin , D. Quan , V. Sinha , K. Bakshi , D. Huynh , B. Katz , D. R. Karger . makes good answer ? role context question answering . Proceedings Ninth IFIP TC   International Conference Human-Computer Interaction ( INTERACT      ) , pages   –   ,      . [    ] D. Liu , Y. Gong , J. Fu , Y. Yan , J. Chen , D. Jiang , J. Lv , N. Duan . Rikinet : Reading wikipedia pages natural question answering . arXiv preprint arXiv:    .      ,      . [    ] Y. Liu M. Lapata . Text summarization pretrained encoders . arXiv preprint arXiv:    .      ,      . [    ] Y. Liu , M. Ott , N. Goyal , J . Du , M. Joshi , D. Chen , . Levy , M. Lewis , L. Zettlemoyer , V. Stoyanov . Roberta : robustly optimized bert pretraining approach . arXiv preprint arXiv:    .      ,      . [    ] A. Maas , R. E. Daly , P. T. Pham , D. Huang , A. Y. Ng , C. Potts . Learning word vectors sentiment analysis . Proceedings   th annual meeting association computational linguistics : Human language technologies , pages    –    ,      .    [    ] L. Martin , B. Muller , P. J. O. Suárez , Y. Dupont , L. Romary , É. V. de la Clergerie , D. Seddah , B. Sagot . Camembert : tasty french language model . arXiv preprint arXiv:    .      ,      . [    ] D. Miller . Leveraging bert extractive text summarization lectures . arXiv preprint arXiv:    .      ,      . [    ] S. Narayan , S. B. Cohen , M. Lapata . ’ give details , summary ! topic-aware convolutional neural networks extreme summarization . arXiv preprint arXiv:    .      ,      . [    ] A. Nenkova L. Vanderwende . impact frequency summarization . Microsoft Research , Redmond , Washington , Tech . Rep. MSR-TR-     ,     ,      . [    ] M. L. Olson , L. Zhang , C.-N. Yu . Adapting pretrained language models long document classiﬁcation . OpenReview ,      . [    ] A. v. d. Oord , Y. Li , O. Vinyals . Representation learning contrastive predictive coding . arXiv preprint arXiv:    .      ,      . [    ] M. Oubounyt , Z. Louadi , H. Tayara , K. T. Chong . Deepromoter : Robust promoter predictor using deep learning . Frontiers genetics ,    ,      . [    ] J. Pérez , J. Marinkovi´c , P. Barceló . turing completeness modern neural network architectures . arXiv preprint arXiv:    .      ,      . [    ] J. Qiu , H. , . Levy , S. W.-t. Yih , S. Wang , J. Tang . Blockwise self-attention long document understanding . arXiv preprint arXiv:    .      ,      . [    ] J. W. Rae , A. Potapenko , S. M. Jayakumar , T. P. Lillicrap . Compressive transformers long-range sequence modelling . arXiv preprint arXiv:    .      ,      . [    ] C. Raffel , N. Shazeer , A. Roberts , K. Lee , S. Narang , M. Matena , Y. Zhou , W. Li , P. J. Liu . Exploring limits transfer learning uniﬁed text-to-text transformer . arXiv preprint arXiv:    .      ,      . [    ] S. Rothe , S. Narayan , A. Severyn . Leveraging pre-trained checkpoints sequence generation tasks . arXiv preprint arXiv:    .      ,      . [    ] . See , P. J. Liu , C. D. Manning . Get point : Summarization pointer-generator networks . arXiv preprint arXiv:    .      ,      . [    ] E. Sharma , C. Li , L. Wang . Bigpatent : large-scale dataset abstractive coherent summarization . arXiv preprint arXiv:    .      ,      . [    ] P. Shaw , J. Uszkoreit , A. Vaswani . Self-attention relative position representations . arXiv preprint arXiv:    .      ,      . [    ] D. A. Spielman S.-H. Teng . Spectral sparsiﬁcation graphs . SIAM Journal Computing ,    (   ) :   –     ,      . [    ] S. Subramanian , R. Li , J. Pilault , C. Pal . extractive abstractive neural document summarization transformer language models . arXiv preprint arXiv:    .      ,      . [    ] S. Sukhbaatar , E. Grave , P. Bojanowski , A. Joulin . Adaptive attention span transformers . arXiv preprint arXiv:    .      ,      . [    ] C. Sun , L. Huang , X. Qiu . Utilizing bert aspect-based sentiment analysis via construct- ing auxiliary sentence . arXiv preprint arXiv:    .      ,      . [    ] D. Sussman . Lecture Notes Boston University     Spring      ,      ( accessed June   ,      ) . URL http : //math.bu.edu/people/sussman/MA   _    /     -  -  -Lecture- .html . [    ] I. Sutskever , O. Vinyals , Q. V. Le . Sequence sequence learning neural networks . Advances neural information processing systems , pages     –     ,      .    [    ] A. Tampuu , Z. Bzhalava , J. Dillner , R. Vicente . Viraminer : Deep learning raw dna sequences identifying viral genomes human samples . PloS one ,    (   ) ,      . [    ] Z. Tang , Y. Shen , X. , W. Xu , J. Yu , W. Lu . Multi-hop reading comprehension across documents path-based graph convolutional network . arXiv:    .      ,      . [    ] T. Thongtan T. Phienthrakul . Sentiment classiﬁcation using document embeddings trained cosine similarity . Proceedings   th Annual Meeting Association Computational Linguistics : Student Research Workshop , pages    –    ,      . [    ] T. H. Trinh Q. V. Le . simple method commonsense reasoning . arXiv preprint arXiv:    .      ,      . [    ] R. K. Umarov V. V. Solovyev . Recognition prokaryotic eukaryotic promoters using convolutional deep learning neural networks . PloS one ,    (   ) ,      . [    ] A. Vaswani , N. Shazeer , N. Parmar , J. Uszkoreit , L. Jones , A. N. Gomez , Ł. Kaiser , I. Polosukhin . Attention need . Advances neural information processing systems , pages     –     ,      . [    ] A. Wang , A. Singh , J. Michael , F. Hill , . Levy , S. R. Bowman . Glue : multi- task benchmark analysis platform natural language understanding . arXiv preprint arXiv:    .      ,      . [    ] Z. Wang , P. Ng , X. , R. Nallapati , B. Xiang . Multi-passage bert : globally normalized bert model open-domain question answering . arXiv preprint arXiv:    .      ,      . [    ] D. J. Watts S. H. Strogatz . Collective dynamics ‘ small-world ’ networks . nature ,     (      ) :   –    ,      . [    ] J. Welbl , P. Stenetorp , S. Riedel . Constructing datasets multi-hop reading compre- hension across documents . Transactions Association Computational Linguistics ,   :    –    ,      . [    ] R. Williams . new algorithm optimal  -constraint satisfaction implications . Theoretical Computer Science ,     (  -  ) :   –    ,      . [    ] S. Wiseman , S. M. Shieber , A. M. Rush . Challenges data-to-document generation . arXiv preprint arXiv:    .      ,      . [    ] X. Xiao , Z.-C. Xu , W.-R. Qiu , P. Wang , H.-T. Ge , K.-C. Chou . ipsw (  l ) -pseknc : two-layer predictor identifying promoters strength hybrid features via pseudo k-tuple nucleotide composition . Genomics ,     (   ) :    –     ,      . [    ] Y. Yang , R. Zhang , S. Singh , J. . Exploiting sequence-based features predicting enhancer–promoter interactions . Bioinformatics ,    (    ) : i   –i    ,      . [     ] Z. Yang , P. Qi , S. Zhang , Y. Bengio , W. W. Cohen , R. Salakhutdinov , C. D. Manning . Hotpotqa : dataset diverse , explainable multi-hop question answering . arXiv preprint arXiv:    .      ,      . [     ] Z. Yang , Z. Dai , Y. Yang , J. Carbonell , R. R. Salakhutdinov , Q. V. Le . Xlnet : Generalized autoregressive pretraining language understanding . Advances neural information processing systems , pages     –     ,      . [     ] Z. Yao , S. Cao , W. Xiao , C. Zhang , L. Nie . Balanced sparsity efﬁcient dnn inference gpu . Proceedings AAAI Conference Artiﬁcial Intelligence , volume    , pages     –     ,      . [     ] Z. Ye , Q. Guo , Q. Gan , X. Qiu , Z. Zhang . Bp-transformer : Modelling long-range context via binary partitioning . arXiv preprint arXiv:    .      ,      . [     ] C. Yun , S. Bhojanapalli , A. S. Rawat , S. J. Reddi , S. Kumar . transformers universal approximators sequence-to-sequence functions ? arXiv preprint arXiv:    .      ,      .    [     ] C. Yun , Y.-W. Chang , S. Bhojanapalli , A. S. Rawat , S. J. Reddi , S. Kumar . ( n ) connections expressive enough : Universal approximability sparse transformers . Advances Neural Information Processing Systems ,      . [     ] H. Zhang , C.-L. Hung , M. Liu , X. Hu , Y.-Y . Lin . Ncnet : Deep learning network models predicting function non-coding dna . Frontiers genetics ,    ,      . [     ] J. Zhang , Y. Zhao , M. Saleh , P. J. Liu . Pegasus : Pre-training extracted gap-sentences abstractive summarization . arXiv preprint arXiv:    .      ,      . [     ] X. Zhang , J. Zhao , Y. LeCun . Character-level convolutional networks text classiﬁcation . Advances neural information processing systems , pages    –    ,      . [     ] J. Zhou O. G. Troyanskaya . Predicting effects noncoding variants deep learning– based sequence model . Nature methods ,    (    ) :   –    ,      . [     ] Y. Zhu , R. Kiros , R. Zemel , R. Salakhutdinov , R. Urtasun , A. Torralba , S. Fidler . Aligning books movies : Towards story-like visual explanations watching movies reading books . IEEE international conference computer vision , pages   –   ,      .    Big Bird : Transformers Longer Sequences – Appendix Universal Approximators A.  Notation begin setting notations following Pérez et al . [    ] formally describe complete architecture Transformers . single layer Transformer encoder parametric function Enc receiving sequence X = ( x  , ... , xn ) vectors Rd returning sequence Z = ( z  , ... , zn ) length . zi dimensional vector well . interchangeably treat sequence X matrix Rn×d . Enc two components :   . attention mechanism ATTN takes sequence X returns sequence ( a  , ... , ) length dimensionality ;   . two layer fully connected network takes vector Rd returns vector Rd . i-th output vector Enc ( X ) computed follows : zi = ( ai ) + ai  ai = ATTN ( X ) + xi (   ) remains deﬁne ATTN next . described Sec .   , attention mechanism parameterized three functions : Q , K , V : Rd → Rm . paper , assume simply matrix products : Q ( x ) = xWQ , K ( x ) = xWK , V ( x ) = xWV , WQ , WK , WV ∈ Rd×m WV ∈ Rd×d . reality multi- headed attention used , i.e . one , H-sets Query/Key/Value weight matrices , W h K h =   , ... , H. Thus , directed graph [ n ] , ith output vector generalized attention mechanism would V , W h Q , W h ATTND ( X ) = H ( cid:   ) h=  σ ( cid:  ) ( xiW h Q ) ( XN ( ) W h K ) ( cid:  ) · ( XN ( ) W h V ) ( ) N ( ) denote out-neighbors set node D. words , set arcs ( directed edges ) represents set inner products attention mechanism consider . Also recall σ scoring function softmax hardmax . Lastly , deﬁne output fully connected network follows : ( ai ) = ReLU ( aiW  + b  ) W  · +b  ( FF ) W  ∈ Rd×q , W  ∈ Rq×d , b  ∈ Rp , b  ∈ Rd parameters output network . Additional Notation introduce pieces additional notation useful . Let [ , b ) δ = { , + δ , . . . , + ( cid:   ) b−a δ ( cid:   ) · δ } . Therefore , [   ,   ) δ = {   , δ ,  δ , . . . , (   − δ ) } . use   [ E ] denote indicator variable ;   event E occurs   otherwise . A.  Proof section , present full proof theorem   . proof contain three parts . ﬁrst third part largely follow standard techniques . main innovation lies second part . A. .  Approximate FCD piece-wise constant functions First , consider suitable partition region (   ,   ) grid granularity δ , denote Gδ . using Lemma   Yun et al . [     ] , restate completeness : Lemma   ( Lemma   [     ] ) . given f ∈ FCD   ≤ p ≤ ∞ , exists δ >   exists piece-wise constant function ¯f dp ( f , ¯f ) ≤ ( cid:   )   . Concretely , ¯f deﬁned ¯f ( X ) = ( cid:   ) P ∈Gδ f ( P ) ·   [ ( cid:    ) ReLU ( X − P ) ( cid:    ) ∞ ≤ δ ]    Since transformers learn positional embedding E , without loss generality , consider translated function . particular , deﬁne E =         δ−d δ− d ... δ− ( n−  )   δ−d δ− d   δ−d δ− d . . . . . . . . .   δ−d δ− d       δ− ( n−  ) δ− ( n−  ) . . . δ− ( n−  ) try approximate g ( X ) = f ( X − E ) g deﬁned domain [   ,   ] × [ δ−d , δ−d +   ] × · · · × [ δ− ( n−  ) , δ− ( n−  ) +   ] d. , apply suitable modiﬁcation Lemma   , consider discretized grid GE δ : = [   ,   ] δ × [ δ−d , δ−d +   ] δ × · · · × [ δ− ( n−  ) , δ− ( n−  ) +   ] δ . Therefore , sufﬁces approximate function ¯f : GE δ → Rn×d deﬁned ¯f ( X ) = ( cid:   ) P ∈GE δ f ( P − E ) ·   [ ( cid:    ) ReLU ( X − P ) ( cid:    ) ∞ ≤ δ ] . A. .  Contextual Mappings Sparse Attention Mechanisms Throughout section , assume given function extra global token index   vectors extra dimension appended . latter assumption without loss generality use Feed-Forward Network append sparse dimensions . particular , associate X ∈ R ( n+  ) × ( d+  ) write X = ( x  , x  , . . . , xn ) . Although function δ ⊂ Rn×d , amend function natural way making ignore deﬁned GE ﬁrst column . avoid excessive clutter , assume function value evaluated last n columns . δ , column xi ∈ [ δ− ( i−  ) , δ− ( i−  ) +   ) main idea section use contextual mapping enable Transformers compute discretized function . contextual mapping unique encoding tuple ( X , xi ) X ∈ GE δ ∈ [ n ] . restate deﬁnition adapted setting Deﬁnition   ( Defn  .  [     ] ) . ( Contextual Mapping ) contextual mapping function mapping q : GE δ → Rn satisﬁes following :   . P ∈ GE δ , q ( P ) contains distinct entries .   . two P , P ( cid:   ) ∈ GE δ P ( cid:   ) = P ( cid:   ) , entries q ( P ) q ( P ( cid:   ) ) distinct . key technical novelty proof computing contextual mapping using sparse attention mechanism . create “ selective shift ” operator shifts entries vector lie certain range . use shift operator strategically ensure attain contextual mapping end process . lemma , based parts proof Lemma   [     ] , states implement suitable “ selective ” shift operator using sparse attention mechanism . Lemma   . Given function ψ : R ( n+  ) × ( d+  ) × R  → R ( n+  ) ×  vector u ∈ Rd+  sparse attention mechanism based directed graph , implement selective shift operator receives input matrix X ∈ R ( n+  ) × ( d+  ) outputs X + ρ · ψu ( X , b  , b  ) ψu ( Z ; b  , b  ) = ( cid:   ) ( maxj∈N ( ) uT Zj − minj∈N ( ) uT Zj ) e    b  ≤ uT Zj ≤ b  else . Note e  ∈ Rd+  denotes (   ,   , . . . ,   ) . Proof . Consider function , implemented sparse attention mechanism : ˜ψ ( X , b ) = σH ( cid:    ) ( uT · Xi ) · ( uT XN ( ) − b T N ( ) ) e (   ) ( uT XN ( ) ) ( cid:    )    Key , Query Value functions simply afﬁne transformations X . Given graph , function evaluate following : ˜ψ ( Z ; b ) = ( cid:   ) ( maxj∈N ( ) uT Zj ) e  ( minj∈N ( ) uT Zj ) e  uT Zj > b uT Zj < b Therefore say ˜ψ ( Z ; bQ ) − ˜ψ ( Z ; bQ ( cid:   ) ) satisﬁes ψ ( Z ; b  , b  ) = ( cid:   ) ( maxj∈N ( ) uT Zj − minj∈N ( ) uT Zj ) e    b  ≤ uT Zj ≤ b  else following lemma , heart proof , uses selective shift operators construct contextual mappings . Lemma   . exists function gc : R ( n+  ) × ( d+  ) → R ( n+  ) unique vector u , P ∈ GE δ gc ( P ) : = ( cid:    ) u , g ( P ) ( cid:    ) satisﬁes property gc contextual mapping P . Furthermore , gc ∈  , ,  using composition sparse attention layers long contains star graph .  Proof . Deﬁne u ∈ Rd+  = [   , δ−  , δ−  , . . . , δ−d+  , δ−nd ] let X  = (   , . . . ,   ,   ) . assume ( cid:    ) xi , x  ( cid:    ) =   , assuming columns x  , . . . , xn appended   . successfully encode entire context token , interleave shift operator target original columns   , . . . , n target global column   . column targeted , inner product u encode entire context ﬁrst columns . Next , shift global token take context account . subsequently used remaining columns . ∈ {   ,   , . . . , n } , use li denote innerproducts ( cid:    ) u , xi ( cid:    ) beginning . fi = ( cid:    ) u , xi ( cid:    ) ith column changed ∈ {   , . . . , n } use f k   denote ( cid:    ) u , x  ( cid:    ) kth phase . need distinguish global token ’ inner product change phase . Initially , given X ∈ GE δ , following true : δ− ( i−  ) ≤ ( cid:    ) u , Xi ( cid:    ) ≤ δ−id − δ δ− ( n+  ) = ( cid:    ) u , X  ( cid:    ) Note li ordered distinct buckets l  < l  < · · · < ln < l  . phases indexed ∈ {   , . . . , n } . phase consists two distinct parts : low shift operation : operation form ∈ [ n ] X ← X + δ−dψ ( X , v − δ/  , v + δ/  ) values v ∈ [ δ−id ) , δ− ( i+  ) ) δ . range chosen li range lj j ( cid:   ) = range . shift exactly ith column xi new inner product fi = ( cid:    ) u , xi ( cid:    ) substantially larger li . Furthermore , column X affected . high shift operation : operation form   denote value ˜f X ← X + δ−nd · ψ ( X , v − δ/  , v + δ/  ) values v ∈ [ Si , Ti ) δ . range [ Si , Ti ) δ chosen affect column x  ( corresponding global token ) column . particular , shift global token δ−nd . Let ˜f   = ( cid:    ) u , x  ( cid:    ) end ith high operation . phase interleaves shift operation column updates global token . phase , updated ith column fi = ( cid:    ) u , xi ( cid:    ) contain unique token encoding values l  , . . . , li . high update , ˜f Finally , deﬁne following constants k ∈ {   ,   , . . . , n } .   = ( cid:    ) u , x  ( cid:    ) contain information ﬁrst tokens . Tk = ( δ− ( n+  ) +   ) k · δ−nd − k ( cid:   ) ( δ− ( n+  ) +   ) k−t (  δ−nd−d + δ−nd +   ) δ−td t=  − ( δ− ( n+  ) +   ) k−  ( δ−nd−d + δ−nd ) δ−d − δ− ( k+  ) ( )    Sk = ( δ− ( n+  ) +   ) k · δ−nd − k ( cid:   ) ( δ− ( n+  ) +   ) k−t (  δ−nd−d + δ−nd +   ) δ− ( t−  ) − ( δ− ( n+  ) +   ) k−  ( δ−nd−d + δ−nd ) − δ−kd ( LP ) t=  k phases , maintain following invariants :   . Sk < ˜f k   < Tk k ∈ {   ,   , . . . , n } .   . Tk−  ≤ fk < Sk   . order inner products kth phase lk+  < lk+  · · · < ln < f  < f  < · · · < fk < ˜f k   . Base case case k =   , trivial simply set S  = δ− ( n+  ) , T  = δ− ( n+  ) ·d + δ . ﬁrst nontrivial case k =   . First , low shift operation performed range [ δ− ( k−  ) , δ−kd ) δ Due Inductive Step invariant , know exists one column xk affected shift . particular , column k , maxj∈N ( k ) ( cid:    ) u , xj ( cid:    ) = ( cid:    ) u , x  ( cid:    ) = ˜f k−  . minimum lk . Thus update fk = δ−d ( ˜f k−  . Hence total ordering , operation   − lk ) + lk . Observe small enough δ , fk ≥ ˜f k−      lk +   < lk+  · · · < ln < f  < f  < · · · < ˜f k−    < fk (   ) operate higher selective shift operator range [ Sk−  , Tk−  ) δ . Since global token ’ innerproduct ˜f k−  range , column affected shift operator . global token operates entire range , know Eq . (   ) , fk = maxi∈ [ n ] ( cid:    ) u , xi ( cid:    ) lk+  = mini∈ [ n ] ( cid:    ) u , xi ( cid:    ) . new value ˜f k . Expanding simplifying get ,   = δ−nd · ( fk − lk+  ) + ˜f k−        = δ−nd · ( fk − lk+  ) + ˜f k−  ˜f k   = δ−nd · ( δ−d ( ˜f k−  = δ− ( n+  ) · ( ˜f k−  = ( δ− ( n+  ) +   ) ˜f k−    − lk ) + lk − lk+  ) + ˜f k−    − lk ) + δ−nd ( lk − lk+  ) + ˜f k−    − ( δ−nd−d + δ−nd ) lk − lk+      Expanding recursively , get = ( δ− ( n+  ) +   ) k · ˜f     − k ( cid:   ) ( δ− ( n+  ) +   ) k−t (  δ−nd−d + δ−nd +   ) lt t=  − ( δ− ( n+  ) +   ) k−  ( δ−nd−d + δ−nd ) l  − lk+    < Tk . sufﬁciently small δ , observe Sk ≤ ˜f k   = δ−nd li < δ−id , substitute get Eq . ( ) Since know ˜f   get lower-bound Eq . ( LP ) using li ≥ δ− ( i−  ) . construction , know Sk ≤ ˜f k   < Tk essentially dominant term ≈ ( δ−n ( k+  ) d−kd ) lower order terms matter . result immediate see fk > δ−d ( ˜f k−    − lk ) > Tk−  hence see invariant   also satisﬁed . Since column k global token affected , see invariant   also satisﬁed . n iterations , ˜f n δ . ensure tokens distinct , add additional layer X = X + δ−n dψ ( X , v − δ/  , v + δ/  ) v ∈ [ S  , Tn ) δ . ensures P , P ( cid:   ) ∈ GE   contains unique encoding P ∈ GE δ , entry q ( P ) q ( P ( cid:   ) ) distinct .    previous lemma shows compute contextual mapping using sparse transforms . use following lemma show use contextual mapping feed-forward layers accurately map desired output function ¯f . Lemma   ( Lemma   [     ] ) . Let gc function Lemma   , construct function gv : R ( n+  ) × ( d+  ) → R ( n+  ) ×d composed ( nδ−nd ) feed-forward layers ( hidden dimension q =   ) activations Φ gv deﬁned gv ( Z ) = [ gtkn ( Zn ) ] , j ∈ {   , . . . , n } , ( Z  ) , . . . , gtkn v v gtkn v ( gc ( L ) j ) = f ( L ) j A. .  Approximating modiﬁed Transformers Transformers previous section assumed used Transformers used hardmax operator σH activations functions belonging set Φ . without loss generality following lemma shows . Lemma   ( Lemma   [     ] ) . g ∈ ¯T  , ,    ≤ p ≤ ∞ , ∃g ∈  , ,  dp ( g , ¯g ) ≤ ( cid:   ) /  Combining lemma Lemma   , get main result : Theorem   . Let   ≤ p ≤ ∞ ( cid:   ) >   , exists transformer network g ∈  , ,  achieves ratio dp ( f , g ) ≤ ( cid:   ) sparse graph .   Since sparsity graph associated BIGBIRD contains star network , know express continuous function compact domain . Contemporary work Universal Approximability Sparse Transformers would like note , contemporary work done Yun et al . [     ] , also parallelly explored ability sparse transformers linear connections capture sequence-to-sequence functions compact domain .    B Turing Completeness section , extend results setting Pérez et al . [    ] . exposition largely use proof structure make changes . repeat lemmas amendments make exposition self-contained . B.  Notation Transformer Decoder need encoder decoder transformer simulating Turing machine . utilize notation used App . A.  encoders . decoder similar encoder additional attention external pair key-value vectors ( Ke ∈ Rn×m , V e ∈ Rn×d ) , usually come encoder stack . single layer Transformer decoder parametric function Dec receiving sequence Yj = ( y  , . . . , yj ) vectors Rd plus external ( Ke , V e ) returning sequence vectors Zj = ( z  , . . . , zj ) length . zi dimensional vector well . Dec three components , one Enc :   . attention mechanism ATTN takes sequence Yj returns sequence ( p  , ... , pj ) length dimensionality ;   . cross-attention mechanism CROSSATTN takes sequence ( p  , ... , pj ) plus exter- nal ( Ke , V e ) returns sequence ( a  , ... , aj ) , ai ∈ Rd ;   . two layer fully connected network takes vector Rd returns vector Rd . i-th output vector Dec ( Yj ; Ke , V e ) computed follows : (   )  (   )  (   ) ATTND deﬁned App . A.  remains deﬁne CROSSATTN . ith output vector multi-head cross-attention attention given zi = ( ai ) + ai ai = CROSSATTN ( pi , Ke , V e ) + pi pi = ATTND ( Yj ) + yi CROSSATTN ( Yj ) = ( cid:   ) σ H ( cid:   ) h=  ( yiW h Q ) ( K ( e ) W h K ) ( cid:   ) · ( V ( e ) W h V ) (   ) W h Q , W h K , W h V ∈ Rd×m , W h V ∈ Rd×d , h =   , . . . H heads . Turning Machine use setup Turning Machine used Pérez et al . [    ] ( see section B.  ) . Given Turing Machine = ( Q , Σ , δ , qinit , F ) , use following notation q ( j ) : state Turing machine time j . ( j ) : symbol head time j . v ( j ) : symbol written time j . ( j ) : head direction transition time j . denotes one-hot vector representation Q|Σ| . Vector representations symbol ∈ Σ ,  ( cid:   ) transformer intermediate vectors used simulations dimension =  |Q|+ |Σ|+   . Note use ﬁve extra dimension compared Pérez et al . [    ] . follow convention used Pérez et al . [    ] write vector v ∈ Qd arranged four groups values follows ( cid:   ) v = [ q  , s  , x  , q  , s  , x  , x  , x  , x  , x  , s  , x  , s  , x  , x  , x   , x   , x   , x   , x   , x   , x   ] qi ∈ Q|Q| , si ∈ Q|Σ| , xi ∈ Q . B.  Details Simulation section , give details architecture encoder decoder needed implement simulation strategy .    High Level Overview : Given Turing machine , show transformer appropriate encoder decoder TD simulate step ’ execution . simulation strategy mostly follow Pérez et al . [    ] , except use sparse attention mechanism . main idea maintain current Turing machine state q ( j ) symbol head ( j ) part decoder sequence time step j always simulate corresponding Turing machine transition δ ( q ( j ) , ( j ) ) = ( q ( j ) , v ( j ) , ( j ) ) . key difference rise Lemma B.  Pérez et al . [    ] , full attention used select appropriate symbol tape history one step . accomplish task sparse attention , exploit associative property max break symbol selection multiple steps . Thus , unlike Pérez et al . [    ] one decoding step sparse transformer TD correspond one step Turing machine . particular , two type steps : compute step corresponding update ’ state intermediate steps corresponding aggregating max ( turn used symbol selection ) . Let denote step TD g ( ) denote step simulated step decoder . decoding step want maintain current Turing machine state qg ( ) symbol sg ( ) yi . roughly ( ) intermediate steps state remain , aggregate information relevant past output symbols sparse attention . maintain state intermediate steps , introduce extra switching layer ( App . B. .  ) . Finally , next compute step make transition new state qg ( ) +  , new head movement mg ( ) , new output symbol vg ( ) written . Thereby able completely simulate given Turing machine . result , prove following main theorem : √ Theorem   . exists sparse attention mechanism using ( n ) inner products resulting class Transformer Networks using sparse attention mechanism Turing Complete . Encoder [    ] , use trivial single layer encoder resulting K ( e ) contains position embed- ding V ( e ) contains one-hot symbol representation . Decoder Sparse Self-Attention mechanism Decoder section , consider particular instance sparse graph decoder . deﬁne edges given following relations : ∀j ∈ N+ ,   ≤ k ≤ j +   , ( cid:   ) j ( j +   )   + k , ( cid:   ) k ( k +   )    ( cid:   ) j ( j +   )   + k , j ( j +   )   ( cid:   ) + k k >   else ( cid:   ) j ( j +   )   +   , j ( j +   )   ( cid:   ) . graph seen special case BIGBIRD ﬁrst type edges realizations random second type edges correspond locality . Also note graph satisﬁes left-to-right constraint decoder , i.e . node attends node future . Figure   : Mapping transformer step original Turing machine step .                                                       Transform : TM Step j : Offset k : Embeddings positional encodings construction needs different positional encoding posDec : N → Qd decoder : posDec ( ) = [   , . . . ,   ,   , . . . ,   ,   , . . . ,   ,   , g ( ) +   ,   g ( ) +  ,   ( g ( ) +  )   , h ( ) ,   ,   ,   ,   ] h ( ) = g ( +   ) − g ( ) . Note h ( ) reduces binary indicator ( cid:    ) − + √  + i ( cid:    )    + i = ( cid:    ) − + √  + i   ( cid:    ) ( cid:    ) . g ( ) = variable   ( cid:    ) − + √   Induction Setup next show construct decoder layers produce sequence outputs y  , y  , . . . , yi given : yi = [ , cg ( ) , sg ( ) qg ( ) , ( cid:   ) ( cid:   ) ( cid:   )   , . . . ,   , w ( )  s ,   , , ( cid:   ) ( cid:   )   , u ( )   ,   ,   ,   ,   , u ( ) ( cid:   )   , u ( )   , u ( )   ] , step sparse decoder yi , contain information state turing machine time g ( ) , symbol head time g ( ) , current location head time g ( ) . also placeholder symbol w placeholder scalars u  , u  , u  , whose role clear construction . consider starting vector decoder vector y  = [ # ,   , ( cid:   ) ( cid:   ) , qinit ( cid:   ) ( cid:   )   , . . . ,   ,   , . . . ,   ,   , . . . ,   ] assume start head c (   ) =   , initial state q (   ) = qinit , (   ) = # initialize clean tape . show correctness construction inductive argument : describe architecture piece piece time show every r ≥   , architecture constructs yr+  previous vectors ( y  , . . . , yr ) . Thus , assume y  , . . . , yr satisfy properties stated . Since using positional encodings , actual input ﬁrst layer decoder sequence y  + posDec (   ) , y  + posDec (   ) , . . . , yr + posDec ( r ) . denote yi vector yi plus positional encoding . Thus ∀   ≤ ≤ r yi = [ sg ( ) , cg ( ) , ( cid:   ) qg ( ) , ( cid:   ) ( cid:   ) ( cid:   )   , . . . ,   , w ( )  s ,   , ( cid:   )   , g ( ) +   , , ( cid:   )   g ( ) +  ,   ( g ( ) +  )   , h ( ) , u ( )   , u ( )   , u ( )   , u ( )   ] B. .  Layer   : Simulate Transition Function layer , use cross-attention encoder decoder access input string feed-forward network simulate transition function . ﬁrst self attention Eq . (   ) used layer produce identity . identity function achieved setting queries , keys , values   everywhere plus residual connection . Thus , p  Since p  [    ] use p  ] , know Lemma B.  Pérez et al . attend encoder obtain form [ ,   , g ( ) +   , = yi . , . . . , , . . . , CROSSATTN ( p  , Ke , V e ) = [   , . . . ,   ,   , . . . ,   , αg ( ) +  ( cid:   )   , . . . ,   ( cid:   ) , βg ( ) +  ,  s , ]    α β deﬁned Eq . (    ) [    ] . Thus Eq . (   ) ﬁnally produce vector a   given a  = = [ sg ( ) CROSSATTN ( p  qg ( ) , ( cid:   ) ( cid:   ) ( cid:   )   , . . . ,   , αg ( ) +  ( cid:   ) ( cid:   )   , g ( ) +   , , Ke , V e ) + p   , cg ( ) , ( cid:   ) , βg ( ) +  ,   g ( ) +  , , ( cid:   ) ( cid:   ) ( g ( ) +  )   , h ( ) , u ( ) w ( )     , u ( )   , u ( )   , u ( )   (   ) ] ﬁnal piece ﬁrst decoder layer use function O  ( · ) ( Eq . (   ) ) satisﬁes following lemma . Lemma   ( Lemma B.  [    ] ) . exists two-layer feed-forward network O  : Qd → Qd input vector a  ( Eq . (   ) ) produces output O  ( a  ) = [   , . . . ,   , qg ( ) +  ( cid:   )   , . . . ,   ,   , . . . ,   , ( cid:   ) ( cid:   ) vg ( ) , mg ( ) ,   ,   ,   ,   ( cid:   ) ] , function O  ( · ) simulates transition δ ( qg ( ) , sg ( ) ) construct besides linear transformations . ( cid:   ) qg ( ) +  vg ( ) , ( cid:   ) ( cid:   ) ( cid:   ) , mg ( ) Thus , ﬁnally output ﬁrst decoder layer = O  ( a  z  ) + a  = [ , cg ( ) , qg ( ) , ( cid:   ) ( cid:   ) ( cid:   ) qg ( ) +  ( cid:   ) ( cid:   ) αg ( ) +  ( cid:   ) ( cid:   )   , g ( ) +   , sg ( ) ( cid:   ) vg ( ) , ( cid:   ) ( cid:   ) , βg ( ) +  ,   g ( ) +  , , mg ( ) ,   ,   ,   ,   , , ( cid:   ) ( cid:   ) ( g ( ) +  )   , h ( ) , u ( ) w ( )     , u ( )   , u ( )   , u ( )   ] B. .  Layer   : Finding Head Node layer , use feed-forward network evaluate next location head . self-attention cross-attention set identity function , a  . Recall cg ( ) cell pointing time g ( ) , satisﬁes following recursion cg ( ) +  = cg ( ) + mg ( ) , expanded see cg ( ) +  = (   ) + (   ) + · · · + mg ( ) . difﬁcult see two layer network non-linearity compute cg ( ) + / ( g ( ) +   ) cg ( ) / ( g ( ) +   ) cg ( ) , mg ( ) ,  / ( g ( ) +   ) using relation cg ( ) +  = cg ( ) + mg ( ) . end layer   , obtain = p  = z  = O  ( a  z  ) + a  = [ , cg ( ) , qg ( ) , ( cid:   ) ( cid:   ) ( cid:   ) qg ( ) +  ( cid:   ) ( cid:   ) αg ( ) +  ( cid:   ) ( cid:   )   , g ( ) +   , sg ( ) ( cid:   ) vg ( ) , ( cid:   ) ( cid:   ) , βg ( ) +  ,   g ( ) +  ,   g ( ) +  , , cg ( ) +  , w ( ) , ( cid:   ) ( cid:   ) ( g ( ) +  )   , h ( ) , u ( )     ( g ( ) +  )   , cg ( ) +  g ( ) +  , cg ( ) g ( ) +  ,   , u ( )   , u ( )   , u ( )   ] B. .  Layer   : Distinguishing Node Type additional layer ( present work [    ] ) , propagate computations sparse graph . particular , use layer “ compute ” accumulate state intermediate nodes . make clear . self-attention cross-attention set identity function , a  . layer , use dense attention layers select newly computed states continue previous states . Using idea similar Lemma B.  [    ] , construct dense network = p  = z  ( [ x , , z , b ] ) ) = ( cid:   ) [   ,   ,   ,   ] [   , z − , −z ,   ] b =   , b =   . negatives generated offset results skip connection . utilize network switch Turing machine state position embedding intermediate steps values received    previous time step nothing compute nodes . use h ( ) ﬂipping bit b . Thus , end layer   , obtain = O  ( a  z  ) + a  = [   , . . . ,   , ˆv ( ) ˆq ( ) , ( cid:   ) ( cid:   ) ( cid:   ) ( cid:   ) , ˆβ ( ) ,  s , ˆα ( ) ( cid:   ) ( cid:   )   , ˆu ( )   , ˆu ( )   , ˆu ( ) , ˆc ( ) ,   g ( ) +  ,   ( g ( ) +  )   , cg ( ) +  g ( ) +  , ˆu ( )   ,   , h ( ) ,   ,   ,   ,   ] used h ( ) selecting old states . particular , • copy input state head position intermediate nodes . need transition next Turing machine states nodes . ˆq ( ) = ( cid:   ) qg ( ) +  qg ( ) h ( ) =   h ( ) =   , ˆv ( ) = ( cid:   ) vg ( ) w ( ) h ( ) =   h ( ) =   , ˆc ( ) = ( cid:   ) cg ( ) +  cg ( ) h ( ) =   h ( ) =   . • preserve symbol head intermediate nodes , copy previous symbol α location set β = g ( ) +   , symbol α location copied symbol head next transformer step ﬁnal transformation layer β = g ( ) +   . Thus , correctly preserve previous symbol head Turing machine transition nodes . compute nodes , things happen usual . ˆα ( ) = ( cid:   ) αg ( ) +  sg ( ) h ( ) =   h ( ) =   , ˆβ ( ) = ( cid:   ) βg ( ) +  h ( ) =   g ( ) +   h ( ) =   . • Finally intermediate nodes , copy position embedding corresponding current best symbol w , stored u  , u  , u  . compute node , let position embedding correspond current Turing machine step . ( cid:   ) ˆu ( )   = g ( ) +   h ( ) =   u ( ) h ( ) =     , ˆu ( )   = ( cid:   )   ( g ( ) +  )   u ( )   h ( ) =   h ( ) =   , ˆu ( )   = ˆu ( )   = ( cid:   )   ( g ( ) +  ) u ( )   ( cid:   ) cg ( ) g ( ) +  u ( )   h ( ) =   h ( ) =   , h ( ) =   h ( ) =   . simpliﬁcation note g ( +   ) = g ( ) h ( ) =   else g ( ) +   h ( ) =   . fact , conclude ˆq ( ) = qg ( i+  ) ˆc ( ) = cg ( i+  ) . Thus , write , z  = [   , . . . ,   , qg ( i+  ) ( cid:   ) ˆα ( ) ( cid:   )   , ˆu ( ) ˆv ( ) , ( cid:   ) ( cid:   ) , ˆβ ( ) ,  s , ( cid:   )   , ˆu ( )   , ˆu ( ) , cg ( i+  ) ,   g ( ) +  , ( cid:   )   ( g ( ) +  )   , cg ( ) +  g ( ) +  , ˆu ( )   ,   , h ( ) ,   ,   ,   ,   ] B. .  Layer   : Finding next symbol tape ﬁnd symbol tape next head position cg ( ) +  , try ﬁnd written last location cg ( ) +  . facilitate , following [    ] , deﬁne ( cid:   ) ( j ) last time ( previous j ) pointing position c ( j ) , j −   ﬁrst time pointing c ( j ) . Recall j Turing machine step counter , different sparse transformer step . [    ] could utilize full attention mechanism ﬁnd v ( cid:   ) ( j+  ) one go , multiple steps owing sparse attention mechanism . use similar query , key , value functions used full attention [    ] ∀i : Q  ( z  ) = [   , . . . ,     , . . . ,   ,   , . . . ,   ,   , cg ( ) +  g ( ) +  ,   g ( ) +  ,     ( g ( ) +  )   ,   ,   ,   ,   ,   ]    K  ( z  V  ( z    , ˆu ( ) ) = [   , . . . ,     , . . . ,   ,   , . . . ,   ,   , ˆu ( )   , . . . ,   ,   , . . . ,   ,  s ,   , ( cid:   )   ,   ,   ,   ,   , ˆu ( ) clear three functions linear transformations thus deﬁned feed- forward networks . Notice query vector always formed using current time step position embedding , whereas key value vectors formed using copied entries intermediate nodes using current entries compute node . , ( cid:   )   , ˆu ( )   ,   ,   ,   ,   ,   ) = [   , ˆu ( )   , ˆu ( )   , ˆu ( ) ˆv ( )   ] ] Pérez et al . [    ] ﬁnd desired vl ( j+  ) vm ( j ) using full attention , ( ) = arg min m∈ {   , ... , } χj = arg min m∈ {   , ... , } | ( cid:    ) Q  ( z  j ) , K  ( z  ) ( cid:    ) | Note minimization Turing machine steps , i.e . compute nodes case . show estimates ( j ) parts using sparse attention mechanism . main idea notice minimization problem minm∈ {   , ... , } χj expressed min { · · · min { min { χj } associativity property .   } , ... , χj   } , χj   , χj deﬁnition graph , every intermediate node form j ( j +   ) /  + k , i.e . k >   , g ( ) = j h ( ) =   , attend node k ( k +   ) /  best till copied −   . node k ( k +   ) /  never intermediate node h ( k ( k +   ) /  ) =   k fact corresponds Turing machine ’ step k. help us select key value corresponding min node k ( k +   ) /  −   . words , node form j ( j +   ) /  + k would evaluated ( k ) corresponding value selected : w ( j ( j+  ) / +k+  ) = ˆvm ( k−  ) similarly u ’ . going intermediate nodes , ﬁnally next compute node , i.e . k = j +   , obtain minimum value   ,   , ... , j . implies compute node able recover ( cid:   ) ( g ( ) +   ) corresponding value shown Lemma B.  [    ] . p  given p  = ATTND ( Z  ) + z   = [   , . . . ,   , qg ( i+  ) ( cid:   ) ˆα ( ) ( cid:   )   , ˆu ( ) , ( cid:   ) ( cid:   ) ( cid:   ) , ˆβ ( ) , w ( i+  ) ( cid:   ) ( cid:   ) ( cid:   )   , h ( ) , u ( i+  )   , ˆu ( )   , ˆu ( ) cross-attention feed-forward network set identity , z  , u ( i+  )   , u ( i+  )   , cg ( i+  ) ,   , cg ( ) +  g ( ) +  , ˆu ( )   , ˆv ( )   , (   ) ] , u ( i+  )   = a  = p  . B. .  Final transformation ﬁnish construction using ﬁnal transformation function F ( · ) corresponding lemma Pérez et al . [    ] , slight modiﬁcation . Lemma   ( Lemma B.  [    ] ) . exists function F : Qd → Qd deﬁned feed-forward network F ( z  r ) = [ = , ( cid:   ) ( cid:   ) w ( r+  ) qg ( r+  ) ( cid:   )   , . . . ,   ,  s ,   , ,   ,   ,   ,   ,   , u ( r+  ) yr+  ( cid:   ) ( cid:   )   sg ( r+  ) ) , cg ( r+  ) , ( cid:   ) , u ( r+  )   , u ( r+  )   , u ( r+  )   ] modiﬁcation let w , u  , u  , u  pass . yields desired input transformer next time step intermediate compute node , thereby concluding induction .    C Limitations Finally , show sparse attention mechanisms universally replace dense attention mechanisms , i.e . free lunch . demonstrate natural task solved full attention mechanism (   ) -layers . However , standard complexity theoretic assumptions , show problem require ˜Ω ( n ) -layers sparse attention layers ˜O ( n ) edges ( BIGBIRD ) . ( use standard notation ˜Ω ( n ) hide dependence poly-logarithmic factors . ) consider simple problem ﬁnding furthest vector vector given sequence length n dimension ∈ Ω ( log  n ) . assumption dimension mild , many situations dimension =     actually comparable number n . Task   . Given n unit vectors { u  , . . . , un } , Rd = Θ ( log  n ) , compute f ( u  , . . . , un ) → ( u ∗ , . . . , un∗ ) ﬁxed j ∈ [ n ] , deﬁne j∗ = arg maxk ( cid:    ) uk − uj ( cid:    )     . Finding vectors furthest apart boils minimizing inner product search case unit vectors . full-attention mechanism appropriate query keys , task easy evaluate pair-wise inner products . impossibility sparse-attention follows hardness results stemming Orthogonal Vector Conjecture ( OVC ) [   ,   ,    ,   ] , widely used assumption ﬁne-grained complexity . Informally , states one determine minimum inner product among n Boolean vectors   subquadratic time . Conjecture   ( Orthogonal Vectors Conjecture ) . every ( cid:   ) >   , c ≥   given n Boolean vectors dimension , determine pair orthogonal vectors ( n − ( cid:   ) ) time instances ≥ c log n . Using conjecture   , show reduction show transformer g ∈ H=O ( ) , m=O ( ) , q=O ( ) sparse directed graph completes Task   must require superlinear number layers . Proposition   . exists single layer full-attention network g ∈ H=  , m= d , q=  evaluate Task   , i.e . g ( u  , ... , un ) = [ u ∗ , . . . , un∗ ] , sparse-attention network H=O ( ) , m=O ( ) , q=O ( ) graph ˜O ( n ) edges ( i.e . inner product evaluations ) , would  require ˜Ω ( n −o (   ) ) layers .   Proof . break proof two parts : Part   : full attention mechanism solve problem (   ) layer begin provid- ing explicit construction single layer full self-attention evaluate Task   . Step   embed ui input R d follows : Step   Construct query , key , value functions follows : xi : = E ( ui ) = [ ui ;   ] Q ( [ ; b ] ) = −a K ( [ ; b ] ) = V ( [ ; b ] ) = [   ; ] (   ) (    ) Attn ( Q ( xi ) , K ( X ) , V ( X ) = [   ; uarg maxj ( cid:    ) −ui , uj ( cid:    ) ] . , ai = Attn ( Q ( xi ) , K ( X ) , V ( X ) ) + xi = [ ui ; uarg maxj ( cid:    ) −ui , uj ( cid:    ) ] = [ ui ; ui∗ ] (    ) Step   Let ( ai ) =   , output zi = [ ui ; ui∗ ] desired . complete argument , observe takes ( n ) inner products check pair orthogonal vectors need compare ( cid:    ) ui , ui∗ ( cid:    ) .    Part   : Every Sparse Attention Mechanism need ˜Ω ( n − ( cid:   ) ) layers prove contradiction impossible solve Task   g ∈ H=O ( ) , m=O ( ) , q=O ( ) sparse-attention graph ˜O ( n ) edges . Suppose solve Task   using network g ∈ H=O ( ) , m=O ( ) , q=O ( ) computation one layer : l layers . Recall   ai = ATTND ( Q ( xi ) , K ( XN ( ) ) , V ( XN ( ) ) + xi xi = ( ai ) + ai (    ) AttnD deﬁned eq . ( ) . Thus , total computation per layer ˜O ( nd  ) consequently ˜O ( nld  ) whole network consisting l layers . use result Task   solve orthogonal vector ( OV ) problem ( deﬁned Conjecture   ) linear time . total , able solve instance OV ˜O ( nld  ) time . l = ( n − ( cid:   ) ) ( cid:   ) >   = Θ ( log  n ) , appears able solve OV ˜O ( n − ( cid:   ) ) contradicts Conjecture   . Therefore , need least ˜Ω ( n −o (   ) ) layers .    Implementation details optimize code modern hardware . Hardware accelerators like GPUs TPUs truly shine coalesced memory operations load blocks contiguous bytes . Thus , efﬁcient small sporadic look-ups caused sliding window random element queries . alleviate “ blockifying ” lookups . GPU/TPU Sparsity Ideally , adjacency matrix described Sec .   sparse , one would hope would sufﬁcient speed implementation . Unfortunately , well known [    ,     ] , sparse multiplications efﬁciently implemented GPUs . GPUs thousands cores performing operations parallel . Thus , efﬁciently perform sparse matrix multiplication mentioned section Sec .   . result propose ﬁrst blockify attention pattern i.e . pack sets query keys together deﬁne attention blocks . easier explain process using example shown Fig .   . Suppose ,    query    key vectors attend . Using block size   , split query matrix   /  =   blocks similarly key matrix   /  =   blocks . three different building components BIGBIRD deﬁned block matrix . particular three different components :   . Random attention : query block attends r random key blocks . Fig .  a , r =   block size   . implies query block size   randomly attends key block size   .   . Window local attention : creating block , ensure number query blocks number key blocks . helps us deﬁning block window attention . Every query block index j attends key block index j − ( w −   ) /  j + ( w −   ) /  , including key block j . Fig .  b , w =   block size   . means query block j ( size   queries ) attends key block j −   , j , j +   .   . Global attention : Global attention remains deﬁned Sec .   , compute terms blocks . Fig .  c , g =   block size   . BIGBIRD-ITC implies one query key block , attend everyone . resulting overall attention matrix shown Fig .  d . Unfortunately , simply trying compute attention score multiplying arbitrary pairs query key vectors would require use gather operation , inefﬁcient . Upon closer examination window global attention , observe compute attention scores without using gather operation . Recall , full dense attention scores calculated simple matrix product query key matrix cost ( n d ) , illustrated Fig .  a . note blockify query key matrix multiply , ( nbd ) cost obtain block diagonal portion attention score , depicted Fig .  b . elaborate lets assume Q , K ∈ Rn×d query key matrix corresponding n tokens Qi . = xiWQ Ki . = xiWK . reshape n × query ( ) Random Attention ( b ) Window Attention ( c ) Global Attention ( ) BIGBIRD Figure   : Building blocks block-attention mechanism used BIGBIRD block size =   . implies attention matrix split blocks size   ×   . previous BIGBIRD parameters work block unit . White color indicates absence attention . ( ) random attention r =   , ( b ) sliding window attention w =   ( c ) global attention g =   . ( ) combined BIGBIRD model .    ( ) Full pair attention obtained direct matrix multiplication query key matrix . Groupings shown guidance . ( b ) Block diagonal attention computed “ blockifying ” query key matrix ( c ) Window local attention obtained “ blockifying ” query/key matrix , copying key matrix , rolling resulting key tensor ( Obtaining rolled key-block tensor illustrated detail Fig .   ) . ensures every query attends least one block two blocks keys size b side . ( ) Window + Random attention obtained following procedure along gathering random key blocks . Figure   : Idea behind fast sparse attention computation BIGBIRD .    KeyQueryA B C DE F G HI J K LM N PQ R TU V X Yd                                       bd b                               A B C DE F G HI J K LM N PQ R TU V X Y        KeyQueryd bbdA B C DE F G H                                       A B C DE F G HI J K LM N PQ R TU V X Y        I J K LKeyQueryA B C DE F G HU V X Y        Q R TU V X YA B C DQ R TE F G HI J K LA B C D                                               A B C DE F G HI J K LM N PQ R TU V X Y        I J K LKeyQueryA B C DE F G HU V X Y        Q R TU V X YA B C DQ R TE F G HI J K LA B C DA B C DE F G HU V X YRandom edgesLocalityedges                               A B C DE F G HI J K LM N PQ R TU V X Y         Figure   : Construction rolled key-block tensor . Make w copies key matrix . Index copies − ( w −   ) /  ≤ j ≤ ( w −   ) /  . Roll jth copy j blocks . Positive roll means circular shift entries left likewise negative roll corresponds right shift . Finally , reshape grouping blocks along new axis obtain key-blocked tensor . illustration purpose w =   chosen . matrix , Q , key matrix , K , along sequence length obtain ( cid:    ) n/b ( cid:    ) × b × tensors Q ( cid:   ) K ( cid:   ) respectively . multiply two tensors Ajst = ( cid:   ) u Q ( cid:   ) jsuK ( cid:   ) jtu , j =   ,   , ... , ( cid:    ) n/b ( cid:    ) (    ) resulting tensor size ( cid:    ) n/b ( cid:   ) × b × b reshaped correspond block diagonal portion full attention pattern . extend attention block diagonal window , i.e . query block index j attends key block index j − ( w −   ) /  j + ( w −   ) /  , make w copies reshaped key tensor K ( cid:   ) . “ roll ” copy key-block tensor incrementally along ﬁrst axis length ( cid:    ) n/b ( cid:    ) illustrated Fig .   . Multiplying w rolled key-block tensors query-block tensor would yield desired window attention scores ( Fig .  c ) . Likewise global component , always include ﬁrst g blocks key tensor corresponding global tokens . Finally , random attention , small ( r =   experiments ) , resort using gather ops ( Fig .  d ) . Also note design , query block attends exactly r random blocks . Thus , result three components basically compact dense tensor K ( cid:   ) ( cid:   ) size ( cid:    ) n/b ( cid:    ) × ( g + w + r ) b × shown Fig .   . Computing ﬁnal attention score boils dense tensor multiplication , TPU/GPU efﬁcient . Speciﬁcally , need multiply Q ( cid:   ) ( size : ( cid:    ) n/b ( cid:    ) × b × ) K ( cid:   ) ( cid:   ) ( size : ( cid:    ) n/b ( cid:    ) × ( g + w + r ) b × ) cost ( n ( g + w + r ) bd ) yield desired attention score tensor size ( cid:    ) n/b ( cid:    ) × b × ( g + w + r ) b , reshaped obtain attention scores according BigBird pattern . Figure   : Overview BIGBIRD attention computation . Structured block sparsity helps compactly packing operations sparse attention , thereby making method efﬁcient GPU/TPU . left , depict transformed dense query key tensors . query tensor obtained simply blocking reshaping ﬁnal key tensor concatenating three transformations : ﬁrst green columns , corresponding global attention , ﬁxed . middle blue columns correspond window local attention obtained appropriately rolling illustrated Fig .   . ﬁnal orange columns , corresponding random attentions , need use computationally inefﬁcient gather operation . Dense multiplication query key tensors efﬁciently calculates sparse attention pattern ( except ﬁrst row-block , computed direct multiplication ) , using ideas illustrated Fig .   . resultant matrix right shown Fig .  d .    B C DE F G HI J K LM N PQ R TU V X Y  Copies KeyA B C DE F G HI J K LM N PQ R TU V X YA B C DE F G HI J K LM N PQ R TU V X YU V X YA B C DE F G HI J K LM N PQ R TRolled KeyQ R TA B C DE F G HI J K LM N PU V X YA B C DE F G HI J K LM N PQ R TU V X YI J K LKeyA B C DE F G HU V X YQ R TU V X YA B C DQ R TE F G HI J K LA B C DRollBlockK K K K K K Q Q Q Q Q Q =FixedRoll Key Matrix LeftRoll Key Matrix RightGattherQ Q Q Q Q db= db= K K K K K K K K K K K K K K K K K K K K K K K K K  E NLP experiments details E.  MLM Pretraining use four publicly available datasets Books [     ] , CC-News [    ] , Stories [    ] Wikipedia pretrain BIGBIRD . borrow sentencepiece vocabulary RoBERTa ( turn borrowed GPT  ) . split document longer      multiple documents join documents much smaller      . Following original BERT training , mask    % tokens four datasets , train predict mask . warm start RoBERTa ’ checkpoint . train two different models : BIGBIRD-ITC-base BIGBIRD-ETC-base . hyper-parameters two models given Tab .   . experiments use learning rate warmup ﬁrst   ,    steps , linear decay learning rate . Similar norm , trained large version model well ,    layers    heads hidden dimension      . Following observation RoBERTa , pretrain larger batch size      size . BIGBIRD-ITC block length kept base size , BIGBIRD-ETC block length almost doubled     . remaining parameters . Parameter BIGBIRD-ITC BIGBIRD-ETC Block length , b # global token , g Window length , w # random token , r Max . sequence length # heads # hidden layers Hidden layer size Batch size Loss Activation layer Dropout prob Attention dropout prob Optimizer Learning rate Compute resources      × b   × b   × b                    MLM gelu  .   .  Adam   −    ×   TPUv           × b                      MLM gelu  .   .  Adam   −    ×   TPUv  Table   : Hyperparameters two BIGBIRD base models MLM . E.  Question Answering detailed statistics four datasets used given Tab .    . hyperparameters BIGBIRD , used creating Tab .   shown Tab .    submitted get Tab .   shown Tab .    . use two types regularization training : • used variant contrastive predictive coding [    ] dual encoder model . • use position embedding ITC relative position encoding [    ] ETC . Next , mention dataset/task speciﬁc part model . Dataset # tokens Avg . doc len . Model Base Large Books [     ] CC-News [    ] Stories [    ] Wikipedia  . B  . B  . B  . B   K      . K     RoBERTa ( sqln :     ) Longformer ( sqln :      ) BIGBIRD-ITC ( sqln :      ) BIGBIRD-ETC ( sqln :      )  .     .     .     .     .     .     .     .    Table   : Dataset used pre training . Table    : MLM performance held-out set .    Instances Instance Length Dataset Training Dev Median Max HotpotQA-distractor [     ] Natural Questions [    ] TriviaQA [    ] WikiHop [    ]                                                                                         Table    : Question Answering Datasets Parameter HotpotQA NaturalQ TriviaQA WikiHop Global token location # global token , g Window length , w # random token , r Max . sequence length # heads # hidden layers Hidden layer size Batch size Loss Compute resources ITC ETC ITC ETC ITC ETC ITC ETC                                                           cross-entropy golden spans   ×   TPUv                                                              cross-entropy golden spans   ×   TPUv                                                            cross-entropy noisy spans [    ]   ×   TPUv                                                            cross-entropy ans choices   ×   TPUv  Table    : Hyperparameters base BIGBIRD model used Question Answering i.e . numbers reported Tab .   HotpotQA data consists question multiple evidence paragraphs . ﬁltered    QA answer given evidences . BIGBIRD-ITC , use ﬁrst     global tokens . BIGBIRD-ETC , one global token question token , one evidence paragraph , one sentence within paragraph , maximum     global token . use dense layer output corresponding global token evidence paragraph predict whether supporting fact threshold output logits . answer type ( yes/no/span ) predicted single dense layer global CLS token . span based answers , spans predicted dense layers sequence distance start end positions    words . spans ranked sum start end logits . Natural Questions also data consists question supporting evidence , form single , potentially long , document multiple paragraphs . largely follow setup [   ] . documents , longer      , sliding window approach used stride      . use CLS token beginning , followed question followed separator token followed document input . BIGBIRD-ITC , make ﬁrst     tokens global . BIGBIRD-ETC , make global token CLS , question , one token paragraphs . train four predictors ﬁnal layer predict long answer start , long answer end , short answer start short answer end respectively . Instead independently predicting start end answers ﬁrst predict start predict best end location beyond start . short answer , limit distance start end positions    words . answer type ( null , yes , , short , long ) predicted CLS token output embedding . logit yes/no answer higher logits short , long null answer , replace short answer corresponding yes/no text . TriviaQA data consists question-answer pairs Wikipedia articles “ noisy ” sup- porting evidence . call noisy given Wikipedia articles may may contain answer . Moreover , answer entities annotated appropriate span article , rather occurrences found using fuzzy string matching listed . use CLS token beginning , followed question followed separator token followed document input . BIGBIRD-ITC , make ﬁrst     tokens global . BIGBIRD-ETC , make global token CLS , question , one token sentence maximum     global tokens . Given    Parameter HotpotQA NaturalQ TriviaQA WikiHop Global token location # global token , g Window length , w # random token , r Max . sequence length # heads # hidden layers Hidden layer size Batch size Loss Num epochs Optimizer Learning rate Compute resources ETC                              cross-entropy {   ,   } Adam   ×   −    ×   TPUv  ETC                              cross-entropy {   ,   } Adam {   ,    } ×   −    ×   TPUv  ETC                              cross-entropy {   ,   } Adam {   ,   } ×   −    ×   TPUv  ETC                              cross-entropy {   ,    } LAMB {   ,   } ×   −    ×   TPUv  Table    : Hyperparameters large BIGBIRD model Question Answering submitted test i.e . numbers reported Tab .   noisy nature answer span , follow Clark Gardner [    ] training . use dense layer sequence predict answer span article independently , distance start end positions    words . article span maximum start logit + end logit chosen . normalize documents associated question . WikiHop question WikiHop , given upto    candidates ,    supporting paragraphs . BIGBIRD-ITC model , following Beltagy et al . [   ] , concatenate answer question special tokens , [ q ] Question [ /q ] [ ans ] Ans  [ /ans ] . . . [ ans ] AnsN [ /ans ] along context . start text , always contains questions followed answers , make ﬁrst     token attend globally . BIGBIRD-ETC model , need insert special [ ans ] , [ /ans ] etc . design global tokens appropriately . Along global tokens question , one per candidate answer maximum     . , linked answer tokens mentions using relative position label . Lastly , use dense layer takes output vector corresponding candidate answer , predicts score current candidate correct answer . apply dense layer candidate independently candidate best score picked ﬁnal answer . worthwhile note explicitly designed attention connection ETC works slightly better , random connection based ITC pretty competative . E.  Relationship Contemporary Work Longformer Child et al . [    ] introduced localized sliding window reduce computation . recent version , includes localized sliding windows global tokens introduced independently Longofrmer [   ] . Although BIGBIRD contains additional random tokens , also differences way global local tokens realized . particular even random token , used get SoTA question answering , two key differences Longformer BIGBIRD-etc ( see [   ] ) :   . use global-local attention relative position encodings enables better handle structured inputs   . Unlike Longformer , train global tokens using CPC loss learn use ﬁnetuning . E.  Classiﬁcation try two types classiﬁcation task . Document classiﬁcation experiment datasets different lengths contents , listed Tab .    . particular , look sentiment analysis ( IMDb [    ] Yelp-  [     ] ) task topic    Parameter IMDb Arxiv Patents Hyperpartisan Yelp-       ×   −      ×              Batch size Learning rate Num epochs TPUv  slice # heads # hidden layers Hidden layer size Block length , b Global token location # global token , g Window length , w # random token , r Max . sequence length Vocab size Activation layer Dropout prob Attention dropout prob Loss Optimizer      ×   −       ×        ×   −       ×        ×   −      ×        ×   −       ×                ITC   × b   × b   × b            gelu  .   .  cross-entropy Adam Table    : Hyperparameters document classiﬁcation . Model IMDb [    ] Yelp-  [     ] Arxiv [    ] Patents [    ] Hyperpartisan [    ] # Examples # Classes Excess fraction          .             .             .   SoTA RoBERTa BIGBIRD [    ]   .    .  ±  .    .  ±  .  [   ]   .     .     .   [    ]   .     .     .                .   [    ]   .     .     .          .   [    ]   .    .  ±  .    .  ±  .  Table    : Classiﬁcation results . report F  micro-averaged score datasets . Experiments smaller IMDb Hyperpartisan datasets repeated   times average performance presented along standard deviation . assignment ( Arxiv [    ] , Patents [    ] , Hyperpartisan [    ] ) task . Following BERT , used one layer cross entropy loss top ﬁrst [ CLS ] token BIGBIRD encoder consuming      tokens . report results document classiﬁcation experiments Tab .    . compare state-of-the-art ( SoTA ) methods dataset plain RoBERTa model     tokens truncation . experiments use learning rate warmup ﬁrst    % steps , linear decay learning rate detail list remaining hyperparameters provided Tab .    . better quantitative evaluation , compute fraction dataset exceeds     tokens , i.e . length document often truncated . see gains using BIGBIRD signiﬁcant longer documents fewer training examples . instance , using base sized model , BIGBIRD improves state-of-the-art Arxiv dataset   % points . Patents dataset , System BERT XLNet RoBERTa BIGBIRD MNLI- ( m/mm ) QQP QNLI    k    k    k SST-  CoLA STS-B MRPC RTE  . k  . k   k  . k  . k   . /  .    . /-   . /-   . /  .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .  Table    : GLUE Dev results base sized models . Number training examples reported task . MCC score reported CoLA , F  score reported MRPC , Spearman correlation reported STS-B , accuracy scores reported tasks .    improvement using simple BERT/RoBERTa , given large size training data improvement SoTA ( BERT based ) signiﬁcant . Note performance gain seen much smaller IMDb dataset . Along experimental setup detail , present detailed results App . E.  show competitive performance . GLUE General Language Understanding Evaluation ( GLUE ) benchmark [    ] , test lan- guage models   different natural language understanding tasks . used training parameters mentioned https : //github.com/pytorch/fairseq/blob/master/ examples/roberta/README.glue.md . model parameters b =    , g =   × b , w =   × b , r =   × b ( used BIGBIRD-ITC base model pretrained MLM task ) . compare performance BIGBIRD BERT , XLNet [     ] RoBERTa Tab .    . ﬁnd even task much smaller context , performance competitive full attention models . E.  Summarization discussed Sec .  .  , given small length output sequence , used sparse BIGBIRD attention encoder , keeping full attention decoder . number hidden layers , number heads , hidden dimension encoder decoder . hyperparameters detailed Tab .    . summarize result Tab .    . experiments , use learning rate warmup ﬁrst   ,    steps , square root decay learning rate . Parameter Base : BIGBIRD-RoBERTa Large : BIGBIRD-Pegasus Block length , b Global token location # global token , g Window length , w # random token , r Max . encoder sequence length Max . decoder sequence length Beam size Length penalty # heads # hidden layers Hidden layer size Batch size Loss Activation layer Dropout prob Attention dropout prob Optimizer Learning rate Compute resources BBC-XSUM : CNN/DM : Others : BBC-XSUM : CNN/DM : Others : BBC-XSUM : Others :    ITC   × b   × b   × b                              .   .                teacher forced cross-entropy gelu  .   .  Adam   ×   −    ×   TPUv     ITC   × b   × b   × b                              .   .                 teacher forced cross-entropy gelu  .   .  Adafactor   ×   −    ×   TPUv  Table    : Encoder hyperparameters Summarization . use full attention decoder Instances Input Length Output Length Dataset Training Dev Test Median    % -ile Median    % -ile Arxiv [    ] PubMed [    ] BigPatent [    ]                                                                                                              Table    : Statistics datasets used summarization .    Instances Input Length Output Length Dataset Training Dev Test Median    % -ile Median    % -ile BBC XSum [    ] CNN/DailyMail [    ]                                                                    Table    : Shorter summarization dataset statistics . Model Lead PtGen [    ] ConvS S [    ] MMN [    ] Bottom-Up [    ] TransLM [    ] UniLM [    ] Extr-Abst-BERT [    ] BART [    ] Transformer [    ] + RoBERTa [    ] + Pegasus [     ] BIGBIRD-RoBERTa  r  r   r P e   B e Pegasus ( Reported ) [     ] g Pegasus ( Re-eval ) r  BIGBIRD-Pegasus L BBC XSum CNN/DailyMail R-  R-  R-L R  R  R-L   .     .     .     .   − − −   .     .     .     .     .     .     .     .     .    .    .     .     .   − − −   .     .    .     .     .     .     .     .     .     .     .     .     .   − − −   .     .     .     .     .     .     .     .     .     .     .   − −   .     .     .     .     .     .     .     .     .     .     .     .     .     .   − −   .     .     .     .     .     .     .     .     .     .     .     .     .     .   − −   .     .     .     .     .     .     .     .     .     .     .     .   Table    : Summarization ROUGE score shorter documents . Following success several recent works [    ,    ] , warm start encoder-decoder BIGBIRD transformer model pretrained weights weights encoder decoder shared . particular , query/key/value matrix self-attention feedforward layers shared encoder decoder . variable initialized randomly encoder-decoder attention . base sized model , utilize MLM pretrained model      sequence length App . E.  , turn initialized using public RoBERTa checkpoint . large size model , lift weight state-of-the-art Pegasus model [     ] , pretrained using objective designed summarization task . check sparse attention causes signiﬁcant degradation compared full attention , experiment two shorter popular datasets , full attention used without signiﬁcantly truncating document . statistics two datasets Tab .    . see perfor- mance competitive , shows sparse attention achieve similar performance full attention models .    F Genomics experiments details section provide details experimental setup BIGBIRD genomics data . F.  Pretraining try keep experimental setup close typical NLP pipeline . regard , take human reference GRCh    convert documents D. document ∈ sequence sentences , sentence sequence fragments DNA . construct documents follows :   . Start empty document set = ∅ .   . chromosome C , repeat following procedure    times . ( ) Pick uniformly random starting point q base pairs          ’ end . ( b ) Repeat q > |C| . Pick uniformly random number        denote number sentences per document . ii . Constructs document containing sentences using consecutive base pairs ( bps ) . length sentence chosen uniformly random    -     . Thus resulting document    ,     -     ,     bps . iii . = ( cid:   ) iv . q = q + |d| procedure end-up approximately    K documents . Next run sentencepiece [    ] tokenization resulting documents . particular , using   characters building blocks ( four bases - , , C , G one missing symbol N ) , construct byte pair encoding table size   k , token representing  .   base pairs average . Using constructed documents , construct dataset two pretraining tasks following Devlin et al . [    ] : • Masked Language Model ( MLM ) : order train deep bidirectional representation , BERT training introduces MLM task , simply mask    % input tokens random , predict masked tokens . simply replace masked tokens [ MASK ] placeholder , leads distribution mis-match downstream tasks placeholders . mitigate issue ,    % tokens selected masking : –    % tokens actually replaced token [ MASK ] . –    % time tokens replaced random token . –    % time tokens left unchanged , still predicted output . run entire sequence BIGBIRD transformer encoder predict corre- sponding masked positions , based context provided non-masked tokens sequence . • Next Sentence Prediction ( NSP ) : order understand relationship two sequences , BERT training introduces NSP task , predict given pair sequences contiguous . training model gets input pairs sequences separated [ SEP ] token along [ CLS ] token start . Overall input pattern : [ CLS ] sequence [ SEP ] sequence B [ SEP ] .    % time second sequence comes true sequence ﬁrst one . Remaining    % time random sequence full dataset . model required predict relationship using output corresponding [ CLS ] token , fed simple binary classiﬁcation layer .  https : //www.ncbi.nlm.nih.gov/assembly/GCF_         .      Figure   : Visual description masked language modeling data generated raw DNA dataset . raw DNA sequences GRCh   , split random positions create documents   -    sentences sentence    -     base pairs ( bps ) . Thus document continuous strand      -   ,    bps DNA . process repeated    times create    sets document chromosome GRCH   . resulting set documents passed Sentencepiece created tokens average  bp . pretraining used masked language model masked    % tokens trained predicting masked tokens . sequence steps visually elaborated Fig .   . model trained MLM NSP together . Training hyperparame- ter provided second columns Tab .    . experiments use learning rate warmup ﬁrst   ,    steps , linear decay learning rate . additionally performed simple ablation study validate hypothesis , similar NLP , larger context improves performance . use MLM task described test BIG- BIRD performed sequences different length . Accuracy MLM task increasing sequence length shown Fig .   . longer context improves ﬁnal accuracy , also leads faster learning , opportunities masking . F.  Promoter Region Prediction Figure   : BIGBIRD accuracy context length . promoter region plays important role transcription initiation thus recognition important area interest ﬁeld bioinformatics . Following Oubounyt et al . [    ] , use datasets Eukaryotic Promoter Database ( EPDnew ) [    ] , contains   ,    promoter region human genome . Around transcription start site ( TSS ) , extract sequence      bp ( -     +     bp ) human reference genome GRCh   . Since EPDnew uses newer GRCh   , convert GRCh   coordinates using LiftOver [    ] .    TGGGCTAACAAGCAAATGATCTGTCreate Document & SentenceTGGGCTAACAAGCAAATGATCTGTSentencepieceTGGGCTAACAAGCAAATGATCTGT ... TGGGCTAACAAGCAAATGATCTGTMasking ......       Steps e               MLM Accuracy             Figure   : Visual description DNA segment predict chromatin proﬁle given non-coding region raw DNA sequences GRCh   . take      bps DNA given non-coding region context . complete fragment DNA including context side , tokenized form input sequence tokens . task predict     chromatin proﬁle including     transcription factors ( TF ) binding proﬁles     different TFs ,     DNase sensitivity ( DHS ) proﬁles     histone-mark ( HM ) proﬁles Following Oubounyt et al . [    ] promoter region example , negative example ( non-promoter sequences ) size positive one constructed follow : positive sequence divided    subsequences . ,    subsequences picked randomly substituted randomly . remaining   subsequences conserved . process illustrated Figure   [    ] . Applying process positive set results new non-promoter sequences conserved parts promoter sequences ( unchanged subsequences ,   subsequences    ) . parameters enable generating negative set       % sequences containing conserved portions promoter sequences . preﬁx append example [ CLS ] [ SEP ] token respectively . output correspond- ing [ CLS ] token BIGBIRD transformer encoder fed simple binary classiﬁcation layer . ﬁne-tune pretrained BIGBIRD App . F.  using hyper-parameters described Tab .    . note high performance surprising due overlap nature negative example generation MLM pretraining . F.  Chromatin-Proﬁle Prediction ﬁrst step sequence-based algorithmic framework predicting non-coding effects build model predict , large scale chromatic proﬁle [     ] . paper , use dataset provided Parameter Pretraining Promoter Region Chromatin-Proﬁle Block length , b Global token location # global token , g Window length , w # random token , r Max . Sequence Length # heads # hidden layers Hidden layer size Batch Size Vocab Size Loss Dropout prob Optimizer Learning rate # steps Compute Resources    ITC   × b   × b   × b                          MLM+NSP  .  Adam  .               ×   TPUv     ITC   × b   × b   × b                          BCE  .  Adam  .           ×   TPUv     ITC   × b   × b   × b                              x +ve upweighted BCE  .  Adam  .              ×   TPUv  Table    : Table hyperparameters Computational biology .    Context     bpTGGTAACAGCAATGCTGT ...... Predict Epigenetic Features of    bp non-coding regionContext     bp ... GGTAACAGCAATGCTG ......... SentencepieceGTAACAG ... G ... CAATG ... Zhou Troyanskaya [     ]   , train BIGBIRD predict chromatic proﬁle . training sample consists  ,   -bp sequence human GRCh   reference genome centered    -bp bin paired label vector     chromatin features . , preﬁx append example [ CLS ] [ SEP ] token respectively . output corresponding [ CLS ] token BIGBIRD transformer encoder fed linear layer     heads . Thus jointly predict     independent binary classiﬁcation problems . ﬁne-tune pretrained BIGBIRD App . F.  using hyper-parameters described Tab .    . data highly imbalanced data ( way negative examples positive examples ) , upweighted loss function positive examples factor   . used training testing split provided Zhou Troyanskaya [     ] using chromosomes strictly non-overlapping . Chromosome     excluded training test chromatin feature prediction performances , rest autosomes used training validation .  ,    samples chromosome   spanning genomic coordinates   ,   ,   –  ,   ,    used validation set . predicted probability sequence DeepSea Zhou Troyanskaya [     ] computed ensemble average probability predictions forward complementary sequence pairs , also predict using ensemble two BIGBIRD model trained independently .  http : //deepsea.princeton.edu/media/code/deepsea_train_bundle.v . .tar . gz    