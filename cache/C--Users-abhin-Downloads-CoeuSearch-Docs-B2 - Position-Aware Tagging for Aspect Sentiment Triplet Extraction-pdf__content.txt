Position-Aware Tagging for Aspect Sentiment Triplet Extraction Lu Xu* 1, 2, Hao Li* 1, 3, Wei Lu1, Lidong Bing2 1 StatNLP Research Group, Singapore University of Technology and Design 2 DAMO Academy, Alibaba Group 3ByteDance xu lu@mymail.sutd.edu.sg, hao.li@bytedance.com luwei@sutd.edu.sg, l.bing@alibaba-inc.com 1 2 0 2 r a M 9 ] L C . s c [ 3 v 9 0 6 2 0 . 0 1 0 2 : v i X r a Abstract Aspect Sentiment Triplet Extraction (ASTE) is the task of extracting the triplets of target entities, their associated sentiment, and opin- ion spans explaining the reason for the senti- ment. Existing research efforts mostly solve this problem using pipeline approaches, which break the triplet extraction process into several stages. Our observation is that the three el- ements within a triplet are highly related to each other, and this motivates us to build a joint model to extract such triplets using a se- quence tagging approach. However, how to ef- fectively design a tagging approach to extract the triplets that can capture the rich interac- tions among the elements is a challenging re- search question. In this work, we propose the ﬁrst end-to-end model with a novel position- aware tagging scheme that is capable of jointly extracting the triplets. Our experimental re- sults on several existing datasets show that jointly capturing elements in the triplet using our approach leads to improved performance over the existing approaches. We also con- ducted extensive experiments to investigate the model effectiveness and robustness1. 1 Introduction Designing effective algorithms that are capable of automatically performing sentiment analysis and opinion mining is a challenging and important task in the ﬁeld of natural language processing (Pang and Lee, 2008; Liu, 2010; Ortigosa et al., 2014; Smailovi´c et al., 2013; Li and Wu, 2010). Re- (Pon- cently, Aspect-based Sentiment Analysis ∗ Equal contribution. Lu Xu is under the Joint PhD Pro- gram between Alibaba and Singapore University of Technol- ogy and Design. The work was done when Hao Li was a PhD student in Singapore University of Technology and Design. Accepted as a long paper in EMNLP 2020 (Conference on Empirical Methods in Natural Language Processing). 1We release our code at https://github.com/ xuuuluuu/Position-Aware-Tagging-for-ASTE 0 food was so so but excited to see many vegan options + Figure 1: ASTE with targets in bold in solid squares, their associated sentiment on top, and opinion spans in dashed boxes. The arc indicates connection between a target and the corresponding opinion span. tiki et al., 2014) or Targeted Sentiment Analy- sis (Mitchell et al., 2013) which focuses on extract- ing target phrases as well as the sentiment asso- ciated with each target, has been receiving much attention. In this work, we focus on a relatively new task – Aspect Sentiment Triplet Extraction (ASTE) proposed by Peng et al. (2019). Such a task is required to extract not only the targets and the sentiment mentioned above, but also the corresponding opinion spans expressing the sen- timent for each target. Such three elements: a target, its sentiment and the corresponding opin- ion span, form a triplet to be extracted. Figure 1 presents an example sentence containing two tar- gets in solid boxes. Each target is associated with a sentiment, where we use + to denote the positive polarity, 0 for neutral, and − for negative. Two opinion spans in dashed boxes are connected to their targets by arcs. Such opinion spans are im- portant, since they largely explain the sentiment polarities for the corresponding targets (Qiu et al., 2011; Yang and Cardie, 2012). This ASTE problem was basically untouched before, and the only existing work that we are aware of (Peng et al., 2019) employs a 2-stage pipeline approach. At the ﬁrst stage, they em- ploy a uniﬁed tagging scheme which fuses the tar- get tag based on the BIOES2 tagging scheme, and sentiment tag together. Under such a uni- 2BIOES is a common tagging scheme for sequence la- beling tasks, and BIOES denotes “begin, inside, outside, end and single” respectively.             0 food was so so but excited to see many vegan + options S 0 2,3 O O O O O O O O B + −4,−4 E Figure 2: The position-aware tagging scheme for the example instance. ﬁed tagging scheme, they proposed methods based on Long Short-Term Memory networks (LSTM) (Hochreiter and Schmidhuber, 1997), Conditional Random Fields (CRF) (Lafferty et al., 2001) and Graph Convolutional Networks (GCN) (Kipf and Welling, 2017) to perform sequence labeling to extract targets with sentiment as well as opinion spans. At the second stage, they use a classiﬁer based on Multi-Layer Perceptron (MLP) to pair each target (containing a sentiment label) with the corresponding opinion span to obtain all the valid triplets. One important observation is that the three ele- ments in a triplet are highly related to each other. Speciﬁcally, sentiment polarity is largely deter- mined by an opinion span as well as the target and its context, and an opinion span also depends on the target phrase in terms of wording (e.g., an opin- ion span “fresh” usually describes food targets in- stead of service). Such an observation implies that jointly capturing the rich interaction among three elements in a triplet might be a more effective ap- proach. However, the BIOES tagging scheme on which the existing approaches based comes with a severe limitation for this task: such a tagging scheme without encoding any positional informa- tion fails to specify the connection between a tar- get and its opinion span as well as the rich interac- tions among the three elements due to the limited expressiveness. Speciﬁcally, BIOES uses the tag B or S to represent the beginning of a target. For example, in the example sentence in Figure 1, “ve- gan” should be labeled with B, but the tagging scheme does not contain any information to spec- ify the position of its corresponding opinion “ex- cited”. Using such a tagging scheme inevitably leads to an additional step to connect each target with an opinion span as the second stage in the pipeline approach. The skip-chain sequence mod- els (Sutton and McCallum, 2004; Galley, 2006) are able to capture interactions between given in- put tokens which can be far away from each other. However, they are not suitable for the ASTE task where the positions of targets and opinion spans are not explicitly provided but need to be learned. Motivated by the above observations, we present a novel approach that is capable of pre- dicting the triplets jointly for ASTE. Speciﬁ- cally, we make the following contributions in this work: • We present a novel position-aware tagging scheme that is capable of specifying the struc- tural information for a triplet – the connec- tion among the three elements by enriching the label semantics with more expressiveness, to address the above limitation. • We propose a novel approach, JET, to Jointly Extract the Triplets based on our novel position-aware tagging scheme. Such an ap- proach is capable of better capturing interac- tions among elements in a triplet by comput- ing factorized features for the structural infor- mation in the ASTE task. • Through extensive experiments, the results show that our joint approach JET outper- forms baselines signiﬁcantly. 2 Our Approach Our objective is to design a model JET to ex- tract the triplet of Target, Target Sentiment, and Opinion Span jointly. We ﬁrst introduce the new position-aware tagging scheme, followed by the model architecture. We next present our simple LSTM-based neural architecture for learning fea- ture representations, followed by our method to calculate factorized feature scores based on such feature representations for better capturing the in- teractions among elements in a triplet. Finally, we also discuss a variant of our model. 2.1 Position-Aware Tagging Scheme To address the limitations mentioned above, we propose our position-aware tagging scheme by en- riching expressiveness to incorporate position in- formation for a target and the corresponding opin- ion span. Speciﬁcally, we extend the tag B and tag S in the BIOES tagging scheme to new tags respectively: Bǫ j,k, Sǫ j,k where Bǫ j,k with the sub-tag3 B still denotes the beginning of a target, and Sǫ j,k with the sub-tag S denotes a single-word target. Note that ǫ ∈ {+, 0, −} denotes the sentiment polarity for the target, and j, k indicate the position information which are the distances between the two ends of an opinion span and the starting position of a tar- get respectively. Here, we use the term “offset” to denote such position information for convenience. We keep the other tags I, E, O as is. In a word, we use sub-tags BIOES for encoding targets, ǫ for sentiment, and offsets for opinion spans under the new position-aware tagging scheme for the struc- tural information. For the example in Figure 1, under the proposed tagging scheme, the tagging result is given in Fig- ure 2. The single-word target “food” is tagged with S0 2,3, implying the sentiment polarity for this target is neutral (0). Furthermore, the positive off- sets 2, 3 indicate that its opinion span is on the right and has distances of 2 and 3 measured at the left and right ends respectively, (i.e., “so so”). The second target is “vegan options” with its ﬁrst word tagged with B+ −4,−4 and the last word tagged with E, implying the sentiment polarity is positive (+). Furthermore, the negative offsets −4, −4 indicate that the opinion span “excited” appears on the left of the target, and has distances of 4 and 4 mea- sured at the left and right ends respectively, (i.e., “vegan”). Our proposed position-aware tagging scheme has the following theoretical property: Theorem 2.1. There is a one-to-one correspon- dence between a tag sequence and a combination of aspect sentiment triplets within the sentence as long as the targets do not overlap with one another, and each has one corresponding opinion span.4 Proof. For a given triplet, we can use the fol- lowing process to construct the tag sequence. First note that the sub-tags of our proposed tags Bǫ j,k, are B, I, O, E, S. The stan- dard BIOES tagset is capable of extracting all j,k, I, O, E, Sǫ 3We deﬁne the sub-tags of Bǫ j,k, Sǫ j,k as B and S respec- tively, and the sub-tags of I, O, E as themselves. 4See the Appendix for detailed statistics on how often this condition is satisﬁed. possible targets when they do not overlap with one another. Next, for each speciﬁed target, the posi- tion information j, k that speciﬁes the position of its corresponding opinion span can be attached to the B (or S) tag, resulting in Bj,k (or Sj,k). Note that the opinion span can be any span within the sentence when j, k are not constrained. Finally, we assign each extracted target its sentiment polar- ity ǫ by attaching it to the tag B (or S), resulting in Bǫ j,k). This construction process is unique for each combination of triplets. Similarly, given a tag sequence, we can reverse the above process to recover the combination of triplets. j,k (or Sǫ We would like to highlight that our proposed position-aware tagging scheme is capable of han- dling some special cases where the previous ap- proach is unable to. For example, in the sentence “The salad is cheap with fresh salmon”, there are two triplets, (“salad”, “cheap with fresh salmon”, positive)5 and (“salmon”, “fresh”, positive). The previous approach such as (Peng et al., 2019), which was based on a different tagging scheme, will not be able to handle such a case where the two opinion spans overlap with one another. 2.2 Our JET Model our design JET model with novel We CRF (Lafferty et al., 2001) and Semi-Markov CRF (Sarawagi and Cohen, 2004) based on our position-aware tagging scheme. Such a model is capable of encoding and factorizing both token-level features for targets and segment-level features for opinion spans. Given a sentence x with length n, we aim to produce the desired output sequence y based on the position-aware tagging scheme. The probabil- ity of y is deﬁned as: )) (1) p(y|x) = exp (s(x, y)) Py′∈Yx,M exp(s(x, y′ where s(x, y) is a score function deﬁned over the sentence x and the output structure y, and Yx,M represents all the possible sequences under our position-aware tagging scheme with the offset con- straint M , indicating the maximum absolute value of an offset. The score s(x, y) is deﬁned as: s(x, y) = n X i=0 ψ¯yi,¯yi+1 + n X i=1 Φyi(x, i) (2) where ¯yi ∈ {B, I, O, E, S} returns the sub-tag 5We use the format (target, opinion spans, sentiment). B I O E S 0+ − ft(hi) fs(gi+j,i+k; ←− hi) fo(gi+j,i+k) fr(j, k) hi = [ −→ hi; ←− hi] ga,b BiLST M ei 2.2.2 Factorized Feature Score We explain how to compute the factorized fea- ture scores (the second part of Equation 2) for the position-aware tagging scheme based on the neural architecture described above. Such factor- ized feature scores involve 4 types of scores, as illustrated in the solid boxes appearing in Figure 3 (top). Basically, we calculate the factorized feature Figure 3: Neural Module for Feature Score score for the tag yi as follows: In our model, of yi, ψ¯yi,¯yi+1 represents the transition score: the weight of a “transition feature” – a feature de- ﬁned over two adjacent sub-tags ¯yi and ¯yi+1, and Φyi(x, i) represents the factorized feature score with tag yi at position i. the calculation of transition score ψ¯yi,¯yi+1 is similar to the one in CRF6. For the factorized feature score Φyi(x, i), we will explain computation de- tails based on a simple LSTM-based neural net- work in the following two subsections. Such a factorized feature score is able to encode both token-level features as in standard CRF, segment- level features as in Semi-Markov CRF as well as the interaction among a target, its sentiment and an opinion span in a triplet. 2.2.1 Neural Module We deploy a simple LSTM-based neural architec- ture for learning features. Given an input token se- quence x = {x1, x2, · · · , xn} of length n, we ﬁrst obtain the embedding sequence {e1, e2, · · · , en}. As illustrated in Figure 3, we then apply a bi- directional LSTM on the embedding sequence and obtain the hidden state hi for each position i, which could be represented as: ←− hi] hi = [ −→ hi; (3) −→ hi and where ward and backward LSTMs respectively. ←− hi are the hidden states of the for- Motivated by (Wang and Chang, 2016; Stern et al., 2017), we calculate the segment represen- tation ga,b for an opinion span with boundaries of a and b (both inclusive) as follows: −→ h b − ←− h n+1 = 0 and 1 ≤ a ≤ b ≤ n. ga,b = [ −→ h 0 = 0, −→ h a−1; ←− h b+1] ←− h a − where (4) 6We calculate the transition parameters among ﬁve sub- tags BIOES for targets. (5) Φyi(x, i) = ft(hi)¯yi where the linear layer ft is used to calculate the score for local context for targets. Such a linear layer takes the hidden state hi as the input and re- turns a vector of length 5, with each value in the vector indicating the score of the corresponding sub-tag among BIOES. The subscript ¯yi indi- cates the index of such a sub-tag. j,k, Sǫ When yi ∈ {Bǫ j,k}, we need to calculate 3 additional factorized feature scores for capturing structural information by adding them to the basic score as follows: Φyi(x, i) += (6) fs([gi+j,i+k; ←− hi])ǫ + fo(gi+j,i+k) + fr(j, k) Note that the subscript of the variable g is repre- sented as i + j, i + k which are the absolute po- sitions since j, k are the offsets. We explain such 3 additional factorized scores appearing in Equa- tion 6. • fs([gi+j,i+k; ←− hi])ǫ calculates the score for the sentiment. A linear layer fs takes the concate- nation of the segment representation gi+j,i+k ←− hi for an opinion span and the local context for a target, since we believe that the sen- timent is mainly determined by the opinion span as well as the target phrase itself. Note that we only use the backward hidden state ←− hi here, because the end position of a target is not available in the tag and the target phrase appears on the right of this position i. The lin- ear layer fs returns a vector of length 3, with each value representing the score of a certain polarity of +, 0, −. The subscript ǫ indicates the index of such a polarity. • fo(gi+j,i+k) is used to calculate a score for an opinion span. A linear layer fo takes the segment representation gi+j,i+k of an opin- ion span and returns one number representing the score of an opinion span. Dataset Train Dev Test #S 1266 0,310 0, 492 14Rest # + # 0 1692 404 773 166 54 66 # - 480 119 155 #S 906 219 328 14Lap # + # 0 817 169 364 126 36 63 # - 517 141 116 #S 605 148 322 15Rest # + # 0 783 185 317 25 11 25 # - 205 53 143 #S 857 210 326 16Rest # + # 0 1015 252 407 50 11 29 # - 329 76 78 Table 1: Statistics of 4 datasets. (#S denotes number of sentences, and # +, # 0, # - denote numbers of positive, neutral and negative triplets respectively.) 0 food was so so but excited to see many vegan options + O O B 0 −2,−2 E O S + 4,5 O O O O O Figure 4: The gold tagging sequence of JETo for the example sentence. • fr(j, k) is used to calculate a score for offsets, since we believe the offset is an important fea- ture. A linear layer fr returns one number representing the score of offsets j, k which again are the distances between a target and two ends of the opinion span. Here, we intro- duce the offset embedding wr randomly ini- tialized for encoding different offsets. Specif- ically, we calculate the score as follows7: fr(j, k) = Wrwr[min (j, k)] + br where Wr and br are learnable parameters. (7) 2.3 One Target for Multiple Opinion Spans The approach JET described above allows multi- ple targets to point to the same opinion span. One potential issue is that such an approach is not able to handle the case where one target is associated with multiple opinion spans. To remedy such an issue, we could swap a target and an opinion span to arrive at a new model as a model variant, since they are both text spans which are characterized by their boundaries. Speciﬁcally, in such a model variant, we still use the extended tags Bǫ j,k and Sǫ j,k, where we use sub-tags BIOES to encode an opinion span, the offsets j, k for the target and ǫ for the sentiment polarity. We use a similar pro- cedure for the feature score calculation. To differentiate with our ﬁrst model, we name our ﬁrst model as JETt and such a model variant as JETo. The superscripts t and o indicate the use of the sub-tags B and S to encode a target and an opinion span respectively. Figure 4 presents the gold tagging sequence of JETo. 2.4 Training and Inference The loss function L for the training data D is de- ﬁned as: L = − X log p(y|x). (8) (x,y)∈D The overall model is analogous to that of a neu- ral CRF (Peng et al., 2009; Do et al., 2010; Lam- ple et al., 2016); hence the inference and decod- ing follow standard marginal and MAP inference8 procedures. For example, the prediction of y fol- lows the Viterbi-like MAP inference procedure during decoding. Notice that the number of la- bels at each position under the position-aware tag- ging scheme is O(M 2), since we need to compute segment representation for text spans of lengths within M . Hence, the time complexity for infer- ence is O(nM 2). When M ≪ n (empirically, we found n can be up to 80 in our datasets, and we set M ∈ [2, 6]), this complexity is better than the existing work with complexity O(n2) (Peng et al., 2019). 3 Experiments 3.1 Data We reﬁne the dataset previously created by Peng et al. (2019)9. We call our reﬁned dataset ASTE- Data-V2, and the original version as ASTE-Data- V110. Note that ASTE-Data-V1 does not contain cases where one opinion span is associated with multiple targets. For example, there are two tar- gets, “service” and “atmosphere”, in the sentence “Best service and atmosphere”. The opinion span “Best” is associated with such two targets, result- ing in two triplets. However, we found that not all such triplets are explicitly annotated in ASTE- Data-V1. We reﬁne the dataset with these addi- tional missing triplets in our dataset ASTE-Data- 8See the Appendix for detailed algorithm. 9https://github.com/xuuuluuu/ SemEval-Triplet-data 7We use min (j, k) since we care the offset between the 10We also report the results on ASTE-Data-V1 in the Ap- starting positions of an opinion span and a target. pendix. V211. Table 1 presents the detailed statistics for 4 datasets.12 14Rest, 15Rest, 16Rest are the datasets of restaurant domain and 14Lap is of laptop domain. Such datasets were all created based on the datasets originally released by Se- mEval (Pontiki et al., 2014, 2015, 2016; Fan et al., 2019). 3.2 Baselines Our JET approaches are compared with the fol- lowing baselines using pipeline. • RINANTE+ (Peng et al., 2019) modiﬁes RI- NANTE (Dai and Song, 2019) which is de- signed based on LSTM-CRF (Lample et al., 2016), to co-extract targets with sentiment, and opinion spans. Such an approach also fuses mined rules as weak supervision to cap- ture dependency relations of words in a sen- tence at the ﬁrst stage. At the second stage, it generates all the possible triplets and applies a classiﬁer based on MLP on such triplets to determine if each triplet is valid or not. • CMLA+ (Peng et al., 2019) modiﬁes CMLA (Wang et al., 2017) which leverages attention mechanism to capture dependencies among words, to co-extract targets with senti- ment, and opinion spans at the ﬁrst stage. At the second stage, it uses the same method to obtain all the valid triplets as RINANTE+. • Li-uniﬁed-R (Peng et al., 2019) modiﬁes the model (Li et al., 2019) to extract targets with sentiment, as well as opinion spans re- spectively based on a customized multi-layer LSTM neural architecture. At the second stage, it uses the same method to obtain all the valid triplets as RINANTE+. • Peng et al. (2019) proposed an approach mo- tivated by Li-uniﬁed-R to co-extract targets with sentiment, and opinion spans simulta- neously. Such an approach also fuses GCN to capture dependency information to facili- tate the co-extraction. At the second stage, it uses the same method to obtain all the valid triplets as RINANTE+. 3.3 Experimental Setup Following the previous work (Peng et al., 2019), we use pre-trained 300d GloVe (Pennington et al., 11We also remove triplets with sentiment originally labeled as “conﬂict” by SemEval. 12See the Appendix for more statistics. 2014) to initialize the word embeddings. We use 100 as the embedding size of wr (offset embed- ding). We use the bi-directional LSTM with the hidden size 300. For experiments with contextu- alised representation, we adopt the pre-trained lan- guage model BERT (Devlin et al., 2019). Specif- ically, we use bert-as-service (Xiao, 2018) to gen- erate the contextualized word embedding without ﬁne-tuning. We use the representation from the last layer of the uncased version of BERT base model for our experiments. Before training, we discard any instance from the training data that contains triplets with offset larger than M . We train our model for a maximal of 20 epochs using Adam (Kingma and Ba, 2014) as the optimizer with batch size 1 and dropout rate 0.513. We select the best model parameters based on the best F1 score on the development data and apply it to the test data for evaluation. Following the previous works, we report the pre- cision (P.), recall (R.) and F1 scores for the cor- rect triplets. Note that a correct triplet requires the boundary14 of the target, the boundary of the opin- ion span, and the target sentiment polarity to be all correct at the same time. 3.4 Main Results Table 2 presents the main results, where all the baselines as well as our models with different maximum offsets M are listed. In general, our joint models JETt and JETo, which are selected based on the best F1 score on the dev set, are able to outperform the most competitive baseline of Peng et al. (2019) on the 4 datasets 14Rest, 15Rest, 16Rest, and 14Lap. Speciﬁcally, the best models selected from JETt and JETo outper- form Peng et al. (2019) signiﬁcantly15 on 14Rest and 16Rest datasets with p < 10−5 respectively. Such results imply that our joint models JETt and JETo are more capable of capturing interactions among the elements in triplets than those pipeline In addition, we observe a general approaches. trend from the results that the F1 score increases as M increases on the 4 datasets when M ≤ 5. We observe that the performance of JETt and JETo 13See the Appendix for experimental details. We use a dif- ferent dropout rate 0.7 on the dataset 14Lap based on pre- liminary results since the domain is different from the other 3 datasets. 14We deﬁne a boundary as the beginning and ending posi- tions of a text span. 15We have conducted signiﬁcance test using the bootstrap resampling method (Koehn, 2004). Models 14Rest Dev F1 P. R. F1 Dev F1 14Lap P. R. F1 Dev F1 15Rest P. R. F1 Dev F1 16Rest P. R. - - - - 39.18 31.42 41.04 43.24 47.13 39.38 67.35 63.66 72.46 70.02 69.67 62.23 66.76 45.67 50.87 50.31 52.41 53.14 CMLA+ RINANTE+ Li-uniﬁed-R Peng et al. (2019) JETt (M = 2) JETt (M = 3) JETt (M = 4) JETt (M = 5) JETt (M = 6) JETo (M = 2) JETo (M = 3) JETo (M = 4) JETo (M = 5) JETo (M = 6) + Contextualized Word Representation (BERT) JETt (M = 6)+ BERT 56.00 JETo (M = 6)+ BERT 56.89 32.29 42.76 47.38 48.39 49.09 30.48 41.45 46.88 47.18 55.13 66.89 65.29 67.63 71.49 61.50 41.72 49.41 51.56 53.35 53.54 54.12 55.94 63.44 70.56 42.79 34.95 51.00 51.46 44.68 53.09 56.41 54.44 56.58 41.88 50.71 55.38 56.85 58.14 - - - - 35.69 42.34 45.90 48.26 47.68 36.12 41.95 45.66 45.83 45.61 30.09 21.71 40.56 37.38 57.39 56.86 48.77 54.84 52.00 54.34 58.89 54.55 55.98 53.03 36.92 18.66 44.28 50.38 24.31 31.31 32.78 34.44 35.91 21.92 31.12 35.36 35.36 33.89 33.16 20.07 42.34 42.87 34.15 40.38 39.21 42.31 42.48 31.23 40.72 42.91 43.34 41.35 - - - - 42.34 52.02 52.50 54.97 55.06 43.39 48.72 56.73 59.57 60.97 34.56 29.88 44.72 48.07 64.81 59.87 64.50 55.67 59.77 52.31 58.28 58.54 61.39 64.37 39.84 30.06 51.39 57.51 28.87 36.91 40.82 43.51 42.27 28.04 34.85 43.09 40.00 44.33 37.01 29.97 47.82 52.32 39.94 45.66 50.00 48.84 49.52 36.51 43.61 49.64 48.44 52.50 - - - - 43.27 52.13 57.69 57.83 58.45 43.24 53.36 58.26 55.92 60.90 41.34 25.68 37.33 46.96 68.75 67.22 64.64 61.63 63.59 63.86 72.40 69.81 66.06 70.94 42.10 22.30 54.51 64.24 38.52 47.47 47.67 48.44 50.97 35.41 47.47 49.03 49.61 57.00 F1 41.72 23.87 44.31 54.21 49.38 55.64 54.87 54.25 56.59 45.56 57.34 57.60 56.67 63.21 58.41 62.40 50.40 48.84 53.53 55.39 43.28 47.33 47.86 51.04 59.86 64.78 68.20 64.45 42.89 51.96 52.66 57.53 60.67 63.75 65.28 70.42 51.95 58.37 57.85 63.83 Table 2: Main results on our reﬁned dataset ASTE-Data-V2. The underlined scores indicate the best results on the dev set, and the highlighted scores are the corresponding test results. The experimental results on the previous released dataset ASTE-Data-V1 can be found in the Appendix. on the dev set of 14Lap drops when M = 6. For the dataset 14Rest, JETo(M = 6) achieves the best results on F1 scores among all the JETo models. Such a JETo(M = 6) model outperforms the strongest baseline Peng et al. (2019) by nearly 7 F1 points. JETt(M = 6) also achieves a good performance with 56.58 in terms of F1 score. Comparing results of our mod- els to baselines, the reason why ours have bet- ter F1 scores is that our models JETt(M ≥ 4) and JETo(M ≥ 4) both achieve improvements of more than 15 precision points, while we main- tain acceptable recall scores. Similar patterns of results on the datasets 14Lap, 15Rest and 16Rest are observed, except that JETt(M = 5) and JETo(M = 5) achieves the best F1 score on the dev set of 14Lap. Furthermore, we dis- cover that the performance of both JETo and JETt on 14Rest and 16Rest datasets is better than on14Lap and 15Rest datasets. Such a behavior can be explained by the large distribution differ- ences of positive, neutral and negative sentiment between the train and test set of the 14Rest and 16Rest datasets, shown in Table 1. Furthermore, we also conduct additional experi- ments on our proposed model with the contextual- ized word representation BERT. Both JETt (M = 6)+ BERT and JETo (M = 6)+ BERT achieve new state- of-the-art performance on the four datasets. 4 Analysis 4.1 Robustness Analysis We analyze the model robustness by assessing the performance on targets, opinion spans and offsets target offset opinion Span 100 80 60 40 20 0 2 1 3 5 (a) JETt(M = 6)+ BERT 4 100 80 60 40 20 0 6 target offset opinion span 2 1 3 5 (b) JETo(M = 6)+ BERT 4 6 Figure 5: F1(%) scores (y-axis) of different lengths (x-axis) for targets, opinion spans and offsets on the dataset 14Rest. of different lengths for two models JETt(M = 6)+ BERT and JETo(M = 6)+ BERT on the four Figure 5 shows the results on the datasets. 14Rest dataset16. As we can see, JETo(M = 6)+ BERT is able to better extract triplets with targets of lengths ≤ 3 than JETt(M = 6)+ BERT. Fur- thermore, JETo(M = 6)+ BERT achieves a better F1 score for triplets whose opinion spans are of length 1 and 4. However, JETo(M = 6)+ BERT performs comparably to JETt(M = 6)+ BERT for triplets whose opinion spans are of length 2 and 3. In addition, JETo(M = 6)+ BERT is able to outper- form JETt(M = 6)+ BERT with offset of length 4 and above. We also observe that the performance drops when the lengths of targets, opinion spans and offsets are longer. This conﬁrms that model- ing the boundaries are harder when their lengths are longer. Similar patterns of results are observed on 14Lap, 15Rest, and 16Rest17. We also investigate the robustness on different 16See the Appendix for the statistics of accumulative per- centage of different lengths for targets, opinion spans and off- sets. 17See the Appendix for results on the other 3 datasets. Gold Peng et al. (2019) JETt JETo + Food is fresh and hot ready to eat + Food is fresh and hot ready to eat + Food is fresh and hot ready to eat + Food is fresh and hot ready to eat 0 with a quaint bar and good food + 0 with a quaint bar and good food + 0 with a quaint bar and good food + 0 with a quaint bar and good food + Table 3: Qualitative Analysis (T, Op, S) (T, O, S) (Tp, O, S) 70 60 50 40 (T, Op, S) (T, O, S) (Tp, O, S) 70 60 50 40 14Rest 14Lap 15Rest 16Rest (a) JETt(M = 6)+ BERT 14Rest 14Lap 15Rest 16Rest (b) JETo(M = 6)+ BERT Figure 6: F1 for different evaluation methods. evaluation methods, as presented in Figure 6. T (Target), O (Opinion Span) and S (Sentiment) are the elements to be evaluated. The subscript p on the right of an element in the legend de- notes “partially correct”. We deﬁne two bound- aries to be partially correct if such two bound- aries overlap. (T, O, S) is the evaluation method (Tp, O, S) requires used for our main results. the boundary of targets to be partially correct, and the boundary of opinion spans as well as (T, Op, S) the sentiment to be exactly correct. requires the boundary of opinion spans to be partially correct, and the boundary of targets as well as the sentiment to be exactly correct. The results based on (T, Op, S) yield higher im- provements in terms of F1 points than results based on (Tp, O, S), compared with (T, O, S) for JETt(M = 6)+ BERT except on 15Rest. The re- sults based on (Tp, O, S) yield higher F1 improve- ments than results based on (T, Op, S), compared with (T, O, S) for JETo(M = 6)+ BERT except on 15Rest. Such a comparison shows the bound- aries of opinion spans or target spans may be bet- ter captured when the sub-tags BIOES are used to model the opinion or target explicitly. 4.2 Qualitative Analysis To help us better understand the differences among these models, we present two example sentences selected from the test data as well as predictions by Peng et al. (2019), JETt and JETo in Table 3 18. As we can see, there exist 2 triplets in the gold data in the ﬁrst example. Peng et al. (2019) predicts 18See the Appendix for more examples. Model M = 6+ BERT +char embedding −offset features −opinion span features M = 6+ BERT +char embedding −offset features −opinion span features 14Rest 14Lap JETt JETo JETt JETo 58.41 59.13 55.36 57.93 62.40 62.23 61.24 62.04 47.86 47.71 44.16 47.66 51.04 51.38 49.58 50.48 15Rest 16Rest JETt 52.66 51.28 48.74 51.37 JETo 57.53 56.84 53.68 56.92 JETt 57.85 57.11 52.83 57.16 JETo 63.83 63.95 61.72 62.71 Table 4: Ablation Study (F1) an incorrect opinion span “hot ready” in the sec- ond triplet. JETt only predicts 1 triplet due to the model’s limitation (JETt is not able to handle the case of one target connecting to multiple opinion spans). JETo is able to predict 2 triplets correctly. In the second example, the gold data contains two triplets. Peng et al. (2019) is able to correctly pre- dict all the targets and opinion spans. However, it incorrectly connects each target to both two opin- ion spans. Our joint models JETt and JETo are both able to make the correct prediction. 4.3 Ablation Study We also conduct an ablation study for JETt(M = 6)+ BERT and JETo(M = 6)+ BERT on dev set of the 4 datasets, presented in Table 4. “+char embed- ding” denotes concatenating character embedding into word representation. The results show that concatenating character embedding mostly has no much positive impact on the performance, which we believe is due to data sparsity. “−offset fea- tures” denotes removing fr(j, k) in the feature score calculation, Equation 6. F1 scores drop more on the JETt(M = 6)+ BERT, this further con- ﬁrms that modeling the opinion span is more dif- ﬁcult than target. “−opinion features” denotes re- moving fo(gi+j,i+k) in the feature score calcula- tion in Equation 6. F1 scores drop consistently, implying the importance of such features for opin- ion spans. 4.4 Ensemble Analysis As mentioned earlier, JETo is proposed to over- come the limitation of JETt, and vice versa. We Dataset Model P. R. 14Rest 14Lap 15Rest 16Rest JETt JETo JETo→t JETt→o JETt JETo JETo→t JETt→o JETt JETo JETo→t JETt→o JETt JETo JETo→t JETt→o 63.44 70.56 61.28 61.10 53.53 55.39 48.68 49.57 68.20 64.45 61.41 61.75 65.28 70.42 61.94 62.50 54.12 55.94 63.38 63.98 43.28 47.33 51.01 53.22 42.89 51.96 53.81 55.26 51.95 58.37 62.06 63.23 F1 58.41 62.40 62.31 62.51 47.86 51.04 49.82 51.33 52.66 57.53 57.36 58.32 57.85 63.83 62.00 62.86 Table 5: Results for Ensemble. We use the models JETt and JETo (with BERT, M = 6) as base models for building two ensemble models on 4 datasets. believe that such two models complement each other. Hence, we propose two ensemble models JETo→t and JETt→o to properly merge the results produced by JETt and JETo. JETo→t merges results of JETo towards JETt by adding distinct triplets from JETo to JETt, and analogously for JETt→o. We discuss how we build the ensemble models based on the two models JETt and JETo (with BERT, M = 6). First we call two triplets are overlap with one another if two targets overlap and any of their opinions overlap with one another. The ensemble model JETo→t merges results from JETo towards JETt. Speciﬁcally, within the same instance, if a triplet produced by JETo does not overlap with any triplet produced by JETt, we augment the prediction space with such an addi- tional triplet. After going through each triplet pro- duced by JETo, we regard the expanded predic- tions as the output of the ensemble model JETo→t. Similarly, we merge the result from JETt towards JETo to obtain the result for the ensemble model JETt→o. We report results for ensemble models JETo→t and JETt→o presented in Table 5. As we can see, on 14Rest, 14Lap and 15Rest, the ensemble model JETt→o is able to achieve better F1 score than JETt and JETo. However, such a simple en- semble approach appears to be less effective on 16Rest. It is worth highlighting that the ensem- ble models have signiﬁcant improvements in terms of recall score. Note that the recall score reﬂects the number of gold triplets extracted. Such im- provement conﬁrms our earlier hypothesis that the two models largely complement each other. 5 Related Work ASTE is highly related to another research topic – Aspect Based Sentiment Analysis (ABSA) (Pon- tiki et al., 2014, 2016). Such a research topic fo- cuses on identifying aspect categories, recogniz- ing aspect targets as well as the associated senti- ment. There exist a few tasks derived from ABSA. Target extraction (Chernyshevich, 2014; San Vi- cente et al., 2015; Yin et al., 2016; Lample et al., 2016; Li et al., 2018b; Ma et al., 2019) is a task that focuses on recognizing all the targets which are either aspect terms or named entities. Such a task is mostly regarded as a sequence labeling problem solvable by CRF-based methods. Aspect sentiment analysis or targeted sentiment analysis is another popular task. Such a task either refers to predicting sentiment polarity for a given tar- get (Dong et al., 2014; Chen et al., 2017; Xue and Li, 2018; Wang and Lu, 2018; Wang et al., 2018; Li et al., 2018a; Peng et al., 2018; Xu et al., 2020) or joint extraction of targets as well as sentiment associated with each target (Mitchell et al., 2013; Zhang et al., 2015; Li and Lu, 2017; Ma et al., 2018; Li and Lu, 2019; Li et al., 2019). The former mostly relies on different neural networks such as self-attention (Liu and Zhang, 2017) or memory networks (Tang et al., 2016) to generate an opinion representation for a given target for further classi- ﬁcation. The latter mostly regards the task as a se- quence labeling problem by applying CRF-based approaches. Another related task – target and opin- ion span co-extraction (Qiu et al., 2011; Liu et al., 2013, 2014, 2015; Wang et al., 2017; Xu et al., 2018; Dai and Song, 2019) is also often regarded as a sequence labeling problem. 6 Conclusion In this work, we propose a novel position-aware tagging scheme by enriching label expressiveness to address a limitation associated with existing works. Such a tagging scheme is able to specify the connection among three elements – a target, the target sentiment as well as an opinion span in an aspect sentiment triplet for the ASTE task. Based on the position-aware tagging scheme, we propose a novel approach JET that is capable of jointly extracting the aspect sentiment triplets. We also design factorized feature representations so as to effectively capture the interaction. We conduct extensive experiments and results show that our models outperform strong baselines signiﬁcantly with detailed analysis. Future work includes ﬁnd- ing applications of our novel tagging scheme in other tasks involving extracting triplets as well as extending our approach to support other tasks within sentiment analysis. Acknowledgements We would like to thank the anonymous reviewers for their helpful comments. This research is par- tially supported by Ministry of Education, Singa- pore, under its Academic Research Fund (AcRF) Tier 2 Programme (MOE AcRF Tier 2 Award No: MOE2017-T2-1-156). Any opinions, ﬁndings and conclusions or recommendations expressed in this material are those of the authors and do not reﬂect the views of the Ministry of Education, Singapore. References Heike Adel and Hinrich Sch¨utze. 2017. Global normal- ization of convolutional neural networks for joint en- tity and relation classiﬁcation. In Proc. of EMNLP. Giannis Bekoulis, Johannes Deleu, Thomas Demeester, and Chris Develder. 2018. Adversarial training for multi-context joint entity and relation extraction. In Proc. of EMNLP. Peng Chen, Zhongqian Sun, Lidong Bing, and Wei Yang. 2017. Recurrent attention network on mem- In Proc. of ory for aspect sentiment analysis. EMNLP. Maryna Chernyshevich. 2014. IHS r&d belarus: Cross-domain extraction of product features using CRF. In Proc. of SemEval. Hongliang Dai and Yangqiu Song. 2019. Neural as- pect and opinion term extraction with mined rules as weak supervision. In Proc. of ACL. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing. In Proc. of NAACL. Trinh Do, Thierry Arti, et al. 2010. Neural conditional random ﬁelds. In Proc. of AISTATS. Li Dong, Furu Wei, Chuanqi Tan, Duyu Tang, Ming Zhou, and Ke Xu. 2014. Adaptive recursive neural network for target-dependent twitter sentiment clas- siﬁcation. In Proc. of ACL. Zhifang Fan, Zhen Wu, Xin-Yu Dai, Shujian Huang, and Jiajun Chen. 2019. Target-oriented opinion words extraction with target-fused neural sequence labeling. In Proc. of NAACL. Michel Galley. 2006. A skip-chain conditional random ﬁeld for ranking meeting utterances by importance. In Proc. of EMNLP. Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. 2015. Explaining and harnessing adversar- ial examples. In Proc. of ICLR. Sepp Hochreiter and J¨urgen Schmidhuber. 1997. Long short-term memory. Neural computation, 9:1735– 80. Yoon Kim. 2014. Convolutional neural networks for sentence classiﬁcation. In Proc. of EMNLP. Diederik P Kingma and Jimmy Ba. 2014. Adam: A In Proc. of method for stochastic optimization. ICLR. Thomas N Kipf and Max Welling. 2017. Semi-su- pervised classiﬁcation with graph convolutional net- works. In Proc. of ICLR. Philipp Koehn. 2004. Statistical signiﬁcance tests for machine translation evaluation. In Proc. of EMNLP. John Lafferty, Andrew McCallum, and Fernando CN Pereira. 2001. Conditional random ﬁelds: Prob- abilistic models for segmenting and labeling se- quence data. In Proc. of ICML. Guillaume Lample, Miguel Ballesteros, Sandeep Sub- ramanian, Kazuya Kawakami, and Chris Dyer. 2016. Neural architectures for named entity recognition. In Proc. of NAACL. Hao Li and Wei Lu. 2017. Learning latent sentiment scopes for entity-level sentiment analysis. In Proc. of AAAI. Hao Li and Wei Lu. 2019. Learning explicit and im- plicit structures for targeted sentiment analysis. In Proc. of EMNLP. Nan Li and Desheng Dash Wu. 2010. Using text min- ing and sentiment analysis for online forums hotspot detection and forecast. Decision support systems, 48(2). Qi Li and Heng Ji. 2014. Incremental joint extraction of entity mentions and relations. In Proc. of ACL. Xin Li, Lidong Bing, Wai Lam, and Bei Shi. 2018a. Transformation networks for target-oriented senti- ment classiﬁcation. In Proc. of ACL. Xin Li, Lidong Bing, Piji Li, and Wai Lam. 2019. A uniﬁed model for opinion target extraction and target sentiment prediction. In Proc. of AAAI. Xin Li, Lidong Bing, Piji Li, Wai Lam, and Zhimou Yang. 2018b. Aspect term extraction with history In Proc. of attention and selective transformation. IJCAI. Bing Liu. 2010. Sentiment analysis and subjectivity. Handbook of natural language processing. Jiangming Liu and Yue Zhang. 2017. Attention model- ing for targeted sentiment. In Proc. of EACL. Kang Liu, Liheng Xu, and Jun Zhao. 2013. Syntactic patterns versus word alignment: Extracting opinion targets from online reviews. In Proc. of ACL. Kang Liu, Liheng Xu, and Jun Zhao. 2014. Extract- ing opinion targets and opinion words from online reviews with graph co-ranking. In Proc. of ACL. Pengfei Liu, Shaﬁq Joty, and Helen Meng. 2015. Fine– grained opinion mining with recurrent neural net- works and word embeddings. In Proc. of EMNLP. Dehong Ma, Sujian Li, and Houfeng Wang. 2018. Joint learning for targeted sentiment analysis. In Proc. of EMNLP. Dehong Ma, Sujian Li, Fangzhao Wu, Xing Xie, and Houfeng Wang. 2019. Exploring sequence-to-se- quence learning in aspect term extraction. In Proc. of ACL. Margaret Mitchell, Jacqueline Aguilar, Theresa Wilson, and Benjamin Van Durme. 2013. Open domain tar- geted sentiment. In Proc. of EMNLP. Makoto Miwa and Mohit Bansal. 2016. End-to-end re- lation extraction using LSTMs on sequences and tree structures. In Proc. of ACL. Makoto Miwa and Yutaka Sasaki. 2014. Modeling joint entity and relation extraction with table repre- sentation. In Proc. of EMNLP. Alvaro Ortigosa, Jos´e M Mart´ın, and Rosa M Carro. 2014. Sentiment analysis in facebook and its appli- cation to e-learning. Computers in Human Behavior, 31. Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Foundations and trends in infor- mation retrieval, 2(1-2). Haiyun Peng, Yukun Ma, Yang Li, and Erik Cam- bria. 2018. Learning multi-grained aspect target se- quence for chinese sentiment analysis. Knowledge- Based Systems, 148:167–176. Haiyun Peng, Lu Xu, Lidong Bing, Fei Huang, Wei Lu, and Luo Si. 2019. Knowing what, how and why: A near complete solution for aspect-based sentiment analysis. In Proc. of AAAI. Jian Peng, Liefeng Bo, and Jinbo Xu. 2009. Condi- tional neural ﬁelds. In Proc. of NIPS. Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. Glove: Global vectors for word rep- resentation. In Proc. of EMNLP. Maria Pontiki, Dimitris Galanis, Haris Papageor- giou, Ion Androutsopoulos, Suresh Manandhar, Mo- hammed AL-Smadi, Mahmoud Al-Ayyoub, Yanyan Zhao, Bing Qin, Orph´ee De Clercq, et al. 2016. Se- meval-2016 task 5: Aspect based sentiment analysis. In Proc. of SemEval. Maria Pontiki, Dimitris Galanis, Haris Papageorgiou, Suresh Manandhar, and Ion Androutsopoulos. 2015. SemEval-2015 task 12: Aspect based sentiment analysis. In Proc. of SemEval. Maria Pontiki, Dimitris Galanis, John Pavlopoulos, Harris Papageorgiou, Ion Androutsopoulos, and Suresh Manandhar. 2014. Semeval-2014 task 4: As- pect based sentiment analysis. In Proc. of SemEval. Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen. 2011. Opinion word expansion and target extrac- tion through double propagation. Computational Linguistics, 37(1):9–27. I˜naki San Vicente, Xabier Saralegi, and Rodrigo Agerri. 2015. EliXa: A modular and ﬂexible ABSA plat- form. In Proc. of SemEval. Sunita Sarawagi and William W Cohen. 2004. Semi– markov conditional random ﬁelds for information extraction. In Proc. of NIPS. Jasmina Smailovi´c, Miha Grˇcar, Nada Lavraˇc, and Mar- tin ˇZnidarˇsiˇc. 2013. Predictive sentiment analysis of tweets: A stock market application. In Human- Computer Interaction and Knowledge Discovery in Complex, Unstructured, Big Data. Springer. Mitchell Stern, Jacob Andreas, and Dan Klein. 2017. A minimal span-based neural constituency parser. In Proc. of ACL. Charles Sutton and Andrew McCallum. 2004. Col- lective segmentation and labeling of distant entities in information extraction. Technical report, MAS- SACHUSETTS UNIV AMHERST DEPT OF COM- PUTER SCIENCE. Duyu Tang, Bing Qin, and Ting Liu. 2016. Aspect level sentiment classiﬁcation with deep memory net- work. In Proc. of EMNLP. Bailin Wang and Wei Lu. 2018. Learning latent opin- In ions for aspect-level sentiment classiﬁcation. Proc. of AAAI. Shuai Wang, Sahisnu Mazumder, Bing Liu, Mianwei Zhou, and Yi Chang. 2018. Target-sensitive memory networks for aspect sentiment classiﬁcation. In Proc. of ACL. Wenhui Wang and Baobao Chang. 2016. Graph-based dependency parsing with bidirectional lstm. In Proc. of ACL. Wenya Wang, Sinno Jialin Pan, Daniel Dahlmeier, and Xiaokui Xiao. 2017. Coupled multi-layer attentions for co-extraction of aspect and opinion terms. In Proc. of AAAI. Han Xiao. 2018. bert-as-service. https://github. com/hanxiao/bert-as-service. Hu Xu, Bing Liu, Lei Shu, and Philip S. Yu. 2018. Dou- ble embeddings and cnn-based sequence labeling for aspect extraction. In Proc. of ACL. Lu Xu, Lidong Bing, Wei Lu, and Fei Huang. 2020. Aspect sentiment classiﬁcation with aspect-speciﬁc opinion spans. In Proc. of EMNLP. Wei Xue and Tao Li. 2018. Aspect based sentiment analysis with gated convolutional networks. In Proc. of ACL. Bishan Yang and Claire Cardie. 2012. Extracting opin- ion expressions with semi-Markov conditional ran- dom ﬁelds. In Proc. of EMNLP. Yichun Yin, Furu Wei, Li Dong, Kaimeng Xu, Ming Zhang, and Ming Zhou. 2016. Unsupervised word and dependency path embeddings for aspect term ex- traction. In Proc. of IJCAI. Meishan Zhang, Yue Zhang, and Duy-Tin Vo. 2015. Neural networks for open domain targeted sentiment. In Proc. of EMNLP. A More Data Statistics We present the statistics of accumulative percent- age of different lengths for targets, opinion spans and offsets in the training data on 4 datasets 14Rest, 14Rest, 15Rest and 16Rest in Fig- ure 7. As we mentioned in the main paper, similar patterns are observed on accumulative statistics on these 4 datasets. We also present the statistics of the number of targets with a single opinion span and with multiple opinion spans, and the number of opinion associated with a single target span and with multiple target spans, shown in Table 6. B Experimental Details We test our model on Intel(R) Xeon(R) Gold 6132 CPU, with PyTorch version 1.40. The average run time is 3300 sec/epoch, 1800 sec/epoch, 1170 sec/epoch, 1600 sec/epoch on 14Rest, 14Rest, 15Rest and 16Rest datasets respectively when M = 6. The total number of parameters is 2.5M. For hyper-parameter, we use pre-trained 300d GloVe (Pennington et al., 2014) to initialize the word embeddings. We use 100 as the embed- ding size of wr (offset embedding). For out-of- vocabulary words as well as wr, we randomly sam- ple their embeddings from the uniform distribution U (−0.1, 0.1), as done in (Kim, 2014). We use the bi-directional LSTM with the hidden size 300. We train our model for a maximal of 20 epochs us- ing Adam (Kingma and Ba, 2014) as the optimizer with batch size 1 and dropout rate 0.5 for datasets in restaurant domain and 0.7 for laptop domain. We manually tune the dropout rate from 0.4 to 0.7, and select the best model parameters based on the best F1 score on the development data and apply it to the test data for evaluation. For experiments with contextualised representation, we adopt the pre-trained language model BERT (Devlin et al., 2019). Speciﬁcally, we use bert-as-service (Xiao, 2018) to generate the contextualized word embed- ding without ﬁne-tuning. We use the representa- tion from the last layer of the uncased version of BERT base model for our experiments. C Experimental Results Table 7 presents the experimental result on the pre- vious released dataset by (Peng et al., 2019). target opinion Span offset D Decoding based on Viterbi 100 90 80 70 60 50 40 30 20 target opinion Span offset 4 5 3 6 (a) 14Rest 7 8 9 1 2 3 100 90 80 70 60 50 40 30 20 100 90 80 70 60 50 40 30 20 1 2 1 2 4 5 (b) 14Lap 6 7 8 9 100 90 80 70 60 50 40 30 20 target opinion Span offset target opinion Span offset j,k, Sǫ Let T = {Bǫ j,k, I, E, O} as the new tag set under our position-aware tagging scheme, where ǫ denotes the sentiment polarity for the target, and j, k indicate the position information which are the distances between the two ends of an opinion span and the starting position of a target respectively. As we know, |j| ≤ |k| ≤ M , ǫ ∈ {+, 0, −}. 4 5 3 6 (c) 15Rest 7 8 9 1 2 4 3 5 (d) 16Rest 6 7 8 9 O(|T |) = O(|ǫ|M 2) = O(M 2) Figure 7: Accumulative percentage (y-axis) in the train- ing data of different lengths (x-axis) for targets, opin- ion spans and offsets on the 4 datasets. We deﬁne the sub-tags of Bǫ j,k as B and S respectively, and the sub-tags of I, O, E as them- selves. We use the bar on top to denote the sub-tag. For example, ¯u is the subtag of u ∈ T . j,k, Sǫ Dataset 14Rest 14Lap 15Rest 16Rest Train Dev Test Train Dev Test Train Dev Test Train Dev Test # of Target with # of Target with # of Opinion with # of Opinion with One Opinion Span Multiple Opinion Spans One Target Span Multiple Target Spans 1809 433 720 1121 252 396 734 180 385 1029 258 396 242 67 128 160 44 67 128 33 47 169 38 56 1893 444 767 1114 270 420 893 224 438 1240 304 452 193 59 87 154 34 54 48 12 23 67 15 23 Models 14Rest Dev F1 P. R. F1 Dev F1 14Lap P. R. F1 Dev F1 15Rest P. R. F1 Dev F1 16Rest P. R. Table 6: Statistics of 4 datasets. - - - - 40.11 31.07 41.44 44.18 46.63 37.63 68.79 62.99 70.00 73.15 70.25 66.20 70.39 47.06 56.15 57.47 59.15 59.51 CMLA+ RINANTE+ Li-uniﬁed-R Peng et al. (2019) JETt (M = 2) JETt (M = 3) JETt (M = 4) JETt (M = 5) JETt (M = 6) JETo (M = 2) JETo (M = 3) JETo (M = 4) JETo (M = 5) JETo (M = 6) + Contextualized Word Representation (BERT) JETt (M = 6)+ BERT 61.01 JETo (M = 6)+ BERT 60.86 34.92 43.62 49.30 49.77 51.86 45.02 53.14 58.19 57.94 58.66 35.38 43.16 52.44 54.99 56.84 66.30 62.31 63.84 64.31 62.26 53.02 60.32 70.20 67.97 43.12 34.03 51.68 51.89 46.59 54.65 57.94 56.82 59.72 46.14 50.99 57.58 59.29 59.43 - - - - 35.00 43.72 43.19 45.47 45.83 33.01 38.99 40.87 43.23 42.50 31.40 23.10 42.25 40.40 63.69 54.18 57.46 59.50 57.98 50.43 55.37 49.86 52.36 52.01 34.60 17.60 42.78 47.24 23.27 30.41 31.43 33.88 36.33 23.88 33.67 36.33 40.82 39.59 32.90 20.00 42.47 43.50 34.08 38.95 40.63 43.17 44.67 32.41 41.88 42.03 45.87 44.96 - - - - 47.13 53.23 58.05 59.37 60.00 46.80 54.59 57.14 59.51 60.32 34.40 29.40 43.34 40.97 64.80 66.52 64.77 64.14 61.99 58.88 55.99 57.57 52.02 63.25 37.60 26.90 50.73 54.68 27.91 33.19 42.42 40.88 43.74 25.49 38.02 42.64 48.13 46.15 35.90 28.00 46.69 46.79 39.02 44.28 51.26 49.93 51.29 35.58 45.29 48.99 50.00 53.37 - - - - 42.32 50.50 53.57 54.16 55.88 40.33 47.87 53.99 56.08 55.63 43.60 27.10 38.19 46.76 70.76 66.35 68.79 66.86 68.99 60.47 69.45 73.98 66.91 66.58 39.80 20.50 53.47 62.97 35.91 44.95 48.82 50.32 51.18 39.14 46.45 54.41 58.71 57.85 F1 41.60 23.30 44.51 53.62 47.65 53.59 57.11 57.42 58.77 47.52 55.67 62.70 62.54 61.91 60.41 63.92 49.07 45.76 51.48 58.47 42.65 43.67 46.65 50.00 62.96 64.12 62.14 58.35 47.25 51.43 53.68 54.67 60.41 60.17 71.12 64.77 57.20 61.29 63.41 62.98 Table 7: The experimental results on the previous released datasets ASTE-Data-V1. The underlined scores indicate the best results on the dev set, and the highlighted scores are the corresponding test results. We use π(i, v) to denote the score for the opti- i } among all the possible mal sequence {y∗ sequences whose last tag is v. 1 · · · y∗ Given the input x of length n, we aim to obtain the optimal sequence y∗ = {y∗ 1 · · · y∗ n}. • Base Case for all the v ∈ T If v ∈ {I, E, O}: π(1, v) = ψST ART,¯v + ft(h1)¯v j,k, Sǫ If v ∈ {Bǫ j,k}: π(1, v) = ψST ART,¯v + Φv(x, 1) = ψST ART,¯v + ft(h1)¯v + fs([g1+j,1+k; + fr(j, k) ←− h1])ǫ + fo(g1+j,1+k) ft(hi)¯v, ←− h1])ǫ, where fo(g1+j,1+k), and fr(j, k) are the fac- torized feature score mentioned in the section 2.2.2. fs([g1+j,1+k; • Loop forward for i ∈ {2, · · · , n} and all the v ∈ T If v ∈ {I, E, O}: If v ∈ {Bǫ π(i, v) = max u∈T j,k, Sǫ π(i, v) = max u∈T {π(i − 1, u) + ψ¯u,¯v + ft(hi)¯v} j,k}: {π(i − 1, u) + ψ¯u,¯v + Φv(x, i)} = { max (u∈T ; j,k∈[−M,M ]; ǫ∈{+,0,−}) π(i − 1, u) + ψ¯u,¯v + ft(hi)¯v ←− hi])ǫ + fo(gi+j,i+k) + fs([gi+j,i+k; + fr(j, k)} • Backtrack y∗ = {y∗ for 1 · · · y∗ n} the optimal sequence y∗ n = arg max {π(n, v) + ψ¯v,ST OP } v∈T Loop for i ∈ {n − 1, · · · , 1} y∗ i = arg max v∈T {π(i, v) + ψ¯v,¯y∗ i+1} Note that ST ART appears before the start of the input sentence and ST OP appears after the end of the input sentence. The time complexity is O(n|T |) = O(nM 2). E Analysis E.1 Robustness Analysis We present the performance on targets, opinion spans and offsets of different lengths for two mod- els JETt(M = 6) and JETo(M = 6) with BERT on 3 datasets 14Lap,15Rest and 16Rest in Figure 8, Figure 9 and Figure 10 respectively. E.2 Qualitative Analysis We present one additional example sentence se- lected from the test data as well as predictions by Peng et al. (2019), JETt and JETo in Table 8. As we can see, the gold data contains two triplets. Peng et al. (2019) only predicts 1 opinion span, and therefore incorrectly assigns the opinion span JETt is able to “Good” to the target “price”. make the correct predictions. JETo only predicts 1 triplet correctly. The qualitative analysis helps us to better understand the differences among these models. F More Related Work The task of joint entity and relation extraction is also related to joint triplet extraction. Different from our task, such a relation extraction task aims to extract a pair of entities (instead of a target and an opinion span) and their relation as a triplet in a joint manner. Miwa and Sasaki (2014) and Li and Ji (2014) used approaches motivated by a table- ﬁlling method to jointly extract entity pairs as well as their relations. The tree-structured neural net- works (Miwa and Bansal, 2016) and CRF-based approaches (Adel and Sch¨utze, 2017) were also adopted to capture rich context information for triplet extraction. Recently, Bekoulis et al. (2018) used adversarial training (Goodfellow et al., 2015) for this task and results show that it performs more robustly in different domains. Although these ap- proaches may not be applied to our task ASTE, they may provide inspirations for future work. Algorithm 1 Decoding based on Viterbi Initialization for i = 1 do for ¯v ∈ {I, E, O} do v = ¯v π(1, v) = ψST ART,¯v + ft(h1)¯v end for ¯v ∈ {B, S} do for j ∈ [−M, M ] do for k ∈ [j, M ] do for ǫ ∈ {+, 0, −} do v = ¯vǫ j,k π(1, v) = ft(h1)¯v +fs([g1+j,1+k; fo(g1+j,1+k) + fr(j, k) ψST ART,¯v + ←− h1])ǫ+ end end end end end Loop Forward for i ∈ {2, · · · , n} do for ¯v ∈ {I, E, O} do v = ¯v π(i, v) maxu∈T {π(i − 1, u) + ψ¯u,¯v + ft(hi)¯v} = end for ¯v ∈ {B, S} do for j ∈ [−M, M ] do for k ∈ [j, M ] do for ǫ ∈ {+, 0, −} do v = ¯vǫ j,k π(i, v) = maxu∈T {π(i − 1, u) + ψ¯u,¯v + ft(hi)¯v + ←− fs([gi+j,i+k; hi])ǫ + fo(gi+j,i+k) + fr(j, k)} end end end end end Backward for 1 · · · y∗ {y∗ if i = n then the optimal sequence y∗ = n} for i ∈ {n, · · · , 1} do y∗ n = arg maxv∈T {π(n, v) + ψ¯v,ST OP } end else y∗ i = arg maxv∈T {π(i, v) + ψ¯v,¯y∗ i+1} end end Gold Peng et al. (2019) JETt JETo 0 Good food at the right price , + 0 Good food at the right price , + 0 Good food at the right price , + 0 Good food at the right price , Table 8: Qualitative Analysis target offset opinion span target offset opinion span 100 80 60 40 20 5 6 0 1 3 2 5 (b) JETo(M = 6) 4 6 2 3 (a) JETt(M = 6) 4 100 80 60 40 20 0 1 Figure 8: F1(%) scores (y-axis) of different lengths (x-axis) for targets, opinion spans and offsets on the dataset 14Lap. 100 80 60 40 20 0 1 target offset opinion span target offset opinion span 100 80 60 40 20 5 6 0 1 3 2 5 (b) JETo(M = 6) 4 6 2 3 (a) JETt(M = 6) 4 Figure 9: F1(%) scores (y-axis) of different lengths (x-axis) for targets, opinion spans and offsets on the dataset 15Rest. 100 80 60 40 20 0 1 target offset opinion span 100 80 60 40 20 5 6 0 1 2 3 (a) JETt(M = 6) 4 target offset opinion span 3 2 5 (b) JETo(M = 6) 4 6 Figure 10: F1(%) scores (y-axis) of different lengths (x-axis) for targets, opinion spans and offsets on the dataset 16Rest. 