Knowing What, How and Why: A Near Complete Solution for Aspect-based Sentiment Analysis Haiyun Peng1, Lu Xu∗1,2, Lidong Bing1, Fei Huang1, Wei Lu2, Luo Si1 1 DAMO Academy, Alibaba Group 2 Singapore University of Technology and Design {haiyun.p, lu.x, l.bing, f.huang, luo.si}@alibaba-inc.com, luwei@sutd.edu.sg 9 1 0 2 v o N 1 2 ] L C . s c [ 4 v 6 1 6 1 0 . 1 1 9 1 : v i X r a Abstract Target-based sentiment analysis or aspect-based sentiment analysis (ABSA) refers to addressing various sentiment anal- ysis tasks at a ﬁne-grained level, which includes but is not limited to aspect extraction, aspect sentiment classiﬁcation, and opinion extraction. There exist many solvers of the above individual subtasks or a combination of two subtasks, and they can work together to tell a complete story, i.e. the dis- cussed aspect, the sentiment on it, and the cause of the senti- ment. However, no previous ABSA research tried to provide a complete solution in one shot. In this paper, we introduce a new subtask under ABSA, named aspect sentiment triplet extraction (ASTE). Particularly, a solver of this task needs to extract triplets (What, How, Why) from the inputs, which show WHAT the targeted aspects are, HOW their sentiment polarities are and WHY they have such polarities (i.e. opin- ion reasons). For instance, one triplet from “Waiters are very friendly and the pasta is simply average” could be (‘Waiters’, positive, ‘friendly’). We propose a two-stage framework to address this task. The ﬁrst stage predicts what, how and why in a uniﬁed model, and then the second stage pairs up the predicted what (how) and why from the ﬁrst stage to output triplets. In the experiments, our framework has set a bench- mark performance in this novel triplet extraction task. Mean- while, it outperforms a few strong baselines adapted from state-of-the-art related methods. Introduction Target-based sentiment analysis (TBSA) or aspect-based sentiment analysis (ABSA1) refers to addressing various sentiment analysis tasks at a ﬁne-grained level (Liu 2012; Pontiki 2014), which includes but is not limited to as- pect/target term extraction (ATE), opinion term extraction (OTE), aspect/target term sentiment classiﬁcation (ATC), etc. Given an example sentence such as ‘Waiters are very friendly and the pasta is simply average’, the ATE is to ex- tract ‘Waiters’ and ‘pasta’, and the ATC is to classify them to positive and negative sentiment, respectively. The OTE ∗Lu Xu is under the Joint PhD Program between Alibaba and Singapore University of Technology and Design. Copyright c(cid:13) 2020, Association for the Advancement of Artiﬁcial Intelligence (www.aaai.org). All rights reserved. 1Interchangeable with TBSA in this paper. Figure 1: The road map to aspect-based sentiment analysis tasks. The bottom blue-ﬁlled circle anchors our task. is to extract ‘friendly’ and ‘average’. Although these tasks seem to be intersecting and confusing at the ﬁrst glance, they distinguish from each other black and white when fulﬁlling the three goals in ABSA. As shown in Fig 1, the top three squares represent ultimate goals for ABSA, where the aspect term represents an explicit mention of discussed target, such as ‘Waiters’ in the example. The opinion term represents the opinionated comment terms/phrases, like ‘friendly’. The as- pect category refers to certain predeﬁned categories, such as SERVICE and FOOD in the previous example (Wang et al. 2019; Pontiki 2015). Each circle in the middle layer denotes a direct subtask to realize the goal. The ‘Sentiment’ circle linked to as- pect terms refers to ATC which attracts a heated research popularity (Dong et al. 2014; Tang, Qin, and Liu 2016; Nguyen and Shirai 2015; Wang et al. 2016b; Ma et al. 2017; Tay, Luu, and Hui 2017; Ma, Peng, and Cambria 2018; Hazarika et al. 2018; Li et al. 2018a; Wang et al. 2018; Xue and Li 2018; He et al. 2018; Peng et al. 2018; Bailin and Lu 2018; Li et al. 2019b). The ‘Extract’ circle linked to aspect term denotes ATE, such as (Qiu et al. 2011; Liu, Xu, and Zhao 2013; Liu, Xu, and Zhao 2014; Liu, Joty, and Meng 2015; Yin et al. 2016; Wang et al. 2016a; Wang et al. 2017; He et al. 2017; Li and Lam 2017; Opinion termAspect/TargettermAspect/TargetcategoryExtractCategorizeExtractSentimentSentiment+ExtractSentimentCo-Extract+CategorizeSentiment+Co-extractSentiment(aspect term, opinion term, sentiment)            Table 1: Tagging schema and relative position index, where B denotes begin, I denotes inside, E denotes end, S denotes single and O denotes outside. The check and cross marks denote valid and invalid aspect-opinion pairs. sashimi is E-POS O O 0 0 Input Uniﬁed tag (aspect+sentiment) Opinion tag the world O I 0 0 (Waiters,friendly)(cid:88) (fugu sashimi, friendly)(cid:55) friendly O S 2 3 Waiters S-POS O 2 0 fugu B-POS O 0 3 and O O 0 0 out O B 0 0 the O O 0 0 are O O 0 0 Position index of O I 0 0 . O O 0 0 O E 0 0 O 0 3 Li et al. 2018b; Xu et al. 2018). The same also applies to other circles in the middle layer. Researchers also re- alized that solving these subtasks individually is insufﬁ- cient so they proposed to couple two subtasks as a com- pound task, such as aspect term extraction and sentiment classiﬁcation (Li et al. 2019a; Mitchell et al. 2013; Zhang, Zhang, and Vo 2015; Li and Lu 2017; He et al. 2019), as- pect term and opinion term co-extraction (Wang et al. 2017; Dai and Song 2019), aspect category and sentiment classiﬁ- cation (Hu et al. 2018), as circles illustrated at the bottom. Nevertheless, the above compound tasks are still not enough to get a complete picture regarding sentiment. For instance, in the previous example, knowing a positive senti- ment towards aspect term ‘waiters’ does not give a clue of why it is positive. Only by knowing ‘friendly’ will people understand the cause of sentiment. Fan et al. (2019) aim to extract the opinion terms for a given target, thus the extrac- tion can be regarded as the cause for certain sentiment on the target, through sentiment prediction is not in the scope of their paper. Note that Fan et al. (2019) assume the targets are given in advance. On the other hand, the co-extraction methods fail to tackle pairing of multiple aspects and opin- ion expressions in a single sentence (Wang et al. 2017; Dai and Song 2019). Li et al. (2019a) couple the tasks of aspect extraction and sentiment classiﬁcation with the uni- ﬁed tags (e.g. “B-POS” standing for the beginning of a pos- itive aspect) but they do not extract the opinion terms for the extracted aspects, leaving blank the sentiment cause. So did the modular architectures presented by Zhang and Gold- wasser (2019). In summary, no previous ABSA research try to handle such a requirement in one shot, namely knowing What target is being discussed (e.g. ‘waiters’), How is the sentiment (e.g. ‘positive’) and Why is this sentiment (e.g. ‘friendly’). Moreover, the mutual inﬂuence among the three questions lacks study either. To this end, we introduce an as- pect sentiment triplet extraction task (ASTE), shown in the blue-ﬁlled circle at the bottom in Fig 1. We propose a two-stage framework to address this task. In the ﬁrst stage, we aim to extract potential aspect terms, together with their sentiment, and extract potential opin- ion terms. The task is formulated as a labeling problem with two label sequences (Mitchell et al. 2013; Wang et al. 2017). Speciﬁcally, we couple a uniﬁed tagging system by following (Li et al. 2019a) for aspect extraction and senti- ment classiﬁcation, and a BIO-like tagging system for opin- ion extraction, as shown in Table 1. For the uniﬁed tag- ging system, it builds on top of two stacked Bidirectional Long Short Term Memory (BLSTM) networks. The upper one produces the aspect term and sentiment tagging results based on the uniﬁed tagging schema. The lower performs an auxiliary prediction of aspect boundaries with the aim for guiding the upper BLSTM. Gate mechanism is explicitly designed to maintain the sentiment consistency within each multi-word aspect. For the opinion term tagging system, it builds on top of a BLSTM layer and a Graph Convolutional Network (GCN) to make full use of semantic and syntactic information in a sentence. According to the task deﬁnition (Pontiki 2014; Pontiki 2015; Pontiki 2016; Li et al. 2018b; Li et al. 2019a), for a term/phrase being regarded as an aspect, it should co-occur with some “opinion terms” that indicate a sentiment polarity on it. Therefore, aspect in- formation is beneﬁcial to extracting opinion terms, as al- ready demonstrated in (Zhang et al. 2017; Wang et al. 2017; Dai and Song 2019). We speciﬁcally design a target-guiding module to transfer aspect information for opinion term ex- traction. After the ﬁrst stage, we have obtained a bunch of aspects with sentiment polarities and a bunch of opinion expres- sions. In the second stage, the goal is to pair up aspects with the corresponding opinion expressions. As we observed, for sentences with multiple aspects and opinions, word distance is very indicative for correctly pairing up an aspect and its opinion as shown in the bottom section of Table 1. Thus, we design distance embeddings to capture the distance be- tween aspects and opinion expressions that are predicted from the stage one. With a BLSTM encoder, we encode sentence-level contexts into aspect and opinion terms for the ﬁnal classiﬁcation of candidate pairs. In the experiments, our framework has set a benchmark performance in this novel sentiment triplet extraction task. Meanwhile, our framework outperforms the state-of-the-art methods (with modiﬁcation to ﬁt in our task) and the strongest sequence taggers on sev- eral benchmark datasets. We also conduct extensive ablation tests to validate the rationality of our framework design. Proposed Framework Problem Formulation For a given input sentence X = {x1, . . . , xT } with length T , the ASTE task is to extract sentiment triplets (What, How, Why), consisting of the aspects/targets (i.e. ‘What’), the sentiment polarity on them (i.e. ‘How’), and the opin- ions causing such a sentiment (i.e. ‘Why’). 2 Here, we for- mulate the task in two stages. In the stage one, the task in- cludes two sequence labeling (SL) subtasks, the uniﬁed tag SL and the opinion tag SL. The uniﬁed tag schema is Y T S = {B-POS, I-POS, E-POS, S-POS, B-NEG, I-NEG, 2Note that the opinion expressions should be paired with the targets/aspects it modiﬁes in a many-to-many setting. and Y OPT . For predicting Y T S , the left half of stage one model resembles the state-of-the-art work (Li et al. 2019a) for uniﬁed tag schema, and adapts one of its original compo- nent as a shared one (i.e. TG) with the right part for predict- ing Y OPT . Speciﬁcally, the left side contains two stacked BLSTM. The lower one BLSTMT performs an auxiliary prediction of target boundaries (i.e. BIO) for producing sig- nals for the upper BLSTM, the boundary guidance (BG), and the target guidance (TG). The hidden states from the upper BLSTMS are ﬁrst manipulated by the sentiment consistency (SC), and then used as the major signal to predict the uni- ﬁed tags by the BG component, which also transforms the pure target boundary tag prediction to guide uniﬁed tag pre- diction. Our design distinguishes from Li et al. (2019a) in the speciﬁc injection of opinion information for predicting uniﬁed tag (i.e. hOP T is used by BG). The right part of the stage one is for the opinion term prediction, i.e. Y OPT . The sentence is fed into a GCN to learn the mutual inﬂuence of target and opinion terms via dependency relations4. Afterwards, this signal will be sent to two different modules, TG and BLST M OP T . The TG component in the middle is the concatenation of pure target boundary information and the GCN output, which leverages the target information for opinion term extraction. Unlike Li et al. (2019a) whose opinion information is weak supervi- sion from sentiment lexicon lookup, our design speciﬁcally constructs a component sharing both target and opinion in- formation. This component is strongly supervised by opin- ion term extraction, therefore, both BLST M T and GCN can beneﬁt from its backpropagation. Meanwhile, the out- put from BLST M OP T will carry the sentence context on top of the GCN output. It will be sent for opinion term ex- traction, as well as for guiding uniﬁed tag prediction. The stage two model ﬁrstly uses the aspects and opinion expressions predicted from stage one to generate all possible pairs in each sentence. Based on the distance between target and opinion expression in each pair, a position embedding is applied for each target and opinion terms. Non-target/non- opinion term will have the same position embedding, which is zero in our experiments. After a BLSTM encoder, the hid- den states from the aspect and opinion expressions will be concatenated for binary classiﬁcation. Stage One Uniﬁed aspect boundary and sentiment labeling. As demonstrated by Li et al. (2019a), a target boundary tag is beneﬁcial to uniﬁed tag prediction. We implement a similar structure for uniﬁed aspect and sentiment tag la- beling. In order to learn the target boundary labeling, we employ a BLSTMT layer on top of sentence word em- beddings. The sequence output of this BLSTMT , hT = ←−−−− −−−−→ LSTMT (x)], will be fed into a softmax clas- LSTMT (x); [ siﬁer to predict the target sequence tag without sentiment, whose tag set Y T is {B, I, E, S, O}. With this supervision learning, hT is expected to carry target boundary informa- tion. Thus we input it to the second BLSTMS layer to ac- 4https://spacy.io/ Figure 2: The framework of our proposed two stage model. 1 T E-NEG, S-NEG, B-NEU, I-NEU, E-NEU, S-NEU} ∪ {O}, which locates aspects and labels their sentiment. The opin- ion tag schema is Y OPT = {B, I, E, S} ∪ {O}. The uniﬁed SL predicts a tag sequence YT S = {yT S T }, where yT S ∈ Y T S , while the opinion SL predicts a sequence i YOPT = {yOPT }, where yOPT , . . . , yOPT ∈ Y OPT . , . . . , yT S 1 In the stage two, given the sets of aspects {T1, T2, ..., Tn} and opinion expressions {O1, O2, ..., Om} labeled from the same sentence in the stage one, where there are n aspects 3 and m opinion expressions, a candidate pair pool is con- structed by coupling the elements from the two sets as {(T1, O1), (T1, O2), ..., (Tn, Om)}. The goal of this stage is to identify the legitimate ones from the candidate pool, and outputs them as the ﬁnal results. Note that the ‘How’ is em- bedded in Ti with the uniﬁed tags. i Model Overview Fig. 2 shows the overview of our two-stage framework. Re- call that the stage one predicts two kinds of labels, i.e. Y T S 3Each aspect could contain one or multiple terms. The same also applies to the opinion expression. In our model setting, there should be at least one aspect and one opinion expression. BGSC!"#$%&!"#$%'GCN!"#$%()'TGSentenceAt each time step:+*BLSTMPretrained word embeddingsPosition embeddingsPosition embedding of target wordHidden states of opinion word+Stage oneStage two…Hidden states of target wordPosition embedding of opinion word++,.2,4ℎ&ℎ'ℎ2&3ℎ2&3ℎ245&ℎ'ℎ(ℎ(+,.3ℎ()'ℎ()'ℎ'ℎ78&8&93ℎ&3ℎ&Target 1Target 2Opinion 1Opinion 2CandidatePair Pool+,.7SentenceHidden statessequenceAverageAverageℒ'&ℒ'ℒ'<ℒ()'8'&+,.5+,.6cumulate sentiment information. Speciﬁcally, the sequence ←−−−− output of this BLSTMS is hS = [ LSTMS (x)]. The expected tag set for each time step in this sequence is Y S = {B-POS, I-POS, E-POS, S-POS, B-NEG, I-NEG, E-NEG, S-NEG, B-NEU, I-NEU, E-NEU, S-NEU}, which appends each tag in Y T with three sentiment polarities. −−−−→ LSTMS (x); As illustrated in Table 1, some aspects may contain more than one term. In our task formulation, however, we pre- dict uniﬁed sequence tag term by term. It is possible, al- though contradictory, to have ‘fugu’ labeled as positive but ‘sashimi’ labeled as negative. To avoid such situation, a Sen- timent Consistency (SC) module (Li et al. 2019a) was de- signed with a gate mechanism: gt = σ(WghS ˜hS t = gt (cid:12) hS t + bg) t + (1 − gt) (cid:12) ˜hS t−1 (1) where Wg and bg are model parameters of the SC mod- ule, and (cid:12) is the element-wise multiplication. σ is a sigmoid function. With this gate mechanism, current time step pre- diction will also inherit features from the previous time step, reducing the risk of drastic sentiment label change. Given an aspect boundary tag, it can only be transformed into one of its three legitimate sentiment-appended uni- ﬁed tags. The Boundary Guidance (BG) module consoli- dates this observation into a constraint matrix transforma- tion Wtr ∈ R|Y T |×|Y S |. This is a probability transformation matrix in which Wtr i,j indicates the probability of tag Y Ti transforming to tag Y Sj . For instance, if Y Ti is I and Y Sj is B-POS, then the Wtr I,B-POS will be zero because I cannot transform to B-POS. With this transformation matrix, the aspect boundary probability distribution can now be trans- formed into uniﬁed probability distribution as: t |xt) = Softmax(WT hT t = p(yT zT t ) t = (Wtr)(cid:62)zT zS t (2) (cid:48) WT is the model parameter and zS t tag probability distribution. (cid:48) is the obtained uniﬁed Up to this moment, for the uniﬁed tag prediction, we have not directly utilized the opinion term information, which should apparently affect the detection of aspect. To this end, we integrate the opinion term information hOPT (which will be introduced in the next subsection) with ˜hS by concatenat- ing them together to form a reinforced representation hU for uniﬁed tag prediction with a softmax classiﬁer: t |xt) = Softmax(WS hU t ) (3) where the Y S is the probability distribution and WS is the model parameter. zS is the obtained uniﬁed tag probability t distribution. Note that the integration of hOPT for uniﬁed tag prediction was not used in Li et al. (2019a). t = p(yS zS Next, we design a fusion mechanism to merge this rein- forced uniﬁed tag probability with the previous transformed uniﬁed tag probability. We calculate a fusion weight score αt ∈ R with the concentration score ct from the target boundary tagger, deﬁned as below: where the concentration score ct, with a maximum value of 1, represents how conﬁdent the target boundary tagger predicts. The higher the score, the more conﬁdent is the tar- get boundary tagger. The hyper-parameter (cid:15) (we empirically set as 0.5) controls the proportional weight that transformed uniﬁed tag probability contributes in the ﬁnal decision. Then the ﬁnal fused score between transformed and reinforced uniﬁed tag probability is given as: t = αtzS zT S t + (1 − αt)zS t . (cid:48) (5) Opinion term extraction. Previous studies (Wang et al. 2017; Dai and Song 2019) suggest that aspect extraction and opinion extraction are mutually beneﬁcial. We also ob- serve that aspects are usually co-occur with opinion terms and especially so on our datasets (see Table 2). This drives us to utilize the target information to guide opinion term ex- traction. Particularly, we feed the sentence embedding to a GCN module to learn the mutual dependency relations be- tween different words. The adjacency matrix for GCN is constructed based on the dependency parsing of the sen- tence, namely WGCN ∈ R|L|×|L| , where L is the length of the sentence. If the ith word has dependency relation with the jth word, WGCN and WGCN will both have value 1, j,i otherwise, value 0. This operation is designed to capture the relation between aspects and opinion terms, as they are con- structed as syntactic modifying pairs. i,j To utilize the target information for opinion term ex- traction, we design an auxiliary task to integrate the tar- get boundary information with the output from GCN with a Target Guidance (TG) module. If a sentence contains an aspect-opinion pair, the opinion expression should modify its aspect following syntactic rules. Thus, given a target sig- nal from BLST M T , it is intuitive to use it to guide opinion term extraction. We have tried various implementations of TG, and in the end a simple concatenation achieved the best performance. The concatenation will be fed into a softmax classiﬁer for opinion tag classiﬁcation in the tag space of Y T G = {B, I, E, S} ∪ {O}: t = p(yOPT zT G t |xt) = Softmax(WT G[hT t ; hO t ]). (6) Next, the sequence of hidden states from GCN (hO) is sent to a BLSTMOPT for sequence learning, namely to en- code the contextual information within the sentence, and the output, hOP T , will be sent to both the BG component to as- sist uniﬁed tag prediction (Eq.3) and a softmax classiﬁer to predict opinion prediction: zOPT t = p(yOPT t |xt) = Softmax(WOPT hOPT t ). (7) Stage one training. Stage one is trained with stochastic gradient descent optimizer. The loss of each output signal is computed using crossentropy as: LI = − 1 T T (cid:88) t=1 I(yI,g t ) ◦ log(zI t ) (8) t )(cid:62)zT t ct = (zT αt = (cid:15)ct (4) where I is the symbol of task indicator and its possible val- ues are T , T S, T G and OPT . I(y) represents the one-hot vector with the y-th component being 1 and yI,g is the gold standard tag for the task I at the time step t. The total train- ing objective of stage one is to minimize the sum of individ- ual loss from each output signal, J (θ): t J (θ) = LT + LT S + LT G + LOPT . (9) Stage Two After stage one, for each sentence, we output two sets of text segments, i.e., aspect terms and opinion expres- sions, denoted as {T1, T2, ..., Tn} and {O1, O2, ..., Om} re- spectively, where there are n aspects and m opinion ex- pressions. Then, we generate a candidate pair pool as {(T1, O1), (T1, O2), ..., (Tn, Om)} by enumerating all pos- sible aspect-opinion pairs. Stage two is to classify whether each of these pair is valid or not. Position embeddings. In order to utilize the position re- lation between an aspect and an opinion expression, we calculate the word-length distance between the center of the aspect and that of the opinion expression by counting how many words appear in the middle. The absolute dis- tance will be treated as relative position information that en- codes the position relation between them. For the ease of training, we create position embeddings by treating the dis- tance as position index for aspects and opinions, and zero to non-aspect and non-opinion words. For instance, position indexes of a true pair (‘Waiter’,‘friendly’) and a fake pair (‘fugu sashimi’,‘friendly’) are shown in Table 1. Pair encoder and classiﬁcation. As shown in Fig. 2, we concatenate the pretrained GloVe word embeddings (Pen- nington, Socher, and Manning 2014) with our position em- beddings to form word representation. The position embed- ding is randomly initialized and kept trainable in the training step. We then feed the sentence to a BLSTM layer to encode sentence contextual information into aspects and opinion ex- pressions. Based on the sentence term index, we average the hidden states output from BLSTM for both aspect and opin- ion expression respectively as their features. Next, we con- catenate the two features and send it to softmax layer for bi- nary classiﬁcation. For the training of classiﬁer, we used the gold pairs annotated in the training set of our experimental datasets. During testing stage, we freeze the classiﬁer pa- rameters tuned against the validation sets, and directly test on the pairs generated in the candidate pool. Experiments Dataset Our datasets 5 originate from SemEval Challenges (Pontiki 2014; Pontiki 2015; Pontiki 2016). The annotation (opinion label) is derived from (Fan et al. 2019), where they already annotated opinion terms. In addition, we merge samples that are of the same sentence but have different annotations on targets and opinions. Each sample includes the original sen- tence, a sequence with uniﬁed aspect/target tags and a se- quence with opinion tags. Since each sentence might have 5https://github.com/xuuuluuu/SemEval-Triplet-data Table 2: Dataset. (#s and #p denote number of sentences and target-opinion pairs, respectively.) Dataset train valid test 14res #s 1300 323 496 # p 2145 524 862 14lap #p 1265 337 490 #s 920 228 339 15res #s 593 148 318 #p 923 238 455 16res #p 1289 316 465 #s 842 210 320 more than one aspect/targets and opinions, we pair up indi- vidual aspects/targets and their opinions. Below is an exam- ple: The best thing about this laptop is the price along with some of the newer features . The=O best=O thing=O about=O this=O laptop=O is=O the=O price=T-POS along=O with=O some=O of=O the=O newer=O features=TT-POS .=O The=O best=S thing=O about=O this=O laptop=O is=O the=O price=O along=O with=O some=O of=O the=O newer=SS features=O .=O The example consists of two target and opinion pairs, the ﬁrst pair is ‘price’ and ‘best’, the second pair is ‘feature’ and ‘newer’. Note that ‘TT-POS’ is only used for indicating the pairing relation with ‘SS’, for model training, the used tags are ‘T-POS’ and ‘S’. We also correct a small number of samples whose targets and opinions are overlapped. The validation set is randomly selected 20% of data from train- ing set. Table 2 shows the detailed statistics. Experimental Setting Our framework is evaluated on a two-stage setting due to our framework design. Since the output of our stage one contains both aspects and opinion terms, we compared with other as- pect and opinion co-extraction methods in the ﬁrst stage. The compared methods are as follows. RINANTE (Dai and Song 2019): It is an aspect and opinion co-extraction method that mines aspect and opinion term extraction rules based on the dependency relations of words in a sentence. CMLA (Wang et al. 2017): A co-extraction model that leverages at- tention mechanism to utilize the direct and direction depen- dency relations. Note that both CMLA and RINANTE use BIO tags for aspect and opinion extraction. For compari- son, we train them with uniﬁed tags for aspect extraction, and BIO tags for opinion extraction. IOG (Fan et al. 2019): A top performing opinion term extraction method with an Inward-Outward LSTM. Li-uniﬁed: (Li et al. 2019a) The state-of-the-art uniﬁed model for aspect extraction and sen- timent classiﬁcation. It also serves as a base model in our design and its results are compared on aspect extraction and sentiment classiﬁcation. Note that it does not conduct opin- ion extraction. Li-uniﬁed-R: A modiﬁed model variant of Li-uniﬁed by us, which adapts their original OE component for opinion extraction. Our–BLSTMOPT : The ﬁrst vari- ant of our model that removes the BLSTMOPT component. Thus, it may fail to consider sentence contextual information for opinion term extraction. Our–TG: The second variant of our model that removes the TG component, which does not Table 3: Stage one results of aspect extraction and sentiment classiﬁcation. (All models were trained in the uniﬁed tag setting.) RINANTE CMLA Li-uniﬁed Li-uniﬁed-R Our–BLSTMOPT Our–TG Our–T Our P 48.97 67.80 74.43 73.15 70.00 74.41 69.42 76.60 14res R 47.36 73.69 69.26 74.44 74.20 73.97 72.2 67.84 F 48.15 70.62 71.75 73.79 72.04 74.19 70.79 71.95 P 41.20 54.70 68.01 66.28 65.99 64.35 64.14 63.15 14lap R 33.20 59.20 56.72 60.71 54.62 60.29 60.63 61.55 F 36.70 56.90 61.86 63.38 59.77 62.26 62.34 62.34 P 46.20 49.90 61.39 64.95 63.41 59.28 62.28 67.65 15res R 37.40 58.00 67.99 64.95 65.19 61.92 66.35 64.02 F 41.30 53.60 64.52 64.95 64.29 60.57 64.25 65.79 P 49.40 58.90 66.88 66.33 69.74 64.57 62.65 71.18 16res R 36.70 63.60 71.40 74.55 71.62 66.89 71.4 72.30 F 42.10 61.20 69.06 70.20 70.67 65.71 66.74 71.73 The two rows below are results of aspect extraction only, without evaluating the correctness of sentiment polarity. RINANTE CMLA 75.89 84.21 70.34 89.83 73.00 86.93 70.80 71.50 52.80 82.20 60.50 76.40 72.64 75.10 51.68 89.30 60.39 81.50 67.10 72.00 55.20 87.60 60.60 79.00 Table 4: Stage one results of opinion term extraction. P 58.39 64.57 81.06 69.47 82.85 81.20 80.41 81.77 80.61 84.72 14res R 43.59 52.72 72.05 74.53 77.38 83.18 86.19 84.80 85.38 80.39 F 49.92 58.04 76.29 71.91 80.02 82.13 83.15 83.21 82.88 82.45 P 50.13 45.09 78.20 51.80 73.24 76.62 78.06 76.87 76.69 78.22 14lap R 33.86 31.57 62.70 65.30 69.63 74.90 68.98 75.31 73.88 71.84 F 40.42 37.14 69.60 57.70 71.35 75.70 73.19 76.03 75.21 74.84 P 54.12 65.49 77.40 60.80 76.06 79.18 74.29 75.98 78.13 78.07 15res R 39.96 48.88 57.00 65.30 70.71 75.88 80.48 76.32 75.22 78.07 F 45.97 55.98 65.70 62.90 73.25 77.44 77.21 76.10 76.60 78.02 P 61.90 76.03 75.00 74.50 85.25 79.84 82.12 82.33 77.14 81.09 16res R 44.57 56.19 42.40 69.00 78.51 86.88 84.95 85.16 87.10 86.67 F 51.83 64.62 54.10 71.70 81.69 83.16 83.46 83.67 81.77 83.73 Distance rule Dependency rule RINANTE CMLA IOG Li-uniﬁed-R Our–BLSTMOPT Our–TG Our–T Our have the mutual information exchange between aspect ex- traction and opinion extraction. Our–T: The third variant that eliminates the loss LT from the training. For the stage two evaluation, we cannot ﬁnd a baseline to compare under identical settings. Thus we stack our stage two model directly on the best performed stage one baselines to construct different pipeline models. In addition to evalu- ating the triplets (eg. (Waiter-friendly-POS))6, we also eval- uate the performances on the pairs (eg. (Waiter-friendly)). The implementations all use GloVe (Pennington, Socher, and Manning 2014) embeddings of 300 dimension and re- move domain embeddings for a fair comparison. We train up to 40 epochs with SGD optimizer with an initial learning rate 0.1 and decay rate at 0.001. Dropout rate of 0.5 is ap- plied on the ultimate features before prediction. We report testing results of the epoch that has the best validation per- formance. Results and Analysis Stage one. Table 3 presents the uniﬁed performance of the stage one for aspect extraction and sentiment classi- ﬁcation. Our model outperforms existing strong baselines (i.e. RINANTE, CMLA, and Li-Uniﬁed) on all datasets, es- pecially compared with the Li-uniﬁed model which is the state-of-the-art in the uniﬁed task. Interestingly, the baseline Li-uniﬁed-R, derived from Li-uniﬁed, performs very com- petitive, i.e. better than Li-uniﬁed on all datasets. It shows 6We switch TS-OPT pairs to target-opinion-sentiment triplets. that given the ground-truth label of opinion words, explicitly modeling opinion extraction can help upgrade the perfor- mance of aspect extraction. We also notice that Li-uniﬁed-R outperforms our full model on 14res and 14lap. Another in- sight is that the performance of RINANTE and CMLA re- duced a lot in the uniﬁed tag setting, comparing with their original setting. We believe this is due to the lack of speciﬁc design to utilize sentiment information. Thus, for reference, we evaluate them under their original setting, i.e. only con- sidering the target boundary and ignoring the sentiment po- larity. The results shown in the last two rows increase dras- tically compared with those in the uniﬁed tag setting. Table 4 illustrates the stage one performances of opin- ion term extraction. In terms of F score, our core model has again achieved the best performance compared with all existing baselines. Li-uniﬁed-R is generally not as good as our model on the restaurant datasets, but still performs very competitive and event better than our model on 14lap. Our– TG variant model has outperformed all baselines in the lap- top domain. RINANTE, CMLA and IOG only learned the mutual inﬂuence of aspect and opinion term. Compared with these baseline models, our model learns the multi-lateral in- formation ﬂow among the three tasks, i.e., aspect extrac- tion, sentiment classiﬁcation and opinion term extraction. In the case of opinion term extraction, it would be relatively straightforward to locate opinion terms if their sentiment po- larities are given. Speciﬁcally, the hOPT is used for uniﬁed tag prediction and thus the sentiment classiﬁcation signals are backpropagated to BLST M OP T , therefore, the opinion P P F Table 5: Stage two results in both pair and triplet setting. (+ denotes cascading our stage two module.) 16res 14lap R R 97.91 94.36 27.00 26.20 47.90 46.30 64.55 52.94 70.50 58.47 20.50 17.60 39.80 34.60 53.47 42.78 62.97 47.24 RINANTE+ CMLA+ Li-uniﬁed-R+ Our RINANTE+ CMLA+ Li-uniﬁed-R+ Our 14res R 97.59 51.08 53.42 73.67 68.10 37.63 46.63 68.79 62.99 15res R 99.61 33.90 46.70 61.75 65.70 26.90 37.60 50.73 54.68 42.32 45.17 44.37 47.76 31.07 40.11 41.44 44.18 34.40 42.10 52.29 50.00 23.10 31.40 42.25 40.40 35.70 52.50 46.11 52.35 27.10 43.60 38.19 46.76 37.10 42.70 52.75 49.22 29.40 34.40 43.34 40.97 29.70 44.10 52.56 53.85 20.00 32.90 42.47 43.50 35.40 44.60 56.85 56.23 28.00 35.90 46.69 46.79 46.29 48.95 55.34 56.10 34.03 43.12 51.68 51.89 P F F P Classiﬁer F1 Pair Triplet F 30.70 50.00 53.75 60.04 23.30 41.60 44.51 53.62 prediction can leverage such information. Stage two. After obtaining all the possible candidate triplets from the stage one, each triplet is sent to a binary classiﬁer. The classiﬁer was trained on the ground truth as- pect and opinion pairs in the training set. The model per- forming the best on the validation set was used as the stage two classiﬁer for evaluating both our model and baselines. (The performance of the classiﬁer on the validation set is shown in the ﬁrst row of Table 5). The last section in Table 5 shows the performance for the ﬁnal triplet extraction. We can observe that our model has achieved steady advantage over other baselines. In addition to evaluating the triplet, we also examine the pure pairing performance for coupling as- pects and opinion terms. The results are shown in the middle section of Table 5. In both sections, Li-uniﬁed-R+, a variant of Li-uniﬁed implemented by us, achieved competitive per- formance on the ﬁrst three datasets, and even slightly better than ours on 15res in the pairing evaluation. Ablation test. To evaluate the rationality of our model design, we also conducted ablation tests by introducing three model variants, our–BLSTMOPT , our–T and our–TG, where ‘–’ means without the component followed behind. As we introduced before, BLSTMOPT is expected to en- code sentence contextual information which is beneﬁcial to both uniﬁed tag prediction and opinion term extraction. From Table 3 and 4, for most datasets, we can ﬁnd the appar- ent performance reduction after removing the BLSTMOPT module, which validates the effectiveness of this component. Nevertheless, the contribution of TG is more complex. In the uniﬁed tag prediction task, the removal of TG module brings down the performance in all datasets reasonably. In the opinion term extraction, the removal even boosts the per- formances on 14res and 14lap datasets, especially the latter. Since TG module studies the mutual inﬂuence between as- pects and opinion terms, we suspect that their mutual rela- tion is not that strong. Instead of bringing useful informa- tion, TG module could potentially brings in noise as well. Our assumption is validated by the classiﬁer performance trained on gold labels in Table 5. 14lap and 14res have lower performances than the other two, particularly 14lap, which indicates that their target-opinion pairs are intrinsically more heterogeneous. Case Study Some triplet prediction cases are given in Table 6. In gen- eral, our model outputs more reasonable results. For the ﬁrst case, our model can predict more accurate opinions and sen- timent polarity such as “was n’t so fresh”. However, it faces some problem in pairing prediction. The baselines cannot well capture the negated opinion. For the second case, all pipelines are hindered by the target “log on”, our model can predict a partial target “log”. For the third case, Li-uniﬁed-R predicts three opinions, but “feeding” is a wrong one, while RINANTE fails to predict tags and thus cannot output any triplet. One might notice that in the table, some aspects ex- tracted in the stage one are coupled with multiple opinions, which usually brings in false positive triplets in the stage two. It might be plausible to set a heuristic rule to constrain that the pairing algorithm can only output a certain number (say equal to the number of extracted aspects) of triplets ac- cording to the classiﬁcation probability. However, we did try it and found it was not consistently proﬁtable. Conclusions We introduce a sentiment triplet extraction task that answers what is the aspect, how is its sentiment and why is the sen- timent in one shot by coupling together aspect extraction, aspect term sentiment classiﬁcation and opinion term ex- traction in a two-stage framework. The ﬁrst stage generates candidate aspects with sentiment polarities and candidate opinion terms by utilizing mutual inﬂuence between aspects and opinion terms. The second stage pairs up the correct as- pects and opinion terms. Experiments validate the feasibility and effectiveness of our model, and set a benchmark perfor- mance for this task. References [Bailin and Lu 2018] Bailin, W., and Lu, W. 2018. Learning latent opinions for aspect-level sentiment classiﬁcation. In AAAI. [Dai and Song 2019] Dai, H., and Song, Y. 2019. Neural aspect and opinion term extraction with mined rules as weak supervision. In ACL, 5268–5277. [Dong et al. 2014] Dong, L.; Wei, F.; Tan, C.; Tang, D.; Zhou, M.; and Xu, K. 2014. Adaptive recursive neural network for target- dependent twitter sentiment classiﬁcation. In ACL, 49–54. [Fan et al. 2019] Fan, Z.; Wu, Z.; Dai, X.; Huang, S.; and Chen, J. 2019. Target-oriented opinion words extraction with target-fused neural sequence labeling. In NAACL-HLT, 2509–2518. Table 6: Case study on ﬁnal output. (False positives were marked with cross.) Example Rice is too dry , tuna was n’t so fresh either . I am pleased with the fast log on, speedy WiFi connection and the long battery life. The service was exceptional - sometime there was a feeling that we were served by the army of friendly waiters . Ground truth (Rice-too dry-NEG), (tuna-was n’t so fresh-NEG) (log on-pleased-POS), (log on-fast-POS), (WiFi connection-speedy-POS), (battery life-long-POS) Our model (Rice-too dry-NEG), (tuna-was n’t so fresh-NEG), (Rice-was n’t so fresh-NEG)(cid:55), (tuna-too dry-NEG)(cid:55) (log-pleased-POS)(cid:55), (log-fast-POS)(cid:55), (WiFi connection-speedy-POS), (battery life-long-POS) (service-exceptional-POS), (waiters-friendly-POS) (service-exceptional-POS), (waiters-friendly-POS) Li-uniﬁed-R+ (Rice-dry-POS)(cid:55), (Rice-n’t-POS)(cid:55), (tuna-dry-POS)(cid:55), (tuna-fresh-POS)(cid:55) (WiFi connection-speedy-POS), (battery life-long-POS) CMLA+ RINANTE+ (Rice-dry-POS)(cid:55), (tuna-dry-POS)(cid:55) (tuna-dry-POS)(cid:55), (tuna-n’t so fresh either-POS)(cid:55) (WiFi connection-speedy-POS), (WiFi connection-long-POS)(cid:55), (battery life-fast-POS), (battery life-long-POS) (fast log-pleased-POS)(cid:55), (fast log-speedy-POS)(cid:55), (WIFI-long-POS)(cid:55), (battery life-long-POS) (service-exceptional-POS), (waiters-friendly-POS) (service-feeling-POS)(cid:55) (service-exceptional-POS), (waiters-friendly-POS) Empty [Hazarika et al. 2018] Hazarika, D.; Poria, S.; Vij, P.; Krishna- murthy, G.; Cambria, E.; and Zimmermann, R. 2018. Modeling inter-aspect dependencies for aspect-based sentiment analysis. In NAACL-HLT, 266–270. [He et al. 2017] He, R.; Lee, W. S.; Ng, H. T.; and Dahlmeier, D. 2017. An unsupervised neural attention model for aspect extrac- tion. In ACL. [He et al. 2018] He, R.; Lee, W. S.; Ng, H. T.; and Dahlmeier, D. 2018. Exploiting document knowledge for aspect-level sentiment classiﬁcation. In ACL, 579–585. [He et al. 2019] He, R.; Lee, W. S.; Ng, H. T.; and Dahlmeier, D. 2019. An interactive multi-task learning network for end-to-end aspect-based sentiment analysis. In ACL. [Hu et al. 2018] Hu, M.; Zhao, S.; Zhang, L.; Cai, K.; Su, Z.; 2018. Can: Constrained attention arXiv preprint Cheng, R.; and Shen, X. networks for multi-aspect sentiment analysis. arXiv:1812.10735. [Li and Lam 2017] Li, X., and Lam, W. 2017. Deep multi-task In learning for aspect term extraction with memory interaction. EMNLP. [Li and Lu 2017] Li, H., and Lu, W. 2017. Learning latent senti- In AAAI, 3482– ment scopes for entity-level sentiment analysis. 3489. [Li et al. 2018a] Li, X.; Bing, L.; Lam, W.; and Shi, B. 2018a. Transformation networks for target-oriented sentiment classiﬁca- tion. In ACL. [Li et al. 2018b] Li, X.; Bing, L.; Li, P.; Lam, W.; and Yang, Z. 2018b. Aspect term extraction with history attention and selective transformation. In IJCAI. [Li et al. 2019a] Li, X.; Bing, L.; Li, P.; and Lam, W. 2019a. A uni- ﬁed model for opinion target extraction and target sentiment pre- diction. In AAAI, volume 33, 6714–6721. [Li et al. 2019b] Li, Z.; Wei, Y.; Zhang, Y.; Xiang, Z.; and Li, X. 2019b. Exploiting coarse-to-ﬁne task transfer for aspect-level sen- timent classiﬁcation. In AAAI. [Liu, Joty, and Meng 2015] Liu, P.; Joty, S.; and Meng, H. 2015. Fine-grained opinion mining with recurrent neural networks and word embeddings. In EMNLP, 1433–1443. [Liu, Xu, and Zhao 2013] Liu, K.; Xu, L.; and Zhao, J. 2013. Syn- tactic patterns versus word alignment: Extracting opinion targets from online reviews. In ACL, 1754–1763. [Liu, Xu, and Zhao 2014] Liu, K.; Xu, L.; and Zhao, J. 2014. Ex- tracting opinion targets and opinion words from online reviews with graph co-ranking. In ACL. [Liu 2012] Liu, B. 2012. Sentiment analysis and opinion mining. Synthesis lectures on human language technologies 5(1):1–167. [Ma et al. 2017] Ma, D.; Li, S.; Zhang, X.; and Wang, H. 2017. Interactive attention networks for aspect-level sentiment classiﬁca- tion. In IJCAI. [Ma, Peng, and Cambria 2018] Ma, Y.; Peng, H.; and Cambria, E. 2018. Targeted aspect-based sentiment analysis via embedding commonsense knowledge into an attentive lstm. In AAAI. [Mitchell et al. 2013] Mitchell, M.; Aguilar, J.; Wilson, T.; and Van Durme, B. 2013. Open domain targeted sentiment. In EMNLP. 2015. Phrasernn: Phrase recursive neural network for aspect-based sen- timent analysis. In EMNLP, 2509–2514. [Nguyen and Shirai 2015] Nguyen, T. H., and Shirai, K. [Peng et al. 2018] Peng, H.; Ma, Y.; Li, Y.; and Cambria, E. 2018. Learning multi-grained aspect target sequence for chinese senti- ment analysis. Knowledge-Based Systems 148:167–176. [Pennington, Socher, and Manning 2014] Pennington, J.; Socher, R.; and Manning, C. 2014. Glove: Global vectors for word rep- resentation. In EMNLP. [Pontiki 2014] Pontiki, M. 2014. Semeval-2014 task 4: Aspect based sentiment analysis. In SemEval, 27–35. [Pontiki 2015] Pontiki, M. e. a. 2015. Semeval-2015 task 12: As- pect based sentiment analysis. In SemEval, 486–495. [Pontiki 2016] Pontiki, M. e. a. 2016. Semeval-2016 task 5: Aspect based sentiment analysis. In SemEval, 19–30. [Qiu et al. 2011] Qiu, G.; Liu, B.; Bu, J.; and Chen, C. 2011. Opin- ion word expansion and target extraction through double propaga- tion. Computational Linguistics 37(1). [Tang, Qin, and Liu 2016] Tang, D.; Qin, B.; and Liu, T. 2016. As- pect level sentiment classiﬁcation with deep memory network. In EMNLP. [Tay, Luu, and Hui 2017] Tay, Y.; Luu, A. T.; and Hui, S. C. 2017. Learning to attend via word-aspect associative fusion for aspect- based sentiment analysis. In AAAI. [Wang et al. 2016a] Wang, W.; Pan, S. J.; Dahlmeier, D.; and Xiao, X. 2016a. Recursive neural conditional random ﬁelds for aspect- based sentiment analysis. In EMNLP, 616–626. [Wang et al. 2016b] Wang, Y.; Huang, M.; zhu, x.; and Zhao, L. 2016b. Attention-based lstm for aspect-level sentiment classiﬁca- tion. In EMNLP, 606–615. [Wang et al. 2017] Wang, W.; Pan, S. J.; Dahlmeier, D.; and Xiao, X. 2017. Coupled multi-layer attentions for co-extraction of aspect and opinion terms. In AAAI, 3316–3322. [Wang et al. 2018] Wang, S.; Mazumder, S.; Liu, B.; Zhou, M.; and Chang, Y. 2018. Target-sensitive memory networks for aspect sen- timent classiﬁcation. In ACL, 957–967. [Wang et al. 2019] Wang, J.; Sun, C.; Li, S.; Liu, X.; Si, L.; Zhang, 2019. Aspect sentiment classiﬁcation to- M.; and Zhou, G. wards question-answering with reinforced bidirectional attention network. In ACL. [Xu et al. 2018] Xu, H.; Liu, B.; Shu, L.; and Yu, P. S. 2018. Double embeddings and cnn-based sequence labeling for aspect extraction. In ACL. [Xue and Li 2018] Xue, W., and Li, T. 2018. Aspect based senti- ment analysis with gated convolutional networks. In ACL, 2514– 2523. [Yin et al. 2016] Yin, Y.; Wei, F.; Dong, L.; Xu, K.; Zhang, M.; and Zhou, M. 2016. Unsupervised word and dependency path embed- dings for aspect term extraction. In IJCAI, 2979–2985. [Zhang and Goldwasser 2019] Zhang, X., and Goldwasser, D. 2019. Sentiment tagging with partial labels using modular archi- tectures. arXiv preprint arXiv:1906.00534. [Zhang et al. 2017] Zhang, X.; Jiang, Y.; Peng, H.; Tu, K.; and Goldwasser, D. 2017. Semi-supervised structured prediction with neural crf autoencoder. In EMNLP, 1701–1711. [Zhang, Zhang, and Vo 2015] Zhang, M.; Zhang, Y.; and Vo, D. T. In 2015. Neural networks for open domain targeted sentiment. EMNLP, 612–621. 