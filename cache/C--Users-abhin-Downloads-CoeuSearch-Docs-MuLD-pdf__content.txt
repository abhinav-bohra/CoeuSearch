MuLD : Multitask Long Document Benchmark G Thomas Hudson , Noura Al Moubayed Durham University Department Computer Science , Durham University , Durham , United Kingdom { g.t.hudson , noura.al-moubayed } @ durham.ac.uk Abstract impressive progress NLP techniques driven development multi-task benchmarks GLUE SuperGLUE . benchmarks focus tasks one two input sentences , exciting work designing efﬁcient techniques processing much longer inputs . paper , present MuLD : new long document benchmark consisting documents   ,    tokens . modifying existing NLP tasks , create diverse benchmark requires models successfully model long-term dependencies text . evaluate existing models perform , ﬁnd benchmark much challenging ‘ short document ’ equivalents . Furthermore , evaluating regular efﬁcient transformers , show models increased context length better able solve tasks presented , suggesting future improvements models vital solving similar long document problems . release data code baselines encourage research efﬁcient NLP models . Keywords : Long Documents , Benchmark , Multitask learning , NLP   . Introduction Pretrained language models highly inﬂuen- tial Natural Language Processing ( NLP ) , leading state-of-the-art results across wide range tasks . Based transformer architecture , language models shown capable text classiﬁcation , ques- tion answering , translation among many NLP problems . rise pretrained language models NLP driven inﬂuential benchmarks GLUE ( Wang et al. ,      ) , SuperGLUE ( Wang et al. ,      ) , combine multiple existing datasets provide standardised evaluation general-purpose Natural Language Understanding ( NLU ) approaches . Similar benchmarks created evaluate mul- tilingual approaches ( Yao et al. ,      ) , types NLP task Natural Language Generation ( NLG ) ( Liu et al. ,      ) . However , key component success trans- former model - self-attention - also major limitation comes processing longer sequences . Com- paring token tokens previous layer yields ( n  ) complexity , limiting ability standard transformers hundred thousand tokens standard hardware . many real world tasks involving need process documents range tens thousands tokens , impor- tant problem solve . Recently , approaches Longformer ( Beltagy et al. ,      ) , Reformer ( Kitaev et al. ,      ) , Lin- former ( Wang et al. ,      ) explored techniques improving efﬁciency transformers , allowing operate much longer sequences . However , mainly evaluated artiﬁcial datasets , limited evaluation real-world data . work creating long-text Figure   : Comparison lengths multitask long document benchmarks . maximum input length gpt  Longformer ( LED ) included comparison . datasets . notable example NarrativeQA ( Koˇcisk´y et al. ,      ) challenges models answer ques- tions plot entire novels movie scripts average length around   ,    tokens - well beyond input size current typical efﬁ- cient transformer models . however lit- tle work developing benchmarks length across wide variety NLP tasks .         b e F     ] L C .  c [   v           .         : v  X r     ,     ,     ,     ,     ,     ,   Words per InputContractNLIQasperQuALITYSummScreenGovReportQMSumNarrativeQAHotpotQAVLSPMovieCharacterTypesAO StyleChangeOpenSubtitlesgpt LEDOther DatasetsMuLD      Many ‘ long document ’ benchmarks use datasets con- sisting thousand tokens ( Figure   ) . Notably , Long-Range Arena ( Tay et al. ,      ) uses maximum length  K tokens , QuALITY becnhmark ( Pang et al. ,      ) uses maximum  K tokens . paper , argue lengths akin short essays , common usage term ‘ long document ’ would imply documents tens thousands tokens - similar length novels . end present MuLD : Multitask Long Document benchmark . set six long document tasks input least   ,    to- kens , spanning range dataset sizes , genres , for- mulations designed speciﬁcally test ability dif- ferent approaches model long-term dependencies real world text . datasets formed ﬁltering , extending , modifying existing NLP datasets . create baseline results standard approaches ( T  ) efﬁcient transformer methods ( Longformer ) , ﬁnding Longformer ’ extended input context al- lows perform better . MuLD dataset available www.github . com/ghomasHudson/muld .   . Related Work Benchmarks inﬂuence many NLP bench- marks success GLUE challenged mod- els solve   language understanding tasks including question answering , coreference resolution , senti- ment analysis . success GLUE resulted ad- vancement point successor challenging tasks required : SuperGLUE . ap- proach replicated across languages ( Yao et al. ,      ) , well task domains NLG ( Liu et al. ,      ) . Recently attempts design benchmarks test ability models under- stand long documents . Long Range Arena designed benchmark evaluating performance efﬁcient transform- ers long input sequences ( Tay et al. ,      ) . benchmark challenges models perform   synthetic real-world multitmodal tasks documents   ,    tokens . Howerver , tasks forms classiﬁcation , noted Shaham et al . (      ) one two NLP tasks : LRA , uses byte tokenization way artiﬁcially increasing token count much smaller number words . Guan et al . (      ) introduced benchmark long doc- ument Chinese tasks based short stories . Four tasks set including cloze tests , sentence position predic- tion , plot completion , story generation . contrast , focus English text , conventional ’ real-world ’ tasks , crucially documents minimum length   ,    tokens consider true ’ long documents ’ . QuALITY benchmark ( Pang et al. ,      ) , de- signed multiple-choice question answering dataset selects questions ’ answered brieﬂy skimming text . maximum docu- ment length used :  ,    , much shorter min- imum length used work   ,    tokens . notable recent development SCROLLS : Standardized CompaRison Long Language Se- quences ( Shaham et al. ,      ) used   long doc- ument tasks create benchmark range in- put lengths median tasks ( excluding long NarrativeQA ) falling        ,    words - argue still short reliable evaluation longer transformer models LED Longformer variant . Additionally , un- like benchmark   different types task , SCROLLS focuses question answering , summariza- tion NLI . limits type length expected outputs short sentences paragraphs . contrast , MuLD explores outputs lengths single words way outputs equivalent length input . Long Document Models numerous attempts improve memory footprint computational cost transformers , thus allowing use longer inputs . One way tackling high com- plexity full attention make attention sparse . done chunking input sequence ﬁxed blocks ( Qiu et al. ,      ) , applying strided win- dows attention matrix , method learn- ing tokens attend ( Kitaev et al. ,      ) . models transformer-XL ( Dai et al. ,      ) Infty-former ( Martins et al. ,      ) solve prob- lem augmenting model memory mecha- nism ( Katharopoulos et al. ,      ) . form kernel trick used well reduce complexity . techniques effective optimising mem- ory computation usage transformer models , little analysis techniques effect ability models solve wide variety NLP tasks , seek solve . detailed overview efﬁcient transformers , see Tay et al . (      ) .   . MuLD Benchmark MuLD benchmark based six long document NLP tasks , span wide range domains .  .  . Desiderata pick tasks based following principles : (   ) Long input size : many benchmarks use ‘ long-text ’ mean inputs thousand tokens , consider ‘ true ’ long documents tens thou- sands tokens long . closely matches common usage term , essay may thousand words long considered fairly ‘ short ’ , common everyday examples long documents novels reports may   ,   -   ,    Dataset task metrics # documents avg . # tokens NarrativeQA Train Valid Test HotpotQA Train Valid AO  Style Change detection Train Valid Test Movie Character Types Train Test Long Scientiﬁc Papers Test Open Subtitles Train Test Question Answering bleu , rouge , meteor Question Answering bleu , rouge , meteor Style change detection F -score Classiﬁcation F -score Summarization bleu , rouge , meteor Translation bleu , rouge , meteor   ,     ,      ,      ,     ,     ,         ,                ,     ,    Table   : MuLD data statistics   ,      ,      ,      ,      ,      ,      ,      ,      ,      ,      ,      ,      ,    words length . reason in- clude documents   ,    tokens benchmark , many documents exceeding    ,    tokens . (   ) Variety dependency input : Within long document tasks , may require understand- ing input others - either analysing whole text using relevant sections . ex- ample summarization , model must holistic overview document , tasks question answering , answer often given referencing sections document . reality , tasks fall sommewhere two extremes endeavour capture variety input dependency benchmark . (   ) Variety output length : input length long , range different possible output lengths ranging single word classiﬁcation label , short answer , way output equivalent length input . (   ) Existing Task Formulation ’ seek in- vent new types task , instead use proven tasks already agreed challenging reg- ular ‘ short ’ transformer models . Instead , tasks used created either ﬁltering existing datasets , expand- ing length existing datasets additional text , replicating methodology existing datasets longer source text . (   ) Easily Evaluated pick tasks perfor- mance easily measured multiple automatic metrics . also acknowledge tasks , current metrics used short documents ’ fully capture challenges long document evaluation poses .  .  . Tasks six long document tasks described summarized Table   . NarrativeQA NarrativeQA Reading Compre- hension Challenge Dataset ( Koˇcisk´y et al. ,      ) con- sists user-submitted questions regarding plot movies novels . Annotators given access human-written plot summary encourage general questions require full understanding nar- rative . given question-answering system along full text narrative ( either movie script novel text ) , test reading comprehension . sentence annotator read summarise plot entire chapter/scene book movie , models evaluated full text must model dependencies multiple sections narrative . majority documents longer   ,   token minimum - simply ﬁlter documents shorter . HotpotQA HotpotQA dataset consists ques- tions crowd workers require information multiple Wikipedia articles order answer , thus testing ability models perform multi- hop question answering . data commonly pre- sented list paragraphs containing relevant infor- mation plus setting addition ’ distractor paragraphs ’ fully test ability model com- prehend information relevant question asked . transform long document , ex- pand paragraph full Wikipedia page well adding additional distractor articles sim- ilar topics ( randomly chosen links exist- ing pages ) order meet   ,    token minimum Figure   : Dataset lengths ( # tokens ) length requirement benchmark . articles shufﬂed concatenated form model input . Character Archetype Classiﬁcation introduce character archetype classiﬁcation dataset based methodology Skowron et al . (      ) . dataset , example consists movie script along named character task classify whether character Hero/Protagonist Villain/Antagonist based understanding role narrative . gather data ﬁrst pick scripts web following Koˇcisk´y et al . (      )   , matched summaries plot Wikipedia using combination matching names titles additional manual veriﬁcation . extract character name candidates using num- ber methods . Firstly , many scripts use common format character name given be- fore line dialogue ( e.g . ‘ HARRY : gone ? ’ ) . Secondly , Wikipedia pages many ﬁlms include list characters parsed matched script . ﬁlter character  Primarily Internet movie script database : www . imsdb.com name candidates include also appear plot summary , eliminating false matches well minor characters ’ impact plot . Annotators Amazon Turk given description task character types Skowron et al . (      ) . provided plot sum- mary asked select character type character name candidate extracted previously . Multi- ple annotators used example ensure accuracy labels . process eliminated character types Mentor , Sidekick , Spouse lot disagreement classes , perhaps due use movies genres rather limited set action movies used Skowron et al . (      ) . Open Subtitles Open Subtitles corpus ( Lison et al. ,      )   consists aligned subtitles movies TV shows website opensubtitles . org    languages used machine translation . Importantly rather individual lines , data consists subtitles entire individ- ual movie tv show , many long  opus.nlpl.eu/OpenSubtitles    .php NarrativeQAHotpotQAStyleChangeCharacterTypesVLSPOpensubtitles   ,     ,     ,     ,      ,      ,      ,      ,      ,      ,      ,      ,      ,      ,      ,   Dataset # Tokens ﬁles ﬁlter remove document less   ,    tokens . One common mistakes made models ’ consider document-level context mistranslation pronouns . example , En- glish phrase “ cold. ” , pronoun “ ” translated differently depending proceeding context “ ice formed night ” “ cam- era left outside ” . reason make use English-German pairs used ContraPro an- notation open subtitles ( M¨uller et al. ,      ) explicitly tests model ’ ability disambiguate possibilities . allow future use con- trastive evaluation , models asked pick two possible translations must show knowledge context . AO  style change detection Style change detection task identifying points author changes document constructed work multiple authors . PAN      Style Change detec- tion shared task ( Zangerle et al. ,      ) introduced task , forming documents StackExchange com- ments produce challenging dataset multiple increasingly difﬁcult subtasks . Task   binary clas- siﬁcation task identify whether document con- tains multiple authors single author . Task   identify points style changes , Task   assign paragraph uniquely author number authors document . Task   challenging ( answers two constructed output task   ) , report scores benchmark . extend approach long document dataset , instead use stories contributed fanﬁction web- site Archive , contains large num- ber different works submitted fans popular ﬁlms , tv , game , book characters . use stories written four popu- lar character relationships : Sherlock Holmes & John Watson , Castiel & Dean Winchester , Steve Rogers & Tony Stark , Draco Malfoy & Harry Potter . Since want task test style change detection topic change detection , constructed doc- ument contains paragraphs taken relationship ensure difference sec- tions author style topic . Additionally , reserve “ Draco Malfoy & Harry Potter ” documents test set . downloading , clean data removing images special formatting char- acters , split stories paragraphs , removing less     characters . construct style change detection documents , ﬁrst randomly choose ﬁrst randomly choosing mini- mum document length (   ,   -  ,    ) number authors (  -  )    % documents sin- gle author . assign authors document , randomly partition lengths section , ﬁ- nally draw , shufﬂe , concatenate paragraphs form complete text . Long Scientiﬁc Papers ( VLSP ) follow process Scientiﬁc papers ( Cohan et al. ,      ) summarization dataset , extracting papers open-access preprint server Arxiv.org using arxiv short abstract one included thesis ( available ) reference summaries . contrast Cohan et al . (      ) , rather remov- ing long documents , explicitly include - removing document less   ,    tokens . new ﬁltering , dataset mostly consists theses like scientiﬁc papers dataset , regular structure consisting multiple chapters . Due long time required compile large docu- ments text ﬁles , provide small test set     documents , train models original smaller-length scientiﬁc papers dataset . box-plot lengths six tasks shown Figure   . see NarrativeQA longest documents , tasks median length   ,       ,    tokens .   . Baselines section describe models evaluate MuLD benchmark . experiment ﬁrstly model based ’ standard ’ transformer architecture : T  , encoder-decoder network pretrained multiple text-to-text tasks take maximum input length     tokens ( Raffel et al. ,      ) . compare Longformer model : ’ efﬁcient transformer ’ uses sliding window attention ( global attention predetermined tokens ) sup- ports      tokens (   ,    encoder- decoder variant ) allowing us see beneﬁts us- ing longer contexts benchmark ( Beltagy et al. ,      ) . possible directly feed entirety many long documents models reasonable hardware ( even longformer ) , use chunking techniques divide , solve , recombine parts input . use following methods : • NarrativeQA/HotpotQA - follow ap- proach Koˇcisk´y et al . (      ) ﬁrst dividing document chunks     tokens . calculate cosine similarity TF- IDF representations chunk ques- tion text order score every chunk . Using metric , pick top    chunks concate- nate together pass model along question input . • Open Subtitles - simple baseline , split document regular chunks respecting line breaks ( never end chunk middle line ) . chunk passed model translated , translated chunks concatenated together form translated doc- ument . Task Bleu-  Bleu-  RougeL Meteor F  NarrativeQA HotpotQA Style Change Character Type VLSP OpenSubtitles NarrativeQA HotpotQA Style Change Character Type VLSP OpenSubtitles T   .     .     .     .     .     .    .    .     .     .    .    .     .     .    .     .   Longformer   .     .    .     .     .     .    .    .     .     .    .    .     .     .    .     .     .     .     .     .   Table   : T  Longformer results benchmark • Style Change Detection - use methodol- ogy Zhang et al . (      ) , training classi- ﬁer paragraph pairs target predict- ing whether two paragraphs author . subtask report ( task   ) , use methodology Strøm (      ) : assign ﬁrst paragraph ﬁrst author , check next paragraph similar previous one . assign author similar paragraph threshold . add new author . Although report score task   , classiﬁer could also used solve remaining two subtasks : task   , pass con- secutive paragraph pairs model out- put ” single-author ” paragraphs pairs author , otherwise classify document ” multi-author ” . task   simply append model outputs together form list style changes . • Character Archetype detection - select chunks containing ﬁrst mention , last mention , frequent mention character con- cern . concatenated , passed model , used predict character type class . • VLSP - split document chunks based article section headings summarize text ﬁrst introduction section thesis onwards ( ignoring contents , list tables , list ﬁgures sections ) . character type classiﬁcation tasks , simply report F -score .   . Results Discussion results benchmark tasks pre- sented Table   T  Longformer mod- els . Longformer model consistently outperforms T  model across many tasks , suggesting models able make use longer context perform well benchmark . models ﬁnd NarrativeQA dataset chal- lenging HotpotQA , hypothesise due longer average length , higher complex- ity narrative understanding involved NarrativeQA contrast factual Wikipedia data HotpotQA typically involves limited number hops . Additionally , HotpotQA question commonly in- volves understanding small number sentences ( even though may widely distributed through- document ) . T  model also outperforms Longformer OpenSubtitles translation task . suggest due challenge output much longer sequence (     vs      ) tokens greater bene- ﬁt gained lines correct translation depends context     tokens away ( M¨uller et al . (      ) ﬁnd around half cases , an- tecedent previous sentence ,   sentences away ) . Following ( Koˇcisk´y et al. ,      ) , evaluate tasks require text-generation ( QA , Summarization , Translation ) using Bleu-  , Bleu-  ( Papineni et al. ,      ) , Meteor ( Denkowski Lavie ,      ) , Rouge-L ( Lin ,      ) , using multiple references available . style change detection   . Conclusion enable evaluation long document models , introduce MuLD : benchmark varied NLP tasks document consists   ,    to- kens . six tasks benchmark created ﬁltering , extending , modifying existing NLP tasks designed require long context high per- formance . evaluate simple chunking-based baselines , ﬁnd Longformer model able outperform T  model suggesting benchmark good test ability models make use longer contexts . believe technique explored work augmenting extending existing ‘ short document ’ datasets , applied many NLP tasks . performance efﬁcient transformers improves , anticipate need update benchmark challenging tasks . focused creating benchmark tests model ’ ability solve real-world long document tasks , also expect im- provements efﬁciencies models may make datasets    ,    to- kens necessary may require fundamentally dif- ferent approach creating long document datasets . leave future work development im- proved chunking methods , efﬁcient trans- formers make methods unnecessary . hope MuLD benchmark encourage research efﬁcient models long doc- ument NLP . end provide data , base- line models , code www.github.com/ ghomasHudson/muld .   . Bibliographical References Beltagy , I. , Peters , M. E. , Cohan , . (      ) . Longformer : long-document transformer . arXiv preprint arXiv:    .      . Cohan , A. , Dernoncourt , F. , Kim , D. S. , Bui , T. , Kim , S. , Chang , W. , Goharian , N. (      ) . discourse-aware attention model abstractive sum- Proceedings marization long documents .      Conference North American Chap- ter Association Computational Linguistics : Human Language Technologies , Volume   ( Short Papers ) , pages    –    , New Orleans , Louisiana , June . Association Computational Linguistics . Dai , Z. , Yang , Z. , Yang , Y. , Carbonell , J. , Le , Q. V. , Salakhutdinov , R. (      ) . Transformer-xl : At- tentive language models beyond ﬁxed-length con- text . arXiv preprint arXiv:    .      . Denkowski , M. Lavie , . (      ) . Meteor  .  : Au- tomatic metric reliable optimization evalua- tion machine translation systems . Proceedings Sixth Workshop Statistical Machine Trans- lation , pages   –   , Edinburgh , Scotland , July . As- sociation Computational Linguistics . Guan , J. , Feng , Z. , Chen , Y. , , R. , Mao , X. , Fan , C. , Huang , M. (      ) . Lot : benchmark evaluating chinese long text understanding gen- eration . arXiv preprint arXiv:    .      . Katharopoulos , A. , Vyas , A. , Pappas , N. , Fleuret , F. (      ) . Transformers rnns : Fast autoregres- Inter- sive transformers linear attention . national Conference Machine Learning , pages     –     . PMLR . Kitaev , N. , Kaiser , L. , Levskaya , . (      ) . Re- former : efﬁcient transformer . International Conference Learning Representations . Koˇcisk´y , T. , Schwarz , J. , Blunsom , P. , Dyer , C. , Her- mann , K. M. , Melis , G. , Grefenstette , E. (      ) . narrativeqa reading comprehension challenge . Transactions Association Computational Linguistics ,  :   –    , Dec . Lin , C.-Y . (      ) . ROUGE : package automatic Text Summarization evaluation summaries . Branches , pages   –   , Barcelona , Spain , July . Association Computational Linguistics . Lison , P. , Tiedemann , J. , Kouylekov , M. (      ) . Opensubtitles     : Statistical rescoring sentence alignments large , noisy parallel corpora .  LREC . Liu , D. , Yan , Y. , Gong , Y. , Qi , W. , Zhang , H. , Jiao , J. , Chen , W. , Fu , J. , Shou , L. , Gong , M. , Wang , P. , Chen , J. , Jiang , D. , Lv , J. , Zhang , R. , Wu , W. , Zhou , M. , Duan , N. (      ) . Glge : new general lan- guage generation evaluation benchmark . FIND- INGS . Martins , P. H. , Marinho , Z. , Martins , A. F. (      ) . Inﬁnite memory transformer . arXiv infty-former : preprint arXiv:    .      . M¨uller , M. , Rios , A. , Voita , E. , Sennrich , R . (      ) . large-scale test set evaluation context-aware pronoun translation neural machine translation . arXiv preprint arXiv:    .      . Pang , R. Y. , Parrish , A. , Joshi , N. , Nangia , N. , Phang , J. , Chen , A. , Padmakumar , V. , , J. , Thompson , J. , , H. , Bowman , S. R . (      ) . QuAL- ITY : Question answering long input texts , yes ! arXiv preprint arXiv:    .      . Papineni , K. , Roukos , S. , Ward , T. , Zhu , W.-J . (      ) . Bleu : method automatic evaluation machine translation . Proceedings   th An- nual Meeting Association Computational Linguistics , pages    –    , Philadelphia , Pennsyl- vania , USA , July . Association Computational Linguistics . Qiu , J. , , H. , Levy , O. , Yih , W.-t. , Wang , S. , Tang , J . (      ) . Blockwise self-attention long document understanding . Findings Associ- ation Computational Linguistics : EMNLP      , pages     –     , Online , November . Association Computational Linguistics . Raffel , C. , Shazeer , N. , Roberts , A. , Lee , K. , Narang , S. , Matena , M. , Zhou , Y. , Li , W. , Liu , P. J . (      ) . Exploring limits transfer learning uniﬁed text-to-text transformer . Journal Ma- chine Learning Research ,    (     ) : –   . Shaham , U. , Segal , E. , Ivgi , M. , Efrat , A. , Yoran , O. , Haviv , A. , Gupta , A. , Xiong , W. , Geva , M. , Berant , J. , Levy , . (      ) . Scrolls : Standardized com- parison long language sequences . Skowron , M. , Trapp , M. , Payr , S. , Trappl , R . (      ) . Automatic identiﬁcation character types ﬁlm dialogs . Applied Artiﬁcial Intelligence ,    (    ) :   –    . Strøm , E. (      ) . Multi-label style change detection solving binary classiﬁcation problem . CLEF . Tay , Y. , Dehghani , M. , Bahri , D. , Metzler , . (      ) . Efﬁcient transformers : survey . ArXiv , abs/    .      . Tay , Y. , Dehghani , M. , Abnar , S. , Shen , Y. , Bahri , D. , Pham , P. , Rao , J. , Yang , L. , Ruder , S. , Metzler , D. (      ) . Long range arena : benchmark ef- ﬁcient transformers . International Conference Learning Representations . Wang , A. , Singh , A. , Michael , J. , Hill , F. , Levy , O. , Bowman , S. (      ) . Glue : multi-task bench- mark analysis platform natural language un- Proceedings      EMNLP derstanding . Workshop BlackboxNLP : Analyzing Interpreting Neural Networks NLP . Association Computa- tional Linguistics . Wang , A. , Pruksachatkun , Y. , Nangia , N. , Singh , A. , Michael , J. , Hill , F. , Levy , O. , Bowman , S. R. (      ) . SuperGLUE : stickier benchmark general-purpose language understanding systems . arXiv preprint     .      . Wang , S. , Li , B . Z. , Khabsa , M. , Fang , H. , , H . (      ) . Linformer : Self-attention linear com- plexity . arXiv preprint arXiv:    .      . Yao , Y. , Dong , Q. , Guan , J. , Cao , B. , Zhang , Z. , Xiao , C. , Wang , X. , Qi , F. , Bao , J. , Nie , J. , Zeng , Z. , Gu , Y. , Zhou , K. , Huang , X. , Li , W. , Ren , S. , Lu , J. , Xu , C. , Wang , H. , Zeng , G. , Zhou , Z. , Zhang , J. , Li , J. , Huang , M. , Yan , R. , , X. , Wan , X. , Zhao , X. , Sun , X. , Liu , Y. , Liu , Z. , Han , X. , Yang , E. , Sui , Z. , Sun , M. (      ) . Cuge : chinese language understanding generation evaluation benchmark . ArXiv , abs/    .      . Zangerle , E. , Mayerl , M. , Potthast , M. , Stein , B . (      ) . Overview style change detection task pan . Zhang , Z. , Han , Z. , Kong , L. , Miao , X. , Peng , Z. , Zeng , J. , Cao , H. , Zhang , J. , Xiao , Z. , Peng , X . (      ) . Style change detection based writing style simi- larity . PAN CLEF      , pages     –     . present examples MuLD benchmark give reader sense tasks included ( added ellipsis brevity ) : Appendix NarrativeQA Input : Oscar related Dana ? [ ... ] EXT . MANHATTAN ISLAND - DAY high AERIAL SHOT island features Statue Liberty prominently foreground TRAVELS ACROSS harbor , Battery Lower Manhattan Greenwich Village [ ... ] DANA ( exasperated ) Frank , think could give hand bags ? FRANK ’ doorman , Miss Barrett . ’ building superintendent [ ... ] Output : son HotpotQA Input : Scott Derrickson Ed Wood nationality ? Doctor Strange      American superhero film based Marvel Comics character name . Produced Marvel Studios distributed Walt Disney Studios Motion Pictures ,   th film Marvel Cinematic Universe ( MCU ) [ ... ] Scott Derrickson ( born July    ,      ) American filmmaker . best known directing films Exorcism Emily Rose , Sinister , Deliver Us Evil , Doctor Strange [ ... ] Edward Davis Wood Jr. ( October    ,      - December    ,      ) American filmmaker , actor , author [ ... ] Output : Yes Style Change Input : John ’ stomach new home . settled throat , painfully dry want man . `` story would like hear , dear ? '' John ’ fingers looped around curls delicately , lesser effort ’ moved hair . thought beautiful mind beneath curls . storing every memory like computer weakened age- always beautiful . John gulped lump panic crawled throat . know ? `` Right , still reckon ’ keep pretending ’ . '' `` John , need pretend couple , things like spontaneous physical contact expected maintain- '' `` ’ mention , '' Sherlock responded soft passion ’ keep voice . words came murmur . - '' continued , delighted John joined harmonize flawlessly latter half phrase [ ... ] Output :  , , , , , ,  , [ ... ] , , , ,  , [ ... ] ,  Character Types Input : Indiana Jones [ ... ] INDY . Barranca looks evilly Indy ’ hand upon . Indy releases smiles friendly way . INDY ’ need . Satipo watches confrontation concern . BARRANCA carry supplies . INDY ’ leave . ’ got , ’ able reach plane dusk [ ... ] Output : Hero VLSP Input : # # Chapter   Basic Definitions Concepts # # #  .  Basics simplicial complexes Let @ xmath , @ xmath denote subsets @ xmath size @ xmath . collection @ xmath subsets @ xmath called ( finite abstract ) simplicial complex closed inclusion , i.e . @ xmath implies @ xmath . Note @ xmath empty ( assume ) @ xmath . @ xmath -th skeleton @ xmath @ xmath . elements @ xmath called faces ; @ xmath dimension . @ xmath -dimensional faces called vertices , @ xmath -dimensional faces called edges maximal faces respect inclusion called facets . facets dimension , @ xmath pure . @ xmath - vector ( face vector ) @ xmath @ xmath @ xmath . dimension K @ xmath ; e.g .  -dimensional simplicial complex simple graph . @ xmath - polynomial @ xmath @ xmath [ ... ] Output : thesis focuses algebraic shifting applications f-vector theory simplicial complexes general graded posets . particular , several approaches partial results concerning g-conjecture simplicial spheres presented . OpenSubtitles Input :      big year . Russians put Sputnik outer space . Dodgers played last game Ebbets Field said goodbye Brooklyn . guy , shot Frank Costello head missed . Gallo brothers whacked Albert Anastasia barbershop . total chaos [ ... ] Output :      war ein bedeutendes Jahr . Die Russen schossen ihren Sputnik ins . Die Dodgers spielten zum letzten Mal Ebbets Field und sagten Brooklyn Adieu . Dieser Kerl schoss auf Frank Costello und verfehlte ihn . Die Gallo-Bruder legten Albert Anastasia beim Friseur um . Es war totales Chaos [ ... ] 