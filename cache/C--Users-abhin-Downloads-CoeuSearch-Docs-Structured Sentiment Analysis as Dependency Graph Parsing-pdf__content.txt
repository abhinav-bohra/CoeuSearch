Structured Sentiment Analysis as Dependency Graph Parsing Jeremy Barnes*, Robin Kurtz†, Stephan Oepen*, Lilja Øvrelid*and Erik Velldal* *University of Oslo, Department of Informatics †National Library of Sweden, KBLab { jeremycb | oe | liljao | erikve } @ifi.uio.no robin.kurtz@kb.se Abstract Structured sentiment analysis attempts to ex- tract full opinion tuples from a text, but over time this task has been subdivided into smaller and smaller sub-tasks, e.g., target extraction or targeted polarity classiﬁcation. We argue that this division has become counterproduc- tive and propose a new uniﬁed framework to remedy the situation. We cast the structured sentiment problem as dependency graph pars- ing, where the nodes are spans of sentiment holders, targets and expressions, and the arcs are the relations between them. We perform experiments on ﬁve datasets in four languages (English, Norwegian, Basque, and Catalan) and show that this approach leads to strong improvements over state-of-the-art baselines. Our analysis shows that reﬁning the sentiment graphs with syntactic dependency information further improves results. 1 Introduction Structured1 sentiment analysis, i.e., the task of predicting a structured sentiment graph like the ones in Figure 1, can be theoretically cast as an information extraction problem in which one at- tempts to ﬁnd all of the opinion tuples O = Oi, . . . , On in a text. Each opinion Oi is a tuple (h, t, e, p) where h is a holder who expresses a po- larity p towards a target t through a sentiment expression e, implicitly deﬁning pairwise relation- ships between elements of the same tuple. Liu (2012) argues that all of these elements2 are essen- tial to fully resolve the sentiment analysis problem. 1We use the term ‘structured sentiment’ distinctly from Al- mars et al. (2017), who use it to refer to the latent hierarchical structure of sentiment aspects. We instead use ‘structured’ to refer to predicting sentiment graphs as a structured prediction task, as opposed to the many text classiﬁcation task that are found in sentiment analysis. 2Liu (2012)’s deﬁnition replaces sentiment expression with the time when the opinion was expressed. However, most research on sentiment analysis fo- cuses either on a variety of sub-tasks, which avoids performing the full task, or on simpliﬁed and ideal- ized tasks, e.g., sentence-level binary polarity clas- siﬁcation. We argue that the division of structured senti- ment into these sub-tasks has become counterpro- ductive, as reported experiments are often not sen- sitive to whether a given addition to the pipeline improves the overall resolution of sentiment, or do not take into account the inter-dependencies of the various sub-tasks. As such, we propose a uni- ﬁed approach to structured sentiment which jointly predicts all elements of an opinion tuple and their relations. Moreover, we cast sentiment analysis as a dependency graph parsing problem, where the sentiment expression is the root node, and the other elements have arcs which model the relationships between them. This methodology also enables us to take advantage of recent improvements in seman- tic dependency parsing (Dozat and Manning, 2018; Oepen et al., 2020; Kurtz et al., 2020) to efﬁciently learn a sentiment graph parser. This perspective also allows us to unify a num- ber of approaches, including targeted, and opinion tuple mining. We aim to answer RQ1: whether graph-based approaches to structured sentiment outperform state-of-the-art sequence labeling ap- proaches, and RQ2: how to best encode structured sentiment as parsing graphs. We perform experi- ments on ﬁve standard datasets in four languages (English, Norwegian, Basque, Catalan) and show that graph-based approaches outperform state-of- the-art baselines on all datasets on several standard metrics, as well as our proposed novel (unlabeled and labeled) sentiment graph metrics. We further propose methods to inject linguistic structure into the sentiment graphs using syntactic dependencies. Our main contributions are therefore 1) proposing a holistic approach to structured sentiment through Figure 1: A structured sentiment graph is composed of a holder, target, sentiment expression, their relationships and a polarity attribute. Holders and targets can be null. sentiment graph parsing, 2) introducing new eval- uation metrics for measuring model performance, and 3) extensive experimental results that outper- form state-of-the-art baselines. Finally, we release the code and datasets3 to enable future work on this problem. 2 Related Work Structured sentiment analysis can be broken down into ﬁve sub-tasks: i) sentiment expression extraction, ii) sentiment target extraction, iii) senti- ment holder extraction, iv) deﬁning the relationship between these elements, and v) assigning polarity. Previous work on information extraction has used pipeline methods which ﬁrst extract the holders, targets, and expressions (tasks i - iii) and subse- quently predict their relations (task iv), mostly on the MPQA dataset (Wiebe et al., 2005). CRFs and a number of external resources (sentiment lexicons, dependency parsers, named-entity taggers) (Choi et al., 2006; Yang and Cardie, 2012) are strong base- lines. Given the small size of the training data and the complicated task, these techniques often still outperform neural models, such as BiLSTMs (Kati- yar and Cardie, 2016). Transition-based end-to- end approaches have shown some potential (Zhang et al., 2019). However, all of this work ignores the polarity classiﬁcation subtask. Targeted sentiment analysis only concentrates on extracting sentiment targets (task ii) and classi- fying the polarity directed towards them (task iv) (Jiang et al., 2011; Mitchell et al., 2013). Recent shared tasks on Aspect-Based Sentiment Analysis (ABSA) (Pontiki et al., 2014, 2015, 2016) also in- clude target extraction and polarity classiﬁcation subtasks. Joint approaches perform on par with pipeline methods (Li et al., 2019a) and multitask models can perform even better (He et al., 2019). Finally, pretrained language models (Devlin et al., 2019) can also lead to improvements on the ABSA data (Li et al., 2019b). End2End sentiment analysis is a recently pro- posed subtask which combines targeted sentiment (tasks ii and v) and sentiment expression extraction (task i), without requiring the resolution of relation- ships between targets and expressions. Wang et al. (2016) augment the ABSA datasets with sentiment expressions, but provide no details on the annota- tion process or any inter-annotator agreement. He et al. (2019) make use of this data and propose a multi-layer CNN (IMN) to create hidden represen- tations h which are then fed to a target and opinion extraction module (AE), which is also a multi-layer CNN. This module predicts ˆyae, a sequence of BIO tags4 that predict the presence or absence of targets and expressions. After jointly predicting the targets and expressions, a second multi-layer CNN with a ﬁnal self-attention network is used to classify the polarity, again as sequence labeling task (AS). This second module combines the information from h and ˆyae by incorporating the predicted probabil- ity of a token to be a target in the formulation of self-attention. Finally, an iterative message-passing algorithm updates h using the predictions from all the modules at the previous timestep. Chen and Qian (2020) instead propose Relation- Aware Collaborative Learning (RACL). This model creates task speciﬁc representations by ﬁrst embedding a sentence, passing through a shared feed-forward network and ﬁnally a task-speciﬁc CNN. This approach then models interactions be- tween each pair of sub-tasks (target extraction, ex- pression extraction, sentiment classiﬁcation) by cre- ating pairwise weighted attention representations. These are then concatenated and used to create the task-speciﬁc predictions. The authors ﬁnally stack several RACL layers, using the output from the previous layer as input for the next. 3Code and datasets available at https://github. com/jerbarnes/sentiment_graphs. 4The tags include {BIO}-{target,expression} Some others give the new UMUC 5 stars - don't believe them . positivenegativeholdertargetexpressiontargetexpressionBoth models perform well on the augmented Se- mEval data, but it is unlikely that these annotations are adequate for full structured sentiment, as Wang et al. (2016) only provide expression annotations for sentences that have targets, generally only in- clude sentiment-bearing words (not phrases), and do not specify the relationship between target and expression. Finally, the recently proposed aspect sentiment triplet extraction (Peng et al., 2019; ?) attempts to extract targets, expressions and their polarity. How- ever, the datasets used are unlikely to be adequate, as they augment available targeted datasets, but do not report annotation guidelines, procedure, or inter-annotator agreement. Graph parsing: Syntactic dependency graphs are regularly used in applications, supplying them with necessary grammatical information (Mintz et al., 2009; Cui et al., 2005; Bj¨orne et al., 2009; Johansson and Moschitti, 2012; Lapponi et al., 2012). The dependency graph structures used in these systems are predominantly restricted to trees. While trees are sufﬁcient to encode syntactic depen- dencies, they are not expressive enough to handle meaning representations, that require nodes to have multiple incoming arcs, or having no incoming arcs at all (Kuhlmann and Oepen, 2016). While much of the early research on parsing these new structures (Oepen et al., 2014, 2015) focused on specialized decoding algorithms, Dozat and Manning (2018) presented a neural dependency parser that essen- tially relies only on its neural network structure to predict any type of dependency graph without restrictions to certain structures. Using the parser’s ability to learn arbitrary dependency graphs, Kurtz et al. (2020) phrased the task of negation resolution (Morante and Blanco, 2012; Morante and Daele- mans, 2012) as a graph parsing task. This trans- formed the otherwise ﬂat representations to depen- dency structures that directly encode the often over- lapping relations between the building blocks of multiple negation instances at the same time. In a simpler fashion, Yu et al. (2020) exploit the parser of Dozat and Manning (2018) to predict spans of named entities. are shown in Table 1. The largest available struc- tured sentiment dataset is the NoReCFine dataset (Øvrelid et al., 2020), a multi-domain dataset of professional reviews in Norwegian, annotated for structured sentiment. MultiBEU and MultiBCA (Barnes et al., 2018) are hotel reviews in Basque and Catalan, respectively. MPQA (Wiebe et al., 2005) annotates news wire text in English. Finally, DSUnis (Toprak et al., 2010) annotate English re- views of online universities and e-commerce. In our experiments, we use only the university re- views, as the e-commerce reviews have a large number of ‘polar targets’, i.e., targets with a polar- ity, but no accompanying sentiment expression. While all the datasets annotate holders, targets, and expressions, the frequency and distribution of these vary. Regarding holders, MPQA has the most (2,054) and DSUnis has the fewest (94), whereas NoReCFine has the largest proportion of targets (8,923) and expressions (11,115). The aver- age length of holders (2.6 tokens) and targets (6.1 tokens) in MPQA is also considerably higher than the others. It is also worth pointing out that MPQA and DSUnis additionally include neutral polarity. In the case of MPQA the neutral class refers to verbs which are subjective but do not convey polarity, e.g., ‘say’, ‘opt for’. In DSUnis, however, the neutral la- bel tends to indicate expressions that could entail mixed polarity or are polar under the right condi- tions, e.g., ‘the classes were not easy’ is considered neutral, as it is possible for difﬁcult classes to be desirable at a university. MultiBEU, and MultiBCA also have labels for strong positive and strong neg- ative, which we map to positive and negative, re- spectively. Finally, NoReCFine includes intensity annotations (strong, normal, slight), which we dis- regard for the purposes of these experiments. 4 Modeling This section describes how we deﬁne and encode sentiment graphs, detail the neural dependency graph models, as well as two state-of-the-art base- lines for end-to-end sentiment analysis (target and expression extraction, plus polarity classiﬁcation). 3 Datasets 4.1 Graph Representations We here focus on datasets that annotate the full task of structured sentiment as described initially. We perform experiments on ﬁve structured sentiment datasets in four languages, the statistics of which Structured sentiment graphs as in Figure 1 are di- rected graphs, that are made up of a set of labeled nodes and a set of unlabeled edges connecting pairs of nodes. Nodes in the structured sentiment graphs sentences holders targets expressions polarity # avg. # avg. max # avg. max # avg. max + neu − NoReCFine MultiBCA MultiBEU MPQA DSUnis train dev test train dev test train dev test train dev test train dev test 8634 1531 1272 1174 168 336 1064 152 305 4500 1622 1681 2253 232 318 16.7 16.9 17.2 15.6 13.3 14.7 10.5 10.7 10.7 25 23 24 20 9 20 898 120 110 169 15 52 205 33 58 1306 377 371 65 17 12 1.1 1.0 1.0 1.1 1.5 1.1 1.1 1.1 1.1 2.6 2.6 2.8 1.2 1.1 1.3 12 3 3 4 7 5 6 2 2 27 16 32 2 3 4 6778 1152 993 1695 211 430 1285 153 337 1382 449 405 1252 151 198 1.9 2.0 2.0 2.4 2.3 2.6 1.4 1.3 1.4 6.1 5.3 6.4 1.2 1.2 1.2 35 15 20 18 10 12 9 6 8 56 41 42 5 3 6 8448 1432 1235 1981 258 518 1684 204 440 1656 552 479 837 106 139 4.9 5.1 4.9 2.6 2.6 2.7 2.2 2.5 2.2 2.4 2.1 2.0 1.9 1.7 2.0 40 31 30 19 9 14 10 8 9 14 8 8 9 6 5 5684 988 875 1272 151 313 1406 168 375 675 241 166 495 40 77 0 0 0 0 0 0 0 0 0 271 105 89 149 19 18 2756 443 358 708 107 204 278 36 65 658 202 199 610 92 103 Table 1: Statistics of the datasets, including number of sentences and average length (in tokens) per split, as well as average and max lengths (in tokens) for holder, target, and expression annotations. Additionally, we include the distribution of polarity – restricted to positive, neutral, and negative – in each dataset. can span over multiple tokens and may have mul- tiple incoming edges. The resulting graphs can have multiple entry points (roots), are not neces- sarily connected, and not every token is a node in the graph. The sentence’s sentiment expressions correspond to the roots of the graphs, connecting explicitly to their respective holders and targets. In order to apply the algorithm of Dozat and Manning (2018), we simplify these structures into bi-lexical dependency graphs visualized in Figure 2. Here, nodes correspond one-to-one to the tokens of the se- quence and follow the same linear order. The edges are drawn as arcs in the half-plane above the sen- tence, connecting heads to dependents. Similarly to the source structures, the graphs can have multiple roots and nodes can have multiple or no incoming arcs. For some rare instances of structured senti- ment graphs, the reduction to dependency graphs is lossy, as they do not allow multiple arcs to share the same head and dependent. This results in a slight mismatch of the learned and aimed-for rep- resentations. The choice of how to encode the sentiment graphs as parsing graphs opens for several alter- nate representations depending on the choice of head/dependent status of individual tokens in the target/holder/expression spans of the sentiment graph. We here propose two simple parsing graph representations: head-ﬁrst and head-ﬁnal, which Metric Name Level Strictness +/− Holder F1 Target F1 Exp. F1 Targeted F1 UF1 LF1 NSF1 SF1 Token-level Token-level Token-level Token-level Graph arcs Graph arcs Sentiment- graph Sentiment- graph Partial Partial Partial Exact Exact Exact Exact graph, partial token Exact graph, partial token No No No Yes No Yes No Yes Table 2: Metrics used to evaluate performance. Col- umn +/− indicates whether polarity is included or not. The main metrics are Targeted F1, which allows us to compare to methods that do not perform the full task, and SF1, which best represents the full task. are shown in Figure 2. For head-ﬁrst, we set the ﬁrst token of the sentiment expression as a root node, and similarly set the ﬁrst token in each holder and token span as the head of the span with all other tokens within that span as dependents. The labels simply denote the type of relation (target/holder) and for sentiment expressions, additionally encode the polarity. Head-ﬁnal is similar, but instead sets the ﬁnal token of spans as the heads, and the ﬁnal token of the sentiment expression as the root node. holder exp:pos exp:neg holder target target target exp:pos target exp:neg Some others give the new UMUC 5 stars - don’t believe them. (a) holder exp:pos exp:neg holder target target target exp:pos exp:neg target Some others give the new UMUC 5 stars - don’t believe them. (b) Figure 2: Two parsing graph proposals to encode the sentiment graph: (a) head-ﬁrst, where the ﬁrst token of any span is the head, and (b) head-ﬁnal, where the ﬁnal token is the head. 4.2 Proposed model The neural graph parsing model used in this work is a reimplementation of the neural parser by Dozat and Manning (2018) which was used by Kurtz et al. (2020) for negation resolution. The parser learns to score each possible arc to then ﬁnally predict the output structure simply as a collection of all positively scored arcs. The base of the net- work structure is a bidirectional LSTM (BiLSTM), that processes the input sentence both from left-to- right and right-to-left, to create contextualized rep- resentations c1, . . . , cn = BiLSTM(w1, . . . , wn) where wi is the concatenation of a word embed- ding, POS tag embedding, lemma embedding, and character embedding created by a character-based LSTM for the ith token. In our experiments, we further augment the token representations with pre- trained contextualized embeddings from multilin- gual BERT (Xu et al., 2019). We use multilingual BERT as several languages did not have available monolingual BERT models at the time of the ex- periments (Catalan, Norwegian). The contextualized embeddings are then pro- cessed by two feedforward neural networks (FNN), creating specialized representations for potential heads and dependents, hi = FNNhead(ci) and di = FNNdep(ci). The scores for each possible arc- label combination are computed by a ﬁnal bilinear transformation using the tensor U . Its inner dimen- sion corresponds to the number of sentiment graph labels plus a special NONE label, indicating the ab- sence of an arc, which allows the model to predict arcs and labels jointly, score(hi, dj) = h(cid:62) i U dj. 4.3 Baselines We compare our proposed graph prediction ap- proach with three state-of-the-art baselines5 for extracting targets and expressions and predicting the polarity: IMN6, RACL7, as well as RACL- BERT, which also incorporates contextualized em- beddings. Instead of using BERTLarge, we use the cased BERT-multilingual-base in order to fairly compare with our own models. Note, however, that our model does not update the mBERT representa- tions, putting it at a disadvantage to RACL-BERT. We also compare with previously reported extrac- tion results from Barnes et al. (2018) and Øvrelid et al. (2020). 5 Evaluation As we are interested not only in extraction or clas- siﬁcation, but rather in the full structured sentiment task, we propose metrics that capture the relations between all predicted elements, while enabling comparison with previous state-of-the-art models on different subtasks. The main metrics we use to rank models are Targeted F1 and Sentiment Graph F1. 5Despite having state-of-the-art results on MPQA, we do not compare with Katiyar and Cardie (2016) as they use dif- ferent dataset splits, 10-fold cross-validation, and their code is not available. 6IMN code available at https://github.com/ ruidan/IMN-E2E-ABSA. 7https://github.com/NLPWM-WHU/RACL. Dataset Model Spans Targeted Parsing Graph Sent. Graph Holder F1 Target F1 Exp. F1 F1 UF1 LF1 NSF1 SF1 NoReCFine MultiBEU MultiBCA MPQA DSUnis Øvrelid et al. (2020) IMN RACL RACL-BERT Head-ﬁrst Head-ﬁnal Barnes et al. (2018)† IMN RACL RACL-BERT Head-ﬁrst Head-ﬁnal Barnes et al. (2018)† IMN RACL RACL-BERT Head-ﬁrst Head-ﬁnal IMN RACL RACL-BERT Head-ﬁrst Head-ﬁnal IMN RACL RACL-BERT Head-ﬁrst Head-ﬁnal 42.4 - - - 51.1 60.4∗ 54.0 - - - 60.4 60.5 56.0 - - - 43.0 37.1 - - - 43.8 46.3 - - - 28.0 37.4 31.3 35.9 45.6 47.2 50.1 54.8 57.0 48.2 55.4 59.9 64.0 64.0 64.0 56.3 65.4 67.5 72.5 71.2 24.3 32.6 20.0 51.0 49.5 33.0 39.3 44.6 39.9 42.1 31.3 48.7 55.4 56.3 54.4 55.5 54.0 65.2 70.7 72.6 73.9 72.1 52.0 60.9 67.6 70.3 71.1∗ 67.1 29.6 37.8 31.2 48.1 46.0 27.4 40.2 38.2 40.3 45.5∗ - 18.0 20.1 30.3 30.5 31.9 - 39.5 48.2 56.8 57.8 56.9 - 32.5 49.1 52.4 55.0∗ 53.9 1.2 11.8 17.8 33.5∗ 18.6 17.9 22.8 27.3 26.7 29.6 - - - - - - - - - - - - - - - - 39.2 48.0∗ 31.5 37.7∗ 37.0 39.2∗ 29.5 31.2∗ - - - - - - - - - - - - - - - - 64.6 60.8 60.0 56.0 58.0 58.0 54.7 54.7 - - - - - - - - - - - - - - - - 66.8∗ 62.7 62.1∗ 58.1 62.0 59.7 56.8 53.7 - - - - - - - - - - - - 40.0 41.4 36.9 38.0 24.5 26.1 17.4 18.8 - - - - - - - - - - - - 35.3 38.1 31.4 33.9 31.0 34.3∗ 25.0 26.5 Table 3: Experiments comparing our sentiment graph approaches (Head-ﬁrst/Head-ﬁnal) using mBERT with the sequence-labeling baselines (IMN, RACL, RACL-BERT). Underlined numbers indicate the best result for the metric and dataset. ∗ indicates approach is signiﬁcantly better than second best (p < 0.05), as determined by a bootstrap with replacement test. † indicates results that are not comparable, as they were calculated with 10-fold cross-validation. Token-level F1 for Holders, Targets, and Ex- pressions To easily compare our models to pipeline models, we evaluate how well these mod- els are able to identify the elements of a sentiment graph with token-level F1. Targeted F1 This is a common metric in targeted sentiment analysis (also referred to as F1-i (He et al., 2019) or ABSA F1 (Chen and Qian, 2020)). A true positive requires the combination of exact extraction of the sentiment target, and the correct polarity. Parsing graph metrics We additionally com- pute graph-level metrics to determine how well the models predict the unlabeled and labeled arcs of the parsing graphs: Unlabeled F1 (UF1), Labeled F1 (LF1). These measure the amount of (in)correctly predicted arcs and labels, as the harmonic mean of precision and recall (Oepen et al., 2014). These metrics inform us of the local properties of the graph, and do not overly penalize a model if a few edges of a graph are incorrect. Sentiment graph metrics The two metrics that measure how well a model is able to capture the full sentiment graph (see Figure 1) are Non-polar Sentiment Graph F1 (NSF1) and Sentiment Graph F1 (SF1). For NSF1, each sentiment graph is a tu- ple of (holder, target, expression), while for SF1 we include polarity (holder, target, expression, polar- ity). A true positive is deﬁned as an exact match at graph-level, weighting the overlap in predicted and gold spans for each element, averaged across all three spans. For precision we weight the number of correctly predicted tokens divided by the total number of predicted tokens (for recall, we divide instead by the number of gold tokens). We allow for empty holders and targets. 6 Experiments All sentiment graph models use token-level mBERT representations in addition to word2vec skip-gram embeddings openly available from the NLPL vector repository8 (Fares et al., 2017). We train all models for 100 epochs and keep the model that performs best regarding LF1 on the dev set (Targeted F1 for the baselines). We use default hyperparameters from Kurtz et al. (2020) (see Ap- pendix) and run all of our models ﬁve times with different random seeds and report the mean (stan- dard deviation shown as well in Table 8 in the Appendix). We calculate statistical difference be- tween the best and second best models through a bootstrap with replacement test (Berg-Kirkpatrick et al., 2012). As there are 5 runs, we require that 3 of 5 be statistically signiﬁcant at p < 0.05. Table 3 shows the results for all datasets. On NoReCFine, the baselines IMN, RACL, and RACL-BERT perform well at extracting targets (35.9, 45.6, and 47.2 F1, respectively) and expres- sions (48.7/55.4/56.3), but struggle with the full tar- geted sentiment task (18.0/20.1/30.3). The graph- based models extract targets better (50.1/54.8) and have comparable scores for expressions (54.4/55.5). The holder extraction scores have a similar range (51.1/60.4). These patterns hold throughout the other datasets, where the proposed graph models nearly always perform best on extracting spans, although RACL-BERT achieves the best score on extracting targets on DSUnis (44.6 vs. 42.1). The graph models also outperform the strongest baseline (RACL-BERT) on targeted sentiment on all 5 datasets, although this difference is often not statistically signiﬁcant (NoReCFine Head-ﬁrst, MultiBEU Head-ﬁnal) and RACL-BERT is better than Head-ﬁrst on DSUnis. Regarding the Graph metrics, the results depend highly on the dataset, with UF1 and LF1 rang- ing from 35.3/31.4 (DSUnis Head-ﬁrst) to 66.8/62.1 (MultiBCA Head-ﬁrst). Sentiment Graph metrics NSF1 and SF1 have a similar, though slightly lower range (24.5/17.7 – 62.0/56.8). The graph and senti- ment graph metrics do not correlate perfectly, how- ever, as UF1 and LF1 on MPQA are relatively good 8Nordic Language Processing Laboratory vector repo.: http://vectors.nlpl.eu/repository/. We used 300-dimensional embeddings trained on English Wikipedia and Gigaword for English (model id 18 in the repo.), and 100- dimensional embeddings trained on the 2017 CoNLL corpora for all others; Basque (id 32), Catalan (id 34), and Norwegian Bokm˚al (id 58). # H.ﬁrst H.ﬁnal RACL NoReCFine MultiBEU MultiBCA MPQA DSUnis 147 45 74 40 10 63.3 68.9 72.2 55.4 56.9 67.8 65.9 73.7 58.5 43.1 65.6 29.2 28.2 28.8 31.4 Table 4: Number of sentences with multiple targets (#) and Macro F1 on the target extraction task for Head- ﬁnal and RACL. Head-ﬁnal is consistently better than RACL on extracting multiple targets. (40.0/36.9 and 41.4/38.0 for Head-ﬁrst and Head- ﬁnal, respectively), but the NSF1 and SF1 are poor (24.5/17.4 and 26.1/18.8). On average IMN is the weakest baseline, fol- lowed by RACL and then RACL-BERT. The main improvement that RACL-BERT gives over RACL on these datasets is seen in the Targeted metric, i.e., the contextualized representations improve the polarity classiﬁcation more than the extraction task. The proposed graph-based models are consistently the best models across the metrics and datasets. Regarding graph representations, the differ- ences between Head-ﬁrst and Head-ﬁnal are gen- erally quite small. Head-ﬁrst performs better on MultiBCA and slightly better on MultiBEU, while for the others (NoReCFine, MPQA, and DSUnis) Head-ﬁnal is better. This suggests that the main beneﬁt is the joint prediction of all spans and rela- tionships, and that the speciﬁc graph representation matters less. 7 Analysis In this section we perform a deeper analysis of the models in order to answer the research questions. 7.1 Do syntactically informed sentiment graphs improve results? Our two baseline graph representations, Head-ﬁrst and Head-ﬁnal, are crude approximations of lin- guistic structure. In syntactic and semantic depen- dency graphs, heads are often neither the ﬁrst or last word, but rather the most salient word accord- ing to various linguistic criteria. First, we enrich the dependency labels to distinguish edges that are internal to a holder/target/expression span from those that are external and perform experiments by adding an ‘in label’ to non-head nodes within the graph, which we call +inlabel. We further in- form the head selection of the parsing graphs with syntactic information in the Dep. edges parsing Spans Targeted Graph Sent. Graph Holder F1 Target F1 Exp. F1 NoReCFine MultiBEU MultiBCA MPQA DSUnis 1.2 2.9 0.4 8.2 7.9 5.0 0.6 1.6 8.8 1.2 3.4 0.8 1.6 5.2 4.3 F1 4.2 1.1 2.1 7.2 6.4 UF1 LF1 NSF1 SF1 2.8 1.0 2.0 6.6 3.9 2.7 1.4 1.8 7.3 5.7 4.6 1.2 3.3 5.4 3.6 4.0 1.4 2.8 5.1 6.0 Table 5: Average gains in percentage points by including mBERT representations. Dep. edges has the strongest positive effect on the NSF1 and SF1 (an avg. 2.52 and 2.22 per- centage point (pp) over Head-ﬁnal, respectively). However, this average is pulled down by poorer per- formance on the English datasets. Removing these two, the average beneﬁt is 5.2 and 4.2 for NSF1 and SF1, respectively. On span extraction and targeted sentiment, however, Dep. edges leads to poorer scores overall. Dep. labels does not lead to any consistent improvements. These results indicate that incorporating syntactic dependency informa- tion is particularly helpful for the full structured sentiment task, but that these beneﬁts do not always show at a more local level, i.e., span extraction. 7.2 Do graph models perform better on sentences with multiple targets? We hypothesize that predicting the full sentiment graph may have a larger effect on sentences with multiple targets. Therefore, we create a subset of the test data containing sentences with multiple targets and reevaluate Head-ﬁrst, Head-ﬁnal, and RACL-BERT on the target extraction task. Table 4 shows the number of sentences with multiple tar- gets and the Target span extraction score for each model. On this subset, Head-ﬁrst and Head-ﬁnal outperform RACL-BERT on 9 of 10 experiments, conﬁrming the hypothesis that the graph models improve on examples with multiple targets. 7.3 How much does mBERT contribute? We also perform experiments without mBERT (shown in Table 7 in the Appendix) and show the average gains (over all 6 graph setups) of including it in Table 5. Adding the mBERT features leads to average improvements in all experiments: for extracting spans an average gain of 4.1 pp for hold- ers, 3.4 for targets, and 3.1 for expressions. For targeted sentiment there is a larger gain of 4.2 pp, while for the parsing graph metrics UF1 and lF1 the gains are more limited (3.3 pp/ 3.8 pp) and similarly for NSF1 and SF1 (3.6 pp/ 3.9 pp). The gains are Figure 3: Average beneﬁt of each graph annotation scheme (y-axis) on the evaluation metrics (x-axis) in percentage points. The results are averaged across datasets. graphs, where we compute the dependency graph for each sentence9 and set the head of each span to be the node that has an outgoing edge in the corresponding syntactic graph. As there can be more than one such edge, we default to the ﬁrst. A manual inspection showed that this approach sometimes set unlikely dependency label types as heads, e.g., punct, obl. Therefore, we suggest a ﬁnal approach, Dep. labels, which ﬁlters out these unlikely heads. The full results are shown in Table 8 in the Appendix. The implementation of the graph structure has a large effect on all met- rics, although the speciﬁc results depend on the dataset. We plot the average effect of each imple- mentation across all datasets in Figure 3, as well as each individual dataset (Figures 4–8 in the Ap- pendix). +inlabel tends to improve results on the non-English datasets, consistently increasing target and expression extraction and targeted sentiment. It also generally improves the graph scores UF1 and LF1 on the non-English datasets. 9We use SpaCy (Honnibal et al., 2020) for English, Stanza (Qi et al., 2020) for Basque and Catalan and UDPipe (Straka and Strakov´a, 2017) for Norwegian. Holder F1Target F1Exp F1Targeted F1UF1LF1NSF1SF1Dep. labelsDep. edgesHead-final+inlabelHead-finalHead-first+inlabelHead-first42024NoReCFine MultiBEU MultiBCA MPQA DSUnis 57.0 (1.5) 75.7 (0.8) 71.7 (2.4) 38.5 (1.4) 44.5 (2.4) Table 6: Polarity F1 scores (unweighted and weighted) of models augmented with mBERT on the head-ﬁnal setup. We report average and standard deviation over 5 runs. largest for the English datasets (MPQA, DSUnis) followed by NoReCFine, and ﬁnally MultiBCA and MultiBEU. This corroborates the bias towards En- glish and similar languages that has been found in multilingual language models (Artetxe et al., 2020; Conneau et al., 2020) and motivates the need for language-speciﬁc contextualized embeddings. sentiment graph parsing, either by augmenting the token-level representations with contextualized vec- tors from their heads in a dependency tree (Kurtz et al., 2020) or by multi-task learning to depen- dency parse. We would also like to explore differ- ent graph parsing approaches, e.g., PERIN (Samuel and Straka, 2020). Acknowledgements This work has been carried out as part of the SANT project (Sentiment Analysis for Norwegian Text), funded by the Research Council of Norway (grant number 270908). The computations were performed on resources provided by UNINETT Sigma2 - the National In- frastructure for High Performance Computing and Data Storage in Norway. 7.4 Analysis of polarity predictions References In this section we zoom in on polarity, in order to quantify how well models perform at predict- ing only polarity. As the polarity annotations are bound to the expressions, we consider true positives to be any expression that overlaps the gold expres- sion and has the same polarity. Table 6 shows that the polarity predictions are best on and MultiBCA, followed by NoReCFine and DSUnis, and ﬁnally MPQA. This is likely due to the number of do- mains and characteristics of the data. NoReCFine contains many domains and has longer expressions, while MPQA contains many highly ambiguous po- lar expressions, e.g., ‘said’, ‘asked’, which have different polarity depending on the context. 8 Conclusion In this paper, we have proposed a dependency graph parsing approach to structured sentiment analysis and shown that these models outperform state-of-the-art sequence labeling models on ﬁve benchmark datasets. Using parse trees as input has shown promise for sentiment analysis in the past, either to guide a tree-based algorithm (Socher et al., 2013; Tai et al., 2015) or to create features for sentiment models (Nakagawa et al., 2010; Almeida et al., 2015). However, to the authors’ knowledge, this is the ﬁrst attempt to directly predict dependency- based sentiment graphs. In the future, we would like to better exploit the similarities between dependency parsing and Abdulqader Almars, Xue Li, Xin Zhao, Ibrahim A. Ibrahim, Weiwei Yuan, and Bohan Li. 2017. Struc- tured sentiment analysis. In Advanced Data Mining and Applications, pages 695–707, Cham. Springer International Publishing. Mariana S. C. Almeida, Cl´audia Pinto, Helena Figueira, Pedro Mendes, and Andr´e F. T. Martins. 2015. Aligning opinions: Cross-lingual opinion mining with dependencies. In Proceedings of the 53rd An- nual Meeting of the Association for Computational Linguistics and the 7th International Joint Confer- ence on Natural Language Processing (Volume 1: Long Papers), pages 408–418, Beijing, China. As- sociation for Computational Linguistics. Mikel Artetxe, Sebastian Ruder, and Dani Yogatama. 2020. On the cross-lingual transferability of mono- lingual representations. In Proceedings of the 58th Annual Meeting of the Association for Computa- tional Linguistics, pages 4623–4637, Online. Asso- ciation for Computational Linguistics. Jeremy Barnes, Toni Badia, and Patrik Lambert. 2018. MultiBooked: A corpus of basque and Catalan ho- tel reviews annotated for aspect-level sentiment clas- In Proceedings of the Eleventh Interna- siﬁcation. tional Conference on Language Resources and Eval- uation (LREC-2018), Miyazaki, Japan. European Languages Resources Association (ELRA). Taylor Berg-Kirkpatrick, David Burkett, and Dan Klein. 2012. An Empirical Investigation of Statis- In Proceedings of the tical Signiﬁcance in NLP. 2012 Joint Conference on Empirical Methods in Nat- ural Language Processing and Computational Nat- ural Language Learning, pages 995–1005, Jeju Is- land, Korea. Association for Computational Linguis- tics. Jari Bj¨orne, Juho Heimonen, Filip Ginter, Antti Airola, Tapio Pahikkala, and Tapio Salakoski. 2009. Extracting Complex Biological Events with Rich In Proceedings of the Graph-based Feature Sets. Workshop on Current Trends in Biomedical Natural Language Processing: Shared Task, BioNLP ’09, pages 10–18, Stroudsburg, PA, USA. Association for Computational Linguistics. Zhuang Chen and Tieyun Qian. 2020. Relation-aware collaborative learning for uniﬁed aspect-based sen- timent analysis. In Proceedings of the 58th Annual Meeting of the Association for Computational Lin- guistics, pages 3685–3694, Online. Association for Computational Linguistics. Yejin Choi, Eric Breck, and Claire Cardie. 2006. Joint extraction of entities and relations for opinion recog- In Proceedings of the 2006 Conference on nition. Empirical Methods in Natural Language Processing, pages 431–439, Sydney, Australia. Association for Computational Linguistics. Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm´an, Edouard Grave, Myle Ott, Luke Zettle- moyer, and Veselin Stoyanov. 2020. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Asso- ciation for Computational Linguistics, pages 8440– 8451, Online. Association for Computational Lin- guistics. Hang Cui, Renxu Sun, Keya Li, Min-Yen Kan, and Tat- Seng Chua. 2005. Question answering passage re- trieval using dependency relations. In Proceedings of the 28th Annual International ACM SIGIR Con- ference on Research and Development in Informa- tion Retrieval, SIGIR ’05, pages 400–407, Salvador, Brazil. Association for Computing Machinery. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- In Proceedings of the 2019 Conference standing. of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota. Associ- ation for Computational Linguistics. Timothy Dozat and Christopher D. Manning. 2018. Simpler but more accurate semantic dependency In Proceedings of the 56th Annual Meet- parsing. ing of the Association for Computational Linguis- tics (Volume 2: Short Papers), pages 484–490, Mel- bourne, Australia. Association for Computational Linguistics. Murhaf Fares, Andrey Kutuzov, Stephan Oepen, and Erik Velldal. 2017. Word vectors, reuse, and replica- bility: Towards a community repository of large-text In Proceedings of the 21st Nordic Con- resources. ference on Computational Linguistics, pages 271– 276, Gothenburg, Sweden. Association for Compu- tational Linguistics. Ruidan He, Wee Sun Lee, Hwee Tou Ng, and Daniel Dahlmeier. 2019. An interactive multi-task learn- ing network for end-to-end aspect-based sentiment analysis. In Proceedings of the 57th Annual Meet- ing of the Association for Computational Linguis- tics, pages 504–515, Florence, Italy. Association for Computational Linguistics. Matthew Honnibal, Ines Montani, Soﬁe Van Lan- deghem, spaCy: and Adriane Boyd. 2020. Industrial-strength Natural Language Processing in Python. Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and Tiejun Zhao. 2011. Target-dependent twitter senti- ment classiﬁcation. In Proceedings of the 49th An- nual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 151–160, Portland, Oregon, USA. Association for Computational Linguistics. Richard Johansson and Alessandro Moschitti. 2012. Relational Features in Fine-Grained Opinion Anal- ysis. Computational Linguistics, 39(3):473–509. Arzoo Katiyar and Claire Cardie. 2016. Investigating LSTMs for joint extraction of opinion entities and relations. In Proceedings of the 54th Annual Meet- ing of the Association for Computational Linguistics (Volume 1: Long Papers), pages 919–929, Berlin, Germany. Association for Computational Linguis- tics. Marco Kuhlmann and Stephan Oepen. 2016. Towards a Catalogue of Linguistic Graph Banks. Computa- tional Linguistics, 42(4):819–827. Robin Kurtz, Stephan Oepen, and Marco Kuhlmann. 2020. End-to-end negation resolution as graph pars- ing. In Proceedings of the 16th International Con- ference on Parsing Technologies and the IWPT 2020 Shared Task on Parsing into Enhanced Universal De- pendencies, pages 14–24, Online. Association for Computational Linguistics. Emanuele Lapponi, Erik Velldal, Lilja Øvrelid, and Jonathon Read. 2012. UiO2: Sequence-labeling Negation Using Dependency Features. In Proceed- ings of the First Joint Conference on Lexical and Computational Semantics - Volume 1: Proceedings of the Main Conference and the Shared Task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation, SemEval ’12, pages 319–327, Stroudsburg, PA, USA. Association for Computational Linguistics. Xin Li, Lidong Bing, Piji Li, and Wai Lam. 2019a. A uniﬁed model for opinion target extraction and target In Proceedings of the AAAI sentiment prediction. Conference on Artiﬁcial Intelligence, pages 6714– 6721. Xin Li, Lidong Bing, Wenxuan Zhang, and Wai Lam. 2019b. Exploiting BERT for end-to-end aspect- based sentiment analysis. In Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019), pages 34–41, Hong Kong, China. Association for Computational Linguistics. Parsing. Proceedings of the 9th International Work- shop on Semantic Evaluation (SemEval 2015), pages 915–926. Bing Liu. 2012. Sentiment Analysis and Opinion Min- ing. Morgan & Claypool Publishers. Mike Mintz, Steven Bills, Rion Snow, and Daniel Ju- rafsky. 2009. Distant supervision for relation ex- In Proceedings of traction without labeled data. the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 1003–1011, Suntec, Singapore. Association for Computational Linguistics. Margaret Mitchell, Jacqui Aguilar, Theresa Wilson, and Benjamin Van Durme. 2013. Open domain tar- In Proceedings of the 2013 Con- geted sentiment. ference on Empirical Methods in Natural Language Processing, pages 1643–1654, Seattle, Washington, USA. Association for Computational Linguistics. Roser Morante and Eduardo Blanco. 2012. *SEM 2012 Shared Task: Resolving the Scope and Focus of Negation. In *SEM 2012: The First Joint Confer- ence on Lexical and Computational Semantics – Vol- ume 1: Proceedings of the Main Conference and the Shared Task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (Se- mEval 2012), pages 265–274, Montr´eal, Canada. As- sociation for Computational Linguistics. Roser Morante and Walter Daelemans. 2012. ConanDoyle-neg: Annotation of negation cues In Pro- and their scope in Conan Doyle stories. ceedings of the Eighth International Conference on Language Resources and Evaluation (LREC’12), Istanbul, Turkey. European pages 1563–1568, Language Resources Association (ELRA). Tetsuji Nakagawa, Kentaro Inui, and Sadao Kurohashi. 2010. Dependency tree-based sentiment classiﬁca- tion using CRFs with hidden variables. In Human Language Technologies: The 2010 Annual Confer- ence of the North American Chapter of the Associa- tion for Computational Linguistics, pages 786–794, Los Angeles, California. Association for Computa- tional Linguistics. Stephan Oepen, Omri Abend, Lasha Abzianidze, Jo- han Bos, Jan Hajic, Daniel Hershcovich, Bin Li, Tim O’Gorman, Nianwen Xue, and Daniel Zeman. 2020. MRP 2020: The Second Shared Task on Cross-Framework and Cross-Lingual Meaning Rep- In Proceedings of the CoNLL resentation Parsing. 2020 Shared Task: Cross-Framework Meaning Rep- resentation Parsing, pages 1–22, Online. Associa- tion for Computational Linguistics. Stephan Oepen, Marco Kuhlmann, Yusuke Miyao, Daniel Zeman, Silvie Cinkova, Dan Flickinger, Jan Hajic, and Zdenka Uresova. 2015. SemEval 2015 Task 18: Broad-Coverage Semantic Dependency Stephan Oepen, Marco Kuhlmann, Yusuke Miyao, Daniel Zeman, Dan Flickinger, Jan Hajic, Angelina Ivanova, and Yi Zhang. 2014. SemEval 2014 Task 8: Broad-Coverage Semantic Dependency Parsing. Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 63–72. Lilja Øvrelid, Petter Mæhlum, Jeremy Barnes, and Erik Velldal. 2020. A ﬁne-grained sentiment dataset for In Proceedings of the 12th Language Norwegian. Resources and Evaluation Conference, pages 5025– 5033, Marseille, France. European Language Re- sources Association. Haiyun Peng, Lu Xu, Lidong Bing, Fei Huang, Wei Lu, and Luo Si. 2019. Knowing what, how and why: A near complete solution for aspect-based sentiment analysis. Maria Pontiki, Dimitris Galanis, Haris Papageorgiou, Ion Androutsopoulos, Suresh Manandhar, Moham- mad AL-Smadi, Mahmoud Al-Ayyoub, Yanyan Zhao, Bing Qin, Orph´ee De Clercq, V´eronique Hoste, Marianna Apidianaki, Xavier Tannier, Na- talia Loukachevitch, Evgeniy Kotelnikov, Nuria Bel, Salud Mar´ıa Jim´enez-Zafra, and G¨uls¸en Eryi˘git. 2016. SemEval-2016 task 5: Aspect based senti- ment analysis. In Proceedings of the 10th Interna- tional Workshop on Semantic Evaluation (SemEval- 2016), pages 19–30, San Diego, California. Associa- tion for Computational Linguistics. Maria Pontiki, Dimitris Galanis, Haris Papageorgiou, Suresh Manandhar, and Ion Androutsopoulos. 2015. SemEval-2015 task 12: Aspect based sentiment In Proceedings of the 9th International analysis. Workshop on Semantic Evaluation (SemEval 2015), pages 486–495, Denver, Colorado. Association for Computational Linguistics. Maria Pontiki, Dimitris Galanis, John Pavlopoulos, Harris Papageorgiou, Ion Androutsopoulos, and Suresh Manandhar. 2014. SemEval-2014 task 4: As- pect based sentiment analysis. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 27–35, Dublin, Ireland. As- sociation for Computational Linguistics. Peng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton, and Christopher D. Manning. 2020. Stanza: A python natural language processing toolkit for many In Proceedings of the 58th An- human languages. nual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 101– 108, Online. Association for Computational Linguis- tics. David Samuel and Milan Straka. 2020. ´UFAL at MRP 2020: Permutation-invariant semantic pars- ing in PERIN. In Proceedings of the CoNLL 2020 Shared Task: Cross-Framework Meaning Represen- tation Parsing, pages 53–64, Online. Association for Computational Linguistics. Processing and Computational Natural Language Learning, pages 1335–1345, Jeju Island, Korea. As- sociation for Computational Linguistics. Juntao Yu, Bernd Bohnet, and Massimo Poesio. 2020. Named Entity Recognition as Dependency Parsing. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6470–6476, Online. Association for Computational Linguistics. Meishan Zhang, Qiansheng Wang, and Guohong Fu. 2019. End-to-end neural opinion extraction with a transition-based model. Information Systems, 80:56 – 63. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment tree- In Proceedings of the 2013 Conference on bank. Empirical Methods in Natural Language Processing, pages 1631–1642, Seattle, Washington, USA. Asso- ciation for Computational Linguistics. Milan Straka and Jana Strakov´a. 2017. Tokenizing, pos tagging, lemmatizing and parsing ud 2.0 with udpipe. In Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Univer- sal Dependencies, pages 88–99, Vancouver, Canada. Association for Computational Linguistics. Kai Sheng Tai, Richard Socher, and Christopher D. Manning. 2015. Improved semantic representations from tree-structured long short-term memory net- In Proceedings of the 53rd Annual Meet- works. ing of the Association for Computational Linguistics and the 7th International Joint Conference on Natu- ral Language Processing (Volume 1: Long Papers), pages 1556–1566, Beijing, China. Association for Computational Linguistics. Cigdem Toprak, Niklas Jakob, and Iryna Gurevych. 2010. Sentence and expression level annotation of In Proceed- opinions in user-generated discourse. ings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 575–584, Up- psala, Sweden. Association for Computational Lin- guistics. Wenya Wang, Sinno Jialin Pan, Daniel Dahlmeier, and Xiaokui Xiao. 2016. Recursive neural conditional random ﬁelds for aspect-based sentiment analysis. In Proceedings of the 2016 Conference on Empiri- cal Methods in Natural Language Processing, pages 616–626, Austin, Texas. Association for Computa- tional Linguistics. Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005. Annotating expressions of opinions and emo- tions in language. Language Resources and Evalua- tion, 39(2-3):165–210. Hu Xu, Bing Liu, Lei Shu, and Philip Yu. 2019. BERT post-training for review reading comprehension and aspect-based sentiment analysis. In Proceedings of the 2019 Conference of the North American Chap- ter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2324–2335, Minneapolis, Minnesota. Association for Computational Linguis- tics. Bishan Yang and Claire Cardie. 2012. Extracting opin- ion expressions with semi-Markov conditional ran- dom ﬁelds. In Proceedings of the 2012 Joint Con- ference on Empirical Methods in Natural Language A Appendix Figure 4: Average beneﬁt of each graph annotation scheme (y-axis) on the evaluation metrics (x-axis) for NoReCFine. Figure 7: Average beneﬁt of each graph annotation scheme (y-axis) on the evaluation metrics (x-axis) in percentage points for MPQA. Figure 5: Average beneﬁt of each graph annotation scheme (y-axis) on the evaluation metrics (x-axis) for MultiBEU. Figure 8: Average beneﬁt of each graph annotation scheme (y-axis) on the evaluation metrics (x-axis in per- centage points) for DSUnis. Figure 6: Average beneﬁt of each graph annotation scheme (y-axis) on the evaluation metrics (x-axis) in percentage points for MultiBCA. Figure 9: Average beneﬁt of each graph annota- tion scheme (y-axis) on the evaluation metrics (x- axis) in percentage points. The results on NoReCFine, MultiBEU, MultiBCA. Holder F1Target F1Exp F1Targeted F1UF1LF1NSF1SF1Dep. labelsDep. edgesHead-final+inlabelHead-finalHead-first+inlabelHead-first42024Holder F1Target F1Exp F1Targeted F1UF1LF1NSF1SF1Dep. labelsDep. edgesHead-final+inlabelHead-finalHead-first+inlabelHead-first42024Holder F1Target F1Exp F1Targeted F1UF1LF1NSF1SF1Dep. labelsDep. edgesHead-final+inlabelHead-finalHead-first+inlabelHead-first42024Holder F1Target F1Exp F1Targeted F1UF1LF1NSF1SF1Dep. labelsDep. edgesHead-final+inlabelHead-finalHead-first+inlabelHead-first42024Holder F1Target F1Exp F1Targeted F1UF1LF1NSF1SF1Dep. labelsDep. edgesHead-final+inlabelHead-finalHead-first+inlabelHead-first42024Holder F1Target F1Exp F1Targeted F1UF1LF1NSF1SF1Dep. labelsDep. edgesHead-final+inlabelHead-finalHead-first+inlabelHead-first42024Spans Targeted Graph Sent. Graph Holder F1 Target F1 Exp. F1 e n i F C e R o N U E B i t l u M A C B i t l u M A Q P M s i n U S D IMN RACL Head-ﬁrst +inlabel Head-ﬁnal +inlabel Dep. edges Dep. labels IMN RACL Head-ﬁrst +inlabel Head-ﬁnal +inlabel Dep. edges Dep. labels IMN RACL Head-ﬁrst +inlabel Head-ﬁnal +inlabel Dep. edges Dep. labels IMN RACL Head-ﬁrst +inlabel Head-ﬁnal +inlabel Dep. edges Dep. labels IMN RACL Head-ﬁrst +inlabel Head-ﬁnal +inlabel Dep. edges Dep. labels - - 48.4 (2.2) 50.4 (4.0) 57.0 (3.3) 57.9 (1.7) 54.4 (3.9) 51.6 (2.6) - - 60.8 (3.8) 59.8 (1.6) 57.0 (2.0) 53.7 (1.2) 53.1 (1.9) 52.0 (3.8) - - 41.9 (2.8) 42.4 (2.6) 40.4 (2.5) 36.4 (2.2) 42.6 (6.1) 43.8 (3.4) - - 35.2 (1.1) 35.6 (1.4) 37.1 (1.3) 37.0 (0.5) 35.4 (1.5) 36.7 (0.6) - - 25.6 (5.1) 22.9 (5.7) 29.2 (8.4) 30.2 (8.9) 33.9 (5.4) 21.3 (18.1) 35.9 45.6 47.1 (1.6) 47.6 (2.5) 49.4 (0.9) 50.1 (1.3) 49.0 (2.5) 46.5 (3.0) 48.2 55.4 64.1 (1.4) 64.3 (0.9) 66.0 (1.6) 64.0 (2.4) 63.8 (1.7) 63.0 (1.1) 56.3 65.4 69.8 (1.7) 70.9 (0.8) 69.9 (1.5) 69.1 (1.1) 69.1 (0.5) 70.3 (0.8) 24.3 32.6 40.5 (1.8) 41.6 (1.1) 42.1 (1.0) 42.3 (1.2) 39.1 (2.0) 39.2 (2.3) 33.0 39.3 36.8 (3.5) 38.6 (3.9) 38.1 (2.0) 38.2 (2.9) 39.2 (2.9) 40.0 (1.5) 48.7 55.4 52.0 (1.6) 51.0 (1.3) 52.1 (1.8) 52.6 (0.4) 51.4 (1.7) 50.7 (2.7) 65.2 70.7 72.2 (0.7) 71.9 (0.8) 72.2 (0.6) 72.9 (0.6) 71.0 (1.1) 71.3 (1.9) 60.9 67.6 68.9 (1.4) 69.9 (0.9) 66.8 (0.8) 65.4 (0.6) 67.3 (0.6) 67.2 (1.1) 29.6 37.8 41.7 (1.7) 42.4 (2.3) 41.9 (0.8) 41.6 (1.6) 41.6 (1.1) 40.4 (2.1) 27.4 40.2 39.0 (1.5) 38.6 (2.8) 39.5 (2.4) 38.8 (2.0) 39.3 (3.4) 38.4 (2.4) F1 18.0 20.1 33.0 (1.4) 27.3 (1.1) 26.0 (0.6) 29.6 (0.6) 26.7 (3.1) 26.7 (1.9) 39.5 48.2 53.9 (1.8) 57.9 (1.9) 55.5 (1.7) 54.9 (2.0) 53.7 (1.7) 54.0 (1.0) 32.5 53.1 57.3 (2.0) 50.9 (1.3) 50.8 (2.6) 52.9 (0.9) 50.6 (1.3) 50.6 (1.8) 1.2 11.8 22.6 (3.1) 14.0 (0.9) 13.3 (1.9) 15.2 (2.5) 12.3 (0.8) 12.6 (1.2) 17.9 22.8 23.4 (1.8) 18.3 (2.7) 21.8 (1.1) 24.4 (2.7) 22.2 (2.7) 21.4 (4.3) UF1 LF1 NSF1 - - 37.6 (0.5) 36.9 (0.5) 45.1 (1.2) 45.0 (1.0) 39.3 (1.1) 36.7 (1.1) - - 29.8 (0.4) 29.4 (0.8) 35.2 (1.1) 35.2 (0.5) 31.5 (1.3) 28.3 (0.8) - - 32.9 (1.6) 32.9 (1.1) 34.4 (0.7) 35.1 (1.6) 47.2 (0.9) 33.4 (1.8) SF1 - - 26.1 (1.5) 25.8 (0.5) 27.2 (0.9) 27.0 (1.3) 36.0 (1.1) 25.4 (1.8) - - - - 62.9 (0.6) 62.6 (0.6) 60.2 (0.8) 60.1 (1.5) 59.0 (1.3) 59.5 (1.1) 58.2 (0.3) 57.5 (1.1) 55.5 (0.9) 54.9 (1.7) 54.5 (1.6) 54.9 (1.1) 58.5 (2.3) 57.3 (1.5) 59.6 (0.8) 57.1 (3.2) 59.0 (1.6) 58.6 (2.8) 54.7 (2.6) 53.6 (1.3) 56.3 (1.0) 53.5 (3.3) 55.6 (1.8) 54.6 (2.4) - - - - 64.2 (0.7) 64.4 (0.9) 60.9 (0.6) 60.6 (0.9) 59.3 (0.7) 61.0 (0.5) 59.9 (0.8) 59.6 (0.7) 57.1 (0.8) 57.0 (0.6) 55.7 (1.1) 57.1 (0.8) 58.2 (2.3) 55.7 (2.0) 57.7 (1.0) 58.0 (1.9) 57.5 (2.1) 57.8 (1.4) 53.3 (2.2) 50.7 (2.1) 53.3 (1.3) 53.5 (2.0) 52.8 (1.8) 52.7 (1.9) - - - - 32.2 (1.3) 32.9 (0.8) 35.7 (0.8) 35.5 (0.7) 28.9 (1.7) 28.9 (1.6) 28.2 (1.4) 28.9 (0.9) 31.7 (0.5) 31.7 (0.6) 24.8 (1.5) 24.5 (1.1) 19.4 (1.5) 20.4 (1.0) 18.7 (0.7) 19.6 (0.6) 19.0 (0.9) 18.9 (0.8) 12.4 (1.7) 13.2 (1.2) 12.5 (1.6) 12.6 (0.8) 11.9 (1.1) 11.6 (1.0) - - - - 32.9 (1.7) 33.7 (2.8) 31.1 (2.1) 32.4 (2.1) 32.4 (2.1) 32.1 (1.5) 25.9 (1.7) 26.4 (1.6) 26.0 (1.0) 28.4 (2.1) 26.8 (2.0) 27.2 (1.5) 27.8 (1.4) 25.9 (3.4) 29.1 (3.3) 28.1 (2.9) 29.3 (1.5) 28.2 (1.1) 18.8 (2.0) 15.2 (2.2) 20.4 (2.0) 22.4 (3.4) 19.8 (1.2) 20.5 (3.1) Table 7: Experiments without contextualized embeddings. Spans Targeted Graph Sent. Graph Holder F1 Target F1 Exp. F1 RACL-BERT - 47.2 56.3 Head-ﬁrst +inlabel Head-ﬁnal +inlabel Dep. edges Dep. labels 51.1 (3.2) 51.6 (2.8) 60.4 (1.2) 57.1 (3.0) 54.0 (3.4) 52.7 (5.6) 50.1 (3.4) 52.7 (0.7) 54.8 (1.6) 55.2 (1.0) 53.6 (1.5) 53.6 (0.3) 54.4 (1.6) 54.6 (1.4) 55.5 (1.5) 56.3 (1.3) 55.0 (0.9) 54.4 (1.5) F1. 30.3 30.5 (2.3) 32.2 (1.4) 31.9 (1.3) 34.8 (1.0) 32.7 (1.6) 32.7 (1.6) UF1 - 39.2 (0.5) 39.6 (0.8) 48.0 (1.3) 48.7 (1.2) 41.5 (0.7) 40.7 (0.8) LF1 - 31.5 (0.5) 32.0 (0.7) 37.7 (1.4) 38.3 (1.0) 33.8 (0.4) 32.2 (0.5) NSF1 - 37.0 (2.6) 37.6 (1.2) 39.2 (1.7) 40.5 (1.1) 50.9 (0.3) 38.2 (1.4) SF1 - 29.5 (2.4) 29.5 (1.2) 31.2 (1.6) 31.7 (1.1) 39.4 (0.4) 30.0 (1.2) RACL-BERT - 59.9 72.6 56.8 - - - - Head-ﬁrst +inlabel Head-ﬁnal +inlabel Dep. edges Dep. labels 60.4 (2.2) 59.6 (1.9) 60.5 (2.2) 58.1 (2.4) 58.8 (4.2) 56.3 (2.1) 64.0 (2.4) 65.9 (0.9) 64.0 (2.3) 64.7 (1.1) 64.8 (1.4) 65.4 (0.9) 73.9 (1.0) 74.2 (0.7) 72.1 (1.2) 72.0 (0.7) 71.2 (0.8) 72.9 (1.1) 57.8 (2.4) 59.2 (0.9) 56.9 (1.7) 58.5 (1.4) 54.0 (1.8) 54.9 (0.8) 64.6 (1.0) 64.7 (0.7) 60.8 (0.8) 60.6 (1.1) 59.9 (0.4) 60.0 (0.9) 60.0 (1.6) 60.3 (1.1) 56.0 (1.1) 56.6 (0.7) 55.5 (0.7) 55.6 (0.8) 58.0 (1.1) 59.8 (1.1) 58.0 (2.1) 59.8 (1.6) 60.9 (1.6) 60.5 (1.1) 54.7 (1.6) 56.1 (1.6) 54.7 (1.8) 56.9 (1.8) 57.4 (1.6) 57.1 (1.1) RACL-BERT - 67.5 70.3 52.4 - - - - Head-ﬁrst +inlabel Head-ﬁnal +inlabel Dep. edges Dep. labels 43.0 (1.3) 43.1 (2.2) 37.1 (4.2) 34.9 (4.1) 46.3 (3.1) 45.6 (2.9) 72.5 (1.0) 73.4 (1.0) 71.2 (0.6) 70.7 (1.4) 70.3 (0.6) 70.3 (1.1) 71.1 (0.8) 70.3 (1.0) 67.1 (1.7) 68.2 (1.0) 69.2 (1.4) 69.1 (1.7) 55.0 (0.9) 55.8 (1.8) 53.9 (2.2) 53.5 (0.7) 53.4 (1.5) 53.9 (1.5) 66.8 (0.5) 66.2 (0.3) 62.7 (0.4) 63.4 (0.5) 60.8 (0.4) 62.5 (0.6) 62.1 (0.5) 61.5 (0.6) 58.1 (0.8) 58.7 (0.6) 57.5 (0.6) 59.1 (0.6) 62.0 (1.1) 61.1 (1.0) 59.7 (1.1) 60.9 (1.1) 60.7 (1.0) 60.4 (1.0) 56.8 (0.7) 56.0 (1.0) 53.7 (2.4) 55.1 (1.2) 55.6 (0.9) 55.8 (1.2) RACL-BERT - 20.0 31.2 17.8 - - - - Head-ﬁrst +inlabel Head-ﬁnal +inlabel Dep. edges Dep. labels 43.8 (1.8) 43.1 (1.5) 46.3 (1.8) 45.6 (2.5) 44.0 (1.5) 43.7 (0.9) 51.0 (1.9) 51.5 (1.0) 49.5 (0.9) 49.4 (2.1) 48.5 (1.2) 47.7 (2.3) 48.1 (0.8) 47.5 (1.1) 46.0 (1.1) 45.6 (1.1) 46.3 (1.9) 47.5 (0.8) 33.5 (3.1) 21.3 (0.4) 21.9 (1.4) 20.7 (1.0) 18.9 (2.3) 21.9 (0.7) 40.0 (1.0) 40.6 (0.5) 41.4 (0.7) 40.4 (1.5) 35.4 (1.3) 35.6 (1.2) 36.9 (1.2) 37.5 (0.5) 38.0 (0.5) 37.2 (1.9) 31.9 (1.2) 32.0 (1.3) 24.5 (2.3) 24.5 (1.3) 26.1 (0.7) 25.2 (1.7) 24.2 (1.6) 24.0 (0.8) 17.4 (2.7) 17.3 (1.0) 18.8 (0.7) 17.8 (1.3) 16.3 (1.9) 17.2 (0.8) RACL-BERT - 44.6 38.2 27.3 - - - - Head-ﬁrst +inlabel Head-ﬁnal +inlabel Dep. edges Dep. labels 28.0 (7.7) 30.9 (9.9) 37.4 (11.6) 30.6 (16.4) 32.7 (12.1) 30.8 (5.8) 39.9 (2.2) 38.4 (3.3) 42.1 (2.7) 38.9 (3.1) 39.9 (2.8) 38.9 (0.9) 40.3 (0.6) 40.6 (2.9) 45.5 (2.4) 45.2 (2.7) 44.8 (4.0) 43.1 (1.2) 26.7 (2.1) 26.7 (2.4) 29.6 (1.7) 28.1 (3.7) 28.9 (3.6) 27.8 (1.9) 35.3 (0.9) 34.2 (2.1) 38.1 (1.9) 37.3 (2.7) 37.3 (2.5) 35.7 (1.5) 31.4 (1.3) 30.7 (2.5) 33.9 (2.3) 33.3 (2.1) 33.8 (2.7) 32.1 (1.6) 31.0 (1.4) 30.5 (2.1) 34.3 (4.2) 29.4 (2.8) 33.2 (4.5) 31.3 (1.1) 25.0 (1.3) 25.4 (2.3) 26.5 (3.5) 23.7 (2.4) 27.3 (4.1) 25.3 (2.0) e n i F C e R o N U E B i t l u M A C B i t l u M A Q P M s i n U S D Table 8: Experiments with mBERT. NVIDIA P100, 16 GiB RAM Intel Xeon-Gold 6138 2.0 GHz 00:31:43 (MultiBEU) – 07:40:54 (NoReCFine) https://github.com/jerbarnes/sentiment_graphs/src Best assignment Word2Vec SkipGram 100D GPU Infrastructure CPU Infrastructure Training duration Model implementation Hyperparameter embedding contexualized embedding mBERT embeddings trainable number of epochs batch size beta1 beta2 l2 hidden lstm hidden char lstm layers lstm dim mlp dim embedding dim char embedding early stopping pos style attention model interpolation loss interpolation lstm implementation char implementation emb dropout type bridge dropout embedding dropout edge dropout label dropout main recurrent dropout recurrent char dropout main ff dropout char ff dropout char linear False 100 50 0 0.95 3e-09 200 100 3 200 100 80 0 xpos bilinear 0.5 0.025 drop connect convolved replace dpa+ 0.2 0.2 0.3 0.2 0.3 0.4 0.3 0.3 