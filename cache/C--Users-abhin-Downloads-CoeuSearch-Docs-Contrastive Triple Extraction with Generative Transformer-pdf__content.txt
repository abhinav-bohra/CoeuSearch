Contrastive Triple Extraction Generative Transformer Hongbin Ye ,  * , Ningyu Zhang , ∗† , Shumin Deng ,  , Mosha Chen  , Chuanqi Tan  , Fei Huang  , Huajun Chen , †   Zhejiang University   AZFT Joint Lab Knowledge Engine   Alibaba Group { yehongbin , zhangningyu,   sm , huajunsir } @ zju.edu.cn , { chenmosha.cms , chuanqi.tcq , f.huang } @ alibaba-inc.com         r p    ] L C .  c [   v           .         : v  X r  Abstract Triple extraction essential task information extraction natural language processing knowledge graph con- struction . paper , revisit end-to-end triple ex- traction task sequence generation . Since generative triple extraction may struggle capture long-term dependencies generate unfaithful triples , introduce novel model , contrastive triple extraction generative transformer . Speciﬁcally , introduce single shared transformer mod- ule encoder-decoder-based generation . generate faith- ful results , propose novel triplet contrastive training ob- ject . Moreover , introduce two mechanisms im- prove model performance ( i.e. , batch-wise dynamic attention- masking triple-wise calibration ) . Experimental results three datasets ( i.e. , NYT , WebNLG , MIE ) show approach achieves better performance baselines . Introduction Triple extraction essential information extraction task natural language processing ( NLP ) knowledge graph ( KG ) , used detect pairs entities rela- tions unstructured text . Consider sentence : “ Paris known romantic capital France. ” , ideal triple extraction would comprise ( cid:    ) Paris , Capital , France ( cid:    ) , Capital relation Paris France . Researchers proposed pipeline approaches past ( Lample et al .      ; Zeng et al .      ) typ- ically deconstructed triple extraction problem two separate tasks : named-entity recognition ( NER ) ( used ex- tract entities ) relation classiﬁcation . Thus , ﬁrst rec- ognized entities ; , predicted relationships . Unfortunately , similar pipeline approaches suffer drawbacks ( Roth Yih      ) omit evident correlations entity recognition relation extrac- tion tasks , resulting error propagation . Recently , several neural-network-based models ( Zeng et al .     a ) proposed jointly extract entities relations sentences . models use parameter- sharing mechanism extract entities relations * Equal contribution shared co-ﬁrst authorship . †Corresponding author . Copyright ©      , Association Advancement Artiﬁcial Intelligence ( www.aaai.org ) . rights reserved . Input Output United States President Trump raised borough Queens New York City , lived age    . Trump→president→of→United→States→ [ S S SEQ ] →Trump→born→in→Queens→ [ S S SEQ ] →Trump→live→in→Queens Gold Negative ( Trump , president , United States ) ( Trump , born , Queens ) ( Trump , live , Queens ) ( Trump , president , Queens ) ( Trump , born ,    ) ( Trump , live ,    ) Table   : Contrastive triple extraction sequence genera- tion . encourage model generate gold triples generate negative ones . network . Apart approaches , Zeng et al . (     b ) proposed recurrent neural-network-based encoder-decoder model ( i.e. , CopyRE ) extract triples overlapping entities . end-to-end generative triple ex- traction directly obtain triples mitigate error propagation issue , also enable generation domain entities relations T -style ( Raffel et al .      ) ( text-to-text ) . Besides , Zeng , Zhang , Liu (      ) proposed multi-task learning framework equipped copy mechanism ( i.e. , CopyMTL ) allow prediction multi-token entities . Nayak Ng (      ) introduced representation scheme triples pointer-network- based decoding approach , improved per- formance CopyRE . Encoder-decoder models powerful tools seen success many NLP tasks , including machine translation ( Cho et al .      ) , open information extraction ( Zhang , Duh , Van Durme      ) . Although signiﬁcant progress achieved , remain two key problems existing methods . First , owing intrinsic shortfalls recurrent neural networks ( RNN ) , capture long- term dependencies , results loss important in- formation otherwise reﬂected sentence . draw- back prevents model applied longer texts .       Second , scarcity work focused gen- erating faithful triples . previous study ( Zhu et al .      ) indicated , sequence-to-sequence architecture generate unfaithful sequences create contradictions meaning . example , given sentence “ United States Presi- dent Trump raised borough Queens New York City , lived age    , ” model could gener- ate fact “ ( Trump , born , Queens ) . ” Although logically true , ﬁnd direct evidence given sentence support . address issues , introduce framework Contrastive triple extraction Generative Transformer ( CGT ) , single shared transformer module triplet contrastive object supports encoder-decoder gen- eration . begin , concatenate input sequence target sequence using separator token lever- age partial causal masking ( Du , Rush , Cardie      ) distinguish encoder-decoder representations . model requires additional parameters beyond pre- trained model . , introduce novel triplet contrastive learning object , utilizes ground-truth triples posi- tive instances leverages random token sampling con- struct corrupt triples negative instances . jointly opti- mize triple generation contrastive object , intro- duce batch-wise dynamic attention-masking mechanism , allows us dynamically choose different objects jointly optimize tasks . Lastly , introduce novel triple- wise calibrating algorithm ﬁlter remaining false triples inference stage . contributions work follows : • revisit triple extraction sequence generation task introduce novel CGT model . light added extraction capability , CGT requires additional param- eters beyond found pre-trained language model . • introduce two mechanisms improve model performance ( i.e. , batch-wise dynamic attention-masking triple-wise calibration ) . ﬁrst enables joint op- timization different objects , second ensures faithful inference . • evaluate CGT three benchmark datasets . model empirically outperforms substantially strong baseline models . also demonstrate CGT bet- ter existing triple extraction approaches captur- ing long-term dependencies , thus , achieving better perfor- mance long sentences . Related Work Triple Extraction Two main methods proposed triple extraction : pipeline ( Nadeau Sekine      ; Bunescu Mooney      ; Lin et al .      ; Lin , Liu , Sun      ; Li et al .      ; Wang et al .      ) joint learning ( Miwa Bansal      ; Katiyar Cardie      ; Cao et al .     a ; Zhang et al .     a ; Dai et al .     a ) . pipeline method ﬁrst extracts en- tities , identiﬁes relations ( Hendrickx et al .      ; Zeng et al .      ; Wu et al .      ) . Although pipeline models achieved great progress ( Zhang et al .      ; et al .      ; Zhang et al .     a ,     c ) , introduce error propagation problem ( Li Ji      ) , harm overall performance . joint learning implicitly model correlations tasks , many approaches proposed . Bek- oulis et al . (      ) formulated triple extraction task multi-head selection problem . Takanobu et al . (      ) pro- posed hierarchical reinforcement-learning framework triple extraction . Chen et al . (      ) utilized triplet atten- tion exploit connections relation cor- responding entity pairs . Dai et al . (     b ) introduced position-attention mechanism produce different tag se- quences triple extraction.Wei et al . (     a ) revisited relational triple extraction task proposed novel cascade binary-tagging framework . Apart approaches , Zeng et al . (     a ) proposed CopyRE , joint model based copy mechanism , converted joint extraction task triplet-generation task . researchers intro- duced multiple strategies , multi-task learning ( Zeng , Zhang , Liu      ) one-word generation ( Nayak Ng      ) improve CopyRE . ﬁrst time , utilize transformer encoder-decoder architecture extract triples sentences . Natural Language Generation Natural language generation intensively studied recent literature . models employed encoder- decoder architecture ( i.e. , seq seq ) using RNNs ( Schus- ter Paliwal      ; Zhang et al .     b ; Krause et al .      ) . Recently , owing powerful representation abil- ity transformers , several researchers introduced transformer-based natural language generation methods . Gu , Wang , Zhao (      ) developed Levenshtein trans- former , new partially autoregressive model , de- vised ﬂexible amenable sequence genera- tion . Chen et al . (      ) present novel approach , Condi- tional Masked Language Modeling ( C-MLM ) , enable ﬁnetuning BERT ( Devlin et al .      ) target generation tasks . Dong et al . (      ) proposed new uniﬁed pre-trained language model different masking strategies , used language understanding generation . Du , Rush , Cardie (      ) proposed generative transformer- based encoder-decoder framework document-level infor- mation extraction . Since generation procedure unconditional , non-trivial judge faithfulness generated se- quence . Zhang et al . (     b ) approached factual cor- rectness problem medical domain , space facts limited could depicted descriptor vector . Cao et al . (     b ) extracted relational information article mapped sequence input encoder . decoder attended article tokens relations . Gunel et al . (      ) employed entity- aware transformer structure boost factual correctness abstractive summarization , entities came Wikidata knowledge graph . comparison , model utilizes contrastive learning encourage model im- plicitly generate faithful triples . Figure   : architecture Contrastive triple extraction Generative Transformer ( CGT ) . top-right component refers generative transformer , bottom-right component represents triplet contrastive learning . two parts optimized jointly . left input encoder ( best viewed color ) . Overview Preliminary treat triple extraction sequence-to-sequence task better model cross dependencies entities re- lations . deﬁne input text output triples source target sequence . shown Figure   , source se- quence simply consists tokens input sentence like “ [ CLS ] United States President Trump raised borough Queens ... [ SEP ] ” . concatenate triples entity/relation separated special token token [ S S SEQ ] target sequence . also add begin- ning ( [ SOS ] ) end ( [ EOS ] ) tokens target se- quence : [ SOS ] h (   ) , r (   ) , (   ) . . . [ S S SEQ ] h (   ) , r (   ) , (   ) . . . [ S S SEQ ] h (   ) , r (   ) , (   ) . . . [ S S SEQ ] ... h ( N ) , r ( N ) , ( N ) . . . [ EOS ] , generated tokens contain extracted triples . model CGT consists three components , follows : Input Encoder . utilize input representation BERT ( Devlin et al .      ) tokenize texts WordPiece ( Yonghui et al .      ) . compute repre- sentation summing corresponding token embedding , position embedding , segment embedding . Generative Transformer . use partial causal masking distinguish encoder-decoder representations . in- ference , leverage beam search ( Wiseman Rush      ) generate multiple triples . Triplet Contrastive Learning . introduce triplet contrastive object enhance faithfulness generated triples . introduce batch-wise dynamic attention mask- ing mechanism joint optimization . also provide triple-wise calibrating algorithm faithful triple gen- eration . Model hi , ri , ti refer i-th generated head entity , Input Encoder relation , tail entity . Framework denote sequence input source tokens x  , x  , ... , xm sequence target tokens y  , y  , ... , yn . Note Given input text x , add special start-of-sequence token [ SOS ] beginning target input . use representation whole input output vector . Furthermore , append special token , namely , end-of- sequence [ EOS ] , end output sequence . Token EmbeddingPosition EmbeddingSegment Embedding [ CLS ] United States PresidentTrump raised boroughof Queens New York City , andlived age    . [ SEP ] Transformer Encoder  Transformer Encoder NTransformer Encoder  h h h h h h S S S S Generative TransformerTriplet Contrastive Learning [ Negative triple ] : ( Trump , president_of , Queens ) ( Trump , born_in ,    ) ( Trump , live_in ,    ) [ Positive triple ] : ( Trump , president_of , United States ) ( Trump , born_in , Queens ) ( Trump , live_in , Queens ) [ SOS ] Trump- > president- > of- > United- > States- > [ S S_SEQ ] - > Trump , born- > in- > Queens- > [ S S_SEQ ] - > Trump- > live- > in- > Queens [ EOS ] Input Encoder [ EOS ] token used special token terminate de- coding process triple generation . input representation one used BERT ( Devlin et al .      ) . tokenize text subword units using WordPiece ( Yonghui et al .      ) . example , word , “ forecasted , ” split “ forecast ” “ # # ed , ” “ # # ” refers pieces belong one word . com- pute input token vector representation summing corresponding token embedding , position embedding , segment embedding . Generative Transformer utilize transformer architecture backbone en- code contextual features consist stacked self- attention layers . paper , use   -layer trans- former architecture single shared transformer module encoder-decoder-based generation . input vec- ( cid:  ) . i=  , ﬁrstly feed H  = ( cid:  ) s  , · · · , s|L| tors , { si } |L| , use transformer encode input : Hl = Transformerl ( cid:  ) Hl−  ( cid:  ) . (   ) multiple self-attention heads trans- former block used aggregate output vec- tors previous layer . compute output self- attention head , Al , l-th transformer layer follows : Ql = Hl− WQ l , Kl = Hl− WK l . Mij = ( cid:   )   , −∞ , allow attend prevent attending (   ) (   ) ( cid:   ) ( cid:  ) , (   ) + ( cid:  ) Hl− Vl Al = softmax ( cid:   ) QlK ( cid:   ) √ dk Ql , Kl , Vl ∈ Rdh×dk matrices projection previous layer ’ output . mask ma- trix , ∈ R|L|×|L| , aimed control context attended token . Speciﬁcally , leverage differ- ent mask matrices , , computing contextualized representation . illustrated examples Figure   , triple generation , leverage partial causal masking , upper right part set −∞ block atten- tion source segment target segment ; left part set  s indicates tokens able attend ﬁrst segment . utilize cross-entropy lossgeneration optimize triple generation procedure . also utilize masking strategies elements mask matrix  s triplet contrastive learning . Details provided next section . Formally , gener- ative transformer obtain contextualized representations optimize following object : ˆx  , ˆx  , . . . , ˆxm , ˆy  . . . , ˆyn = Transformer ( x  , x  , . . . , xm , y  , . . . , yn ) (   ) lossgeneration = ( cid:   )  ( cid:   ) (   xilog ( ˆxi ) + n ( cid:   )   yilog ( ˆyi ) ) (   ) Algorithm   Triplet Contrastive Learning   : Require : Train instances X = x  , ... , xN , labels = y  , ... , yN , batch size k , P OS = Φ , N EG = Φ , tem- perature batch = [ ( x , )   , .. , ( x , ) k ] ( x , ) j batch POS = decompose triple ( yj ) pos POS   : ≤ N/k   :   :   :   :   :   :   :    :    :    :    :    :    : return DataLoader neg = random permute entity ( pos ) l pos = Contrastive Classify ( x , pos ) l neg = Contrastive Classify ( x , neg ) z = cat ( [ l pos , l neg ] , dim=  ) labels = zeros (   ) loss = CrossEntropyLoss ( z/t , labels ) loss.backward ( ) update ( Contrastive Classiﬁer.params ) Triplet Contrastive Learning previous generation-based approach usually neglects fact triple faithful consistent input sentence . example , given instance “ Obama born Honolulu , ” engorge model gener- ate triples like “ ( Obama , born , Honolulu ) ” rather “ ( Obama , live , Honolulu ) , ” though latter may cor- rect induced given sentence . Moti- vated , introduce triplet contrastive learning enhance faithfulness generated triples . speciﬁc , leverage triple contrastive learn- ing binary classiﬁcation  s masking . use gold triples positive instances generate corrupt triples replacing one entity random tokens instances . use corrupt triples negative instances . con- catenate input sentence one triple x  , x  , ... , xm [ SEP ] , hi , ri , ti feed input encoder . utilize representation [ CLS ] MLP layer compute classiﬁcation logits z . utilize cross-entropy optimization losscontrastive : ( cid:   ) ( z+ log ( ˆz+ ) + (  −z− losscontrastive = ) ) ) (   ) positive negative logits , respectively . Formally , triplet contrastive learning algo- rithm triple extraction shown Algorithm   . ˆz− ˆz+ ) log ( (  − ˆz− Training Inference Details training stage , entities relations to- kens vocabulary , whereas [ S S SEQ ] , [ SOS ] , [ EOS ] unused tokens ( e.g. , [ unused  ] ) . split entity relation label mentions different tokens dur- ing data preprocessing procedure , meaning entity relation may contain multiple tokens . Note triplet contrastive learning triple generation two different tasks , optimizing jointly non- trivial , owing leakage generated labels . exam- Algorithm   Batch-wise Dynamic Attention Masking   : Require : Train instances X = x  , ... , xN , labels = y  , ... , yN , negative instances ( cid:   ) , batch size k sampling ratio γ old batch = [ ( x , , ( cid:   ) )   , .. , ( x , , ( cid:   ) ) k ] ( x , , ( cid:   ) ) j old batch condition = Bernoulli ( γ ) condition ==     : ≤ N/k   :   :   :   :   :   :   : else instance = Zero Mask ( ( x , , ( cid:   ) ) j ) instance = Partial Causal Mask ( ( x , , ( cid:   ) ) j ) batch ← batch ∩ instance    : DataLoader ← DataLoader ∩ batch    : batch = Φ    :    : return DataLoader ple , optimize generation contrastive learning instance , model see tokens be- cause  s masking . address issue , intro- duce batch-wise dynamic attention masking . , sample instances Bernoulli distribution generation instances , rest sampled contrastive learning sen- tences . Formally , algorithm shown Algorithm   . overall optimization object follows : Dataset NYT WebNLG MIE Domain Relation Triplets News       ,    Web       ,    Medical       ,    Table   : Statistics four datasets domain , number relation types , triple number . nal dataset splitting NYT , WebNLG , MIE . Detailed statistics three datasets shown Table   . Settings utilized UniLM-base-uncased English  Chinese  datasets . utilized Pytorch implement CGT model conducted experiments using four Nvidia     -Ti graphical processing units . employed Adam ( Kingma Ba      ) optimizer . initial learning rate set  e-  , reduced rate    % every eight epochs . batch size    En- glish    Chinese , total number epochs    datasets . beam size set   , α set  .  , γ set  .  , θ set  .  . care- fully tuned hypermeters valid set ( Detailed search space supplementary materials ) . loss = lossgenerative + αlosscontrastive (   ) Baselines Evaluation Metrics α hyperparameter balance different objects . inference stage , ﬁrst generate triplet se- quences via beam search ( Wiseman Rush      ) . , introduce triple-wise calibrating algorithm ﬁlter- unfaithful triples . calculate matching score contrastive classiﬁer ﬁlter triples match score < θ . Besides , also leverage heuristic rules generate reasonable triples relation followed head entities . Experiment Dataset conducted experiments three benchmark datasets : New York Times ( NYT ) , WebNLG  , MIE  . NYT dataset produced using distant supervision method widely used triplet extraction ( Riedel , Yao , McCal- lum      ) . contains   ,    sentences training ,  ,    sentences validation ,  ,    sentences test . WebNLG dataset ( Gardent et al .      ) used natu- ral language generation , later used triplet ex- traction ( Zeng et al .     a ) . consists  ,   /   /    instances training , validation , testing , respectively . MIE ( Zhang et al .     d ) large-scale Chinese dialogue information extraction dataset medical domain . contains     instances training ,     instances val- idation ,     instances testing . used origi- compared performance CGT various baseline models evaluated performance precision , recall , F  score . CGT ( Random ) CGT ( UniLM ) refer model initialized randomly , model initialized UniLM , respectively . Generative Baseline Models : CopyRE ( Zeng et al .     a ) Seq Seq learning frame- work copy mechanism wherein multiple decoders applied generate triples handle overlapping rela- tions . PNDec ( Nayak Ng      ) provides two novel ap- proaches using encoder-decoder architecture triples hav- ing multiple tokens . CopyMTL ( Zeng , Zhang , Liu      ) proposes mul- titask learning framework used complete entities . Extractive Baselines : Tagging ( Zheng et al .     b ) end-to-end method uses novel tagging scheme . HRL ( Takanobu et al .      ) addresses relation extrac- tions regarding related entities arguments relation via hierarchical reinforcement learning . MrMep ( Chen et al .      ) approach utilizes triplet attention exploit connections relations corresponding entity pairs . CasRel ( Wei et al .     a ) approach models re- lations functions , map subjects objects sen- tence .  https : //github.com/weizhepei/CasRel  https : //github.com/nlpir    /MIE-ACL-      https : //github.com/microsoft/unilm  https : //github.com/YunwenTechnology/Unilm Model Extractive Tagging ( Zheng et al .     a ) HRL ( Takanobu et al .      ) MrMep ( Chen et al .      ) CasRel ( Wei et al .     b ) Generative CopyRE ( Zeng et al .     a ) PNDec ( Nayak Ng      ) CopyMTL ( Zeng , Zhang , Liu      )  CGT ( Random ) CGT ( UniLM ) w/o contrastive NYT R   .    .    .    .    .    .    .    .    .    .  P   .    .    .    .    .    .    .    .    .  *   .  F   .    .    .    .    .    .    .    .    .    .  WebNLG R P -   .    .    .    .    .    .    .    .  *   .  -   .    .    .    .    .    .    .    .    .  F -   .    .    .    .    .    .    .    .    .  Table   : Main results NYT WebNLG . top section refers extractive models , middle section indicates generative approaches , bottom model different settings . * indicates pvalue <  .   paired t-test evaluation . Model P R Bi-LSTM MIE-multi CGT ( random ) CGT ( UniLM )   .     .     .     .     .     .     .     .   F    .     .     .     .   Table   : Main results MIE dataset . Bi-LSTM ( Zhang et al .     d ) baseline approach utilizes bi-directional long-short term memory network information extraction . MIE-multi ( Zhang et al .     d ) another baseline model uses max-pooling operation obtain ﬁnal score , considering turn-interaction . Main Results Table   , observe approach achieved sig- niﬁcant improvements compared generation-based baseline models NYT WebNLG datasets . CGT model relative   .  F  score improvement NYT compared PNDec , relative   .  F  score improvement NYT compared CopyMTL , illustrat- ing power proposed model . approach also ob- tained comparable results compared extractive models , CasRel . Note search space generative model much larger extractive ones , indi- cates generative model challenging optimize extractive approaches . contrast , generative methods generate triples beyond entity relation domain , promising open domain setting . empir- ical results reveal generative approach could obtain comparable performance extractive models , motivating future research directions . Table   , observe approach achieved sig- niﬁcant improvements ( relative   .   F  score ) compared baselines MIE dataset . MIE dialogue- based information-extraction dataset challenging optimize . Thus , argue CGT implicitly model relations among entities , boosting performance . Ablation Study conducted ablation studies demonstrate ef- ﬁcacy different strategies model . Table   , notice performance decayed without contrastive ob- ject , illustrates triplet contrastive learning en- hance faithfulness generated triples , thus boosting performance . also observe approach ran- dom initialization CGT ( Random ) achieves signiﬁcantly bet- ter performance generative baselines three bench- mark datasets , indicates improve- ments pre-trained language model also model architecture . Analysis better analyze performance proposed CGT model , conducted detailed analysis attempted answer questions whether CGT capture long-term dependence . Intrusively , transformers self- attention better capture long-term dependencies RNNs . investigate issue , evaluated instances different lengths . Figure   , notice mod- els performance decay sentence length in- creases , indicates sequence generation chal- lenging input sentence long . observe approach could obtain better performance CopyRE sentence length increased . sen- tence longer    , CopyRE archived worse perfor- mance , CGT performed relatively better . demon- strates proposed approach capture long-term de- pendencies , compared RNN-based approaches . Error Analysis analyze drawbacks approach pro- mote future works triple extraction , select instances conduct error analysis . random select incorrect in- stances classify three categories bellows , shown Table   : ( ) NYT ( b ) WebNLG ( c ) MIE Figure   : Model performance # sentence length . Instance Batchoy originates Philippines served soup.Its main ingredients noodles , instance #   pork organs , vegetables , chicken , shrimp beef . generated triple : ( cid:    ) Batchoy , location , Philippines ( cid:    ) ( cid:    ) Batchoy , country , Philippines ( cid:    ) ground truth : instance #   Alan Shepard crew member NASA operated Apollo    died California represented Dianne Feinstein . generated triple : ( cid:    ) Shepard , deathPlace , California ( cid:    ) ground truth : ( cid:    ) Allan Shepard , deathPlace , California ( cid:    ) instance #   Saranac Lake , served Adirondack Regional Airport , part Harrietstown , Essex County , New York , US . generated triple : ( cid:    ) Airport , cityServed , New York ( cid:    ) ground truth : ( cid:    ) Airport , cityServed , York ( cid:    ) Table   : Error anslysis . Distract Context . instance #   shows , observe approach may fail ambiguous contexts may expressed similar context differ ﬁne- grained type entities . argue may caused unbalanced learning problems models tend judge sentence similar context high-frequency relations . Wrong Boundaries . instance #   shows , generated triples incorrect boundaries , indicates difﬁ- culty entity recognition triple extraction . argue since approach end-to-end generation method , challenging capture ﬁne-grained entity boundaries without sequence token information . Wrong Triples . instance #   shows , many generated triples entities exist gold-standard set . Generally , occurs sentences multiple triples . WebNLG datasets noisy , several cases pro- duced incorrect results . leave future works suitable benchmarks . Conclusion Future Work paper , revisited triple extraction sequence generation task , jointly extracts entities relations . address long-term dependence faithfulness is- sues , proposed novel CGT model generate faith- ful triples . best knowledge , ﬁrst integrate sequence generation contrastive learning information extraction , may inspire future research directions motivate new ideas . Experimental results three datasets demonstrated efﬁcacy approach . future , utilize stronger transformer architec- tures , Longformer ( Beltagy , Peters , Cohan      ) generate relational knowledge documents . also delve injection ontology knowledge using condi- tion generation methods . also useful apply approach scenarios , event extractions . Acknowledgments  want viewers  feedback paper . work     YFB       /NSFC        /NSFCU  B     . express gratitude anonymous re- hard work kind comments .  funded thank Ning Ding helpful discussions                                         f  ( % ) sentence length CGT CopyRE                              f  ( % ) sentence length CGT CopyRE                                  f  ( % ) sentence character length CGT MIE-multi References Bekoulis , G. ; Deleu , J. ; Demeester , T. ; Develder , C .      . Joint entity recognition relation extraction multi-head selection problem . Expert Systems Applica- tions     :   –   . Beltagy , I. ; Peters , M. E. ; Cohan , .      . Long- arXiv preprint former : long-document transformer . arXiv:    .      . Bunescu , R. C. ; Mooney , R. J .      . shortest path Proceedings dependency kernel relation extraction . conference human language technology em- pirical methods natural language processing ,    –    . Association Computational Linguistics . Cao , Y. ; Huang , L. ; Ji , H. ; Chen , X. ; Li , J .     a . Bridge Text Knowledge Learning Multi-Prototype Entity Proceedings   th Annual Mention Embedding . Meeting Association Computational Linguistics ( Volume   : Long Papers ) ,     –     . Vancouver , Canada : Association Computational Linguistics . doi:  .     / v /P  -     . URL https : //www.aclweb.org/anthology/P  -      . Cao , Z. ; Wei , F. ; Li , W. ; Li , S.     b . Faithful original : Fact aware neural abstractive summarization . arXiv preprint arXiv:    .      . Chen , J. ; Yuan , C. ; Wang , X. ; Bai , Z .      . MrMep : Joint Extraction Multiple Relations Multiple Entity Pairs Based Triplet Attention . Proceedings   rd Conference Computational Natural Language Learning ( CoNLL ) ,    –    . Chen , Y.-C. ; Gan , Z. ; Cheng , Y. ; Liu , J. ; Liu , J .      . Distilling Knowledge Learned BERT Text Generation . Proceedings   th Annual Meeting Associa- tion Computational Linguistics ,     –     . Cho , K. ; Van Merri¨enboer , B. ; Gulcehre , C. ; Bahdanau , D. ; Bougares , F. ; Schwenk , H. ; Bengio , .      . Learning phrase representations using RNN encoder-decoder sta- tistical machine translation . arXiv preprint arXiv:    .     . Dai , D. ; Xiao , X. ; Lyu , Y. ; Dou , S. ; , Q. ; Wang , H.     a . Joint extraction entities overlapping re- lations using position-attentive sequence labeling . Pro- ceedings AAAI Conference Artiﬁcial Intelligence , volume    ,     –     . Dai , D. ; Xiao , X. ; Lyu , Y. ; Dou , S. ; Wang , H.     b . Joint Extraction Entities Overlapping Relations Using Position-Attentive Sequence Labeling . Proceedings AAAI Conference Artiﬁcial Intelligence    :     –     . Devlin , J. ; Chang , M.-W. ; Lee , K. ; Toutanova , K.      . BERT : Pre-training Deep Bidirectional Transformers Language Understanding . Proceedings      Con- ference North American Chapter Association Computational Linguistics : Human Language Technolo- gies , Volume   ( Long Short Papers ) ,     –     . Min- neapolis , Minnesota : Association Computational Lin- guistics . doi:  .     /v /N  -     . Dong , L. ; Yang , N. ; Wang , W. ; Wei , F. ; Liu , X. ; Wang , Y. ; Gao , J. ; Zhou , M. ; Hon , H.-W.      . Uniﬁed language model pre-training natural language understanding generation . Advances Neural Information Processing Systems ,      –      . Du , X. ; Rush , A. ; Cardie , C.      . Document-level Event-based Extraction Using Generative Template-ﬁlling Transformers . arXiv preprint arXiv:    .      . Gardent , C. ; Shimorina , A. ; Narayan , S. ; Perez- Beltrachini , L.      . Creating training corpora nlg micro-planning .   th annual meeting Association Computational Linguistics ( ACL ) . Gu , J. ; Wang , C. ; Zhao , J .      . Levenshtein trans- former . Advances Neural Information Processing Sys- tems ,      –      . Gunel , B. ; Zhu , C. ; Zeng , M. ; Huang , X .      . Mind Facts : Knowledge-Boosted Coherent Abstractive Text Summarization . arXiv preprint arXiv:    .      . , Z. ; Chen , W. ; Li , Z. ; Zhang , M. ; Zhang , W. ; Zhang , M.      . SEE : Syntax-aware Entity Embedding Neural Relation Extraction . Proceedings AAAI . Hendrickx , I. ; Kim , S. N. ; Kozareva , Z. ; Nakov , P. ; S´eaghdha , D. O. ; Pad´o , S. ; Pennacchiotti , M. ; Romano , L. ; Szpakowicz , S.      . Semeval-     task   : Multi-way classiﬁcation semantic relations pairs nomi- nals . arXiv preprint arXiv:    .      . Katiyar , A. ; Cardie , C.      . Going limb : Joint extraction entity mentions relations without depen- dency trees . Proceedings   th Annual Meeting Association Computational Linguistics ( Volume   : Long Papers ) ,    –    . Kingma , D. P. ; Ba , J .      . Adam : method stochastic optimization . arXiv preprint arXiv:    .     . Krause , B. ; Gotmare , A. D. ; McCann , B. ; Keskar , N. S. ; Joty , S. ; Socher , R. ; Rajani , N. F.      . Gedi : Generative discriminator guided sequence generation . arXiv preprint arXiv:    .      . Lample , G. ; Ballesteros , M. ; Subramanian , S. ; Kawakami , K. ; Dyer , C.      . Neural Architectures Named En- tity Recognition . HLT-NAACL . Li , J. ; Wang , R. ; Zhang , N. ; Zhang , W. ; Yang , F. ; Chen , H.      . Logic-guided Semantic Representation Learning Proceedings Zero-Shot Relation Classiﬁcation .   th International Conference Computational Lin- guistics ,     –     . Barcelona , Spain ( Online ) : Interna- tional Committee Computational Linguistics . URL https : //www.aclweb.org/anthology/    .coling-main.    . Li , Q. ; Ji , H.      . Incremental joint extraction en- tity mentions relations . Proceedings   nd An- nual Meeting Association Computational Linguis- tics ( Volume   : Long Papers ) ,    –    . Lin , Y. ; Liu , Z. ; Sun , M.      . Neural relation extrac- Proceedings ACL , tion multi-lingual attention . volume   ,   –   . Lin , Y. ; Shen , S. ; Liu , Z. ; Luan , H. ; Sun , M.      . Neu- ral relation extraction selective attention instances . Proceedings ACL , volume   ,     –     . Miwa , M. ; Bansal , M.      . End-to-End Relation Ex- traction using LSTMs Sequences Tree Structures . Proceedings ACL , volume   ,     –     . Nadeau , D. ; Sekine , S.      . survey named entity recognition classiﬁcation . Nayak , T. ; Ng , H. T.      . Effective Modeling Encoder-Decoder Architecture Joint Entity Relation Extraction . arXiv preprint arXiv:    .      . Raffel , C. ; Shazeer , N. ; Roberts , A. ; Lee , K. ; Narang , S. ; Matena , M. ; Zhou , Y. ; Li , W. ; Liu , P. J .      . Explor- ing limits transfer learning uniﬁed text-to-text transformer . arXiv preprint arXiv:    .      . Riedel , S. ; Yao , L. ; McCallum , .      . Modeling rela- tions mentions without labeled text . Joint Euro- pean Conference Machine Learning Knowledge Dis- covery Databases ,    –    . Springer . Roth , D. ; Yih , W.-t.      . Global inference entity relation identiﬁcation via linear programming formu- Introduction statistical relational learning    – lation .     . Schuster , M. ; Paliwal , K. K.      . Bidirectional recur- rent neural networks . IEEE transactions Signal Process- ing    (    ) :     –     . Takanobu , R. ; Zhang , T. ; Liu , J. ; Huang , M.      . hi- erarchical framework relation extraction reinforce- ment learning . AAAI , volume    ,     –     . Wang , Z. ; Wen , R. ; Chen , X. ; Huang , S.-L. ; Zhang , N. ; Finding inﬂuential instances Zheng , .      . arXiv preprint distantly supervised relation extraction . arXiv:    .      . Wei , Z. ; Jia , Y. ; Tian , Y. ; Hosseini , M. J. ; Steedman , M. ; Chang , .     a . Novel Cascade Binary Tagging Frame- Proceedings work Relational Triple Extraction . ACL      . Wei , Z. ; Su , J. ; Wang , Y. ; Tian , Y. ; Chang , .     b . Novel Cascade Binary Tagging Framework Relational Triple Extraction . Proceedings ACL ,     –     . Wiseman , S. ; Rush , A. M.      . Sequence-to-sequence arXiv preprint learning beam-search optimization . arXiv:    .      . Wu , T. ; Li , X. ; Li , Y.-F. ; Haffari , R. ; Qi , G. ; Zhu , Y. ; Xu , G.      . Curriculum-Meta Learning Order-Robust Con- tinual Relation Extraction . arXiv preprint arXiv:    .      . Yonghui , W. ; Schuster , M. ; Chen , Z. ; Le , Q. ; Norouzi , M. ; Macherey , W. ; Krikun , M. ; Cao , Y. ; Gao , Q. ; Macherey , K. ; et al .      . Bridging gap human machine translation . arXiv preprint arXiv:    .      . Zeng , D. ; Liu , K. ; Chen , Y. ; Zhao , J .      . Distant su- pervision relation extraction via piecewise convolutional neural networks . Proceedings EMNLP ,     –     . Zeng , D. ; Zhang , H. ; Liu , Q .      . CopyMTL : Copy Mechanism Joint Extraction Entities Relations Multi-Task Learning . AAAI ,     –     . Zeng , X. ; Zeng , D. ; , S. ; Liu , K. ; Zhao , J .     a . Ex- tracting relational facts end-to-end neural model copy mechanism . Proceedings ACL ,    –    . Zeng , X. ; Zeng , D. ; , S. ; Liu , K. ; Zhao , J .     b . Extracting Relational Facts End-to-End Neural Model Copy Mechanism . Proceedings ACL ,    –    . Melbourne , Australia : Association Computational Lin- doi:  .     /v /P  -     . URL https : //www . guistics . aclweb.org/anthology/P  -     . Zhang , N. ; Deng , S. ; Bi , Z. ; Yu , H. ; Yang , J. ; Chen , M. ; Huang , F. ; Zhang , W. ; Chen , H.     a . OpenUE : Pro- Open Toolkit Universal Extraction Text . ceedings      Conference Empirical Methods Natural Language Processing : System Demonstrations ,  –   . Online . doi:  .     /v /    .emnlp-demos.  . Zhang , N. ; Deng , S. ; Li , J. ; Chen , X. ; Zhang , W. ; Chen , H.     b . Summarizing Chinese Medical Answer Graph Convolution Networks Question-focused Findings Association Com- Dual Attention . putational Linguistics : EMNLP      ,   –   . Online . doi :   .     /v /    .ﬁndings-emnlp.  . Zhang , N. ; Deng , S. ; Sun , Z. ; Chen , J. ; Zhang , W. ; Chen , H.     c . Relation Adversarial Network Low Resource Knowledge Graph Completion . Proceedings Web Conference      ,  –   . Zhang , N. ; Deng , S. ; Sun , Z. ; Chen , X. ; Zhang , W. ; Chen , H.      . Attention-Based Capsule Networks Dynamic Routing Relation Extraction . Proceedings EMNLP . Zhang , N. ; Deng , S. ; Sun , Z. ; Wang , G. ; Chen , X. ; Zhang , W. ; Chen , H.     a . Long-tail Relation Extraction via Knowledge Graph Embeddings Graph Convolution Net- works . arXiv preprint arXiv:    .      . Zhang , S. ; Duh , K. ; Van Durme , B .      . Selective decoding cross-lingual open information extraction . Proceedings IJCNLP ,    –    . Zhang , Y. ; Jiang , Z. ; Zhang , T. ; Liu , S. ; Cao , J. ; Liu , K. ; Liu , S. ; Zhao , J .     d . MIE : Medical Information Ex- tractor towards Medical Dialogues . Proceedings ACL ,     –     . Zhang , Y. ; Merck , D. ; Tsai , E. B. ; Manning , C. D. ; Lan- glotz , C. P.     b . Optimizing factual correctness summary : study summarizing radiology reports . arXiv preprint arXiv:    .      . Zheng , S. ; Wang , F. ; Bao , H. ; Hao , Y. ; Zhou , P. ; Xu , B .     a . Joint Extraction Entities Relations Based Novel Tagging Scheme . CoRR abs/    .      . URL http : //arxiv.org/abs/    .      . Zheng , S. ; Wang , F. ; Bao , H. ; Hao , Y. ; Zhou , P. ; Xu , B .     b . Joint Extraction Entities Relations Based Novel Tagging Scheme . Proceedings ACL ,     –     . Zhu , C. ; Hinthorn , W. ; Xu , R. ; Zeng , Q. ; Zeng , M. ; Huang , X. ; Jiang , M.      . Boosting factual correctness arXiv abstractive summarization knowledge graph . preprint arXiv:    .      . 