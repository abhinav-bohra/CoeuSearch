Learning-Based Dimensionality Reduction Computing Compact Effective Local Feature Descriptors Hao Dong Xieyuanli Chen Mihai Dusmanu Viktor Larsson Marc Pollefeys Cyrill Stachniss         p e      ] V C .  c [   v           .         : v  X r  Abstract— distinctive representation image patches form features key component many computer vision robotics tasks , image matching , image retrieval , visual localization . State-of-the-art descriptors , hand- crafted descriptors SIFT learned ones HardNet , usually high dimensional ;     dimensions even . higher dimensionality , larger memory consumption computational time approaches using descriptors . paper , investigate multi- layer perceptrons ( MLPs ) extract low-dimensional high- quality descriptors . thoroughly analyze method unsupervised , self-supervised , supervised settings , eval- uate dimensionality reduction results four representative descriptors . consider different applications , including visual localization , patch veriﬁcation , image matching retrieval . experiments show lightweight MLPs achieve better dimensionality reduction PCA . lower-dimensional descriptors generated approach outperform original higher-dimensional descriptors downstream tasks , especially hand-crafted ones . code available https : //github.com/PRBonn/descriptor-dr . . INTRODUCTION Local feature descriptors [    ] , [   ] , [    ] used rep- resent characteristics image patches designed robust partial occlusions , viewpoint changes , variations illumination . play essential role many robotics applications robot localization [    ] , object recognition [    ] , image retrieval [    ] . traditional pipeline local feature extraction often starts detecting position , scale , orientation keypoints image . , normalized image patch extracted respect estimated keypoint , usually provides basis descriptor computation . Distinctive invariant keypoints descriptors key achieving good performance subsequent matching , retrieval , localization tasks . paper focuses descriptor part pipeline , speciﬁcally dimensional- ity reduction local feature descriptors generate compact time effective features . Multiple visual feature descriptors introduced literature . Among hand-crafted descriptors , SIFT [    ] one famous robustness blurring , translation , rotation , scale changes . advent neural networks , learning- based descriptors proposed [    ] , [    ] , [    ] , [    ] H. Dong , M. Dusmanu , M. Pollefeys ETH Z¨urich , Switzer- land . X. Chen C. Stachniss University Bonn , Germany . V . Larsson Lund University , Sweden . M. Pollefeys additionally Microsoft . C. Stachniss additionally Department Engineering Science University Oxford , UK , Lamarr Institute Machine Learning Artiﬁcial Intelligence , Germany . Fig .   : Overview approach . ﬁrst compute descriptors given image patches . MLP-based network used dimensionality reduction . aim learn MLP-based projection better PCA generate lower-dimensional descriptors . pushed state-of-the-art forward benchmarks image matching , patch veriﬁcation , image retrieval . results promising , common issue hand-crafted learned methods dimension- ality generated descriptors usually high . image database increases , substantial time space might needed computing storing high- dimensional descriptors . may hinder application mobile resource-constraint robots . Several principal component analysis ( PCA ) [    ] based dimensionality reduc- tion methods proposed alleviate problem . example , Valenzuela et al . [    ] apply PCA reduce dimensionality SIFT SURF descriptors . Instead original SIFT ’ smoothed weighted histograms , Ke et al . [    ] apply PCA patches generating lighter descrip- tors . PCA performs linear projection high- low- dimensional descriptor space , limited capabilities generating high-quality dimensionally reduced descriptors . Moreover , components low eigenvalues PCA necessarily less important down-weighted even eliminated , cause information loss performance degradation . Unlike previous PCA-based methods , aim learn MLP-based dimensionality reduction better transform large descriptors lighter ones . end , thoroughly analyze MLP-based network dimensionality reduction design three learning schemes , including unsupervised , self-supervised , supervised methods . unsuper- vised scheme , use auto-encoder reconstruction distance losses improve projection . self-supervised one , iteratively cluster descriptors using k-means use cluster assignments pseudo-labels train MLPs . also propose supervised method uses ground truth patch labels triplet loss supervise MLPs generating distinctive lower-dimensional descriptors . evaluate MLP-based method proposed learning schemes four common descriptors : SIFT [    ] , MKD [    ] , TFeat [   ] , HardNet [    ] . train       network one dataset apply directly datasets different downstream tasks , including visual localization , patch veriﬁcation , image matching , image retrieval . experimental results show method consistently outperforms PCA-based method strong generalization ability . Furthermore , using lower- dimensional descriptors generated supervised MLP , achieve even better performance downstream tasks original higher-dimensional descriptors , especially hand-crafted ones faster speed less memory consumption . Overall , contributions follows : • propose evaluate MLP-based network descriptor dimensionality reduction show superi- ority PCA multiple descriptors various tasks ; • demonstrate lighter descriptors super- vised MLP projection achieve even better performance downstream tasks original descriptors ; • thoroughly analyze improvement using method different descriptors multiple datasets show good generalization method . II . RELATED WORK Various local image descriptors proposed past decades ranging hand-crafted ones like SIFT [    ] , BRIEF [   ] , ORB [    ] learned ones like TFeat [   ] , HardNet [    ] , SOSNet [    ] . Hand-crafted descriptors typically based human insights qualities invariant certain transformations , differential moment invariants , correlations , gradients histograms . example , SIFT Lowe [    ] generates descriptors based gradient distribution detected patches . BRIEF Calonder et al . [   ] uses simple binary intensity comparisons pixels image patch . details classical hand-crafted descriptors found surveys [    ] , [    ] . Beneﬁting large-scale learning-based descriptors recently achieved datasets [   ] , state-of-the-art performances [    ] , [   ] , [    ] , [    ] , [    ] , [    ] . example , TFeat Balntas et al . [   ] uses CNN hard-negative mining anchor swap triplet loss compute descriptors . contrast , Tian et al . [    ] propose L -Net , adding different error terms loss function improve distinctiveness descriptors . HardNet Mishchuk et al . [    ] also uses simple triplet margin loss hard negative mining outperforms descriptors advanced sampling procedure . , state-of-the-art hand-crafted learning-based descriptors often high dimensional . Dimensionality reduction techniques used shorten dimensionalities feature descriptors . Tradi- tional dimensionality reduction usually refers reducing dimension data keeping much information possible . Classical examples backward elimination [    ] , forward selection [    ] , random forests [   ] . Another type ﬁnd combination new features describe data . example . linear dimensionality reduction methods include PCA [    ] , factor analysis [    ] , linear dis- criminant analysis [   ] , non-linear methods including Fig .   : pipeline using auto-encoder unsupervised dimensionality reduction . consists encoder decoder . encoder maps original descriptors lower-dimensional descriptors . decoder tries reconstruct original descriptors projected lower-dimensional descriptors . Kernel PCA [    ] , t-distributed stochastic neighbor embed- ding ( t-SNE ) [    ] , isometric mapping [    ] . Among , PCA [    ] widely used dimensionality reduction image patch-based descriptors . example , Gil et al . [    ] Valenzuela et al . [    ] use PCA reduce dimensionality SIFT SURF descriptors . Ke et al . [    ] apply PCA patches instead using smoothed weighted histograms SIFT . works using neural networks dimensionality reduction . work related one Loquercio et al . [    ] , uses supervised method train linear projection . However , work hand-crafted descriptors like FREAK [   ] focus visual localization task . Different , thoroughly analyze MLP-based non-linear pro- jections using unsupervised , self-supervised , supervised training schemes multiple downstream tasks . Moreover , method works hand-crafted descriptors SIFT [    ] MKD [    ] , learned ones TFeat [   ] HardNet [    ] , shows strong generalization ability . III . METHODOLOGY aim reduce dimensionality local feature descriptors using MLP network . understand ability MLP-based method task , investigate three learning schemes , including unsupervised , self-supervised , introduce supervised methods . section principle approach detail . A. Unsupervised Reduction Auto-encoders [   ] unsupervised learning techniques dimensionality reduction . , use see whether MLP-based network learn good projection high low dimensionality unsupervised way . achieve fast lightweight dimensionality reduction , build auto-encoder using MLPs two hidden layers shown Fig .   . auto-encoder consists symmetric encoder decoder . encoder projects input feature lower-dimensional embeddings , decoder reconstructs original input lower- dimensional embeddings . checking consistency be- tween inputs outputs , auto-encoder learns extract lower-dimensional descriptors original ones Fig .   : Pipeline self-supervised method . iteratively cluster descriptors use clustering assignments pseudo-labels train network . without labels . apply consistency constraints min- imizing reconstruction loss inputs outputs . reconstruction loss LR measures differences input descriptors { xi } N i=  training mini-batch reconstructed output descriptors { x ( cid:   ) } N i=  LR =   N N ( cid:   ) i=  ( cid:    ) xi − x ( cid:   ) ( cid:    )   . (   ) propose additional distance loss LD Hard- Net . distance loss calculates difference distance original high-dimensional descriptors distance lower-dimensional descriptors embedding space . Given two descriptors xi xj , corresponding lower-dimensional descriptors embedding space ˆxi ˆxj . loss LD make ( cid:   )   distance lower-dimensional descriptors ( ˆxi , ˆxj ) similar possible compared original descriptors ( xi , xj ) LD =   N ( N −   ) ( cid:    ) ( cid:    ) ( cid:    ) ( cid:    ) N ( cid:   ) ( cid:   ) i=  j ( cid:   ) =i ( ( xi , xj ) − ( ˆxi , ˆxj ) )   , (   ) ( xi , xj ) = ( cid:    ) xi −xj ( cid:    )   ( cid:   )   distance loss calculated different pairs mini-batch . ﬁnal loss HardNet weighted sum reconstruction distance losses L = LR + αLD , (   ) explanation design given Sec . V . Note need information image patches train auto-encoder . Moreover , non-linearity auto-encoder allows learn better projection PCA , thus generating better lower-dimensional descriptors . shown experimental evaluation . B. Self-Supervised Reduction Unlike unsupervised methods , self-supervised methods usually use traditional heuristic-based methods generate pseudo-labels guide networks learn certain tasks . combines human priors learning-based methods achieve good performance . Inspired deep feature cluster- ing [    ] , [    ] , propose self-supervised method MLP- based dimensionality reduction . main idea apply Fig .   : Pipeline supervised method . training sample includes anchor positive negative patches . extracted descriptors fed MLPs get lower-dimensional descriptors triplet loss applied supervise MLPs . k-means clustering descriptors use clustering assignments pseudo-labels , i.e. , descriptors cluster considered label . classiﬁcation layer [    ] added training guide MLPs learning generate similar lower-dimensional descriptors high-dimensional ones pseudo-label . ﬁrst training epoch , cluster original descriptors different groups use labels supervise MLPs . , use clusters lower- dimensional descriptors previous epoch supervi- sion . Given set descriptors extracted image patches , k-means clustering generates centroid matrix C clustering assignments yi descriptor xi solving min C∈Rd×k   N N ( cid:   ) i=  min yi∈ {  ,  } k ( cid:    ) xi − Cyi ( cid:    )     , s.t . ( cid:   )  k =   , (   )  k vector whose elements   yi one-hot vector . training loss used self-supervised method standard cross-entropy loss pseudo-labels targets . C. Supervised Reduction next introduce supervised method dimension- ality reduction . shown Fig .   , use ground truth patch labels together triplet loss train MLP . batch matching patches denoted { ai , pi } i=  ... N , stands anchor patch p positive patch . non-matching negative patches { ni } i=  ... N sampled hardest-within-batch strategy introduced Mishchuk et al . [    ] . training patches , ﬁrst use existing methods generate higher-dimensional descriptors feed MLPs generate low-dimensional descriptors triplet margin loss applied N ( cid:   ) max (   , + ( f ( ai ) , f ( pi ) ) − ( f ( ai ) , f ( ni ) ) ) , LT =   N i=  (   ) f MLP-based projection . main idea learn MLP-based projection f distance anchor positive descriptors ( f ( ai ) , f ( pi ) ) smaller anchor negative de- scriptors ( f ( ai ) , f ( ni ) ) margin lower- dimensional embedding space . also add distance loss term HardNet used auto-encoder target similarity distance input descriptors embedding descriptors batch . ﬁnal loss HardNet weighted sum triplet margin distance losses L = LT + βLD . (   ) D. Training Parameters use UBC Phototour Liberty dataset [   ] training . training networks dataset , apply trained model tasks datasets without ﬁne-tuning . auto-encoder , train network   epochs using Adam [    ] learning rate  .    batch size      . choose distance loss weighting factor α =  .  HardNet . self-supervised method , train network     epochs using Adam learning rate  .    batch size     . number cluster k-means         . clustering repeated classiﬁcation layer re-initialized every    epochs . supervised method , choose margin =   train network    epochs using Adam learning rate  .    batch size      . learning rate linearly decayed zero within    epochs . choose weighting distance loss β =   HardNet . use ReLU followed batch normalization [    ] linear layer except last one . embeddings ( cid:   )  - normalized . hand-crafted descriptors , use two hidden layers MLPs , learning-based ones use one hidden layer . detailed parameters network architectures method found open- source implementation . PCA baseline , use implementation scikit-learn [    ] . IV . EXPERIMENTS present experiments show capabilities MLP-based methods different tasks . choose SIFT [    ] , MKD [    ] , TFeat [   ] , HardNet [    ] base descriptors . original dimensions     . convert descriptors lower dimensions    ,    ,    ,    apply three publicly available datasets , HPatches [   ] , Aachen Day-Night v .  [    ] , InLoc [    ] different downstream tasks , including visual localization , patch veriﬁcation , image matching , patch retrieval . name unsupervised method ‘ Ours-US ’ , self-supervised method ‘ Ours-SS ’ , supervised method ‘ Ours-SV ’ experiments . . Visual Localization ﬁrst experiment , evaluate well reduced features perform robot visual localization tasks . an- alyze different methods using two challenging localization datasets , severe illumination changes complex in- door scenes . use hierarchical localization toolbox [    ] achieve visual localization , replace feature extrac- tors generated lower-dimensional descriptors . Aachen Day-Night v .  InLoc night  .  /  .  /  .    /   /    day  .   /  .  /  .    /   /    duc   .   /  .  /  .  N / duc   .   /  .  /  .  N / distance [ ] orient . [ deg ]   .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .  SIFT   .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .  PCA-   Ours-US-     .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .  Ours-SS-     .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .  Ours-SV-     .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .  PCA-   Ours-US-     .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .  Ours-SS-     .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .  Ours-SV-     .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .  PCA-     .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .  Ours-US-     .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .  Ours-SS-     .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .  Ours-SV-     .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .  PCA-     .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .  Ours-US-     .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .  Ours-SS-     .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .  Ours-SV-     .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .  TABLE : Evaluation localization Aachen Day-Night v .  InLoc datasets SIFT [    ] features . report recall [ % ] different distances orientation thresholds . overall best results red best results low dimension blue . Aachen Day-Night v .  InLoc night  .  /  .  /  .    /   /    day  .   /  .  /  .    /   /    duc   .   /  .  /  .  N / duc   .   /  .  /  .  N / distance [ ] orient . [ deg ]   .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .  MKD   .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .  PCA-   Ours-US-     .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .  Ours-SS-     .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .  Ours-SV-     .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .  PCA-     .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .  Ours-US-     .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .  Ours-SS-     .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .  Ours-SV-     .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .  PCA-     .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .  Ours-US-     .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .  Ours-SS-     .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .  Ours-SV-     .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .  PCA-   Ours-US-     .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .  Ours-SS-     .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .  Ours-SV-     .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .  TABLE II : Evaluation localization Aachen Day-Night v .  InLoc datasets MKD [    ] features . ﬁrst day-night localization challenge , evaluate different dimensionality reduction methods Aachen Day-Night v .  dataset [    ] . contains       day-time database images old European town       queries (     taken day     night conditions ) . use code evaluation protocol [    ] report percentage day-night queries localized within given error bound estimated camera position orientation . complex indoor localization challenge , exploit InLoc dataset [    ] . challenging indoor localization dataset large differences viewpoint illumination query database images . also use code evaluation protocol [    ] report percentage queries localized within given error bound estimated camera position . Tables IV report quantitative localization results Fig .   shows qualitative results . four de- scriptors , MLP-based method auto-encoder , self- supervised , supervised learning schemes perform better terms visual localization PCA even original descriptors queries lower dimensions . indi- Aachen Day-Night v .  InLoc SIFT-PCA-   SIFT-Ours-SV-   night  .  /  .  /  .    /   /    day  .   /  .  /  .    /   /    duc   .   /  .  /  .  N / duc   .   /  .  /  .  N / distance [ ] orient . [ deg ]   .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .  TFeat   .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .  PCA-   Ours-US-     .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .  Ours-SS-     .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .  Ours-SV-     .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .  PCA-   Ours-US-     .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .  Ours-SS-     .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .  Ours-SV-     .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .  PCA-   Ours-US-     .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .  Ours-SS-     .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .  Ours-SV-     .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .  PCA-   Ours-US-     .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .  Ours-SS-     .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .  Ours-SV-     .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .  TABLE III : Evaluation localization Aachen Day-Night v .  InLoc datasets TFeat [   ] features . Aachen Day-Night v .  InLoc night  .  /  .  /  .    /   /    day  .   /  .  /  .    /   /    duc   .   /  .  /  .  N / duc   .   /  .  /  .  N / distance [ ] orient . [ deg ]   .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .  HardNet   .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .  PCA-   Ours-US-     .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .  Ours-SS-     .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .  Ours-SV-     .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .  PCA-   Ours-US-     .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .  Ours-SS-     .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .  Ours-SV-     .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .  PCA-   Ours-US-     .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .  Ours-SS-     .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .  Ours-SV-     .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .  PCA-   Ours-US-     .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .  Ours-SS-     .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .  Ours-SV-     .  /   .  /   .    .  /   .  /   .    .  /   .  /   .    .  /   .  /   .  TABLE IV : Evaluation localization Aachen InLoc datasets HardNet [    ] features . cates learned lower-dimensional descriptors distinctive invariant challenging environments , improves visual localization performance . Besides , degree improvement hand-crafted descriptors larger learned ones . analyze discuss phenomenon Sec . V . B . Applications Patch Veriﬁcation , Image Matching , Patch Retrieval experiment shows robotics applications using dimensionality-reduced features , including patch pair veriﬁcation , image matching , patch retrieval tasks HPatches dataset [   ] .     sequences  .  million patches dataset .    sequences show signiﬁcant viewpoint changes    sequences signiﬁcant illumination changes . patches divided three groups : easy , hard , tough , based levels geometric noise . Evaluation results SIFT shown Fig .   . Note , models MLP pre-trained Liberty sequence UBC Phototour test directly HPatches dataset zero-shot fashion show good generalization proposed MLP-based method . seen Fig .   , methods supervised self-supervised learning schemes perform better Fig .   : Localization SIFT-PCA-   SIFT-Ours-SV-   Aachen Day-Night v .  InLoc . image pair , left image query right image retrieved database image inlier matches , returned PnP+RANSAC . Red lines represent wrong feature matches red boxes represent wrong localization results . Green lines boxes correct . Best view color zoom . three tasks lower PCA auto-encoder dimensions large margin . Besides , also observe   -dimensional descriptors generated method even outperform original    -dimensional descriptors . Even learned   -dimensional descriptors par performances original    -dimensional SIFT descrip- tors , shows superiority proposed learned MLP-based dimensionality reduction . explanation given Sec . V . C. Ablation Study section , perform several experiments provide in-depth analysis component MLP contributes ﬁnal performance . choose different numbers hidden layers (   ,   ,   ) different sizes    ,     ,     ,     ,       . make evaluations matching task HPatches benchmark . select lower dimension    experiments , Fig .   shows TFeat HardNet . Moreover , performance hand- crafted descriptors beneﬁts complex network architec- tures , learned ones , simple network architectures achieve better results . interpret results follows . SIFT MKD , descriptor space optimized ( cid:   )   metric hand-crafted design , overlaps non-matching patches . applying PCA directly , matching non-matching patches still overlap . However , learning discriminative representation projected descriptor space using triplet distinctive lower-dimensional descriptors . might also method ’ performance hand-crafted descriptors beneﬁts complex network architecture - parameters needed rearrange descriptor space . also reason need additional distance loss hand-crafted ones . loss ,  learning-based descriptors , HardNet TFeat , relatively discriminative descriptor space already learned using triplet loss . Similar features close descriptor space , otherwise apart . Therefore , kind distinctive distance information may also preserved PCA projection . might reason different dimensionality reduction methods perform similarly learning-based descriptors . However , since descriptor space already quite regular , easy MLP overﬁt . Thus simple architectures obtain better results dimensionality reduction . added additional distance loss HardNet restrict learned descriptor space avoid overﬁtting . Since HardNet trained advanced hard-negative mining techniques TFeat , performance TFeat still improved signiﬁcantly using supervised MLP-based method . Due page limitation , put results discussion code repository : https : //github.com/PRBonn/descriptor-dr . VI . CONCLUSION paper , investigated MLP-based network unsupervised , self-supervised , supervised learning schemes dimensionality reduction local feature de- scriptors . thoroughly evaluate method four de- scriptors including hand-crafted learning-based multi- ple datasets various downstream tasks . experimental results show MLP-based projections work better PCA challenging tasks , including visual localization , patch veriﬁcation , image matching , patch retrieval   -dimensional descriptors generated cases . Besides , learning-based projections even outperform orig- inal    -dimensional descriptors . also provided ablation studies analyzed degree improvement different descriptors terms distribution descriptor space . Additional memory runtime experiments show learned lower-dimensional descriptors used saving memory consumption without adding extra runtime thus useful real-world robotics applications . Fig .   : Veriﬁcation , matching , retrieval results SIFT test set ‘ ’ HPatches dataset . None MLPs trained HPatches . Different colors represent different difﬁculties numbers average mAP values . Fig .   : Impact number size hidden layers image matching mAP Hatches dataset . SIFT , number size hidden layers , better results cases . HardNet , trend opposite . ablation study results SIFT HardNet . see number size hidden layers , better results hand-crafted descriptor . However , learned descriptor , hidden layers help . D. Runtime Memory Evaluation image patch , MLP-based method adds less  .    ms extra computational time compared cal- culating original descriptors , basically ignored . Meanwhile , lower-dimensional descriptors save memory   ,   ,   ,   times respectively    ,    ,    ,   -dimensional descriptors , shows advantages proposed MLP-based method . V. DISCUSSION experiments , ﬁnd learned MLP projections using unsupervised , supervised , self- supervised methods achieve better performances reduc- tion using PCA . Furthermore , features partially even better original    -dimensional ones . degree improvement different descriptors different . key result improvement hand-crafted descriptors , like SIFT MKD , larger learned ones , like EasyHardToughSIFTHardNetSV_ _hiddenSV_ _hiddenSV_ _hiddenUS_ _hiddenUS_ _hiddenUS_ _hiddenSS_ _hidden REFERENCES [   ] A. Alahi , R. Ortiz , P. Vandergheynst . Freak : Fast retina keypoint . Proc . IEEE Conf . Computer Vision Pattern Recogni- tion ( CVPR ) ,      . [   ] S. Balakrishnama A. Ganapathiraju . Linear discriminant analysis- Institute Signal information Processing , brief tutorial .    (      ) : –  ,      . [   ] P. Baldi . Autoencoders , unsupervised learning , deep architectures . Proc . Int . Conf . Machine Learning ( ICML ) ,      . [   ] V. Balntas , K. Lenc , A. Vedaldi , K. Mikolajczyk . Hpatches : benchmark evaluation handcrafted learned local descrip- Proc . IEEE Conf . Computer Vision Pattern tors . Recognition ( CVPR ) ,      . [   ] V. Balntas , E. Riba , D. Ponsa , K. Mikolajczyk . Learning local feature descriptors triplets shallow convolutional neural Proc . British Machine Vision Conference ( BMVC ) , networks .      . [   ] H. Bay , T. Tuytelaars , L. Van Gool . Surf : Speeded robust features . Proc . Europ . Conf . Computer Vision ( ECCV ) ,      . [   ] L. Breiman . Random forests . Machine learning ,    (   ) : –   ,      . [   ] M. Brown , G. Hua , S. Winder . Discriminative learning local IEEE Trans . Pattern Analalysis Machine image descriptors . Intelligence ( TPAMI ) ,    (   ) :  –   ,      . [   ] M. Calonder , V. Lepetit , M. Ozuysal , T. Trzcinski , C. Strecha , P. Fua . Brief : Computing local binary descriptor fast . IEEE Trans . Pattern Analalysis Machine Intelligence ( TPAMI ) ,    (   ) :    –     ,      . [    ] M. Caron , P. Bojanowski , A. Joulin , M. Douze . Deep clus- Proc . tering unsupervised learning visual features . Europ . Conf . Computer Vision ( ECCV ) ,      . [    ] J. Deng , J. Guo , N. Xue , S. Zafeiriou . Arcface : Additive angular margin loss deep face recognition . Proc . IEEE Conf . Computer Vision Pattern Recognition ( CVPR ) ,      . [    ] P. Ebel , A. Mishchuk , K.M . Yi , P. Fua , E. Trulls . Beyond cartesian representations local descriptors . Proc . IEEE Intl . Conf . Computer Vision ( ICCV ) ,      . [    ] B . Fan , Z. Wang , F. Wu . Local Image Descriptor : Modern Approaches , volume     . Springer ,      . [    ] R. Fergus , P. Perona , A. Zisserman . Object class recognition unsupervised scale-invariant learning . Proc . IEEE Conf . Computer Vision Pattern Recognition ( CVPR ) ,      . [    ] A. Gil , O. Reinoso , O. Mart´ınez-Mozos , C. Stachniss , W. Burgard . Improving Data Association Vision-based SLAM . Proc . IEEE/RSJ Intl . Conf . Intelligent Robots Systems ( IROS ) ,      . [    ] R.E . Gonz´alez Valenzuela , W.R. Schwartz , H. Pedrini . Dimension- ality reduction pca sift surf descriptors . Proc . IEEE Intl . Conf . Cybernetic Intelligent Systems ( CIS ) ,      . [    ] X. Han , T. Leung , Y. Jia , R. Sukthankar , A.C. Berg . Matchnet : Unifying feature metric learning patch-based matching .  Proc . IEEE Conf . Computer Vision Pattern Recognition ( CVPR ) ,      . [    ] S. Ioffe C. Szegedy . Batch normalization : Accelerating deep network training reducing internal covariate shift . Proc . Int . Conf . Machine Learning ( ICML ) , pages    –    . PMLR ,      . [    ] I.T . Jolliffe . Principal Component Analysis . Springer ,      . [    ] Y. Ke R. Sukthankar . Pca-sift : distinctive representation local image descriptors . Proc . IEEE Conf . Computer Vision Pattern Recognition ( CVPR ) ,      . [    ] D.P . Kingma J. Ba . Adam : method stochastic optimization . arXiv preprint arXiv:    .     ,      . [    ] M. Larsson , E. Stenborg , C. Toft , L. Hammarstrand , T. Sattler , F. Kahl . Fine-grained segmentation networks : Self-supervised segmentation improved long-term visual localization . Proc . IEEE Intl . Conf . Computer Vision ( ICCV ) ,      . [    ] A. Loquercio , M. Dymczyk , B. Zeisl , S. Lynen , I. Gilitschenski , R. Siegwart . Efﬁcient descriptor learning large scale localization . Proc . IEEE Intl . Conf . Robotics & Automation ( ICRA ) ,      . [    ] D.G . Lowe . Distinctive image features scale-invariant keypoints . Proc . IEEE Intl . Conf . Computer Vision ( ICCV ) ,      . [    ] Z. Luo , T. Shen , L. Zhou , S. Zhu , R. Zhang , Y. Yao , T. Fang , L. Quan . Geodesc : Learning local descriptors integrating geometry constraints . Proc . Europ . Conf . Computer Vision ( ECCV ) ,      . [    ] J. , X. Jiang , . Fan , J. Jiang , J. Yan . Image matching Intl . Journal Computer handcrafted deep features : survey . Vision ( IJCV ) ,     (   ) :  –   ,      . [    ] K.Z . Mao . Orthogonal forward selection backward elimination algorithms feature subset selection . IEEE Trans . Systems , Man , Cybernetics , Part B ( Cybernetics ) ,    (   ) :   –    ,      . [    ] K. Mikolajczyk C. Schmid . Indexing based scale invariant interest points . Proc . IEEE Intl . Conf . Computer Vision ( ICCV ) ,      . [    ] A. Mishchuk , D. Mishkin , F. Radenovic , J. Matas . Working hard know neighbor ’ margins : Local descriptor learning loss . Proc . Advances Neural Information Processing Systems ( NIPS ) ,      . [    ] A. Mukundan , G. Tolias , A. Bursuc , H. J´egou , . Chum . Un- Intl . Journal derstanding improving kernel local descriptors . Computer Vision ( IJCV ) ,    :    –     ,      . [    ] F. Pedregosa , G. Varoquaux , A. Gramfort , V. Michel , B. Thirion , O. Grisel , M. Blondel , P. Prettenhofer , R. Weiss , V. Dubourg , J. Van- derplas , A. Passos , D. Cournapeau , M. Brucher , M. Perrot , Journal E. Duchesnay . Scikit-learn : Machine learning Python . Machine Learning Research ,   :    –     ,      . [    ] T.M . Rassias . Properties isometric mappings . Journal Mathe- matical Analysis Applications ,     (   ) :   –    ,      . [    ] E. Rublee , V. Rabaud , K. Konolige , G. Bradski . Orb : efﬁcient alternative sift surf . Proc . IEEE Intl . Conf . Computer Vision ( ICCV ) ,      . [    ] P.E . Sarlin , C. Cadena , R. Siegwart , M. Dymczyk . coarse Proc . ﬁne : Robust hierarchical localization large scale . IEEE Conf . Computer Vision Pattern Recognition ( CVPR ) ,      . [    ] T. Sattler , B. Leibe , L. Kobbelt . Efﬁcient amp ; effective prioritized IEEE Trans . matching large-scale image-based localization . Pattern Analalysis Machine Intelligence ( TPAMI ) ,    (   ) :    –      ,      . [    ] B. Sch¨olkopf , A. Smola , K.R . M¨uller . Kernel principal component Proc . Intl . Conf . artiﬁcial neural networks analysis . ( ICANN ) ,      . [    ] E. Simo-Serra , E. Trulls , L. Ferraz , I. Kokkinos , P. Fua , F. Moreno- Noguer . Discriminative learning deep convolutional feature point Proc . IEEE Intl . Conf . Computer Vision descriptors . ( ICCV ) ,      . [    ] K. Simonyan , A. Vedaldi , A. Zisserman . Learning local feature descriptors using convex optimisation . IEEE Trans . Pattern Analal- ysis Machine Intelligence ( TPAMI ) ,    (   ) :    –     ,      . [    ] H. Taira , M. Okutomi , T. Sattler , M. Cimpoi , M. Pollefeys , J. Sivic , T. Pajdla , A. Torii . InLoc : Indoor Visual Localization Dense Matching View Synthesis . Proc . IEEE Conf . Computer Vision Pattern Recognition ( CVPR ) ,      . [    ] Y. Tian , B . Fan , F. Wu , et al . L -net : Deep learning discriminative Proc . IEEE Conf . patch descriptor euclidean space . Computer Vision Pattern Recognition ( CVPR ) ,      . [    ] Y. Tian , X. Yu , B . Fan , F. Wu , H. Heijnen , V. Balntas . Sosnet : Second order similarity regularization local descriptor learning . Proc . IEEE Conf . Computer Vision Pattern Recognition ( CVPR ) ,      . [    ] C. Toft , W. Maddern , A. Torii , L. Hammarstrand , E. Stenborg , D. Safari , M. Okutomi , M. Pollefeys , J. Sivic , T. Pajdla , F. Kahl , IEEE Trans . T. Sattler . Long-term visual localization revisited . Pattern Analalysis Machine Intelligence ( TPAMI ) ,    (   ) :    –      ,      . [    ] L. van der Maaten G. Hinton . Visualizing data using t-SNE . Journal Machine Learning Research ,  :    –     ,      . [    ] M.W . Watkins . Exploratory factor analysis : guide best practice . Journal Black Psychology ,    (   ) :   –    ,      . [    ] S. Zagoruyko N. Komodakis . Learning compare image patches Proc . IEEE Conf . via convolutional neural networks . Computer Vision Pattern Recognition ( CVPR ) ,      . 