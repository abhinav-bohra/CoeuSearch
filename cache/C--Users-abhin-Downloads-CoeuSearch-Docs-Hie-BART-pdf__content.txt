Hie-BART : Document Summarization Hierarchical BART Kazuki Akiyama Ehime University k_akiyama @ ai.cs.ehime-u.ac.jp Akihiro Tamura Doshisha University aktamura @ mail.doshisha.ac.jp Takashi Ninomiya Ehime University ninomiya @ cs.ehime-u.ac.jp Abstract paper proposes new abstractive sum- marization model documents , hierarchi- cal BART ( Hie-BART ) , captures hierarchical structures documents ( i.e. , sentence-word structures ) BART model . Although existing BART model achieved state-of-the-art performance document summarization tasks , ac- count interactions sentence-level word-level information . machine trans- lation tasks , performance neural ma- chine translation models improved incorporation multi-granularity self-attention ( MG-SA ) , captures rela- tionships words phrases . In- spired previous work , proposed Hie- BART model incorporates MG-SA encoder BART model capturing sentence-word structures . Evaluations per- formed CNN/Daily Mail dataset show proposed Hie-BART model outper- forms strong baselines improves per- formance non-hierarchical BART model ( + .   ROUGE-L ) .   Introduction recent years , improvements abstractive doc- ument summarization models devel- oped incorporation pre-training . BERTSUM model ( Liu Lapata ,      ) proposed pre-training model document summarization tasks . sequence-to- sequence tasks , T  model ( Raffel et al. ,      ) BART model ( Lewis et al. ,      ) proposed part generalized pre-training models . Among existing pre-training models , BART model achieves state-of-the-art perfor- mance document summarization tasks . How- ever , BART model capture hierar- chical structures documents generating summary . Neural machine translation improved capture multiple granularities in- formation input texts “ phrases words ” “ words characters ” . particular , Transformer-based machine translation model improved incorporating multi-granularity self-attention ( MG-SA ) ( Hao et al. ,      ) , considers relationships words phrases decomposing input text el- ements using multiple granularity ( i.e. , words phrases ) assigning granular element ( i.e. , word phrase ) head multi-head Self- Attention Networks ( SANs ) . method enables interactions words also be- tween phrases words , self-attentions . Inspired previous work , paper proposes new abstractive document summarization model , hierarchical BART ( Hie-BART ) , captures document ’ hierarchical structures ( i.e. , sentence- word structures ) SANs BART model . , document divided ele- ments word-level sentence-level granu- larity , element assigned head SANs layers BART encoder . , information multi-granularity captured combining output SANs layers , ratio combining word-level sentence- level information controlled hyperparame- ter . evaluated proposed model abstrac- tive summarization task CNN/Daily Mail dataset . evaluation shows Hie-BART model improves F-score ROUGE-L  .   points relative non-hierarchical BART model , proposed model better strong baselines , BERTSUM T  models .   Background  .  BART BART model ( Lewis et al. ,      ) general- ized pre-training model based Transformer ProceedingsofNAACL-HLT     : StudentResearchWorkshop , pages   –   June –  ,    .©    AssociationforComputationalLinguistics    Figure   : overview architecture BART . en- coder bidirectional model decoder autoregressive model . model ( Vaswani et al. ,      ) . Five pre-training techniques introduced : token masking , sen- tence permutation , document rotation , token dele- tion , text inﬁlling . denoising autoencoder tech- nique adds noise original text re- stores original text . Token masking , used BERT ( Devlin et al. ,      ) , randomly masks tokens . Sentence permutation randomly shuf- ﬂes sentences document . Document ro- tation randomly selects token sentence rotates sentence begins token . Token deletion randomly deletes to- ken original sentence . Text inﬁlling re- places word sequences single mask token inserts mask token randomly selected position . combination sentence permutation text inﬁlling achieves best accuracy techniques . overview BART model given Figure   . encoder bidirectional model decoder autoregressive model . pre- trained BART model ﬁne-tuned various tasks , summarization task , , doc- ument provided encoder , decoder generates document summary .  .  Multi-Granularity Self-Attention ( MG-SA ) MG-SA ( Hao et al. ,      ) used capture multi- granularity information input text di- viding input elements several types granularity preparing heads multi-head SANs type granularity . Provided word-level matrix H , input SANs , method ﬁrst generates phrase-level matrix Hg representing phrase-level information , follows : Figure   : Overview architecture Hie-BART . based Transformer model . SANs encoder divided word sentence levels computed . Hg = Fh ( H ) , Fh ( ・ ) function generates phrase- level matrix h-th head . Speciﬁcally , phrase-level matrix generated running max pooling operation word-level vectors word- level matrix . phrase-level matrix gener- ated , SANs perform following computations : Q , W h Q , HgW h K , HgW h Qh , Kh , V h = HW h Oh = ATT ( Qh , Kh ) V h , V , (   ) (   ) Qh ∈ Rn×dh , Kh ∈ Rp×dh , V h ∈ Rp×dh respectively query , key , value repre- ∈ Rd×dh parame- K , W h sentations，W h V ter matrices , , dh , n , p dimen- sions hidden layer , one head , word vec- tor , phrase vector , respectively . addition , ATT ( X , ) function calculates atten- tion weights X . computa- tions , output Oh head SANs generated . , output MG-SA gener- ated concatenating outputs heads : MG-SA ( H ) = [ O  , ... , ] . outputs head Oh contain information words words phrases . Thus , addition relationships words , relationships words phrases captured MG-SA .    Pre-trainedEncoderPre-trainedDecoderDocumentSummarySummaryMulti-HeadAttentionAdd & NormMulti-HeadSelf-AttentionAdd & NormoutputsEmbeddingsConcatenateAdd & NormFeed FowardSentence LevelMulti-HeadSelf-AttentionLinearSoftmax×N×NFeed FowardAdd & NormoutputprobabilitiesAdd & NormWord LevelMulti-HeadSelf-AttentioninputsEmbeddingsCreateSentence LevelVector Figure   : Behavior create sentence level vector layer . Ewij E [ BOS ] embedded vectors word wij ( j-th word i-th sentence ) [ BOS ] token , respectively . Esi sentence-level embedded vector i-th sentence si . Figure   : example behavior concatenate layer number heads multi-head   join point j =   . blue [ O  w ] designates outputs word-level SANs red [ O  ] shows outputs sentence-level SANs . w , ... , O  , ... , O    Hie-BART  .  Architecture Hie-BART ( Hierarchical-BART ) model sentence-to-word ( sentence-level ) SANs addi- tion word-to-word ( word-level ) SANs original BART model . overview Hie-BART shown Figure   . Hie-BART sentence- level SANs , create sentence level vector layer concatenate layer , addition BART . create sentence level vector layer , sentence- level matrix created word-level matrix . concatenate layer concatenates outputs word-level sentence-level SANs . outputs concatenate layer forwarded sub- sequent feed-forward layer . provide boundary information sentences , sentence preﬁxed [ BOS ] token .  .  Create Sentence Level Vector Layer behavior create sentence level vector layer shown Figure   . Ewij E [ BOS ] embedded vectors word wij ( j-th word i-th sentence ) [ BOS ] token , respectively . Esi sentence-level embedded vector i-th sentence si . create sentence level vector layer uses av- erage pooling generate sentence-level vector word-level vectors . Given word sequence W = ( w  , ... , wN ) , divided sentences = ( s  , ... , sM ) , N total number words , total number sentences , si i-th sentence consisting word subsequence wi  , . . . wiNi , Ni total number words sentence . ele- ment , apply average pooling follows : gm = AVG ( sm ) , AVG ( ・ ) average pooling . formula , G = ( g  , ... , gM ) generated . element W , , G embedded vector . G forwarded sentence- level SANs input .  .  Concatenate Layer w (   ) w , . . . , OH , . . . , OH SANs ( W ) = [ O  SANs ( G ) = [ O  outputs word-level sentence-level SANs combined concate- nate layer . outputs word-level sentence-level SANs layer follows : w ] = OALL , ] = OALL ,  H number heads , [ O  w , . . . , OH w ] = OALL output word-level SANs , w consisting word-level head ’ outputs , [ O  output sentence-level SANs , consisting sentence- level head ’ outputs .  word/sentence-level SANs combined fol- lows : ] = OALL outputs , ... , OH (   )  CONCAT ( OALL w , ... , Oj = [ O  w , OALL , j )  w , Oj+  , ... , OH  ] , (   )    E [ BOS ] EEEE [ BOS ] E [ BOS ] EEEEEEWord Level Input ( Embeddings ) Sentence Level InputEAverage PoolingEEWord LevelOutputSentence LevelOutputConcatenate LayerOutput ( j=  ) Concatenate Model LEAD-  ( Nallapati et al. ,      ) PTGEN ( See et al. ,      ) PTGEN+COV ( See et al. ,      ) BERTSUMEXTABS ( Liu Lapata ,      ) T  ( Raffel et al. ,      ) BART ( Lewis et al. ,      ) BART ( ) Hie-BART ( ) ROUGE-  ROUGE-  ROUGE-L   .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .  ∗ , ∗∗   .     .     .     .     .     .     .     .  ∗∗ Table   : Results CNN/Daily Mail test set . Word : Sentence    :      :      :      :      :      :      :     :     :   ROUGE     .     .     .     .     .     .     .     .     .   L   .     .     .     .     .     .     .     .     .       .     .     .     .     .     .     .     .     .   Table   : Results CNN/Daily Mail validation set . leftmost column shows ratio num- ber multi-heads combine . highest score achieved ratio “ Word : Sentence =   :  ” . CONCAT ( X , , j ) function con- catenates X join point j multi- heads . combined multi-head , heads   j word-level outputs , heads j +   H sentence-level outputs . Figure   shows example behavior concatenate layer Hie-BART , number heads multi-head   join point j =   . output word- level SANs [ O  w ] output sentence-level SANs [ O  ] joined join point j =   , resulting output , O  [ O  ] . w , ... , O  , ... , O  w , O  w , O  w , O  w , O  output concatenate layer forwarded feed-forward layer encoder .   Experiments  .  Dataset used CNN/Daily Mail dataset  ( Her- mann et al. ,      ) , summary corpus En- glish news articles , consisting    ,    train- ing pairs ,   ,    validation pairs ,   ,    test pairs . average , source documents sum- mary sentences        tokens , respec- tively . data preprocessing , followed in- struction provided CNN/Daily Mail dataset  fairseq  .  .  Parameters used pre-trained BART model “ bart.large ” , provided fairseq  Hie-BART . hyper- parameters BART Hie-BART de- termined validation set ; gradient ac- cumulation parameter ( update-freq )    , total number training steps   ,    , number multi-heads set    . ratio number combined heads out- put word-level sentence-level SANs set “ Word : Sentence =   :  ” Hie-BART . followed fairseq ’ settings  hy- perparameters . environments , model    ,   ,    parameters Hie-BART    ,   ,    parameters BART .  .  Results results evaluation Hie-BART model CNN/Daily Mail test data shown Table   . F-score ROUGE-  , ROUGE-  , ROUGE-L ( Lin ,      ) used evalua- tion metrics . calculate ROUGE score ,  CNN/Daily Mail dataset : https : //github.com/abisee/cnn-dailymail   Usage BART faireseq : https : //github.com/pytorch/fairseq/tree/master/examples/bart     [ Source Document ] ( CNN ) dozen Native American actors walked set Adam Sandler movie comedy , saying satirical Western ’ script in- sulting Native Americans women , accord- ing report . ... According ICTMN , Na- tive American adviser hired help ensure movie ’ cultural authenticity also walked set protest . Hill , Choctaw actor , seemed hold hope differences produc- ers Native American cast members could resolved . ... . [ Baseline Model ’ Summary ] walkout occurred set `` Ridicu- lous Six '' near Las Vegas , New Mexico , according report . script called native women ’ names `` Beaver ’ Breath '' `` Bra '' actress portraying Apache woman squat urinate smoking peace pipe , ICTMN reports . [ Proposed Model ’ Summary ] dozen Native American actors walked set Adam Sandler movie comedy , report says . say script insulting Native Americans women , according Indian Country Today Media Network . `` Ridiculous Six '' also stars Forte , Taylor Lautner , Steve Buscemi John Turturro . [ Gold Summary ] dozen Native American actors walk set Adam Sandler comedy , says report . Actors say satirical Western ’ script insulting Native Americans women . Table   : example improved summaries . Note source document partially omitted . [ Source Document ] ( CNN ) ’ kind thing see movies , like Robert Redford ’ role `` Lost '' Ang Lee ’ `` Life Pi . '' , real life , ’ hard swallow idea single person stranded sea days , weeks , months somehow living talk . Miracles happen , though , Hollywood . ’ talking people ﬂoat aimlessly run aground running gas let falter- ing winds picked hours later ... spotted U.S. Coast Guard . crew aboard Japanese Maritime Self-Defense Force destroyer picked smaller rescue boat , took drink offered burst tears , Kyodo reported . Shinkawa told rescuers , `` thought today last day life . '' [ Base Model ’ Summary ] ’ hard swallow idea single person be- ing stranded sea days , weeks , months somehow living talk . Miracles happen , though , Hollywood . Jose Salvador Alvarenga says journey began Paredon Viejo , port Mexico ’ Paciﬁc coast late      . [ Proposed Model ’ Summary ] ’ hard swallow idea single person be- ing stranded sea days , weeks , months somehow living talk . Miracles happen , though , Hollywood . Jose Salvador Alvarenga says journey began Paredon Viejo , port Mexico ’ Paciﬁc coast , late      . Louis Jordan says used laundry trap scoop ﬁsh , rigged makeshift mast sail . [ Gold Summary ] South Carolina man says spent    days alone sea rescued . sole survivor stories include Japanese man washed away tsunami . El Salvador man says drifted Mexico Marshall Islands year . Table   : example summaries im- proved .     used ﬁles rouge  . Hie-BART compared LEAD-  ( Nallapati et al. ,      ) , PTGEN , PT- GEN+COV ( See et al. ,      ) , BERTSUMEXTABS ( Liu Lapata ,      ) , T  ( Raffel et al. ,      ) , BART environment , BART Lewis et al . (      ) . LEAD-  method uses ﬁrst three sentences source document summary . PTGEN sequence-to-sequence model incorporates pointer generator net- work . PTGEN+COV introduces coverage mechanism PTGEN . BERTSUMEXTABS pre-training model adapts BERT sum- marization tasks . T  generalized pre-training model sequence-to-sequence tasks based Transformer model . statistical signiﬁ- cance test performed Wilcoxon-Mann- Whitney test . Table   , * * * indicate comparisons BART ( ) statistically signiﬁcant   % signiﬁcance level    % sig- niﬁcance level , respectively . Hie-BART improved F-score ROUGE-  / /L  .    points average relative BART environment ,  .    points av- erage BART reported ( Lewis et al. ,      ) . Table   also shows Hie-BART model signiﬁcantly improved ROUGE-  ROUGE-L scores baseline BART model .  .  Analysis Table   shows comparison ROUGE scores ratio number multi-heads word sentence levels validation set CNN/Daily Mail dataset . leftmost column shows ratio number multi- heads combine . seen Table   , maximum ROUGE- / /L score achieved `` Word : Sentence =   :  '' . ROUGE- / /L , smaller ratios multi-heads sentence level compared word level , higher score tends . However , number multi-heads sentence level   ( origi- nal BART ) , accuracy lower Hie- BART . Table   shows improved example sum- summaries generated baseline maries : model ( BART ) proposed model ( Hie- BART ) , gold summary . seen Table   , summary proposed model ﬂu- ent close contents gold summary , indicates summary proposed  ﬁles rouge usage : https : //github.com/pltrdy/ﬁles rouge model includes important parts source document . Table   shows example summaries improved . example , baseline model ’ summary proposed model ’ sum- mary include almost contents , far longer gold summary .   Conclusion study , proposed Hie-BART take account relationship words sentences BART dividing self-attention layer encoder word sentence lev- els . experiments , conﬁrmed Hie- BART improved F-score ROUGE-L  .   points relative non-hierarchical BART model , proposed model better strong baselines , BERTSUM T  models CNN/Daily Mail dataset . future work , intend investigate meth- ods incorporate information sentences addition word-to-word word-to-sentence information . References Jacob Devlin , Ming-Wei Chang , Kenton Lee , Kristina Toutanova .      . BERT : Pre-training deep bidirectional transformers language under- Proceedings      Conference standing . North American Chapter Association Computational Linguistics : Human Language Technologies , Volume   ( Long Short Papers ) , pages     –     , Minneapolis , Minnesota . Associ- ation Computational Linguistics . Jie Hao , Xing Wang , Shuming Shi , Jinfeng Zhang , Zhaopeng Tu .      . Multi-granularity self-atten- tion neural machine translation . Proceedings      Conference Empirical Methods Natural Language Processing  th Interna- tional Joint Conference Natural Language Pro- cessing ( EMNLP-IJCNLP ) , pages    –    , Hong Kong , China . Association Computational Lin- guistics . Karl Moritz Hermann , Tomas Kocisky , Edward Grefenstette , Lasse Espeholt , Kay , Mustafa Su- leyman , Phil Blunsom .      . Teaching ma- Advances chines read comprehend . neural information processing systems , pages     –      . Mike Lewis , Yinhan Liu , Naman Goyal , Mar- jan Ghazvininejad , Abdelrahman Mohamed , Omer Levy , Veselin Stoyanov , Luke Zettlemoyer .      . BART : Denoising sequence-to-sequence pre– training natural language generation , translation ,     comprehension . Proceedings   th An- nual Meeting Association Computational Linguistics , pages     –     . Association Com- putational Linguistics . Chin-Yew Lin .      . ROUGE : package auto- matic evaluation summaries . Text Summariza- tion Branches , pages   –   , Barcelona , Spain . Association Computational Linguistics . Yang Liu Mirella Lapata .      . Text summariza- Proceedings tion pretrained encoders .      Conference Empirical Methods Nat- ural Language Processing  th International Joint Conference Natural Language Processing ( EMNLP-IJCNLP ) , pages     –     , Hong Kong , China . Association Computational Linguistics . Ramesh Nallapati , Feifei Zhai , Bowen Zhou .      . SummaRuNNer : recurrent neural network based sequence model extractive . Proceedings Association Advancement Artiﬁcial Intel- ligence . Colin Raffel , Noam Shazeer , Adam Roberts , Katherine Lee , Sharan Narang , Michael Matena , Yanqi Zhou , Wei Li , Peter J. Liu .      . Exploring limits transfer learning uniﬁed text-to-text trans- Journal Machine Learning Research , former .    (     ) : –   . Abigail See , Peter J. Liu , Christopher D. Man- ning .      . Get point : Summarization Proceedings pointer-generator networks .   th Annual Meeting Association Com- putational Linguistics ( Volume   : Long Papers ) , pages     –     , Vancouver , Canada . Association Computational Linguistics . Ashish Vaswani , Noam Shazeer , Niki Parmar , Jakob Uszkoreit , Llion Jones , Aidan N Gomez , Ł ukasz Kaiser , Illia Polosukhin .      . Attention need . Advances Neural Information Pro- cessing Systems , volume    , pages     –     . Cur- ran Associates , Inc .    