1 2 0 2 r p A 9 2 ] L C . s c [ 1 v 0 9 6 4 1 . 4 0 1 2 : v i X r a Entailment as Few-Shot Learner Sinong Wang, Han Fang, Madian Khabsa, Hanzi Mao, Hao Ma Facebook AI, USA sinongwang@fb.com Abstract Large pre-trained language models (LMs) have demonstrated remarkable ability as few-shot learners. However, their success hinges largely on scaling model pa- rameters to a degree that makes it challenging to train and serve. In this paper, we propose a new approach, named as EFL, that can turn small LMs into better few-shot learners. The key idea of this approach is to reformulate potential NLP task into an entailment one, and then ﬁne-tune the model with as little as 8 exam- ples. We further demonstrate our proposed method can be: (i) naturally combined with an unsupervised contrastive learning-based data augmentation method; (ii) easily extended to multilingual few-shot learning. A systematic evaluation on 18 standard NLP tasks demonstrates that this approach improves the various exist- ing SOTA few-shot learning methods by 12%, and yields competitive few-shot performance with 500 times larger models, such as GPT-3. 1 Introduction Recent improvements in NLP models relied largely on the pre-training and ﬁne-tuning paradigm, wherein a language model is ﬁrst pre-trained on a massive text corpora, followed by ﬁnetuning on the downstream tasks (Devlin et al., 2019; Liu et al., 2019; Raffel et al., 2020; Lewis et al., 2020). The performance of the model varies depending on the tasks, and the number of available training examples. However, in practice, there is an overly large number of domains, tasks, and languages, and scaling to a new problem space will require additional labeled data. This leads to an important research area, few-shot learning, which assumes accessing to only a small number of labeled examples. Different from this pre-training/ﬁne-tuning paradigm, GPT-3 (Brown et al., 2020) showed that pre- training alone combined with a sufﬁciently large parameter size yields a model that is capable of achieving nearly SOTA results when prompted with the training examples. More importantly, GPT- 3 demonstrated that the number of examples needed for promoting can be as small as one, and performance can be signiﬁcantly improved when using 16 examples (per class) in a few-shot man- ner. However, the performance of the model depended largely on the number of parameters, and scaling to 175 billion parameters was imperative to achieve these results. Unfortunately, this scale introduces signiﬁcant challenges for both training and serving. Various methods have been proposed to equip smaller language models with few-shot capabili- ties. One alternative approach is to follow the Masked Language Modeling (MLM) paradigm (De- vlin et al., 2019), and reformulate downstream tasks as similar cloze questions (e.g., by appending phrases such as “the correct answer is ”), allowing pre-trained LMs to predict the correct label by reusing MLM head (Schick & Sch¨utze, 2020a,b). The performance of these techniques are limited when the downstream tasks have different data distribution from the pre-trained text corpora. For example, the work (Gao et al., 2020) shows that pre-trained LM performs worse in some tasks such as linguistic acceptability task and natural language inference task since pre-trained LM barely saw this data distribution.             Figure 1: An illustration of (a) standard ﬁne-tuning of a classiﬁcation problem; (b) prompt-based method; (c) our proposed method using entailment-based ﬁne-tuning. Compared to prompt-based methods, the key difference of this approach is reformulating tasks as entailment task instead of cloze questions and design ﬁne-grained label descriptions instead of a single task description. In this paper, we propose a new approach, in which NLP tasks are ﬁrst reformulated as a textual entailment task. The assumption here is that entailment can be used as uniﬁed method to model all classiﬁcation tasks1. As illustrated in Figure 1(c), the key idea is to convert the class label into a natural language sentence which can be used to describe the label, and determine if the example entails the label description. For example, we can reformulate a sentiment classiﬁcation input/label pair: [x : I am in love with these actors[EOS], y : positive] as following textual entailment sample: [x : I am in love with these actors[SEP]This is a great movie[EOS], y : entailment]. By converting tasks into this entailment style, we demonstrate that standard pre-trained language models are very effective few-shot learners. Another beneﬁt is that, since various tasks are refor- mulated as a sentence-pair entailment task, we can utilize contrastive learning to construct pairwise augmented data to further improve the few-shot performance. Experiments on wide variety of tasks including 8 tasks from GLUE benchmark, SNLI, BoolQ from superGLUE, and 8 other popular sentence classiﬁcation tasks show that such off-the-shelf entailment models can improve few-shot performance by 12% compared to various few-shot learning methods. Our methods also established 1.9pt absolute improvement in full training dataset compared to standard ﬁne-tuning of RoBERTa model. We further extend this method to multilingual few-shot learning, which also shows average 19pt improvement compared to standard ﬁne-tuning method. The model’s striking ability to perform few-shot learning could be due to entailment being a true language understanding task, that once the model is capable of performing it correctly, it can easily apply this knowledge to other tasks which are framed as such. While the proposed approach still requires ﬁne-tuning the model for each task, it does not require a prohibitively large language model to achieve strong performance. Moreover, entailment models are widely accessible to everyone to download and ﬁne-tune through various repositories, thus it democratizes few-shot learners and does not limit it to commercial black-box APIs. 2 Related Work There have been many research studies on improving the few-shot learning performance of a pre- trained language model: Language modeling with demonstrations: The series of GPT works (Radford et al., 2019; Brown et al., 2020) proposed to add a task description (prompt) and annotated examples as demonstration to enable few-shot learning, which has been commonly applied to classiﬁcation (Puri & Catanzaro, 2019), QA, commonsense knowledge mining (Davison et al., 2019). It is also explored for probing the knowledge contained within pre-trained LMs. 1This approach does not extend to generative tasks, such as translation or summarization 2 [CLS]TheIAUdowngradePlutoasadwarfplanet[EOS]CLS head(Feed Forward Layer)(a) Standard finetuningLabel:√ science news-business news-sports newsPredictTheIAUdowngradePlutoasadwarfplanet[SEP]What is news about? [MASK]MLM head(Feed Forward Layer)(b) Prompt-based methodLabel:√ science-business-sportsPredict[CLS]TheIAUdowngradePlutoasadwarf planetThis is sciencenews [EOS]Entailment head(Feed Forward Layer)Science:√ Entail-Not entailBusiness:-Entail-Not entailSports:-Entail-Not entailPredictThis is businessnews [EOS]This is sportsnews [EOS]Textual EntailmentLabel descriptions(c) Entailment-based method(ourapproach). Each class has a label description, and we choose the class with maximumprobability ofentailment between input sentence and label description. Figure 2: An illustration of intermediate training: ﬁrst ﬁne-tune the pre-trained LM on a source task with rich annotated data, then further ﬁne-tune the model with 8 annotated samples per class in target task. Figure (a) illustrates the accuracy heatmap between 11 tasks. It shows that sentiment-related task and NLI task have better transferability among each other and non-trivial few-shot results, while other task type such as topic classiﬁcation, subjectivity task has marginal improvements. Figure (b) illustrates the diagram of this approach, which requires 110 times ﬁne-tuning runs to ﬁne the best results among 11 tasks. Task reformulation: Language model is usually pre-trained with a Masked Language Modeling (MLM) objective, which is motivated by Cloze task in (Taylor, 1953). There have been several works reformulating few-shot learning tasks as cloze questions to reuse pre-trained LM such as LM prompt (Jiang et al., 2020), PET (Radford et al., 2019; Schick & Sch¨utze, 2020a), and recent LM-BFF (Gao et al., 2020). It shows a pre-trained LM can achieve non-trivial performance with few annotated samples. There are also some other works transforming NLP tasks as generative QA tasks (Puri & Catanzaro, 2019). Intermediate training: The work by (Phang et al., 2018) shows that supplementing pre-trained LMs with further training on data-rich supervised tasks can obtain additional performance improvements on the GLUE benchmark. More recently, this approach has been further improved by a matching- based few-shot learning method (Yin et al., 2020). General techniques: There are several general techniques to improve the few-shot learning perfor- mance, including: (i) optimization and regularization techniques during the ﬁne-tuning (Howard & Ruder, 2018; Lee et al., 2019; Zhang et al., 2020), (ii) semi-supervised learning to augment training data (Xie et al., 2020), and (iii) supervised contrastive learning as additional objective (Gunel et al., 2021). We anticipate that these studies are largely complementary to ours. Comparing to existing prompt-based few-shot learning methods (Brown et al., 2020; Schick & Sch¨utze, 2020a; Gao et al., 2020), the key differences of our proposed method are: (i) our approach reformulates NLP tasks as textual entailment instead of cloze questions; (ii) provide label-speciﬁc descriptions for each class instead of single task description. 3 Framework of Entailment Training In this section, we introduce our motivation and framework of entailment training. 3.1 Motivation We assume there exists a pre-trained language model M and a new downstream task T with label space Y. Suppose that the training dataset Dtrain = {D1, D2, . . . , D|Y|}, where Dk = {xi, yi}K i=1, k ∈ Y and K is the number of training data for each class. In this paper, we set K = 8 as default. Note that label space Y can be both discrete and continuous space, which corresponds to 3 SST-2MRCRIMDBMPQASubjOSCoLAQQPQNLIBoolQSST-2MRCRIMDBMPQASubjOSCoLAQQPQNLIBoolQ30405060708090[CLS]I am in love with these actors [EOS]CLS head(Feed Forward Layer)[CLS] I am in love with these actors [SEP] This is a great movie [EOS] CLS head(Feed Forward Layer)(a) Standard finetuningLabel:√ Positive-NegativeLabel:√ Entailed-Not entailedThis is a great movieDerivePredictPredictLMSSTIMDBBoolQ. . .SSTIMDBBoolQ. . .(b) Diagram of intermediate training 110combinationssubjmrimdbmpqasst-2crhsCoLAsubjmpqacrimdbsst-2mrhsCoLA607080909080706050Source taskFew-shot taskMR. . .MR. . .(a) Heatmap of intermediate trainingSentiment-related tasksNLItasksAccuracyLabel Template Accuracy Label Template Accuracy SST-2 sentiment classiﬁcation OS hate speech classiﬁcation This review indicates positive experience This review indicates great experience It is great It is great movie It is cute 89.8 (1.2) 90.6 (0.7) 90.8 (1.0) 91.0 (0.6) 87.4 (2.1) This tweet contains hate speech This tweet contains offensive words It is hate speech It is offensive speech It is peace and love 82.6 (2.1) 83.2 (2.6) 76.6 (3.5) 79.3 (2.4) 74.3 (4.7) Table 1: The impact of label descriptions on our proposed EFL method with K = 8. The perfor- mance is relative stable if the designed label descriptions are close to the problem deﬁnition, but degenerates if choosing semantic-unrelated label descriptions. classiﬁcation task and regression task, separately. The objective is to develop a task-agnostic train- ing methodology of language model M on limited training data Dtrain such that it generalizes well to examples on test set Dtest. In the standard NLP tasks, the input xin is usually a single sentence S1 (classiﬁcation, regres- sion) or a sentence pair (S1, S2) (natural language inference, QA). In the standard ﬁne-tuning approach, given a pre-trained language model M, we usually take xin = [CLS]S1[EOS] or xin = [CLS]S1[SEP]S2[EOS], and map xin into a sequence of hidden vectors M(xin). Then we can predict the output classes via a downstream classiﬁcation head softmax(Wh[CLS]), where h[CLS] is the hidden vector of [CLS] token. During the ﬁne-tuning phase, the parameters of both language model M and classiﬁcation head W will be updated to minimize the cross entropy be- tween softmax output and correct label 2. The key challenge of few-shot learning is that we have limited training data, while a huge amount of model parameters need to be updated, e.g., BERT-large has 340M parameters. To resolve this challenge, one alternative approach is to reuse the language model parameters if the model is already pre-trained with a similar task. For example, in Figure 2, we conduct a two-stage experiment across 11 NLP tasks (details of data stats can be seen in Appendix 2). We ﬁrst ﬁne-tune a RoBERTa- Large (Liu et al., 2019) model on the full dataset of one task, then we load the model parameters M and continue ﬁne-tune on another task in a few-shot setting (8 training data per label class in each task, and ﬁne-tuned with 5 random split). We report the average accuracy on full Dtest. As illustrated in Figure 2(a), we can achieve reasonable few-shot learning performance, i.e. 85% accuracy, on most sentiment tasks such as ﬁne-tuning on IMDB ﬁrst, then ﬁne-tune on MR. However, in other tasks such as OS, CoLA and NLI tasks, all the source tasks brings marginal improvement. Moreover, the above approach is not task-agnostic. It requires substantial amount of sweeping over available source tasks (Figure 2(b)), and the domain-knowledge of the downstream task. One natural question is that can we build a single language model M that is task-agnostic and generalize well in few-shot setting? 3.2 Framework To answer this question, we propose a new framework in which NLP tasks are transformed as a textual entailment task. We refer to this framework as EFL, short for Entailment as Few-shot Learner. For instance, we can reformulate a sentiment classiﬁcation task as a textual entailment one with an input sentence S1 as xin = [CLS]S1[SEP]S2[EOS], where S2 = This indicates positive user sentiment, (1) and let the language model M to determine the if input sentence S1 entails the label description S2. It is important to note that this approach consolidates all the one-sentence classiﬁcation/regression tasks and sentence-pair tasks into a uniﬁed template softmax[WM(xin)[CLS]], where xin is de- ﬁned in (1). Then we can reduce the gap between traditional MLM pre-training and ﬁne-tuning by further pre-training the language model with existing entailment tasks or reformulating other tasks as entailment task such that we can reuse the model parameters W and M. In the binary classiﬁcation/regression task, we simply choose a label description p as second sen- tence S2, and ﬁne-tune the model on Dtrain = {(xi, p, yi)}K|Y| i=1 . The remaining challenge is to 2For regression task, we will minimize the mean square error. 4 Dataset |L| #Train #Test Type Labels Descriptions One-sentence Task SST-2 MR CR MPQA Subj OS IMDB CoLA TREC Yelp AG News Sentence-pair task QQP MRPC QNLI SNLI RTE STS-B BoolQ 2 2 2 2 2 2 2 2 6 5 4 2 2 2 3 2 2 2 67,349 8,662 1,775 8,606 8,000 3,000 25000 8,551 5,452 600k 4,000 363,846 3,668 104,743 549,367 2,490 5,749 9,427 872 2,000 2,000 2,000 2,000 3,000 25000 1,042 500 1,000 4,000 40,431 408 5,463 9,842 277 1,500 3,270 sentiment sentiment sentiment opinion polarity subjectivity topic classiﬁcation sentiment acceptability question classiﬁcation sentiment topic classiﬁcation positive, negative experience positive, negative experience positive, negative experience positive/negative opinion subjective/objective hate speech/benign twitters positive/negative user experience grammatical, not grammatical abbr., entity, description, human, loc., num. 5-scale user sentiments world, sports, business, science news paraphrase paraphrase NLI NLI NLI sentence similarity QA equivalent, not equivalent equivalent, not equivalent entailment, not entailment entailment, neutral, contradiction entailment, not entailment [0, 5] continuous score Yes/No Table 2: The datasets evaluated in this work. |L|: # of classes for classiﬁcation tasks. Note that we only sample Dtrain with K = 8 examples from the original training set and evaluate using full dev dataset in our few-shot experiments choose the label descriptions for different tasks. Similar as the existing work (Schick & Sch¨utze, 2020b), we hand-craft the label descriptions for each task. Details can be seen in Table 10. The gen- eral design principle is to intuitively choose some simple words to describe the labels. For instance, in the sentiment classiﬁcation task, we can choose This movie is great or This review indicates pos- itive user experience as label description. In Table 1, we show that our method actually requires minimum domain knowledge to choose the label descriptions. Note that in sentence-pair task such as NLI task, this approach is degenerating to the standard ﬁne-tuning method. In the multi-class classiﬁcation task, we choose a label description pk for each class k ∈ Y. Suppose that the dataset Dtrain = {Di}|Y| j=1 is the data for ith class. Based on the designed label descriptions {pk}k∈Y , we reshape each dataset as an entailment dataset ˆDk = {(xj, pk, yj)}K j=1. To accommodate the entailment training, we reshape the training dataset as i=1, where Di = {(xj, yj)}k ˆDtrain = {( ˆDi, ˆD−i)}|Y| i=1, where ˆD−i is a random subset of { ˆDj}j(cid:54)=i,j∈Y with | ˆD−i| = K. For instance, in AG news clas- siﬁcation (Zhang et al., 2015), we can formulate a simple set of label descriptions {This is worlds news, This is business news, This is sports news, This is science news} for total 4 classes. Note that this approach requires |Y| forward passes during inference time, while standard ﬁne-tuning only requires one forward pass to get the prediction of all the classes. Actually, the inference speed can be easily improved if we reuse the siamese transformer networks (Reimers & Gurevych, 2019). (2) 4 Experiment In this section, we present experiment results of our proposed approach and various existing few- shot learning methods. We also evaluate the impact of training data size and pre-trained model size on those methods. 4.1 Data Statistics We conduct a comprehensive study across 18 NLP tasks (see Table 2), which covers sentiment analysis, topic classiﬁcation, natural language inference, paraphrases, sentence similarity and QA. Our evaluation consists of 8 tasks from the GLUE benchmark (Wang et al., 2018), SNLI (Bowman et al., 2015), BoolQ (Clark et al., 2019) from SuperGLUE and 8 other popular sentence classiﬁcation 5 Method SST-2 (Acc) MR (Acc) CR (Acc) MPQA (Acc) Subj (Acc) IMDB (Acc) OS (Acc) CoLA (Acc.) Full training dataset Majority Fine-tuning EFL 50.9 96.4 (0.2) 96.9 (0.2) 50.0 91.6 (0.2) 92.5 (0.1) 50.0 91.8 (0.7) 92.5 (0.4) 50.0 89.4 (0.9) 90.8 (0.4) 50.0 97.4 (0.1) 97.1 (0.2) 50.0 96.1 (0.2) 96.1 (0.2) 66.8 94.3 (0.1) 95.1 (0.1) 69.1 86.2 (1.6) 86.4 (0.5) Few-shot with K=8 60.5 (3.1) Fine-tuning 64.5 (5.4) Stilts-NLI 85.5 (2.3) Stilts-Close LM-BFF 79.9 (6.0) EFL wo PT 58.6 (6.7) 90.8 (1.0) EFL 60.3 (7.5) 63.3 (4.3) 81.9 (4.8) 85.4 (3.9) 55.1 (1.1) 86.2 (0.8) 61.9 (5.1) 68.8 (7.0) 84.7 (5.4) 88.6 (2.3) 64.0 (5.6) 92.3 (0.4) 59.0 (3.4) 67.7 (5.7) 76.6 (1.3) 74.2 (1.2) 60.4 (3.7) 87.0 (0.6) 78.3 (8.2) 69.1 (5.6) 83.2 (3.4) 81.6 (6.2) 77.9 (6.1) 80.0 (5.4) 73.5 (7.8) 59.7 (3.6) 86.9 (4.0) 92.0 (0.5) 75.5 (9.6) 87.1 (1.2) 70.0 (5.4) 62.4 (5.7) 68.1 (1.4) 71.0 (3.3) 71.2 (3.2) 76.6 (3.5) 70.0 (0.9) 66.3 (3.5) 62.2 (3.9) 69.5 (0.5) 70.4 (1.9) 69.4 (0.9) Table 3: Our main few-shot learning results using RoBERTa-large on one sentence task. All the results are evaluated on full test sets and averaged across 5 different training sets. tasks (Yelp, MR, CR, MPQA, Subj, IMDB, AG News, OS, Trec). See Appendix A for details. We divide our tasks into two categories: (i) One-sentence task: The input of this task is a single sentence, the goal is to predict the binary or multi-class label. For example, predicting whether user sentiment is positive or negative. (ii) Two-sentence task: The goal of this task is to make prediction based on a sentence pair. For example, predicting similarity of two input sentences . 4.2 Evaluation and Training Protocol It is well-known that ﬁne-tuning and evaluating on small datasets can suffer from instability and results may change dramatically given a new split of data. To address this, existing work (Gao et al., 2020) proposed to measure the average performance across 5 different randomly sampled train and dev splits, using a ﬁxed set of seeds. To further reduce the instability of evaluation, we measure the performance on full test dataset instead of randomly downsampled 5 small dev splits. In all of our experiment, we use 8 training samples per class, and the size of evaluation data is listed in TABLE 2. We also follows the principle in (Schick & Sch¨utze, 2020a,b) and assume no access to a development set and adopt a ﬁxed set of hyper-parameters based on practical considerations. For fair comparison, we use the same label descriptions for both prompt-based ﬁne-tuning and entailment-based ﬁne-tuning. For each experiment, we run 5 experiments with 5 different train- ing split and report the average results and standard deviations. We experiment all the methods using a RoBERTa-large model (Liu et al., 2019), which is a 24-layer, 16-heads transformer model with 355M model parameters. Details of label descriptions and hyper-parameter setup can be seen in Appendix A. 4.3 Main Results We conduct both experiments on full-shot and few-shot scenarios, and compare our proposed meth- ods with various baselines, including: Majority: simply take the most frequent class (measured on the full test dataset). Standard FT: standard ﬁne-tuning of pre-trained language model on full/few-shot dataset with cross entropy loss (or mean square error for regression task). STILTS (Phang et al., 2018): STILTS is widely used technique to improve the performance of a target problem by creating intermediate training task. Traditionally, people pre-train M on MNLI and then ﬁne-tune on target data. We also create a new stronger version, named as STILTS-close, which pre-train M across the tasks in Table 2, and report average results of best 4 source tasks. LM-BFF (Gao et al., 2020): add prompts in each input sentence (pair) and replace keywords by masked tokens, then predict masked tokens by reusing the output layer of pre-trained mask lan- guage model. This method has show superior performance across GPT-3 in-context learning and PET (Schick & Sch¨utze, 2020a). 6 Method QQP (F1) QNLI (Acc) SNLI (Acc) RTE (Acc) MRPC (F1) STS-B (Pear.) BoolQ (Acc) Full training dataset Majority Fine-tuning EFL 0.0 89.0 (0.1) 89.2 (0.1) 50.5 92.9 (0.2) 94.5 (0.1) 33.8 77.6 (8.8) 93.1 (0.2) 52.7 90.2 (0.6) 90.5 (0.4) 81.2 89.9 (1.7) 91.0 (0.8) -1 86.1 (0.5) 91.8 (0.3) 62.3 86.1 (0.5) 86.0 (0.2) Few-shot with K=8 Fine-tuning LM-BFF EFL 58.8 (9.9) 68.2 (1.2) 67.3 (2.6) 52.7 (1.8) 61.8 (3.2) 68.0 (3.4) 38.4 (1.3) 52.0 (1.7) 81.0 (1.1) 55.0 (1.3) 63.3 (2.1) 85.8 (0.9) 76.1 (3.9) 78.5 (2.3) 76.2 (1.3) 24.5 (8.4) 66.0 (3.2) 71.0 (1.3) 60.8 (2.8) 71.2 (3.5) 73.9 (1.8) Table 4: Our main few-shot learning results using RoBERTa-large on NLI, paraphrase, similarity and QA tasks. All the results are evaluated on full dev sets and averaged across 5 different training sets. EFL: Our proposed method, which transforms label descriptions as a input sentence and reformu- late original classiﬁcation/regression task as a entailment task. We also create two version of this methods: (i) EFL wo PT refers to directly ﬁne-tune pre-trained language model; (ii) EFL refers to ﬁrst train on MNLI task, then ﬁne-tune on downstream tasks. Table 3 and Table 4 shows the main results of 15 NLP tasks. We observe that our proposed method EFL greatly outperforms standard ﬁne-tuning, LM-BFF, Stilts-NLI and even a very strong baseline Stilts-Close, which assumes the access of development set to sweep the choice of upstream task. In few-shot scenario, K=8, EFL achieves average 8.2% (up to 55%) improvements across 15 tasks among those existing methods. The only exception is the CoLA (the linguistic acceptability task). It is might due to that MLM pre-trainng and entailment training did not see this type of data distribution before. In the full training dataset, EFL shows around 1.9pt average improvements compared to standard ﬁne-tuning of RoBERTa-Large3. These results identify the effectiveness of the proposed entailment-based method as a better approach for few-shot learning, moreover, a uniﬁed approach for various NLP tasks. Furthermore, we observe that if we remove the entailment training step of EFL and directly ﬁne-tune a language model as an entailment task, the performance drops signiﬁcantly in 8-shot scenario, even worse compared to the standard ﬁne-tuning. This is an interesting case as we will show that, in the next section, EFL without entailment training can also perform well if we increase K to 16 or 32. Note that the overall label description we used in our experiments is default choice of prompt-based method (Gao et al., 2020). As shown in Table 1, we observe that our method can achieve even better performance if we optimize the label descriptions. Datasets Majority Acc Macro-F1 Standard FT LM-BFF EFL Acc Macro-F1 Acc Macro-F1 Acc Macro-F1 AG News Yelp Trec 25.0 41.1 27.6 0.08 11.7 8.65 81.5 (2.9) 49.8 (4.3) 53.0 (9.3) 51.1 (9.9) 40.1 (3.0) 23.2 (0.2) 85.4 (2.3) 64.5 (2.5) 81.5 (5.7) 53.5 (5.3) 40.5 (2.9) 49.1 (1.5) 86.1 (2.9) 64.9 (2.6) 80.9 (3.1) 79.5 (3.8) 42.6 (3.9) 69.0 (4.9) Table 5: Our main few-shot learning results using RoBERTa-large on multi-class tasks. Besides accuracy metric, we also report macro-F1 metric since these multi-class datasets are unbalanced across different classes. Table 5 shows the main results of three multi-class benchmarks. Both LM-BFF and our proposed method have better performance than standard ﬁne-tuning. Each multi-class benchmark has unbal- anced data across different classes, using accuracy metric is unrepresentative in this scenario. An interesting observation is that, when we use Macro-F1 metric (average F1 across each class) to mea- sure the performance, our proposed method has signiﬁcantly better performance than other methods. It implies that EFL has better performance in some low-resource classes. 3Note that we have very large improvement on SNLI, if we remove that benchmark, the average improve- ment is 1.0pt. 7 Figure 3: Sample efﬁciency of standard ﬁne-tuning, LM-BFF, our proposed EFL and EFL without entailment pre-training as a function of K (number of instances per class). Figure 4: Impact of pre-trained Language model size on standard ﬁne-tuning, LM-BFF and our proposed method EFL. 4.4 Impact of Training Data Scale We ﬁrst investigate how our proposed method and other methods such as standard ﬁne-tuning and prompt-based method LM-BFF scale as number of training data K increases. In Figure 3, we plot the trends for SST-2 and QNLI. We observe that EFL has bigger improvements when number of annotated samples are small for both benchmarks. For simple tasks such as SST-2, when K=8, EFL already performs as good as K=256. For the harder tasks such as QNLI, EFL keep improving as K increases, and perform best among all the methods. Another interesting observation is that, although EFL without entailment training performs poorly when K = 8, it starts performing better than LM- BFF and standard ﬁne-tuning when K is larger than 16. Based on this observation, we further run ablation studies of our proposed methods without entailment training on two benchmarks: Subj and OS. In Table 6, we observe that the gap between EFL and EFL without entailment training is further reduced as K increases and performs similarly when K = 32. Data Few-shot K=16 Standard FT EFL wo PT EFL Few-shot K=32 Standard FT EFL wo PT EFL Subj OS 90.0 (1.7) 81.2 (2.1) 90.4 (1.5) 84.1 (2.6) 91.4 (1.3) 86.7 (2.0) 92.0 (0.5) 88.0 (0.5) 93.4 (0.9) 90.1 (0.9) 92.6(0.9) 88.8 (1.6) Table 6: Ablation studies of proposed EFL method without entailment training. 4.5 Impact of Model Size We further investigate the impact of model size on different methods. We evaluate standard ﬁne- tuning, LM-BFF and our proposed method on two different-sized pre-trained LMs: RoBERTa-base 8 SST-2QNLISST-2QNLIMRFigure 5: An illustration of our proposed unsupervised contrastive learning-based data augmentation method for entailment-based training. (12 layer, 768 hidden dimension, 125M parameters) and RoBERTa-Large (24 layer, 1024 hidden dimension, 355M parameters). In Figure 4, we experiment with three benchmarks: SST-2, MR and QNLI. The performances of both LM-BFF and our proposed method improve when the size of pre- trained language models increases. Another interesting observation is that standard ﬁne-tuning has better performance on SST-2 and QNLI dataset when uses a smaller language models. We suspect that this phenomenon is due to smaller LM has less number of parameters to be updated in standard ﬁne-tuning method. 5 Optimizations In this section, we discuss two further optimizations of our proposed framework: (i) a natural com- bination with unsupervised contrastive learning-based data augmentation method; (ii) multilingual few-shot learning. 5.1 Unsupervised Contrastive Learning In our proposed framework, we reformulate various NLP tasks as a textual entailment task, which takes sentence pair as input. This approach facilitates leveraging unsupervised techniques to con- struct pairwise data to augment existing limited annotated data. Based on this observation, we propose a new data augmentation strategies, UCA, Unsupervised Contrastive data Augmentation. 1 from S1 via sentence augmentation, and add S1[SEP]S(cid:48) The Figure 5 illustrates our main procedure of UCA. In EFL, each training sample has the format of S1[SEP]S2, where S1 is the original input sentence, S2 is either label descriptions for one- sentence task or 2nd input sentence for sentence-pair task. In one-sentence task, we can construct a new instance S(cid:48) 1[SEP]S2 as a new training sample with positive label. Similarly, in sentence-pair task, we can also create a new pos- itive instance S1[SEP]S(cid:48) 2. In practical implementation, we can alternate between these two augmentations in a probabilistic way. The negative sampling in UCA is straightforward: (i) we randomly sample two sentences R1, R2 from different instances of Dtrain and construct a negative data R1[SEP]R2; (ii) we use multiple sentence augmentation methods to largely change the original sentence meaning and add it as a negative sample. The rest relies on how to construct positive sentence pair from existing data. 1 or S2[SEP]S(cid:48) 1 or S(cid:48) sentence four basic augmentations: word deletion, span deletion, We have reordering and substitution. The basic intuition of deletion is that small portion of deletion in a sentence wouldn’t affect too much of the original semantic meaning. In word deletion, we randomly remove pdel percent of words in the input sentence, while in span deletion, we randomly pick dspan consecutive words and directly delete them. Sentence reordering is originally proposed in BART (Lewis et al., 2020) for auto-denoising, i.e., restoring original sentence from random reordered sentence. We randomly sample dre pairs of span and switch them pairwise to construct the reordering augmentation in our implementation. In synonym substitution (Jia et al., 2019), we sample dsub words and replace them with synonyms to construct one augmentation. This approach is known to be able to improve model’s robustness, which is beneﬁciary for some grammatical acceptability task such as CoLA (Warstadt et al., 2019). 9 I amin love with theseactorsI am inlove with these actorsI am in love with these charactersIn love with these actors, I amCL augmentations[CLS]I am in love with actors [SEP]This is greatmovie [EOS][CLS]This style matches my taste[SEP]This is greatmovie [EOS][CLS]I in love with actors [SEP]This is greatmovie [EOS][CLS]In love with these actors, I am[SEP]I in love with actors [EOS]Entailment head(Feed Forward Layer)-Entail-Not entailPredictAnnotated dataAugmented positivedata[CLS]I am actors [SEP]This is greatmovie [EOS][CLS]I am in love with actors [SEP]This style matches my taste[EOS]Augmentednegativedata(CL sample, label des’)(CL sample, sample)(aggressive CL sample, label des’)random sampleMethod SST-2 (Acc) MR (Acc) CR (Acc) MPQA (Acc) Subj (Acc) IMDB (Acc) OS (Acc) CoLA (Acc.) 90.8 (1.0) EFL EFL + UCA 90.3 (1.2) 86.2 (0.8) 86.3 (1.0) 92.3 (0.4) 92.8 (0.3) 87.0 (0.6) 87.8 (0.5) 80.0 (5.4) 83.3 (1.2) 87.1 (1.2) 87.4 (1.2) 76.6 (3.5) 79.8 (3.3) 69.4 (0.9) 71.2 (2.1) Method QQP (F1) QNLI (Acc) SNLI (Acc) RTE (Acc) MRPC (F1) STS-B (Pear.) BoolQ (Acc) Average EFL 67.3 (2.6) EFL + UCA 81.5 (0.8) 68.0 (3.4) 74.6 (0.8) 81.0 (1.1) 80.9 (1.1) 85.8 (0.9) 87.2 (0.9) 76.2 (1.3) 80.8 (0.8) 71.0 (1.3) 75.7 (1.7) 73.9 (1.8) 73.7 (1.9) 79.5 82.2 Table 7: Few-shot learning results (K = 8) with EFL and unsupervised contrastive data augmenta- tion. Note that bold numbers refer to the new best results achieved compared to results in Table 3 and Table 4. Method + UCA MPQA (∆Acc) Subj (∆Acc) IMDB (∆Acc) OS (∆Acc) CoLA (∆Acc) Standard FT LM-BFF EFL -2.05 +0.10 +0.81 +4.46 +13.9 +3.29 -2.26 -0.76 +0.28 -2.58 -1.90 +3.17 +1.60 +2.89 +1.85 QQP (∆F1) +17.5 +7.79 +14.2 QNLI (∆Acc) STS-B (∆Pear.) Average - +11.9 +2.62 +6.57 +6.21 +2.35 +5.57 +4.35 +3.38 +4.47 Table 8: Results of combining unsupervised contrastive data augmentation (UCA) with different methods. We report average ∆ across 5 different training sets. Note that bold numbers refer to the new best results achieved compared to results in Table 3 and Table 4. As illustrated in Table 7, UCA further brings signiﬁcant improvement across 12 of 15 tasks, and average 2.7pt improvements across 15 tasks. In particular, the improvement is even larger in the sentence-pair task. For example, we have seen 14pt improvement of QQP task. This is intuitive since UCA creates more similarly distributed data via sentence-level augmentation. In the linguistic acceptability task CoLA, the UCA can create some negative data by aggressively delete or reorder words of sentences, which matches the data format of this task. It is also interesting to see how our proposed UCA method impacts other few-shot or ﬁne-tuning methodologies. In Table 8, we further make a direct comparison of combining UCA with standard ﬁne-tuning, LM-BFF and our proposed method on 5 single-sentence task and 3 sentence-pair task. As it is shown, the UCA method improves all 8 tasks for EFL, 5 of 8 task for standard ﬁne-tuning, 6 of 8 tasks for LM-BFF, and brings average 3 to 4pt improvement. It implies our proposed method can beneﬁt different training methods and may not be limited to EFL. Further, we observe that combining UCA method with standard ﬁne-tuning and LM-BFF mostly improves sentence-pair task. The major reason is that only sentence-pair task in standard ﬁne-tuning and LM-BFF has similar input format of UCA data, instead, the training sample created by EFL is similar format as UCA data. 5.2 Multilingual Few-shot Learning When we scale an existing problem to a new language, one challenge is how to obtain the annotated data in the target domain, which leads to an even important research area, multilingual few-shot learning (MFL). In MFL, suppose that we have language space L = {l1, . . . , lm} with m different languages. The training dataset is similarly deﬁned as Dtrain as in previous monolingual scenario. The key difference is the test dataset, DX test, . . . , Dlm test}, test = {Dl1 test, Dl2 where Dlm test is the test dataset in language lm. This problem is even challenging compared to previous scenario since we not only need to generalize to the original unseen test data, but also generalize to unseen languages. There have been existing works leveraging multilingual masked language modeling to develop cross- lingual LM such as XLM (Lample & Conneau, 2019). However, there are no existing works on exploring MFL. As shown in Fig 6, our proposed method can be straightforwardly extended to this scenario. We follow the same steps when we construct the training dataset in monolingual scenario. In test dataset DX test, we don’t need to translate label description into target language, instead, simply use original English label description for various languages. 10 Figure 6: Generalization of our proposed method to multilingual few-shot learning: we ﬁrst ﬁne- tune on English data with few annotated examples and then test it on multilingual data. Method M-SST-2 M-MR (Acc) (Acc) M-CR M-MPQA M-Subj (Acc) (Acc) (Acc) M-OS M-CoLA Average (Acc) (Acc) (Acc) Majority 50.9 50.0 50.0 50.0 50.0 66.8 69.1 55.3 Train + Translate Test + Few-shot K = 8 Standard FT 52.2 (0.8) 52.6 (1.6) EFL wo PT 84.1 (1.1) EFL 54.3 (1.4) 54.3 (1.5) 81.2 (0.8) 55.8 (1.0) 57.2 (3.4) 90.1 (0.4) 56.9 (2.1) 59.9 (1.8) 83.2 (0.5) 77.5 (4.7) 76.5 (6.4) 84.5 (2.0) 67.2 (0.8) 67.0 (0.4) 72.9 (2.7) 69.2 (0.1) 69.2 (0.1) 68.7 (1.2) Translate Train + Translate Test + Few-shot K = 8 Standard FT 55.1 (5.0) 54.6 (3.2) EFL wo PT 84.3 (0.8) EFL 63.8 (7.2) 60.7 (4.9) 81.5 (0.9) 65.5 (4.9) 65.6 (4.1) 90.5 (1.1) 57.0 (1.6) 59.3 (3.3) 83.3 (1.3) 89.4 (1.6) 89.5 (1.3) 88.7 (2.1) 69.4 (7.4) 71.3 (5.4) 73.1 (2.1) 69.1 (0.1) 69.2 (0.1) 67.6 (1.6) 61.9 62.4 80.7 67.0 67.2 81.3 Table 9: Our main multilingual few-shot learning results using XLM-R. All the results are evaluated on full test sets and averaged across 5 different training sets. We add M- in front of each benchmark to represent the multilingual testing. We follow the same evaluation protocol in the monolingual scenario. For evaluation benchmark, we reuse the previous monolingual benchmark by using the translation test (Ott et al., 2018). We also use XLM-R (Conneau et al., 2020) as our pre-trained LM. For our proposed method EFL, we ﬁne-tune the model on XNLI dataset (Conneau et al., 2018) instead of MNLI. Table 9 shows the main results of 7 NLP tasks. We observe that EFL performs signiﬁcantly better in the multilingual scenarios compared to standard ﬁne-tuning, i.e, improved average accuracy from 61.9 to 80.7 across 7 tasks. This result demonstrates the effectiveness of EFL on multilingual scenario. We further experiment with translate training method: translate few-shot English annotated samples into target languages and add them into training data. Interestingly, we observe standard ﬁne-tuning method improves from 61.9 to 67.0, while EFL only improves from 80.7 to 81.3. 6 Conclusion In this paper, we proposed a set of simple and effective few-shot learning methods: (i) reformulates traditional classiﬁcation/regression tasks as a textual entailment task; (ii) unsupervised contrastive learning-based data augmentations. We show through a series of systematic evaluations that our method outperforms various few-shot learning methods by up to 55% (and 12% on average). In the future, we will explore several new directions based on this approach: (i) how to choose the best label descriptions based on reinforcement learning; (ii) how to create a more effective entailment training task instead of MNLI tasks. References Luisa Bentivogli, Peter Clark, Ido Dagan, and Danilo Giampiccolo. The ﬁfth pascal recognizing textual entailment challenge. In TAC, 2009. Samuel R Bowman, Gabor Angeli, Christopher Potts, and Christopher D Manning. A large anno- tated corpus for learning natural language inference. arXiv preprint arXiv:1508.05326, 2015. 11 [CLS]I am in love with these charactersThis is greatmovie [EOS]Entailment head(Feed Forward Layer)Training:√ Entail-Not entailPredict[CLS]Ce film correspond exactement à mon goûtThis is greatmovie [EOS]Entailment head(Feed Forward Layer)Testing:√ Entail-Not entailPredict[CLS]Я смотрю этот фильм всего 10 минутт[CLS]Ich genießees wirklich, Kung Fu zusehenMultilingual GeneralizationTrainTestTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020. Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia. Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation. arXiv preprint arXiv:1708.00055, 2017. Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difﬁculty of natural yes/no questions. arXiv preprint arXiv:1905.10044, 2019. Alexis Conneau, Guillaume Lample, Ruty Rinott, Adina Williams, Samuel R Bowman, Holger Schwenk, and Veselin Stoyanov. Xnli: Evaluating cross-lingual sentence representations. arXiv preprint arXiv:1809.05053, 2018. Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm´an, ´Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Unsuper- vised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 8440–8451, 2020. Ido Dagan, Oren Glickman, and Bernardo Magnini. The pascal recognising textual entailment challenge. In Machine Learning Challenges Workshop, pp. 177–190. Springer, 2005. Thomas Davidson, Dana Warmsley, Michael Macy, and Ingmar Weber. Automated hate speech detection and the problem of offensive language. In Proceedings of the International AAAI Con- ference on Web and Social Media, volume 11, 2017. Joe Davison, Joshua Feldman, and Alexander M Rush. Commonsense knowledge mining from pretrained models. In Proceedings of the 2019 Conference on Empirical Methods in Natural Lan- guage Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 1173–1178, 2019. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171–4186, 2019. William B Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005), 2005. Tianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better few-shot learners. arXiv preprint arXiv:2012.15723, 2020. Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and William B Dolan. The third pascal rec- ognizing textual entailment challenge. In Proceedings of the ACL-PASCAL workshop on textual entailment and paraphrasing, pp. 1–9, 2007. Beliz Gunel, Jingfei Du, Alexis Conneau, and Ves Stoyanov. Supervised contrastive learning for pre-trained language model ﬁne-tuning. ICLR, 2021. R Bar Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and Idan In Proceedings of the Szpektor. The second pascal recognising textual entailment challenge. Second PASCAL Challenges Workshop on Recognising Textual Entailment, 2006. Jeremy Howard and Sebastian Ruder. Universal language model ﬁne-tuning for text classiﬁcation. arXiv preprint arXiv:1801.06146, 2018. Minqing Hu and Bing Liu. Mining and summarizing customer reviews. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 168–177, 2004. 12 Robin Jia, Aditi Raghunathan, Kerem G¨oksel, and Percy Liang. Certiﬁed robustness to adversarial In Proceedings of the 2019 Conference on Empirical Methods in Natural word substitutions. Language Processing and the 9th International Joint Conference on Natural Language Process- ing (EMNLP-IJCNLP), pp. 4120–4133, 2019. Zhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig. How can we know what language models know? Transactions of the Association for Computational Linguistics, 8:423–438, 2020. Guillaume Lample and Alexis Conneau. Cross-lingual language model pretraining. arXiv preprint arXiv:1901.07291, 2019. Cheolhyoung Lee, Kyunghyun Cho, and Wanmo Kang. Mixout: Effective regularization to ﬁnetune large-scale pretrained language models. arXiv preprint arXiv:1909.11299, 2019. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre- training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 7871–7880, 2020. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019. Andrew Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies, pp. 142–150, 2011. Myle Ott, Sergey Edunov, David Grangier, and Michael Auli. Scaling neural machine translation. arXiv preprint arXiv:1806.00187, 2018. Bo Pang and Lillian Lee. A sentimental education: Sentiment analysis using subjectivity summa- rization based on minimum cuts. arXiv preprint cs/0409058, 2004. Bo Pang and Lillian Lee. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. arXiv preprint cs/0506075, 2005. Jason Phang, Thibault F´evry, and Samuel R Bowman. Sentence encoders on stilts: Supplementary training on intermediate labeled-data tasks. arXiv preprint arXiv:1811.01088, 2018. Raul Puri and Bryan Catanzaro. Zero-shot text classiﬁcation with generative language models. arXiv preprint arXiv:1912.10165, 2019. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer. Journal of Machine Learning Research, 21:1–67, 2020. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for machine comprehension of text. arXiv preprint arXiv:1606.05250, 2016. Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert- networks. arXiv preprint arXiv:1908.10084, 2019. Timo Schick and Hinrich Sch¨utze. Exploiting cloze questions for few-shot text classiﬁcation and natural language inference. arXiv preprint arXiv:2001.07676, 2020a. Timo Schick and Hinrich Sch¨utze. It’s not just size that matters: Small language models are also few-shot learners. arXiv preprint arXiv:2009.07118, 2020b. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language pro- cessing, pp. 1631–1642, 2013. 13 Wilson L Taylor. “cloze procedure”: A new tool for measuring readability. Journalism quarterly, 30(4):415–433, 1953. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461, 2018. Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems. arXiv preprint arXiv:1905.00537, 2019. Alex Warstadt, Amanpreet Singh, and Samuel R Bowman. Neural network acceptability judgments. Transactions of the Association for Computational Linguistics, 7:625–641, 2019. Janyce Wiebe, Theresa Wilson, and Claire Cardie. Annotating expressions of opinions and emotions in. To appear in Language Resources and Evaluation, 1:2, 2004. Adina Williams, Nikita Nangia, and Samuel R Bowman. A broad-coverage challenge corpus for sentence understanding through inference. arXiv preprint arXiv:1704.05426, 2017. Qizhe Xie, Zihang Dai, Eduard Hovy, Thang Luong, and Quoc Le. Unsupervised data augmentation for consistency training. Advances in Neural Information Processing Systems, 33, 2020. Wenpeng Yin, Nazneen Fatema Rajani, Dragomir Radev, Richard Socher, and Caiming Xiong. Uni- versal natural language processing with limited annotations: Try few-shot textual entailment as a start. arXiv preprint arXiv:2010.02584, 2020. Tianyi Zhang, Felix Wu, Arzoo Katiyar, Kilian Q Weinberger, and Yoav Artzi. Revisiting few- sample bert ﬁne-tuning. arXiv preprint arXiv:2006.05987, 2020. Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text clas- siﬁcation. arXiv preprint arXiv:1509.01626, 2015. A Experiment Details The label descriptions we use for each method are the same and deﬁned in Table 10. For model hyperparameters, in full data experiment, we use the learning rate 1e-5, batch size 32, weight decay 0.1, max epochs 10 and linear learning rate decay with a warm up ratio 0.06. In few-shot experiments and ablation studies, we use a constant learning rate 1e-5, batch size 8, max epochs 10, and standard Adam optimizer. For unsupervised contrastive data augmentation (UCA), we use the following default setups: we randomly augment 8 data for each class. • Positive generation: We alternate sentence augmentations with 10% probability in deleting character, 10% probability in reordering words, 40% probability in deleting words, and 40% probability in reordering words. In deleting characters, we delete 15% characters of input sentence. In reordering spans, each span cross 5% consecutive characters of input sentence, and we consider switching of 3 pairs of spans. In deleting words, we randomly delete 15% words of input sentence. In reordering words, we randomly switch 2 pairs of words. • Negative generation: It has same probability distribution across different sentence augmen- tations but higher probability of reordering and deletion: In deleting characters, we delete 40% characters of input sentence; In reordering spans, each span cross 25% consecutive characters of input sentence, and we consider switching of 3 pairs of spans. In deleting words, we randomly delete 40% words of input sentence. In reordering words, we ran- domly switch 2 pairs of words and each contains 40% words. For one-sentence task, suppose p1 is the label description, we use the above rule to generate positive sample S(cid:48) 1[SEP]p1 with equal probability. The negative augmented data consists of 70% of randomly pairwise down-sampled 1 from input sentence S1 and add augmented data: S1[SEP]S(cid:48) 1 or S(cid:48) 14 data and and 30% UCA constructed negative samples. In pairwise down-sampling, we randomly sample the ﬁrst sentence Si1 from existing positive sample, and then randomly sample another ﬁrst sentence Sj1 from existing negative sample, and construct Si1[SEP]Sj1 as new sample. For sentence-pair task, suppose that we have one annotated sample S1[SEP]S2, we will add augmented data S1[SEP]S(cid:48) 1[SEP]S2 with equal probability. The negative data follows the similar rule of one-sentence task. 2 or S(cid:48) Dataset Template of Prompt Finetuning Template of EFL SST-2 MR CR MPQA Subj OS IMDB CoLA Yelp AG news Trec QQP MRPC QNLI MNLI SNLI RTW STS-B BoolQ sentence1[SEP]It was[MASK]. (great / terrible) sentence1[SEP]It was[MASK]. (great / terrible) sentence1[SEP]It was[MASK]. (great / terrible) sentence1[SEP]It was[MASK]. (positive / negative) sentence1[SEP]It was[MASK]. (subjective / objective) sentence1[SEP]It was[MASK]. (hatespeech / benign) sentence1[SEP]It was[MASK]. (great / terrible) sentence1[SEP]It was[MASK]. (correct / incorrect) sentence1[SEP]It was[MASK] news. (Great/good/ok/bad/terrible) sentence1[SEP]It was[MASK] news. (World/sports/business/science) sentence1[SEP]It was[MASK] news. (expression/entity/description/human/location/number) sentence1 ?[MASK], sentence2. (yes / no) sentence1 ?[MASK], sentence2. (yes / no) sentence1 ?[MASK], sentence2. (yes / no) sentence1 ?[MASK], sentence2. (yes / maybe / no) sentence1 ?[MASK], sentence2. (yes / maybe / no) sentence1 ?[MASK], sentence2. (yes / no) sentence1 ?[MASK], sentence2. (yes / no) sentence1 ?[MASK], sentence2. (yes / no) sentence1[SEP]It was great sentence1[SEP]It was great sentence1[SEP]It was great sentence1[SEP]It was positive sentence1[SEP]It was objective sentence1[SEP]It was hatespeech sentence1[SEP]It was great sentence1[SEP]It was correct sentence1[SEP]It was (great/good/ok/bad/terrible). sentence1[SEP]It is (World/sports/ business/science) news. sentence1[SEP]It is (expression/ entity/description/human/location/number). sentence1[SEP]sentence2 sentence1[SEP]sentence2 sentence1[SEP]sentence2 sentence1[SEP]sentence2 sentence1[SEP]sentence2 sentence1[SEP]sentence2 sentence1[SEP]sentence2 sentence1[SEP]sentence2 Table 10: Prompts and label descriptions of prompt-based ﬁnetuning method (Gao et al., 2020) and our method used in experiments. B Benchmark Our benchmark includes 8 datasets from GLUE (Wang et al., 2018): CoLA (Warstadt et al., 2019), SST-2 (Socher et al., 2013), MPRC (Dolan & Brockett, 2005), QQP4, STS-B (Cer et al., 2017), MNLI (Williams et al., 2017), QNLI (Rajpurkar et al., 2016), RTE (Dagan et al., 2005; Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009), IMDB (Maas et al., 2011), Yelp, AG News (Zhang et al., 2015), SNLI (Bowman et al., 2015). For the datasets which require a cross- validation evaluation, we follow similar processing path (Gao et al., 2020): MR (Pang & Lee, 2005), CR (Hu & Liu, 2004), MPQA (Wiebe et al., 2004), Subj (Pang & Lee, 2004)—we simply randomly sample 2,000 examples as the testing set and leave them out from training. BoolQ (Clark et al., 2019) is from SuperGlue (Wang et al., 2019), and OS is twitter offensive speech data (Davidson et al., 2017). 4https://www.quora.com/share/First-Quora-Dataset-Release-Question-Pairs 15 