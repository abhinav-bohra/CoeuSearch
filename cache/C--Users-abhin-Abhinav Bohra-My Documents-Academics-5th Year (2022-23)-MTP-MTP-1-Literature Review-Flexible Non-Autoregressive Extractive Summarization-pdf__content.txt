Flexible Non-Autoregressive Extractive Summarization with Threshold: How to Extract a Non-Fixed Number of Summary Sentences Ruipeng Jia,1,2 Yanan Cao,1,2 Haichao Shi,1,2 Fang Fang,1,2(cid:3) Pengfei Yin,1,2 Shi Wang3(cid:3) 1 Institute of Information Engineering, Chinese Academy of Sciences 2 School of Cyber Security, University of Chinese Academy of Sciences 3 Institute of Computing Technology, Chinese Academy of Sciences jiaruipeng, caoyanan, shihaichao, fangfang0703, yinpengfei g @iie.ac.cn f wangshi@ict.ac.cn Abstract Sentence-level extractive summarization is a fundamental yet challenging task, and recent powerful approaches prefer to pick sentences sorted by the predicted probabilities until the length limit is reached, a.k.a. “Top-K Strategy”. This length limit is ﬁxed based on the validation set, resulting in the lack of ﬂexibility. In this work, we propose a more ﬂexible and accurate non-autoregressive method for single document ex- tractive summarization, extracting a non-ﬁxed number of sum- mary sentences without the sorting step. We call our approach ThresSum as it picks sentences simultaneously and individu- ally from the source document when the predicted probabili- ties exceed a threshold. During training, the model enhances sentence representation through iterative reﬁnement and the intermediate latent variables receive some weak supervision with soft labels, which are generated progressively by adjust- ing the temperature with a knowledge distillation algorithm. Speciﬁcally, the temperature is initialized with high value and drops along with the iteration until a temperature of 1. Experi- mental results on CNN/DM and NYT datasets have demon- strated the effectiveness of ThresSum, which signiﬁcantly outperforms BERTSUMEXT with a substantial improvement of 0.74 ROUGE-1 score on CNN/DM. Introduction Encoder-decoder mechanism is widely used for single doc- ument extractive summarization. The encoder is to encode one sentence into vector representation, while the popular decoder with top-k strategy can be divided into three steps: predict the probability scores of those sentence vectors, sort sentences in descending order in line with the probability scores, and pick sentences until exceeding the length limit. (Nallapati, Zhai, and Zhou 2017; Xiao and Carenini 2019; Liu and Lapata 2019; Xu et al. 2020). However, there are still two inherent obstacles for sentence-level extractive summa- rization: 1) Redundant phrases between selected sentences. The naive approach for the ﬁrst prediction step of decoder is to make independent binary decisions for each sentence, leading to the absence of overlap or redundancy modeling between (cid:3)Corresponding authors: Fang Fang and Shi Wang Copyright c(cid:13) 2021, Association for the Advancement of Artiﬁcial Intelligence (www.aaai.org). All rights reserved. the selected target sentences (Xu et al. 2020; Zhong et al. 2020). The ﬁrst natural solution is to introduce an autoregres- sive decoder (Chen and Bansal 2018; Jadhav and Rajan 2018; Liu and Lapata 2019; Xu et al. 2020), extracting sentences one by one and allowing different sentences to inﬂuence each other. Secondly, reinforcement learning is introduced for decoder to consider the semantics of the entire target sum- mary (Narayan, Cohen, and Lapata 2018; Dong et al. 2018; Bae et al. 2019), which combines the maximum-likelihood cross-entropy loss with the rewards from policy gradient to directly optimize the evaluation metric for the summarization task. The third popular solution is to build summarization system with a two-stage decoder (Aliguliyev 2009; Galanis and Androutsopoulos 2010; Zhang et al. 2019a; Zhong et al. 2020), with the ﬁrst stage to extract some fragments of the original text and the second stage to select or modify on the basis of these fragments. Unfortunately, all these approaches with an autoregressive decoder are unstable, for there are the train-inference gap and error propagation when extracting sentences one by one. 2) Fixed number or proportion of summary sentences. SummaRuNNer (Nallapati, Zhai, and Zhou 2017) ﬁrst sets a “sort” step after the “predict” step, and “pick” sorted sentences until the length limit is reached (a.k.a. “Top-K Strategy”), which has been a popular method employed by the following models (Narayan, Cohen, and Lapata 2018; Zhang et al. 2018; Zhang, Wei, and Zhou 2019; Liu and Lapata 2019; Xu et al. 2020; Wang et al. 2020). While a ﬂexible extractor should generate a non-ﬁxed number of summary sentences based on source document length, topics, or other aspects. The reasonable approach is to pick sentences whose predicted probability is over a threshold. However, it may not be an optimal stratety since the training data is very imbalanced in terms of summary-membership of sentences (Nallapati, Zhai, and Zhou 2017). Furthermore, Mendes et al. (2019) introduces the length variable into the decoder and Zhong et al. (2020) can choose any number of sentences by matching candidate summary in semantic space. But the decoders of these two solutions are either autoregressive or two-stage. To address the above two obstacles, we introduce Thres- Sum, a heuristic approach to strengthen the encoder by en- hancing sentence representation through iterative reﬁnement and simplify the decoder by removing the “sort” step: TheThirty-FifthAAAIConferenceonArtificialIntelligence(AAAI-21)13134(cid:15) (cid:15) (cid:15) For the encoder, we ﬁrst map textual tokens into hidden states by contextualized interactions. Secondly, the sen- tence embedding is extracted through hierarchical atten- tion to adaptively aggregate information from its word elements. Finally, we fuse the redundant information be- tween selected sentences by iterative reﬁnement and this process is supervised by knowledge distillation. The decoder is non-autoregressive with low redundancy, for that our encoder has modeled the overlap information between the selected sentences. In that case, our decoder only consists of two steps instead of the former three steps: predict the probability scores of those sentence vectors, and pick sentences simultaneously and individually when the predicted probability exceeds a threshold. The key component of our extractive model includes a weak supervision for the intermediate latent variables of iterative reﬁnement. We design a teacher algorithm of knowledge distillation to produce high entropy soft la- bels at a high temperature, and progressively reduce the temperature along with iteration until a temperature of 1. Therefore, the former iterative steps with high temper- ature are equivalent to minimize the square difference between ground-truth and prediction, and the latter steps with a lower temperature will pay much more attention to matching the positive elements (Hinton, Vinyals, and Dean 2015). Experimental results validate the effectiveness of Thres- Sum, which signiﬁcantly outperforms BERTSUMEXT by 0.74 ROUGE-1 score on CNN/DM. The human evaluation also shows that our model is better in relevance compared with others. Our contributions in this work are concluded as follows: 1) Instead of extracting sentences one by one to form a top-k summary, we formulate a non-autoregressive decoder, which can extract a non-ﬁxed number of summary sentences simultaneously and individually. 2) We propose iterative reﬁnement to strengthen encoder and enhance the sentence representation. Simultaneously, we introduce and expand the knowledge distillation algorithm to progressively supervise the iterative reﬁnement. 3) Our proposed framework has achieved superior per- formance compared with strong baselines. Moreover, we conduct an analysis to investigate where the performance gain of our model comes from. Related Work Extractive Summarization There are two main lines of summarization: abstractive and extractive. The abstractive paradigm (Celikyilmaz et al. 2018; Sharma et al. 2019) focuses on generating a summary word- by-word after encoding the full document. The extractive approach (Cheng and Lapata 2016) directly selects sentences from the document to assemble into a summary. The abstrac- tive approach is more ﬂexible and generally produces less redundant summaries, while the extractive approach enjoys better factuality and efﬁciency (Cao et al. 2018). Recent research work on extractive summarization spans a large range of approaches. These work usually instantiate their encoder-decoder architecture by choosing RNN (Nal- lapati, Zhai, and Zhou 2017; Zhou et al. 2018), Transformer (Wang et al. 2019; Zhong et al. 2019b; Liu and Lapata 2019; Zhang, Wei, and Zhou 2019) or GNN (Wang et al. 2020; Jia et al. 2020b) as encoder, autoregressive (Jadhav and Rajan 2018; Liu and Lapata 2019) or RL-based (Narayan, Cohen, and Lapata 2018; Arumae and Liu 2018; Bae et al. 2019) decoders. Despite the effectiveness, these models are with the top-k strategy essentially. For two-stage summarization, Chen and Bansal (2018) and Bae et al. (2019) follow a hybrid extract-then-rewrite architecture, with policy-based RL to bridge the two net- works together. Lebanoff et al. (2019), Xu and Durrett (2019) and Mendes et al. (2019) focus on the extract-then-compress learning paradigm, which will ﬁrst train an extractor for con- tent selection. Zhong et al. (2020) introduces extract-then- match framework, which employs BERTSUMEXT (Liu and Lapata 2019) as ﬁrst-stage to prune unnecessary information. However, these two-stage approaches are inherently with error propagation and difﬁcult to train. Knowledge Distillation The common formulation of knowledge distillation (KD) is proposed in (Buciluundeﬁned, Caruana, and Niculescu-Mizil 2006; Hinton, Vinyals, and Dean 2015; Kim and Rush 2016), where a smaller student model is trained on soft probability labels provided by a larger teacher model (with temperature T for ﬁnal softmax). More recently, (Tan et al. 2019) applied KD to multilingual NMT, and (Sun et al. 2019) proposed patient KD for BERT model compression. Our distillation focuses on using KD to generalize student model for ﬁtting the cumbersome inter-relationship of sentences, while these previous work mostly focused on model compression. Methodology Problem Deﬁnition Given a document D consisting of a sequence of M sentences (s1; s2; :::; sM ) and a sentence si consisting of a sequence of N words (wi1; wi2; :::; wiN ). We denote by hi and hij the embedding of sentences and words in a continuous space. by The extractive summarizer aims to produce a summary S M ). For each selecting m sentences from D (where m (cid:20) sentence si 0; 1 D, there is ground-truth yi and we g 2 f will predict a label ^yi , where 1 means that si should g si; D; (cid:18)) be included in the summary. We assign a score p(^yi to quantify si’s relevance to the summary, where (cid:18) is the parameters of neural network model. Finally, we assemble a si; D; (cid:18)) summary S exceeds the threshold. by selecting m sentences, in which p(1 0; 1 2 f 2 j j Overview of Architecture ThresSum consists of a powerful encoder and an easy- adjustable decoder, as shown in Figure 1(a). Encoder: In order to learn the contextual representation of words, we utilize the pre-trained ALBERT (enhanced version of BERT) (Lan et al. 2020). The output of ALBERT contains 13135Figure 1: Overview of ThresSum. words hidden state hij, special tokens h[CLS] and h[SEP]. Liu and Lapata (2019) simply choose the h[CLS] as sentence rep- resentation, while we think it is difﬁcult for [CLS] to identify the boundary of sentences. Inspired by Yang et al. (2016), we employ the hierarchical attention mechanism, shown as Fig- ure 1(b), where the context vector uw can be seen as a high level representation of a ﬁxed query “what is the informative word”. i ; ::::hL i ; :::; gL(cid:0)1 i In the process of iterative reﬁnement with L steps, there are different state representations for si: (h0 i ), where hl i is the hidden state of si at l-th iteration. We also intro- duce intermediate random variables (g0 ) for each sentence si, where gl i is the importance for sentence state hl i. With the assistance of these latent variables, the encoder can implicitly aggregate the redundant information between selected summary sentences into hi. 1 ; :::; hL Decoder: For the ﬁnal sentences state (hL M ), our decoder predicts the probabilities (gL 1 ; :::; gL M ) with feed- forward network and sigmoid. Then we adjust the threshold to pick ﬂexible summary sentences. Approximate Models The standard conditional probability distribution of selecting sentence si from D is as below: P (^yi = 1 j D; y1; :::; yi(cid:0)1; yi+1; :::; yM ) (1) where we must have the ground-truth labels of other sen- tences before calculating si. However, there is still no known polynomial algorithm to solve it exactly. Autoregressive originates from the literature on time-series models, where observations from the previous time-steps are used to predict the value at the current time step, i.e., P (^yt = 1 D; y<t) j (2) The autoregressive paradigm is with error propagation in- herently, especially when there is misjudgment for the ﬁrst element. In this paper, we introduce a non-autoregressive architecture, P (^yi = 1 i(cid:0)1; ^y0 (3) where ^y0 i is the pre-predicted label introduced to implicitly capture the bidirectional dependencies among target symbols. i+1; :::; ^y0 1; :::; ^y0 D; ^y0 M ) j Iterative Reﬁnement Our model iteratively updates the latent variables gl ing document semantic information with (gl(cid:0)1 pecially: 1 i by mask- M ), es- ; :::; gl(cid:0)1 j (4) i = 1 ; :::; ^yl(cid:0)1 M ) D; ^yl(cid:0)1 i = P (^yl gl 1 = (cid:27)(FFN(LN(Hl + MHAtt(Hl))))i where FFN, LN, MHAtt are for feed-forward network, layer normalize, and multi-head attention; ^yl(cid:0)1 is the pre-predicted i 1 iteration; Hl is a matrix which contains the label for l (cid:0) sentences hidden state (hl M ) at l-th iteration. 1; :::; hl As shown in Figure 1(a), for l-th iterative reﬁnement, a Transformer-like unit layer is stacked on the top of hidden state: l ~H = LN(Hl(cid:0)1 + MHAtt(Hl(cid:0)1)) l ~H Wr tanh(Rl) l Hl = Wc ~H (cid:0) (cid:12) (5) l where ~H is the input of l-th iteration, and Hl gets updated by reducing redundancy Rl, a dynamic matrix representation of redundancy for each sentence (rl 1; :::; rl means the dot product of the i-th ~hl i in ~Hl and the corresponding rl i, i.e. ~hl iWrtanh(rl operation returns a new M -dimensional M ); i); (cid:12) (cid:12)  5  4  3  1  2 hidden statehidden state 11 10 12 13 22 21 FF & SigmoidAdd & Norm 20 30 40 50 1  FF & SigmoidAdd & Norm 2  3  4  5  1  FF & SigmoidAdd & Norm 2  3  4  5  amb1* amb2* amb3*Flexible Extraction with Adjustable Threshold hidden statehidden statehidden state Iterative Refinement ×LMH AttentionMH AttentionMH Attentionhidden stateHierarchical Atttention 21 22 2    1 2   21 22 22⋯⋯(b) Hierarchical Attention(a) ThresSum13136vector. rl i represents the redundant phrase information of i-th sentence, which is a weighted summation of all other sentence-level hidden states: rl i = X gl(cid:0)1 j ~hl j j2f1;:::;M gnfig (6) where gl(cid:0)1 2 sentence information ~hl j. j [0; 1] is a predicted probability to mask the Finally, ThresSum is trained to predict the label of sen- tences and the overall training is equivalent to optimizing the following conditional probability: El(cid:24)f0;:::;Lg (cid:20) L + l 2L = L (cid:0) Ei(cid:24)f1;:::;M g(cid:30)(^yl i = yl ij (cid:21) si; D; (cid:18)) (7) where we gradually increase the proportion L+l 2L of each reﬁnement according to importance; l and i are the index of randomly sampled iteration step and sentence; (cid:30) is the regular binary cross-entropy loss with respect to the prediction ^yl i against soft ground-truth label yl i: (cid:30)(^yl i = yl ij si; D; (cid:18)) = yl i log(gl i) + (1 yl i) log(1 gl i) (8) (cid:0) (cid:0) Knowledge Distillation for Soft Labels Most summarization datasets only contain human written ab- stractive summaries as ground truth. Thus, a greedy approach (Nallapati, Zhai, and Zhou 2017; Liu and Lapata 2019) is employed that adds one sentence at a time incrementally to the summary, with maximizing the ROUGE score and stop- ping until none of remaining candidate sentences improves the score. Theoretically, soft labels with high entropy will provide much more information than binary hard labels and much less variance in the gradient between training cases (Hinton, Vinyals, and Dean 2015). While, the binary labels can maxi- mize the margin between positive and negative examples by extracting salient sentences and reduce redundancy. Intuitively, the former iterative steps in our architecture serve as a small model with few iteration, and the latter steps are larger with more iterations. Therefore, the iterative reﬁnement with gl i should be gradually trained with soft labels i, until the last step gL yl 0; 1 . i with ground-truth binary label g f In this work, we modify the knowledge distillation (Jia i ; :::; yL et al. 2020a) to design more soft target labels (y0 i ) for the intermediate variables (g0 i ). As Algorithm 1, we design a teacher algorithm of knowledge distillation to produce high entropy soft labels at a high temperature, and progressively reduce the temperature along with the iterations until a temperature of 1. As a result, the former iterative steps with high temperature are equivalent to the regression approach and the latter steps with lower temperature will pay much more attention to matching positive units (Hinton, Vinyals, and Dean 2015). i ; :::; gL We denote ri; r2; :::; rM as the individual ROUGE scores of each sentence against the human-written summary, espe- cially: Algorithm 1: Teacher Algorithm for Soft Labels Initialize Sentence Set D = g Initialize ROUGE r1; :::rM , and Iteration Steps L ; Sort D by r1; :::; rM in descending order ; for l from 0 to L 1 do s1; :::; sM f ; (cid:0) Set the Temperature T as L for t from T to 1 do l ; (cid:0) Temporary Sentence Set: Dtemp  fg Temporary ROUGE of Dtemp: Rtemp for si from D[0] to D[end] do ; 0 ; Dtemp if Rtemp is increasing then Dtemp + si ; D else D si (cid:0) Dtemp Dtemp end si ; (cid:0) end Set the Sentence s in Dtemp with Soft Label t T ; end Set the Sentence s Remained in D with Label 0 ; Record these Soft Labels as (yl M ) ; Re-Initialize Sentence Set D = ; g Re-Sort D by r1; :::; rM in descending order ; 2; :::; yl s1; :::; sM 1; yl f end Datasets Experiments As shown in Table 1, we employ two datasets widely- used with multiple sentences summary: CNN and Dailymail (CNN/DM) (Hermann et al. 2015) and New York Times (NYT) (Sandhaus 2008). CNN/DM. We used the standard split (Hermann et al. 2015) for training, validation and test (90,266/1,220/1,093 for CNN and 196,96/12.148/10,397 for Daily Mail), with splitting sentences by Stanford CoreNLP (Manning et al. 2014) toolkit and pre-processing the dataset following (See, Liu, and Manning 2017) and (Xu et al. 2020). This dataset contains news articles and several associated abstractive high- lights. We use the un-anonymized version as in previous summarization work and each document is truncated to 768 BPE tokens. NYT. Following previous work (Zhang, Wei, and Zhou 2019; Xu and Durrett 2019), we use 137,778, 17,222 and 17,223 samples for training, validation and test, respectively. Input documents were truncated to 768 BPE tokens too. Note that there are different divisions for NYT (Durrett, Berg- Kirkpatrick, and Klein 2016; Liu and Lapata 2019) and sev- eral models are not evaluated on NYT ofﬁcially. e.g. See, Liu, and Manning (2017) and Mendes et al. (2019), so we re-train and evaluate them on NYT with the source code from Github. Parameters & Metrics Our code is based on Pytorch (Paszke et al. 2019) and the pre-trained model employed in ThresSum is ‘albert-xxlarge- 13137        Datasets # docs (train / val / test) avg.doc length words sentences avg.summary length sentences words CNN DailyMail NYT 90,266 / 1,220 / 1,093 196,961 / 12,148 / 10,397 137,778 / 17,222 / 17,223 760.50 653.33 800.04 33.98 29.33 35.55 45.70 54.65 45.54 3.59 3.86 2.44 Table 1: Data Statistics: CNN/Daily Mail and NYT. v2’ (huggingface/transformers1). We train ThresSum (with about 400M parameters) two days for 100,000 steps on 2GPUs(Nvidia Tesla V100, 32GB) with gradient accumula- tion every two steps. Adam with (cid:12)1 = 0:9; (cid:12)2 = 0:999 is used as optimizer. Learning rate schedule follows the strategy with warming-up on ﬁrst 10,000 steps. We have tried the iteration steps of [1; 3; 5; 7] for knowl- edge distillation, and L = 5 is the best choice based on the validation set. In comparison, we have tried to replace bi- nary cross-entropy with regression objective, but the result indicates that regression can’t achieve the performance of cross-entropy. The ﬁnal threshold of extraction is 0.73 for CNN/DM and 0.78 for NYT, which are tuned on the valida- tion set to get the highest ROUGE-1 score. A higher threshold is for a more concise summary and the lower threshold will return more information. We report the F1 ROUGE score of ThresSum by ROUGE- 1.5.5.pl (Lin 2004), which calculates the overlap lexical units between extracted sentences and ground-truth. Our source code will be available on Github.2 Baselines Abstractive Methods: ABS is the normal architecture with RNN-based encoder and decoder. PGC augments the stan- dard Seq2Seq attentional model with pointer and cover- age mechanisms. TransformerABS employs Transformer in text summarization. T5, BART, and ProphetNet are pre- trained on large unlabeled data and perform excellent perfor- mance with Transformer architecture. PEGASUS proposes Transformer-based models with extracted gap-sentences for abstractive summarization. Extractive Methods: Oracle Summary is the extracted summary according to the ground-truth labels. Speciﬁcally, the oracle summary is essential to reveal the upper bound performance of the extractive paradigm. Lead-3 is a base method for extractive text summarization that chooses ﬁrst three sentences as a summary. SummaRuNNer takes con- tent, salience, novelty, and position of each sentence into con- sideration when deciding if a sentence should be included in the extractive summary. Exconsumm ﬁrst extracts sentences from a document and then compresses them. PNBERT tries to employ the unsupervised transferable knowledge. Dis- coBERT extracts sub-sentential discourse units as candi- dates for extractive selection on a ﬁner granularity. BERT- SUMEXT applies pre-trained BERT in text summarization and proposes a general framework for both extractive and 1https://github.com/huggingface/transformers 2https://github.com/coder352/ThresSum abstractive models. MATCHSUM is a two stage method for extract-then-match, and the ﬁrst-stage is BERTSUMEXT. Result & Analysis ROUGE Score The experiment results of ROUGE are shown in Table 2. These scores are in accordance with original papers and the missing ones (only for NYT) are calculated with source code on Github by ourselves. It is obvious that our Thres- Sum outperforms all the baseline models, demonstrating that our enhanced encoder can help to model the relationships across source sentences and selected sentences. Speciﬁcally, our model outperforms MATCHSUM by 0.18 ROUGE-1, 0.29 ROUGE-2 and 0.21 ROUGE-L on CNN/DM. For more in-depth performance analysis, we note that: 1) Pre-trained BERT-like model is so powerful for that it can capture bidirec- tional dependencies by applying deep architecture; 2) Flexi- bility of the summary length is essential, for the large margin between ThresSum/MATCHSUM and BERTSUMEXT. Ablation Studies We propose several strategies to improve the performance of extractive summarization, including knowledge distilla- tion(vs. binary labels), pre-trained ALBERT(vs. BERT), and iterative reﬁnement(vs. None). To investigate the inﬂuence of these factors, we conduct the experiments and list the re- sults in Table 3. Signiﬁcantly, 1) Iterative reﬁnement is more important than ALBERT, for the reason that the redundant information in selected sentences are difﬁcult for ALBERT to model; 2) Knowledge distillation mechanism enlarges the advantage of extractive method, with high entropy for the soft labels (Hinton, Vinyals, and Dean 2015). Human Evaluation for Summarization It is not enough to only rely on the ROUGE evaluation for a summarization system, although the ROUGE correlates well with human judgments (Owczarzak et al. 2012). Therefore, we design an Amazon Mechanical Turk experiment based on ranking method. Following (Cheng and Lapata 2016), (Narayan, Cohen, and Lapata 2018) and (Zhang, Wei, and Zhou 2019), ﬁrstly, we randomly select 40 samples from CNN/DM test set. Then the human participants are presented with one original document and a list of corresponding sum- maries produced by different model systems. Participants are requested to rank these summaries (ties allowed) by taking informativeness (Can the summary capture the important in- formation from the document) and ﬂuency (Is the summary 13138Models Abstractive ABS (2015) PGC (2017) TransformerABS (2017) T5Large (2020) BARTLarge (2019b) PEGASUSLarge (2019b) ProphetNetLarge (2020) Extractive Oracle (Sentence) Lead-3 y SummaRuNNer y ? (2017) Exconsumm z ? (2019) y ?(2019a) PNBERTBase DiscoBERTBase (2020) BERTSUMEXTLarge MATCHSUMBase z ThresSumLarge z ?(2020) (Ours) (cid:15) y ?(2019) CNN/DM R-2 R-1 R-L R-1 35.46 39.53 40.21 43.52 44.16 44.17 44.20 55.61 40.42 39.60 41.7 42.69 43.77 43.85 44.41 44.59 13.30 17.28 17.76 21.55 21.28 21.47 21.17 32.84 17.62 16.20 18.6 19.60 20.85 20.34 20.86 21.15 32.65 36.38 37.09 40.69 40.90 41.11 41.30 51.88 36.67 35.30 37.8 38.85 40.67 39.90 40.55 40.76 42.78 43.93 45.36 - 48.73 - - 64.22 41.80 42.37 43.18 - - 48.51 - 50.08 NYT R-2 25.61 26.85 27.34 - 29.25 - - 44.57 22.60 23.89 24.43 - - 30.27 - 31.77 R-L 35.26 38.67 39.53 - 44.48 - - 57.27 35.00 38.74 38.92 - - 44.65 - 45.21 y means Top-K strategy; z means Dynamically Adjusting Summary Length. ? means with Binary Labels / Hard Labels; (cid:15) means with Soft Labels. Table 2: Automatic Evaluation of ROUGE F1. Models R-1 R-2 R-L ThresSum ThresSum w/o Distillation ThresSum w/o ALBERT ThresSum w/o Iteration 44.59 44.18 44.35 43.98 21.15 20.95 21.03 20.74 40.76 40.42 40.57 40.19 Table 3: Ablation Study on CNN/DM. Models 1st BERTSUMEXT 0.20 0.23 MATCHSUM 0.47 ThresSum 0.70 Ground-Truth 2nd 0.28 0.32 0.28 0.20 3rd 0.30 0.27 0.18 0.08 4th MeanR 0.22 0.18 0.07 0.02 2.54 2.40 1.85 1.42 Table 4: Human Evaluation on CNN/DM. grammatical) into account. Each document is annotated by three different participants separately. The input article and ground truth summaries are also shown to the human participants in addition to the three model summaries (BERTSUMEXT, MATCHSUM and ThresSum). From the results shown in Table 4, it is obvious that ThresSum is better in relevance compared with others. with the previously selected sentences, bringing a remarkable performance improvement on CNN/DM. Whereas there is another statistic on the test set of CNN/DM: (cid:15) (cid:15) (cid:15) (cid:15) 7.35% of the oracle summaries have trigram overlaps within its sentences. 8.64% of the summaries extracted by our ThresSum (with iterative reﬁnement) have trigram overlaps within its sen- tences. 21.47% of the summaries extracted by our ThresSum (with- out iterative reﬁnement) have trigram overlaps within its sentences. 0% of the summaries extracted by BERTSUMEXT (with Trigram-Blocking) have trigram overlaps within its sen- tences. The oracle summaries are the upper bound of the extractive paradigm and 7.35% of them still contain trigram overlaps, while it is 0% for BERTSUMEXT with Trigram-Blocking. Consequently, Trigram-Blocking is a straightforward yet not optimal approach. In this paper, the proposed iterative re- ﬁnement is to model the overlaps between the selected sen- tences, and it can effectively avoid but not empty the overlaps. ThresSum with Iterative Reﬁnement reduces the overlaps from 21.47% to 8.64%, showing superiority over Trigram- Blocking. Trigram-Blocking vs. Iterative Reﬁnement Trigram blocking (Paulus, Xiong, and Socher 2018; Liu and Lapata 2019) skips the sentence that has trigram overlaps Autoregressive Decoders with Threshold ThresSum extracts a non-ﬁxed number of summary sentences with a threshold, but whether there is a barrier that prevents 13139Models ThresSum ThresSum (with Trigram-Blocking) BERTSUMEXT BERTSUMEXT (with Threshold) SummaRuNNer SummaRuNNer (with Threshold) R-1 R-L 44.59 44.03 43.85 41.17 39.60 36.58 40.76 40.25 39.90 36.52 35.30 33.61 Table 5: Threshold Strategy on CNN/DM. (a) SummaRuNNer (b) BERTSUMEXT previous models like SummaRuNNer or BERTSUMEXT to do this? It has been explained by Nallapati, Zhai, and Zhou (2017), that picking all sentences by comparing the predicted probability with a threshold may not be an optimal strategy since the training data is very imbalanced in terms of summary-membership of sentences. Table 5 further summarizes the performance gain of thresh- old strategy. The thresholds for BERTSUMEXT and Sum- maRuNNer are tuned on the validation set individually to get highest ROUGE-1 score. It is obvious that the threshold strategy is not suitable for BERTSUMEXT/SummaRuNNer, for that the independent binary decision is based on the over- laps modeling between selected sentences. On the other hand, the Trigram-Blocking strategy with ﬁxed top-3 summary sen- tences will damage the ﬂexibility of ThresSum. Non-Fixed Number of Summary Sentences Since the extractive summarization requires sentence-level summary membership lables, Nallapati, Zhai, and Zhou (2017) ﬁrst introduces a simple greedy approach to con- vert the abstractive summaries into extractive binary labels. Considering that ThresSum removes the restriction of the summary sentence number, it is necessary to discuss the dis- tribution of summary sentence numbers on the test set of CNN/DM. According to statistics, 5% / 27% / 68% of the test set examples are with 1- / 2- / 3-sentences summary. However, previous extractive approaches (such as SummaRuNNer and BERTSUMEXT) with top-k strategy only extract 3 sentences, which is not suitable for almost half of the examples. In this paper, there are about 6% / 35% / 59% for 1- / 2- / 3-sentences summary in CNN/DM test dataset, extracted by our ThresSum to get the highest ROUGE-1 score. Our threshold is still a hyper-parameter which need to be tuned. A higher threshold is for a more concise summary and the lower threshold will return more information. Visualization We visualize the three types of well-trained sentence represen- tation of SummaRuNNer, BERTSUMEXT, and ThresSum by employing the t-SNE algorithm. T-SNE is used to visualize the representations of sentences learned by models, and a better extractive model should enlarge the distance between different clusters / different colors. Speciﬁcally, we randomly select 1000 sentences in test set and each sentence is repre- sented as one point in the two-dimensional space. (c) ThresSum Figure 2: T-SNE Visualization on CNN/DM. In Figure 2, there are ﬁve different colors, for ﬁve different soft labels in our ThresSum. It is obvious that: 1) Compared with BERTSUMEXT or SummaRuNNer, the sentence clus- ters in ThresSum are more distinguishable and meaningful; 2) The decoder of ThresSum is easy to score these sentences in- dividually and extract summary sentences with an adjustable threshold; 3) That’s why our ThresSum can extract summary sentences by threshold, while other models only extract top-3 sentences. Conclusion In this paper, to remove the restriction that the number of the summary sentences is ﬁxed, we introduce three substantial improvements: strengthen the encoder by enhancing sentence representation through iterative reﬁnement, simplify the de- coder by removing the “sort” step, and weakly supervise the intermediate latent variables of iterative reﬁnement by knowledge distillation. It is amazing that our ThresSum can extract each sentence separately only according to an ad- justable threshold, which is a great improvement by ﬁtting the distribution of sentence number in extractive summariza- tion. Experimental results show that our method signiﬁcantly outperforms previous models on the ROUGE score, the ﬂexi- bility of summary sentence, and the proportion of the over- laps. Our future work will focus on extending the ﬂexible summary-sentences mechanism to unsupervised summariza- tion. Acknowledgements This research is supported by the National Key Research and Development Program of China (NO.2018YFB1004703) and National Natural Science Foundation of China (No.61902394). We thank all authors for their contributions and all anonymous reviewers for their constructive com- ments.  30 20 100102030X 60 40 2002040Y 40 30 20 100102030X 60 40 200204060Y 40 2002040X 40 30 20 100102030Y13140References Aliguliyev, R. M. 2009. The two-stage unsupervised ap- proach to multidocument summarization. Automatic Control and Computer Sciences 276–284. Arumae, K.; and Liu, F. 2018. Reinforced Extractive Summa- rization with Question-Focused Rewards. In ACL, 105–111. Bae, S.; Kim, T.; Kim, J.; and goo Lee, S. 2019. Summary Level Training of Sentence Rewriting for Abstractive Sum- marization. In arXiv preprint arXiv:1909.08752. Buciluundeﬁned, C.; Caruana, R.; and Niculescu-Mizil, A. 2006. Model Compression. In Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 535–541. Cao, Z.; Wei, F.; Li, W.; and Li, S. 2018. Faithful to the Original: Fact Aware Neural Abstractive Summarization. In AAAI, 4784–4791. Celikyilmaz, A.; Bosselut, A.; He, X.; and Choi, Y. 2018. Deep Communicating Agents for Abstractive Summarization. In NAACL-HLT, 1662–1675. Chen, Y.-C.; and Bansal, M. 2018. Fast Abstractive Sum- marization with Reinforce-Selected Sentence Rewriting. In ACL, 675–686. Cheng, J.; and Lapata, M. 2016. Neural summarization by extracting sentences and words. In ACL. doi:10.18653/v1/ p16-1046. Dong, Y.; Shen, Y.; Crawford, E.; van Hoof, H.; and Cheung, J. C. K. 2018. BanditSum: Extractive Summarization as a Contextual Bandit. EMNLP 3739–3748. Durrett, G.; Berg-Kirkpatrick, T.; and Klein, D. 2016. Learning-based single-document summarization with com- In arXiv preprint pression and anaphoricity constraints. arXiv:1603.08887. Galanis, D.; and Androutsopoulos, I. 2010. An extractive supervised two-stage method for sentence compression. In NAACL-HLT, 885–893. Hermann, K. M.; Kocisky, T.; Grefenstette, E.; Espeholt, L.; Kay, W.; Suleyman, M.; and Blunsom, P. 2015. Teaching machines to read and comprehend. In NIPS, 1693–1701. Hinton, G.; Vinyals, O.; and Dean, J. 2015. Distilling the Knowledge in a Neural Network. In NIPS Deep Learning and Representation Learning Workshop. URL http://arxiv. org/abs/1503.02531. Jadhav, A.; and Rajan, V. 2018. Extractive summarization with swap-net: Sentences and words from alternating pointer networks. In ACL, 142–151. Jia, R.; Cao, Y.; Shi, H.; Fang, F.; Liu, Y.; and Tan, J. 2020a. DistilSum: Distilling the Knowledge for Extractive Summa- rization. In CIKM, 2069–2072. Jia, R.; Cao, Y.; Tang, H.; Fang, F.; Cao, C.; and Wang, S. 2020b. Neural Extractive Summarization with Hierarchical Attentive Heterogeneous Graph Network. In EMNLP, 3622– 3631. Kim, Y.; and Rush, A. M. 2016. Sequence-Level Knowledge Distillation. In EMNLP, 1317–1327. Lan, Z.; Chen, M.; Goodman, S.; Gimpel, K.; Sharma, P.; and Soricut, R. 2020. ALBERT: A Lite BERT for Self- supervised Learning of Language Representations. In ICLR. OpenReview.net. URL https://openreview.net/forum?id= H1eA7AEtvS. Lebanoff, L.; Song, K.; Dernoncourt, F.; Kim, D. S.; Kim, S.; Chang, W.; and Liu, F. 2019. Scoring Sentence Singletons and Pairs for Abstractive Summarization. ACL 2175–2189. Lin, C.-Y. 2004. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, 74–81. Liu, Y.; and Lapata, M. 2019. Text summarization with pretrained encoders. In EMNLP, 3728–3738. Manning, C.; Surdeanu, M.; Bauer, J.; Finkel, J.; Bethard, S.; and McClosky, D. 2014. The Stanford CoreNLP Natural Language Processing Toolkit. In ACL, 55–60. Mendes, A.; Narayan, S.; Miranda, S.; Marinho, Z.; Martins, A. F.; and Cohen, S. B. 2019. Jointly Extracting and Com- pressing Documents with Summary State Representations. In NAACL-HLT, 3955–3966. Nallapati, R.; Zhai, F.; and Zhou, B. 2017. Summarunner: A recurrent neural network based sequence model for extractive summarization of documents. In AAAI, 3075–3081. Narayan, S.; Cohen, S. B.; and Lapata, M. 2018. Ranking sentences for extractive summarization with reinforcement learning. In NAACL-HLT, 1747–1759. Owczarzak, K.; Conroy, J. M.; Dang, H. T.; and Nenkova, A. 2012. An assessment of the accuracy of automatic eval- uation in summarization. In Proceedings of Workshop on Evaluation Metrics and System Comparison for Automatic Summarization, 1–9. Paszke, A.; Gross, S.; Massa, F.; Lerer, A.; Bradbury, J.; Chanan, G.; Killeen, T.; Lin, Z.; Gimelshein, N.; Antiga, L.; Desmaison, A.; Kpf, A.; Yang, E.; DeVito, Z.; Raison, M.; Tejani, A.; Chilamkurthy, S.; Steiner, B.; Fang, L.; Bai, J.; and Chintala, S. 2019. PyTorch: An Imperative Style, High- Performance Deep Learning Library. In NIPS, 8024–8035. Paulus, R.; Xiong, C.; and Socher, R. 2018. A Deep Rein- forced Model for Abstractive Summarization. In ICLR. Raffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.; Matena, M.; Zhou, Y.; Li, W.; and Liu, P. J. 2020. Explor- ing the limits of transfer learning with a uniﬁed text-to-text transformer. J. Mach. Learn. Res. 140:1–140:67. Rush, A. M.; Chopra, S.; and Weston, J. 2015. A Neural Attention Model for Abstractive Sentence Summarization. In EMNLP, 379–389. Sandhaus, E. 2008. The new york times annotated corpus. In Linguistic Data Consortium, Philadelphia. See, A.; Liu, P. J.; and Manning, C. D. 2017. Get To The Point: Summarization with Pointer-Generator Networks. In ACL, 1073–1083. 13141Zhong, M.; Wang, D.; Liu, P.; Qiu, X.; and Huang, X. 2019b. A Closer Look at Data Bias in Neural Extractive Summariza- tion Models. In arXiv preprint arXiv:1909.13705. Zhou, Q.; Yang, N.; Wei, F.; Huang, S.; Zhou, M.; and Zhao, T. 2018. Neural document summarization by jointly learning to score and select sentences. In ACL, 654–663. Sharma, E.; Huang, L.; Hu, Z.; and Wang, L. 2019. An Entity-Driven Framework for Abstractive Summarization. In EMNLP, 3278–3289. Sun, S.; Cheng, Y.; Gan, Z.; and Liu, J. 2019. Patient Knowl- edge Distillation for BERT Model Compression. In EMNLP, 4322–4331. Tan, X.; Ren, Y.; He, D.; Qin, T.; Zhao, Z.; and Liu, T.- Y. 2019. Multilingual Neural Machine Translation with Knowledge Distillation. In ICLR. OpenReview.net. URL https://openreview.net/forum?id=S1gUsoR9YX. Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. Attention is all you need. In NIPS, 5998–6008. Wang, D.; Liu, P.; Zheng, Y.; Qiu, X.; and Huang, X. 2020. Heterogeneous Graph Neural Networks for Extractive Docu- ment Summarization. In ACL, 6209–6219. Wang, D.; Liu, P.; Zhong, M.; Fu, J.; Qiu, X.; and Huang, X. 2019. Exploring Domain Shift in Extractive Text Summa- rization. In arXiv preprint arXiv:1908.11664. Xiao, W.; and Carenini, G. 2019. Extractive Summarization of Long Documents by Combining Global and Local Context. In EMNLP, 3009–3019. Xu, J.; and Durrett, G. 2019. Neural Extractive Text Summa- rization with Syntactic Compression. EMNLP 3290–3301. Xu, J.; Gan, Z.; Cheng, Y.; and Liu, J. 2020. Discourse-Aware Neural Extractive Text Summarization. In ACL, 5021–5031. Yan, Y.; Qi, W.; Gong, Y.; Liu, D.; Duan, N.; Chen, J.; Zhang, R.; and Zhou, M. 2020. ProphetNet: Predicting Fu- ture N-gram for Sequence-to-Sequence Pre-training. In arXiv preprint arXiv:2001.04063, 2401–2410. Yang, Z.; Yang, D.; Dyer, C.; He, X.; Smola, A.; and Hovy, E. 2016. Hierarchical Attention Networks for Document Classiﬁcation. In NAACL-HLT, 1480–1489. Zhang, H.; Gong, Y.; Yan, Y.; Duan, N.; Xu, J.; Wang, J.; Gong, M.; and Zhou, M. 2019a. Pretraining-Based Natural Language Generation for Text Summarization. In CoNLL, 789–797. Zhang, J.; Zhao, Y.; Saleh, M.; and Liu, P. J. 2019b. PEGA- SUS: Pre-training with Extracted Gap-sentences for Abstrac- tive Summarization. In arXiv preprint arXiv:1912.08777, 11328–11339. Zhang, X.; Lapata, M.; Wei, F.; and Zhou, M. 2018. Neural In EMNLP, Latent Extractive Document Summarization. 779–784. Zhang, X.; Wei, F.; and Zhou, M. 2019. HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization. In ACL, 5059–5069. Zhong, M.; Liu, P.; Chen, Y.; Wang, D.; Qiu, X.; and Huang, X. 2020. Extractive Summarization as Text Matching. In ACL, 6197–6208. Zhong, M.; Liu, P.; Wang, D.; Qiu, X.; and Huang, X. 2019a. Searching for Effective Neural Extractive Summarization: What Works and Whats Next. In ACL, 1049–1058. 13142