Hie-BART: Document Summarization with Hierarchical BART Kazuki Akiyama Ehime University k_akiyama@ai.cs.ehime-u.ac.jp Akihiro Tamura Doshisha University aktamura@mail.doshisha.ac.jp Takashi Ninomiya Ehime University ninomiya@cs.ehime-u.ac.jp Abstract This paper proposes a new abstractive sum- marization model for documents, hierarchi- cal BART (Hie-BART), which captures the hierarchical structures of documents (i.e., their sentence-word structures) in the BART model. Although the existing BART model has achieved state-of-the-art performance on document summarization tasks, it does not ac- count for interactions between sentence-level and word-level information. In machine trans- lation tasks, the performance of neural ma- chine translation models can be improved with the incorporation of multi-granularity self-attention (MG-SA), which captures rela- tionships between words and phrases. In- spired by previous work, the proposed Hie- BART model incorporates MG-SA into the encoder of the BART model for capturing sentence-word structures. Evaluations per- formed on the CNN/Daily Mail dataset show that the proposed Hie-BART model outper- forms strong baselines and improves the per- formance of a non-hierarchical BART model (+0.23 ROUGE-L). 1 Introduction In recent years, improvements to abstractive doc- ument summarization models have been devel- oped through the incorporation of pre-training. The BERTSUM model (Liu and Lapata, 2019) has been proposed as a pre-training model for document summarization tasks. For sequence-to- sequence tasks, the T5 model (Raffel et al., 2020) and the BART model (Lewis et al., 2020) have been proposed as part of generalized pre-training models. Among the existing pre-training models, the BART model achieves state-of-the-art perfor- mance on document summarization tasks. How- ever, the BART model does not capture the hierar- chical structures of documents when generating a summary. Neural machine translation has been improved by the capture of multiple granularities of in- formation in input texts such as “phrases and words” and “words and characters”. In particular, Transformer-based machine translation model has been improved by incorporating multi-granularity self-attention (MG-SA) (Hao et al., 2019), which considers the relationships between words and phrases by decomposing an input text into its el- ements using multiple granularity (i.e., words and phrases) and assigning each granular element (i.e., a word or a phrase) to a head in multi-head Self- Attention Networks (SANs). This method enables interactions not only between words but also be- tween phrases and words, through self-attentions. Inspired by previous work, this paper proposes a new abstractive document summarization model, hierarchical BART (Hie-BART), which captures a document’s hierarchical structures (i.e., sentence- word structures) through the SANs of the BART model. Here, a document is divided into ele- ments with word-level and sentence-level granu- larity, where each element is assigned to a head of the SANs layers of the BART encoder. Then, information with multi-granularity is captured by combining the output of the SANs layers, where the ratio of combining word-level and sentence- level information is controlled by a hyperparame- ter. We evaluated the proposed model in an abstrac- tive summarization task with the CNN/Daily Mail dataset. Our evaluation shows that our Hie-BART model improves the F-score of ROUGE-L by 0.23 points relative to the non-hierarchical BART model, and the proposed model is better than the strong baselines, BERTSUM and T5 models. 2 Background 2.1 BART The BART model (Lewis et al., 2020) is a general- ized pre-training model based on the Transformer ProceedingsofNAACL-HLT2021:StudentResearchWorkshop,pages159–165June6–11,2021.©2021AssociationforComputationalLinguistics159Figure 1: The overview architecture of BART. The en- coder is a bidirectional model and the decoder is an autoregressive model. model (Vaswani et al., 2017). Five pre-training techniques are introduced: token masking, sen- tence permutation, document rotation, token dele- tion, and text inﬁlling. Each of these is a denoising autoencoder tech- nique that adds noise to the original text and re- stores the original text. Token masking, as used in BERT (Devlin et al., 2019), randomly masks tokens. Sentence permutation randomly shuf- ﬂes the sentences in a document. Document ro- tation randomly selects a token from a sentence and then rotates the sentence so that it begins with that token. Token deletion randomly deletes a to- ken from the original sentence. Text inﬁlling re- places word sequences with a single mask token or inserts a mask token into a randomly selected position. A combination of sentence permutation and text inﬁlling achieves the best accuracy of all techniques. An overview of the BART model is given in Figure 1. The encoder is a bidirectional model and the decoder is an autoregressive model. This pre- trained BART model is ﬁne-tuned to various tasks, such as the summarization task, for which, a doc- ument is provided to the encoder, and the decoder generates a document summary. 2.2 Multi-Granularity Self-Attention (MG-SA) MG-SA (Hao et al., 2019) is used to capture multi- granularity information from an input text by di- viding the input into elements with several types of granularity and preparing heads of multi-head SANs for each type of granularity. Provided with the word-level matrix H, which is an input to the SANs, this method ﬁrst generates a phrase-level matrix Hg representing phrase-level information, as follows: Figure 2: Overview architecture of Hie-BART. This is based on the Transformer model. The SANs in the encoder are divided into word and sentence levels and computed. Hg = Fh(H), where Fh(・) is a function that generates a phrase- level matrix for the h-th head. Speciﬁcally, a phrase-level matrix is generated by running a max pooling operation on word-level vectors in a word- level matrix. After a phrase-level matrix is gener- ated, SANs perform the following computations: Q, W h Q, HgW h K, HgW h Qh, Kh, V h = HW h Oh = ATT(Qh, Kh)V h, V , (1) (2) where Qh ∈ Rn×dh, Kh ∈ Rp×dh, V h ∈ Rp×dh are respectively the query, key, and value repre- ∈ Rd×dh are parame- K, W h sentations，W h V ter matrices, and d, dh, n, and p are the dimen- sions of the hidden layer, one head, a word vec- tor, and a phrase vector, respectively. In addition, ATT(X,Y) is a function that calculates the atten- tion weights of X and Y. From these computa- tions, the output Oh of each head in the SANs is generated. Then, the output of MG-SA is gener- ated by concatenating the outputs from all heads: MG-SA(H) = [O1, ..., ON]. The outputs of each head Oh contain information between words or between words and phrases. Thus, in addition to relationships between words, the relationships between words and phrases can be captured with MG-SA. 160Pre-trainedEncoderPre-trainedDecoderDocumentSummarySummaryMulti-HeadAttentionAdd & NormMulti-HeadSelf-AttentionAdd & NormoutputsEmbeddingsConcatenateAdd & NormFeed FowardSentence LevelMulti-HeadSelf-AttentionLinearSoftmax×N×NFeed FowardAdd & NormoutputprobabilitiesAdd & NormWord LevelMulti-HeadSelf-AttentioninputsEmbeddingsCreateSentence LevelVectorFigure 3: Behavior of the create sentence level vector layer. Ewij and E[BOS] are embedded vectors for the word wij (j-th word in i-th sentence) and [BOS] token, respectively. Esi is the sentence-level embedded vector for the i-th sentence si. Figure 4: An example of the behavior of the concatenate layer where the number of heads of the multi-head is 6 and the join point j = 4. The blue [O1 w] designates the outputs of the word-level SANs and the red [O1 s] shows the outputs of the sentence-level SANs. w, ..., O6 s, ..., O6 3 Hie-BART 3.1 Architecture The Hie-BART (Hierarchical-BART) model has a sentence-to-word (sentence-level) SANs in addi- tion to the word-to-word (word-level) SANs of the original BART model. An overview of Hie-BART is shown in Figure 2. Hie-BART has sentence- level SANs, a create sentence level vector layer and a concatenate layer, in addition to BART. In the create sentence level vector layer, a sentence- level matrix is created from a word-level matrix. The concatenate layer concatenates the outputs of word-level and sentence-level SANs. The outputs of the concatenate layer are forwarded to the sub- sequent feed-forward layer. To provide boundary information between the sentences, each sentence is preﬁxed with a [BOS] token. 3.2 Create Sentence Level Vector Layer The behavior of the create sentence level vector layer is shown in Figure 3. Ewij and E[BOS] are embedded vectors for word wij (j-th word in the i-th sentence) and [BOS] token, respectively. Esi is the sentence-level embedded vector for the i-th sentence si. The create sentence level vector layer uses av- erage pooling to generate a sentence-level vector from word-level vectors. Given the word sequence W = (w1, ..., wN ), it is divided into sentences S = (s1, ..., sM ), where N is the total number of words, M is the total number of sentences, and each si is the i-th sentence consisting of a word subsequence wi1, . . . wiNi, where Ni is the total number of words in the sentence. For each ele- ment of S, we apply average pooling as follows: gm = AVG(sm), where the AVG(・) is average pooling. From this formula, G = (g1, ..., gM ) is generated. Each element of W, S, and G is an embedded vector. G is forwarded to the sentence- level SANs as its input. 3.3 Concatenate Layer w (3) w, . . . , OH s , . . . , OH SANs(W) = [O1 SANs(G) = [O1 The outputs of each of the word-level and sentence-level SANs are combined in the concate- nate layer. The outputs of the word-level and sentence-level SANs layer are as follows: w ] = OALL , s ] = OALL , s where H is the number of heads, [O1 w, . . . , OH w ] = OALL is the output of the word-level SANs, w consisting of the word-level head’s outputs, and [O1 is the output of the sentence-level SANs, consisting of the sentence- level head’s outputs. these word/sentence-level SANs are combined as fol- lows: s ] = OALL The outputs of s , ..., OH (4) s CONCAT(OALL w, ..., Oj = [O1 w , OALL , j) s w, Oj+1 , ..., OH s s ], (5) 161E[BOS]EEEE[BOS]E[BOS]EEEEEEWord Level Input(Embeddings)Sentence Level  InputEAverage PoolingEEWord LevelOutputSentence LevelOutputConcatenate LayerOutput(j=4)ConcatenateModel LEAD-3 (Nallapati et al., 2017) PTGEN (See et al., 2017) PTGEN+COV (See et al., 2017) BERTSUMEXTABS (Liu and Lapata, 2019) T5 (Raffel et al., 2020) BART (Lewis et al., 2020) BART (ours) Hie-BART (ours) ROUGE-1 ROUGE-2 ROUGE-L 17.62 15.66 17.28 19.60 21.55 21.28 21.22 21.37 40.42 36.44 39.53 42.13 43.52 44.16 44.06 44.35∗,∗∗ 36.67 33.42 36.38 39.18 40.69 40.90 40.82 41.05∗∗ Table 1: Results on the CNN/Daily Mail test set. Word : Sentence 16 : 0 15 : 1 14 : 2 13 : 3 12 : 4 11 : 5 10 : 6 9 : 7 8 : 8 ROUGE 2 21.73 21.92 21.92 21.87 21.66 21.81 21.75 21.71 21.77 L 41.43 41.68 41.75 41.64 41.49 41.62 41.51 41.46 41.58 1 44.72 44.95 45.01 44.91 44.74 44.88 44.78 44.70 44.79 Table 2: Results on the CNN/Daily Mail validation set. The leftmost column shows the ratio of the num- ber of multi-heads to combine. The highest score was achieved for the ratio “Word:Sentence = 14:2”. where CONCAT(X, Y, j) is a function that con- catenates X and Y at the join point j of the multi- heads. In the combined multi-head, the heads from 1 to j are word-level outputs, and the heads from j + 1 to H are sentence-level outputs. Figure 4 shows an example of the behavior of the concatenate layer in Hie-BART, where the number of heads of the multi-head is 6 and the join point j = 4. The output of the word- level SANs [O1 w] and the output of the sentence-level SANs [O1 s ] are joined at the join point j = 4, resulting in the output s , O6 [O1 s ]. w, ..., O6 s , ..., O6 w, O4 w, O5 w, O3 w, O2 The output of the concatenate layer is forwarded to the feed-forward layer in the encoder. 4 Experiments 4.1 Dataset We used the CNN/Daily Mail dataset1 (Her- mann et al., 2015), a summary corpus of En- glish news articles, consisting of 287,226 train- ing pairs, 13,368 validation pairs, and 11,490 test pairs. On average, the source documents and sum- mary sentences have 781 and 56 tokens, respec- tively. For data preprocessing, we followed the in- struction provided in the CNN/Daily Mail dataset1 and fairseq2. 4.2 Parameters We used the pre-trained BART model “bart.large”, provided in fairseq2 for Hie-BART. The hyper- parameters for BART and Hie-BART were de- termined for the validation set; the gradient ac- cumulation parameter (update-freq) was 10, the total number of training steps was 20,000, and the number of multi-heads was set to 16. The ratio of the number of combined heads of out- put in word-level and sentence-level SANs was set to “Word:Sentence = 14:2” for Hie-BART. We followed fairseq’s settings2 for the other hy- perparameters. In our environments, the model had 406,291,456 parameters for Hie-BART and 406,290,432 parameters for BART. 4.3 Results The results of the evaluation of the Hie-BART model for the CNN/Daily Mail test data are shown in Table 1. The F-score for ROUGE-1, ROUGE-2, and ROUGE-L (Lin, 2004) were used as evalua- tion metrics. To calculate the ROUGE score, we 1CNN/Daily Mail dataset: https://github.com/abisee/cnn-dailymail 2 Usage of BART by faireseq: https://github.com/pytorch/fairseq/tree/master/examples/bart 162[Source Document] (CNN)About a dozen Native American actors have walked off the set of an Adam Sandler movie comedy, saying the satirical Western’s script is in- sulting to Native Americans and women, accord- ing to a report. ... According to ICTMN, a Na- tive American adviser hired to help ensure the movie’s cultural authenticity also walked off the set in protest. Hill, the Choctaw actor, seemed to hold out hope that differences between the produc- ers and Native American cast members could be resolved . ... . [Baseline Model’s Summary] The walkout occurred on the set of "The Ridicu- lous Six" near Las Vegas, New Mexico, according to a report . The script called for native women’s names such as "Beaver’s Breath" and "No Bra" and an actress portraying an Apache woman to squat and urinate while smoking a peace pipe, ICTMN reports . [Proposed Model’s Summary] About a dozen Native American actors walked off the set of an Adam Sandler movie comedy, a report says . They say the script is insulting to Native Americans and women, according to the Indian Country Today Media Network . "The Ridiculous Six" also stars Will Forte, Taylor Lautner, Steve Buscemi and John Turturro . [Gold Summary] About a dozen Native American actors walk off set of Adam Sandler comedy, says report . Actors say satirical Western’s script is insulting to Native Americans and women . Table 3: An example of improved summaries. Note that the source document is partially omitted. [Source Document] (CNN)It’s the kind of thing you see in movies, like Robert Redford’s role in "All Is Lost" or Ang Lee’s "Life of Pi." But, in real life, it’s hard to swallow the idea of a single person being stranded at sea for days, weeks, if not months and somehow living to talk about it. Miracles do happen, though, and not just in Hollywood. We’re not talking about people who ﬂoat aimlessly or run aground after running out of gas or being let down by falter- ing winds only to be picked up a few hours later ... After being spotted by the U.S. Coast Guard. by crew aboard a Japanese Maritime Self-Defense Force destroyer and picked up in a smaller rescue boat, he took a drink offered to him and burst into tears, Kyodo reported. Shinkawa told his rescuers, "I thought today was the last day of my life." [Base Model’s Summary] It’s hard to swallow the idea of a single person be- ing stranded at sea for days, weeks, if not months and somehow living to talk about it . Miracles do happen, though, and not just in Hollywood . Jose Salvador Alvarenga says his journey began in Paredon Viejo, a port on Mexico’s Paciﬁc coast in late 2012 . [Proposed Model’s Summary] It’s hard to swallow the idea of a single person be- ing stranded at sea for days, weeks, if not months and somehow living to talk about it . Miracles do happen, though, and not just in Hollywood . Jose Salvador Alvarenga says his journey began in Paredon Viejo, a port on Mexico’s Paciﬁc coast, in late 2012 . Louis Jordan says he used laundry to trap and scoop up ﬁsh, rigged a makeshift mast and sail . [Gold Summary] A South Carolina man says he spent 66 days alone at sea before being rescued . Other sole survivor stories include a Japanese man washed away by a tsunami . An El Salvador man says he drifted from Mexico to Marshall Islands over a year . Table 4: An example of summaries that are not im- proved. 163used ﬁles2rouge3. Hie-BART was compared with LEAD-3 (Nallapati et al., 2017), PTGEN, PT- GEN+COV (See et al., 2017), BERTSUMEXTABS (Liu and Lapata, 2019), T5 (Raffel et al., 2020), BART with our environment, and BART with Lewis et al. (2020). The LEAD-3 method uses the ﬁrst three sentences of the source document as a summary. PTGEN is a sequence-to-sequence model that incorporates a pointer generator net- work. PTGEN+COV introduces the coverage mechanism into PTGEN. BERTSUMEXTABS is a pre-training model that adapts BERT for sum- marization tasks. T5 is a generalized pre-training model for sequence-to-sequence tasks based on the Transformer model. The statistical signiﬁ- cance test was performed by the Wilcoxon-Mann- Whitney test. In Table 1, * and ** indicate that the comparisons with BART (ours) are statistically signiﬁcant at 5% signiﬁcance level and 10% sig- niﬁcance level, respectively. Hie-BART improved the F-score of ROUGE- 1/2/L by 0.223 points on average relative to BART with our environment, and by 0.143 points on av- erage from BART reported in (Lewis et al., 2020). Table 1 also shows that our Hie-BART model signiﬁcantly improved ROUGE-1 and ROUGE-L scores of the baseline BART model. 4.4 Analysis Table 2 shows a comparison of ROUGE scores for the ratio of the number of multi-heads at the word and sentence levels with the validation set of the CNN/Daily Mail dataset. The leftmost column shows the ratio of the number of multi- heads to combine. As can be seen in Table 2, the maximum ROUGE-1/2/L score was achieved for "Word:Sentence = 14:2". In ROUGE-1/2/L, smaller ratios of multi-heads at the sentence level that are compared to the word level, the higher the score tends to be. However, when the number of multi-heads at the sentence level is 0 (the origi- nal BART), the accuracy is lower than that of Hie- BART. Table 3 shows an improved example of sum- summaries generated by the baseline maries: model (BART) and the proposed model (Hie- BART), and the gold summary. As can be seen in Table 3, the summary of the proposed model is ﬂu- ent and close to the contents of the gold summary, which indicates that the summary of the proposed 3ﬁles2rouge usage : https://github.com/pltrdy/ﬁles2rouge model includes the important parts of the source document. Table 4 shows an example of summaries that are not improved. In this example, the baseline model’s summary and the proposed model’s sum- mary include almost the same contents, but they are far from and longer than the gold summary. 5 Conclusion In this study, we proposed Hie-BART to can take into account the relationship between words and sentences in BART by dividing the self-attention layer of encoder into word and sentence lev- els. In the experiments, we conﬁrmed that Hie- BART improved the F-score of ROUGE-L by 0.23 points relative to the non-hierarchical BART model, and the proposed model was better than the strong baselines, BERTSUM and T5 models for the CNN/Daily Mail dataset. As future work, we intend to investigate meth- ods to incorporate information between sentences in addition to word-to-word and word-to-sentence information. References Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- In Proceedings of the 2019 Conference standing. of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota. Associ- ation for Computational Linguistics. Jie Hao, Xing Wang, Shuming Shi, Jinfeng Zhang, and Zhaopeng Tu. 2019. Multi-granularity self-atten- tion for neural machine translation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Interna- tional Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), pages 887–897, Hong Kong, China. Association for Computational Lin- guistics. Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Su- leyman, and Phil Blunsom. 2015. Teaching ma- In Advances in chines to read and comprehend. neural information processing systems, pages 1693– 1701. Mike Lewis, Yinhan Liu, Naman Goyal, Mar- jan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising sequence-to-sequence pre– training for natural language generation, translation, 164and comprehension. In Proceedings of the 58th An- nual Meeting of the Association for Computational Linguistics, pages 7871–7880. Association for Com- putational Linguistics. Chin-Yew Lin. 2004. ROUGE: A package for auto- matic evaluation of summaries. In Text Summariza- tion Branches Out, pages 74–81, Barcelona, Spain. Association for Computational Linguistics. Yang Liu and Mirella Lapata. 2019. Text summariza- In Proceedings of tion with pretrained encoders. the 2019 Conference on Empirical Methods in Nat- ural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3730–3740, Hong Kong, China. Association for Computational Linguistics. Ramesh Nallapati, Feifei Zhai, and Bowen Zhou. 2017. SummaRuNNer: A recurrent neural network based sequence model for extractive. In Proceedings of In Association for the Advancement of Artiﬁcial Intel- ligence. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a uniﬁed text-to-text trans- Journal of Machine Learning Research, former. 21(140):1–67. Abigail See, Peter J. Liu, and Christopher D. Man- ning. 2017. Get to the point: Summarization with In Proceedings of the pointer-generator networks. 55th Annual Meeting of the Association for Com- putational Linguistics (Volume 1: Long Papers), pages 1073–1083, Vancouver, Canada. Association for Computational Linguistics. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Pro- cessing Systems, volume 30, pages 5998–6008. Cur- ran Associates, Inc. 165