Flexible Non-Autoregressive Extractive Summarization Threshold : Extract Non-Fixed Number Summary Sentences Ruipeng Jia, ,  Yanan Cao, ,  Haichao Shi, ,  Fang Fang, ,  ( cid:  ) Pengfei Yin, ,  Shi Wang  ( cid:  )   Institute Information Engineering , Chinese Academy Sciences   School Cyber Security , University Chinese Academy Sciences   Institute Computing Technology , Chinese Academy Sciences jiaruipeng , caoyanan , shihaichao , fangfang     , yinpengfei g @ iie.ac.cn f wangshi @ ict.ac.cn Abstract Sentence-level extractive summarization fundamental yet challenging task , recent powerful approaches prefer pick sentences sorted predicted probabilities length limit reached , a.k.a . “ Top-K Strategy ” . length limit ﬁxed based validation set , resulting lack ﬂexibility . work , propose ﬂexible accurate non-autoregressive method single document ex- tractive summarization , extracting non-ﬁxed number sum- mary sentences without sorting step . call approach ThresSum picks sentences simultaneously individu- ally source document predicted probabili- ties exceed threshold . training , model enhances sentence representation iterative reﬁnement intermediate latent variables receive weak supervision soft labels , generated progressively adjust- ing temperature knowledge distillation algorithm . Speciﬁcally , temperature initialized high value drops along iteration temperature   . Experi- mental results CNN/DM NYT datasets demon- strated effectiveness ThresSum , signiﬁcantly outperforms BERTSUMEXT substantial improvement  .   ROUGE-  score CNN/DM . Introduction Encoder-decoder mechanism widely used single doc- ument extractive summarization . encoder encode one sentence vector representation , popular decoder top-k strategy divided three steps : predict probability scores sentence vectors , sort sentences descending order line probability scores , pick sentences exceeding length limit . ( Nallapati , Zhai , Zhou      ; Xiao Carenini      ; Liu Lapata      ; Xu et al .      ) . However , still two inherent obstacles sentence-level extractive summa- rization :   ) Redundant phrases selected sentences . naive approach ﬁrst prediction step decoder make independent binary decisions sentence , leading absence overlap redundancy modeling ( cid:  ) Corresponding authors : Fang Fang Shi Wang Copyright c ( cid:   )      , Association Advancement Artiﬁcial Intelligence ( www.aaai.org ) . rights reserved . selected target sentences ( Xu et al .      ; Zhong et al .      ) . ﬁrst natural solution introduce autoregres- sive decoder ( Chen Bansal      ; Jadhav Rajan      ; Liu Lapata      ; Xu et al .      ) , extracting sentences one one allowing different sentences inﬂuence . Secondly , reinforcement learning introduced decoder consider semantics entire target sum- mary ( Narayan , Cohen , Lapata      ; Dong et al .      ; Bae et al .      ) , combines maximum-likelihood cross-entropy loss rewards policy gradient directly optimize evaluation metric summarization task . third popular solution build summarization system two-stage decoder ( Aliguliyev      ; Galanis Androutsopoulos      ; Zhang et al .     a ; Zhong et al .      ) , ﬁrst stage extract fragments original text second stage select modify basis fragments . Unfortunately , approaches autoregressive decoder unstable , train-inference gap error propagation extracting sentences one one .   ) Fixed number proportion summary sentences . SummaRuNNer ( Nallapati , Zhai , Zhou      ) ﬁrst sets “ sort ” step “ predict ” step , “ pick ” sorted sentences length limit reached ( a.k.a . “ Top-K Strategy ” ) , popular method employed following models ( Narayan , Cohen , Lapata      ; Zhang et al .      ; Zhang , Wei , Zhou      ; Liu Lapata      ; Xu et al .      ; Wang et al .      ) . ﬂexible extractor generate non-ﬁxed number summary sentences based source document length , topics , aspects . reasonable approach pick sentences whose predicted probability threshold . However , may optimal stratety since training data imbalanced terms summary-membership sentences ( Nallapati , Zhai , Zhou      ) . Furthermore , Mendes et al . (      ) introduces length variable decoder Zhong et al . (      ) choose number sentences matching candidate summary semantic space . decoders two solutions either autoregressive two-stage . address two obstacles , introduce Thres- Sum , heuristic approach strengthen encoder en- hancing sentence representation iterative reﬁnement simplify decoder removing “ sort ” step : TheThirty-FifthAAAIConferenceonArtificialIntelligence ( AAAI-   )       ( cid:   ) ( cid:   ) ( cid:   ) encoder , ﬁrst map textual tokens hidden states contextualized interactions . Secondly , sen- tence embedding extracted hierarchical atten- tion adaptively aggregate information word elements . Finally , fuse redundant information be- tween selected sentences iterative reﬁnement process supervised knowledge distillation . decoder non-autoregressive low redundancy , encoder modeled overlap information selected sentences . case , decoder consists two steps instead former three steps : predict probability scores sentence vectors , pick sentences simultaneously individually predicted probability exceeds threshold . key component extractive model includes weak supervision intermediate latent variables iterative reﬁnement . design teacher algorithm knowledge distillation produce high entropy soft la- bels high temperature , progressively reduce temperature along iteration temperature   . Therefore , former iterative steps high temper- ature equivalent minimize square difference ground-truth prediction , latter steps lower temperature pay much attention matching positive elements ( Hinton , Vinyals , Dean      ) . Experimental results validate effectiveness Thres- Sum , signiﬁcantly outperforms BERTSUMEXT  .   ROUGE-  score CNN/DM . human evaluation also shows model better relevance compared others . contributions work concluded follows :   ) Instead extracting sentences one one form top-k summary , formulate non-autoregressive decoder , extract non-ﬁxed number summary sentences simultaneously individually .   ) propose iterative reﬁnement strengthen encoder enhance sentence representation . Simultaneously , introduce expand knowledge distillation algorithm progressively supervise iterative reﬁnement .   ) proposed framework achieved superior per- formance compared strong baselines . Moreover , conduct analysis investigate performance gain model comes . Related Work Extractive Summarization two main lines summarization : abstractive extractive . abstractive paradigm ( Celikyilmaz et al .      ; Sharma et al .      ) focuses generating summary word- by-word encoding full document . extractive approach ( Cheng Lapata      ) directly selects sentences document assemble summary . abstrac- tive approach ﬂexible generally produces less redundant summaries , extractive approach enjoys better factuality efﬁciency ( Cao et al .      ) . Recent research work extractive summarization spans large range approaches . work usually instantiate encoder-decoder architecture choosing RNN ( Nal- lapati , Zhai , Zhou      ; Zhou et al .      ) , Transformer ( Wang et al .      ; Zhong et al .     b ; Liu Lapata      ; Zhang , Wei , Zhou      ) GNN ( Wang et al .      ; Jia et al .     b ) encoder , autoregressive ( Jadhav Rajan      ; Liu Lapata      ) RL-based ( Narayan , Cohen , Lapata      ; Arumae Liu      ; Bae et al .      ) decoders . Despite effectiveness , models top-k strategy essentially . two-stage summarization , Chen Bansal (      ) Bae et al . (      ) follow hybrid extract-then-rewrite architecture , policy-based RL bridge two net- works together . Lebanoff et al . (      ) , Xu Durrett (      ) Mendes et al . (      ) focus extract-then-compress learning paradigm , ﬁrst train extractor con- tent selection . Zhong et al . (      ) introduces extract-then- match framework , employs BERTSUMEXT ( Liu Lapata      ) ﬁrst-stage prune unnecessary information . However , two-stage approaches inherently error propagation difﬁcult train . Knowledge Distillation common formulation knowledge distillation ( KD ) proposed ( Buciluundeﬁned , Caruana , Niculescu-Mizil      ; Hinton , Vinyals , Dean      ; Kim Rush      ) , smaller student model trained soft probability labels provided larger teacher model ( temperature ﬁnal softmax ) . recently , ( Tan et al .      ) applied KD multilingual NMT , ( Sun et al .      ) proposed patient KD BERT model compression . distillation focuses using KD generalize student model ﬁtting cumbersome inter-relationship sentences , previous work mostly focused model compression . Methodology Problem Deﬁnition Given document consisting sequence sentences ( s  ; s  ; : : : ; sM ) sentence si consisting sequence N words ( wi  ; wi  ; : : : ; wiN ) . denote hi hij embedding sentences words continuous space .  extractive summarizer aims produce summary  ) . selecting sentences ( ( cid:   ) sentence si   ;   , ground-truth yi  g   f predict label ^yi ,   means si g si ; ; ( cid:   ) ) included summary . assign score p ( ^yi quantify si ’ relevance summary , ( cid:   ) parameters neural network model . Finally , assemble si ; ; ( cid:   ) ) summary  exceeds threshold . selecting sentences , p (     ;     f   j j Overview Architecture ThresSum consists powerful encoder easy- adjustable decoder , shown Figure   ( ) . Encoder : order learn contextual representation words , utilize pre-trained ALBERT ( enhanced version BERT ) ( Lan et al .      ) . output ALBERT contains       Figure   : Overview ThresSum . words hidden state hij , special tokens h [ CLS ] h [ SEP ] . Liu Lapata (      ) simply choose h [ CLS ] sentence rep- resentation , think difﬁcult [ CLS ] identify boundary sentences . Inspired Yang et al . (      ) , employ hierarchical attention mechanism , shown Fig- ure   ( b ) , context vector uw seen high level representation ﬁxed query “ informative word ” . ; : : : :hL ; : : : ; gL ( cid:  )    process iterative reﬁnement L steps , different state representations si : ( h  ) , hl hidden state si l-th iteration . also intro- duce intermediate random variables ( g  ) sentence si , gl importance sentence state hl . assistance latent variables , encoder implicitly aggregate redundant information selected summary sentences hi .   ; : : : ; hL Decoder : ﬁnal sentences state ( hL ) , decoder predicts probabilities ( gL   ; : : : ; gL ) feed- forward network sigmoid . adjust threshold pick ﬂexible summary sentences . Approximate Models standard conditional probability distribution selecting sentence si : P ( ^yi =   j ; y  ; : : : ; yi ( cid:  )   ; yi+  ; : : : ; yM ) (   ) must ground-truth labels sen- tences calculating si . However , still known polynomial algorithm solve exactly . Autoregressive originates literature time-series models , observations previous time-steps used predict value current time step , i.e. , P ( ^yt =   ; < ) j (   ) autoregressive paradigm error propagation in- herently , especially misjudgment ﬁrst element . paper , introduce non-autoregressive architecture , P ( ^yi =   ( cid:  )   ; ^y  (   ) ^y  pre-predicted label introduced implicitly capture bidirectional dependencies among target symbols . i+  ; : : : ; ^y    ; : : : ; ^y  ; ^y  ) j Iterative Reﬁnement model iteratively updates latent variables gl ing document semantic information ( gl ( cid:  )   pecially :   mask- ) , es- ; : : : ; gl ( cid:  )   j (   ) =   ; : : : ; ^yl ( cid:  )   ) ; ^yl ( cid:  )   = P ( ^yl gl   = ( cid:   ) ( FFN ( LN ( Hl + MHAtt ( Hl ) ) ) ) FFN , LN , MHAtt feed-forward network , layer normalize , multi-head attention ; ^yl ( cid:  )   pre-predicted    iteration ; Hl matrix contains label l ( cid:  ) sentences hidden state ( hl ) l-th iteration .   ; : : : ; hl shown Figure   ( ) , l-th iterative reﬁnement , Transformer-like unit layer stacked top hidden state : l ~H = LN ( Hl ( cid:  )   + MHAtt ( Hl ( cid:  )   ) ) l ~H Wr tanh ( Rl ) l Hl = Wc ~H ( cid:  ) ( cid:   ) (   ) l ~H input l-th iteration , Hl gets updated reducing redundancy Rl , dynamic matrix representation redundancy sentence ( rl   ; : : : ; rl means dot product i-th ~hl ~Hl corresponding rl , i.e . ~hl iWrtanh ( rl operation returns new -dimensional ) ; ) ; ( cid:   ) ( cid:   )                hidden statehidden state                   FF & SigmoidAdd & Norm                FF & SigmoidAdd & Norm                FF & SigmoidAdd & Norm             amb  *  amb  *  amb  * Flexible Extraction Adjustable Threshold hidden statehidden statehidden state Iterative Refinement ×LMH AttentionMH AttentionMH Attentionhidden stateHierarchical Atttention                          ⋯⋯ ( b ) Hierarchical Attention ( ) ThresSum      vector . rl represents redundant phrase information i-th sentence , weighted summation sentence-level hidden states : rl = X gl ( cid:  )   j ~hl j j f  ; : : : ; gnfig (   ) gl ( cid:  )     sentence information ~hl j . j [   ;   ] predicted probability mask Finally , ThresSum trained predict label sen- tences overall training equivalent optimizing following conditional probability : El ( cid:   ) f  ; : : : ; Lg ( cid:   ) L + l  L = L ( cid:  ) Ei ( cid:   ) f  ; : : : ; g ( cid:   ) ( ^yl = yl ij ( cid:   ) si ; ; ( cid:   ) ) (   ) gradually increase proportion L+l  L reﬁnement according importance ; l index randomly sampled iteration step sentence ; ( cid:   ) regular binary cross-entropy loss respect prediction ^yl  soft ground-truth label yl : ( cid:   ) ( ^yl = yl ij si ; ; ( cid:   ) ) = yl log ( gl ) + (   yl ) log (   gl ) (   ) ( cid:  ) ( cid:  ) Knowledge Distillation Soft Labels summarization datasets contain human written ab- stractive summaries ground truth . Thus , greedy approach ( Nallapati , Zhai , Zhou      ; Liu Lapata      ) employed adds one sentence time incrementally summary , maximizing ROUGE score stop- ping none remaining candidate sentences improves score . Theoretically , soft labels high entropy provide much information binary hard labels much less variance gradient training cases ( Hinton , Vinyals , Dean      ) . , binary labels maxi- mize margin positive negative examples extracting salient sentences reduce redundancy . Intuitively , former iterative steps architecture serve small model iteration , latter steps larger iterations . Therefore , iterative reﬁnement gl gradually trained soft labels , last step gL yl   ;   . ground-truth binary label g f work , modify knowledge distillation ( Jia ; : : : ; yL et al .     a ) design soft target labels ( y  ) intermediate variables ( g  ) . Algorithm   , design teacher algorithm knowledge distillation produce high entropy soft labels high temperature , progressively reduce temperature along iterations temperature   . result , former iterative steps high temperature equivalent regression approach latter steps lower temperature pay much attention matching positive units ( Hinton , Vinyals , Dean      ) . ; : : : ; gL denote ri ; r  ; : : : ; rM individual ROUGE scores sentence human-written summary , espe- cially : Algorithm   : Teacher Algorithm Soft Labels Initialize Sentence Set = g Initialize ROUGE r  ; : : : rM , Iteration Steps L ; Sort r  ; : : : ; rM descending order ; l   L   s  ; : : : ; sM f ; ( cid:  ) Set Temperature L   l ; ( cid:  ) Temporary Sentence Set : Dtemp fg Temporary ROUGE Dtemp : Rtemp si [   ] [ end ] ;   ; Dtemp Rtemp increasing Dtemp + si ;  else  si ( cid:  ) Dtemp Dtemp end si ; ( cid:  ) end Set Sentence Dtemp Soft Label ; end Set Sentence Remained Label   ; Record Soft Labels ( yl ) ; Re-Initialize Sentence Set = ; g Re-Sort r  ; : : : ; rM descending order ;   ; : : : ; yl s  ; : : : ; sM   ; yl f end Datasets Experiments shown Table   , employ two datasets widely- used multiple sentences summary : CNN Dailymail ( CNN/DM ) ( Hermann et al .      ) New York Times ( NYT ) ( Sandhaus      ) . CNN/DM . used standard split ( Hermann et al .      ) training , validation test (   ,   / ,   / ,    CNN    ,  /  .   /  ,    Daily Mail ) , splitting sentences Stanford CoreNLP ( Manning et al .      ) toolkit pre-processing dataset following ( See , Liu , Manning      ) ( Xu et al .      ) . dataset contains news articles several associated abstractive high- lights . use un-anonymized version previous summarization work document truncated     BPE tokens . NYT . Following previous work ( Zhang , Wei , Zhou      ; Xu Durrett      ) , use    ,    ,   ,      ,    samples training , validation test , respectively . Input documents truncated     BPE tokens . Note different divisions NYT ( Durrett , Berg- Kirkpatrick , Klein      ; Liu Lapata      ) sev- eral models evaluated NYT ofﬁcially . e.g . See , Liu , Manning (      ) Mendes et al . (      ) , re-train evaluate NYT source code Github . Parameters & Metrics code based Pytorch ( Paszke et al .      ) pre-trained model employed ThresSum ‘ albert-xxlarge-          Datasets # docs ( train / val / test ) avg.doc length words sentences avg.summary length sentences words CNN DailyMail NYT   ,    /  ,    /  ,       ,    /   ,    /   ,       ,    /   ,    /   ,       .      .      .     .     .     .     .     .     .    .    .    .   Table   : Data Statistics : CNN/Daily Mail NYT . v  ’ ( huggingface/transformers  ) . train ThresSum (    M parameters ) two days    ,    steps  GPUs ( Nvidia Tesla V    ,   GB ) gradient accumula- tion every two steps . Adam ( cid:   )   =  :  ; ( cid:   )   =  :    used optimizer . Learning rate schedule follows strategy warming-up ﬁrst   ,    steps . tried iteration steps [   ;   ;   ;   ] knowl- edge distillation , L =   best choice based validation set . comparison , tried replace bi- nary cross-entropy regression objective , result indicates regression ’ achieve performance cross-entropy . ﬁnal threshold extraction  .   CNN/DM  .   NYT , tuned valida- tion set get highest ROUGE-  score . higher threshold concise summary lower threshold return information . report F  ROUGE score ThresSum ROUGE-  . . .pl ( Lin      ) , calculates overlap lexical units extracted sentences ground-truth . source code available Github.  Baselines Abstractive Methods : ABS normal architecture RNN-based encoder decoder . PGC augments stan- dard Seq Seq attentional model pointer cover- age mechanisms . TransformerABS employs Transformer text summarization . T  , BART , ProphetNet pre- trained large unlabeled data perform excellent perfor- mance Transformer architecture . PEGASUS proposes Transformer-based models extracted gap-sentences abstractive summarization . Extractive Methods : Oracle Summary extracted summary according ground-truth labels . Speciﬁcally , oracle summary essential reveal upper bound performance extractive paradigm . Lead-  base method extractive text summarization chooses ﬁrst three sentences summary . SummaRuNNer takes con- tent , salience , novelty , position sentence con- sideration deciding sentence included extractive summary . Exconsumm ﬁrst extracts sentences document compresses . PNBERT tries employ unsupervised transferable knowledge . Dis- coBERT extracts sub-sentential discourse units candi- dates extractive selection ﬁner granularity . BERT- SUMEXT applies pre-trained BERT text summarization proposes general framework extractive  https : //github.com/huggingface/transformers  https : //github.com/coder   /ThresSum abstractive models . MATCHSUM two stage method extract-then-match , ﬁrst-stage BERTSUMEXT . Result & Analysis ROUGE Score experiment results ROUGE shown Table   . scores accordance original papers missing ones ( NYT ) calculated source code Github . obvious Thres- Sum outperforms baseline models , demonstrating enhanced encoder help model relationships across source sentences selected sentences . Speciﬁcally , model outperforms MATCHSUM  .   ROUGE-  ,  .   ROUGE-   .   ROUGE-L CNN/DM . in-depth performance analysis , note :   ) Pre-trained BERT-like model powerful capture bidirec- tional dependencies applying deep architecture ;   ) Flexi- bility summary length essential , large margin ThresSum/MATCHSUM BERTSUMEXT . Ablation Studies propose several strategies improve performance extractive summarization , including knowledge distilla- tion ( vs. binary labels ) , pre-trained ALBERT ( vs. BERT ) , iterative reﬁnement ( vs. None ) . investigate inﬂuence factors , conduct experiments list re- sults Table   . Signiﬁcantly ,   ) Iterative reﬁnement important ALBERT , reason redundant information selected sentences difﬁcult ALBERT model ;   ) Knowledge distillation mechanism enlarges advantage extractive method , high entropy soft labels ( Hinton , Vinyals , Dean      ) . Human Evaluation Summarization enough rely ROUGE evaluation summarization system , although ROUGE correlates well human judgments ( Owczarzak et al .      ) . Therefore , design Amazon Mechanical Turk experiment based ranking method . Following ( Cheng Lapata      ) , ( Narayan , Cohen , Lapata      ) ( Zhang , Wei , Zhou      ) , ﬁrstly , randomly select    samples CNN/DM test set . human participants presented one original document list corresponding sum- maries produced different model systems . Participants requested rank summaries ( ties allowed ) taking informativeness ( summary capture important in- formation document ) ﬂuency ( summary       Models Abstractive ABS (      ) PGC (      ) TransformerABS (      ) T Large (      ) BARTLarge (     b ) PEGASUSLarge (     b ) ProphetNetLarge (      ) Extractive Oracle ( Sentence ) Lead-  SummaRuNNer ? (      ) Exconsumm z ? (      ) ? (     a ) PNBERTBase DiscoBERTBase (      ) BERTSUMEXTLarge MATCHSUMBase z ThresSumLarge z ? (      ) ( ) ( cid:   ) ? (      ) CNN/DM R-  R-  R-L R-    .     .     .     .     .     .     .     .     .     .     .    .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .    .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .    .     .     .     .     .     .     .     .   -   .   - -   .     .     .     .   - -   .   -   .   NYT R-    .     .     .   -   .   - -   .     .     .     .   - -   .   -   .   R-L   .     .     .   -   .   - -   .     .     .     .   - -   .   -   .   means Top-K strategy ; z means Dynamically Adjusting Summary Length . ? means Binary Labels / Hard Labels ; ( cid:   ) means Soft Labels . Table   : Automatic Evaluation ROUGE F  . Models R-  R-  R-L ThresSum ThresSum w/o Distillation ThresSum w/o ALBERT ThresSum w/o Iteration   .     .     .     .     .     .     .     .     .     .     .     .   Table   : Ablation Study CNN/DM . Models  st BERTSUMEXT  .    .   MATCHSUM  .   ThresSum  .   Ground-Truth  nd  .    .    .    .    rd  .    .    .    .    th MeanR  .    .    .    .    .    .    .    .   Table   : Human Evaluation CNN/DM . grammatical ) account . document annotated three different participants separately . input article ground truth summaries also shown human participants addition three model summaries ( BERTSUMEXT , MATCHSUM ThresSum ) . results shown Table   , obvious ThresSum better relevance compared others . previously selected sentences , bringing remarkable performance improvement CNN/DM . Whereas another statistic test set CNN/DM : ( cid:   ) ( cid:   ) ( cid:   ) ( cid:   )  .   % oracle summaries trigram overlaps within sentences .  .   % summaries extracted ThresSum ( iterative reﬁnement ) trigram overlaps within sen- tences .   .   % summaries extracted ThresSum ( with- iterative reﬁnement ) trigram overlaps within sentences .   % summaries extracted BERTSUMEXT ( Trigram-Blocking ) trigram overlaps within sen- tences . oracle summaries upper bound extractive paradigm  .   % still contain trigram overlaps ,   % BERTSUMEXT Trigram-Blocking . Consequently , Trigram-Blocking straightforward yet optimal approach . paper , proposed iterative re- ﬁnement model overlaps selected sen- tences , effectively avoid empty overlaps . ThresSum Iterative Reﬁnement reduces overlaps   .   %  .   % , showing superiority Trigram- Blocking . Trigram-Blocking vs. Iterative Reﬁnement Trigram blocking ( Paulus , Xiong , Socher      ; Liu Lapata      ) skips sentence trigram overlaps Autoregressive Decoders Threshold ThresSum extracts non-ﬁxed number summary sentences threshold , whether barrier prevents       Models ThresSum ThresSum ( Trigram-Blocking ) BERTSUMEXT BERTSUMEXT ( Threshold ) SummaRuNNer SummaRuNNer ( Threshold ) R-  R-L   .     .     .     .     .     .     .     .     .     .     .     .   Table   : Threshold Strategy CNN/DM . ( ) SummaRuNNer ( b ) BERTSUMEXT previous models like SummaRuNNer BERTSUMEXT ? explained Nallapati , Zhai , Zhou (      ) , picking sentences comparing predicted probability threshold may optimal strategy since training data imbalanced terms summary-membership sentences . Table   summarizes performance gain thresh- old strategy . thresholds BERTSUMEXT Sum- maRuNNer tuned validation set individually get highest ROUGE-  score . obvious threshold strategy suitable BERTSUMEXT/SummaRuNNer , independent binary decision based over- laps modeling selected sentences . hand , Trigram-Blocking strategy ﬁxed top-  summary sen- tences damage ﬂexibility ThresSum . Non-Fixed Number Summary Sentences Since extractive summarization requires sentence-level summary membership lables , Nallapati , Zhai , Zhou (      ) ﬁrst introduces simple greedy approach con- vert abstractive summaries extractive binary labels . Considering ThresSum removes restriction summary sentence number , necessary discuss dis- tribution summary sentence numbers test set CNN/DM . According statistics ,   % /    % /    % test set examples  - /  - /  -sentences summary . However , previous extractive approaches ( SummaRuNNer BERTSUMEXT ) top-k strategy extract   sentences , suitable almost half examples . paper ,   % /    % /    %  - /  - /  -sentences summary CNN/DM test dataset , extracted ThresSum get highest ROUGE-  score . threshold still hyper-parameter need tuned . higher threshold concise summary lower threshold return information . Visualization visualize three types well-trained sentence represen- tation SummaRuNNer , BERTSUMEXT , ThresSum employing t-SNE algorithm . T-SNE used visualize representations sentences learned models , better extractive model enlarge distance different clusters / different colors . Speciﬁcally , randomly select      sentences test set sentence repre- sented one point two-dimensional space . ( c ) ThresSum Figure   : T-SNE Visualization CNN/DM . Figure   , ﬁve different colors , ﬁve different soft labels ThresSum . obvious :   ) Compared BERTSUMEXT SummaRuNNer , sentence clus- ters ThresSum distinguishable meaningful ;   ) decoder ThresSum easy score sentences in- dividually extract summary sentences adjustable threshold ;   ) ’ ThresSum extract summary sentences threshold , models extract top-  sentences . Conclusion paper , remove restriction number summary sentences ﬁxed , introduce three substantial improvements : strengthen encoder enhancing sentence representation iterative reﬁnement , simplify de- coder removing “ sort ” step , weakly supervise intermediate latent variables iterative reﬁnement knowledge distillation . amazing ThresSum extract sentence separately according ad- justable threshold , great improvement ﬁtting distribution sentence number extractive summariza- tion . Experimental results show method signiﬁcantly outperforms previous models ROUGE score , ﬂexi- bility summary sentence , proportion over- laps . future work focus extending ﬂexible summary-sentences mechanism unsupervised summariza- tion . Acknowledgements research supported National Key Research Development Program China ( NO.    YFB        ) National Natural Science Foundation China ( No.         ) . thank authors contributions anonymous reviewers constructive com- ments .                 X              Y                   X                Y           X                   Y      References Aliguliyev , R. M.      . two-stage unsupervised ap- proach multidocument summarization . Automatic Control Computer Sciences    –    . Arumae , K. ; Liu , F.      . Reinforced Extractive Summa- rization Question-Focused Rewards . ACL ,    –    . Bae , S. ; Kim , T. ; Kim , J. ; goo Lee , S.      . Summary Level Training Sentence Rewriting Abstractive Sum- marization . arXiv preprint arXiv:    .      . Buciluundeﬁned , C. ; Caruana , R. ; Niculescu-Mizil , .      . Model Compression . Proceedings   th ACM SIGKDD International Conference Knowledge Discovery Data Mining ,    –    . Cao , Z. ; Wei , F. ; Li , W. ; Li , S.      . Faithful Original : Fact Aware Neural Abstractive Summarization . AAAI ,     –     . Celikyilmaz , A. ; Bosselut , A. ; , X. ; Choi , .      . Deep Communicating Agents Abstractive Summarization . NAACL-HLT ,     –     . Chen , Y.-C. ; Bansal , M.      . Fast Abstractive Sum- marization Reinforce-Selected Sentence Rewriting . ACL ,    –    . Cheng , J. ; Lapata , M.      . Neural summarization extracting sentences words . ACL . doi:  .     /v / p  -     . Dong , Y. ; Shen , Y. ; Crawford , E. ; van Hoof , H. ; Cheung , J. C. K.      . BanditSum : Extractive Summarization Contextual Bandit . EMNLP     –     . Durrett , G. ; Berg-Kirkpatrick , T. ; Klein , D.      . Learning-based single-document summarization com- arXiv preprint pression anaphoricity constraints . arXiv:    .      . Galanis , D. ; Androutsopoulos , .      . extractive supervised two-stage method sentence compression . NAACL-HLT ,    –    . Hermann , K. M. ; Kocisky , T. ; Grefenstette , E. ; Espeholt , L. ; Kay , W. ; Suleyman , M. ; Blunsom , P.      . Teaching machines read comprehend . NIPS ,     –     . Hinton , G. ; Vinyals , O. ; Dean , J .      . Distilling Knowledge Neural Network . NIPS Deep Learning Representation Learning Workshop . URL http : //arxiv . org/abs/    .      . Jadhav , A. ; Rajan , V.      . Extractive summarization swap-net : Sentences words alternating pointer networks . ACL ,    –    . Jia , R. ; Cao , Y. ; Shi , H. ; Fang , F. ; Liu , Y. ; Tan , J .     a . DistilSum : Distilling Knowledge Extractive Summa- rization . CIKM ,     –     . Jia , R. ; Cao , Y. ; Tang , H. ; Fang , F. ; Cao , C. ; Wang , .     b . Neural Extractive Summarization Hierarchical Attentive Heterogeneous Graph Network . EMNLP ,     –      . Kim , Y. ; Rush , A. M.      . Sequence-Level Knowledge Distillation . EMNLP ,     –     . Lan , Z. ; Chen , M. ; Goodman , S. ; Gimpel , K. ; Sharma , P. ; Soricut , R.      . ALBERT : Lite BERT Self- supervised Learning Language Representations . ICLR . OpenReview.net . URL https : //openreview.net/forum ? id= H eA AEtvS . Lebanoff , L. ; Song , K. ; Dernoncourt , F. ; Kim , D. S. ; Kim , S. ; Chang , W. ; Liu , F.      . Scoring Sentence Singletons Pairs Abstractive Summarization . ACL     –     . Lin , C.-Y .      . Rouge : package automatic evaluation summaries . Text summarization branches ,   –   . Liu , Y. ; Lapata , M.      . Text summarization pretrained encoders . EMNLP ,     –     . Manning , C. ; Surdeanu , M. ; Bauer , J. ; Finkel , J. ; Bethard , S. ; McClosky , D.      . Stanford CoreNLP Natural Language Processing Toolkit . ACL ,   –   . Mendes , A. ; Narayan , S. ; Miranda , S. ; Marinho , Z. ; Martins , A. F. ; Cohen , S. B .      . Jointly Extracting Com- pressing Documents Summary State Representations . NAACL-HLT ,     –     . Nallapati , R. ; Zhai , F. ; Zhou , B .      . Summarunner : recurrent neural network based sequence model extractive summarization documents . AAAI ,     –     . Narayan , S. ; Cohen , S. B. ; Lapata , M.      . Ranking sentences extractive summarization reinforcement learning . NAACL-HLT ,     –     . Owczarzak , K. ; Conroy , J. M. ; Dang , H. T. ; Nenkova , .      . assessment accuracy automatic eval- uation summarization . Proceedings Workshop Evaluation Metrics System Comparison Automatic Summarization ,  –  . Paszke , A. ; Gross , S. ; Massa , F. ; Lerer , A. ; Bradbury , J. ; Chanan , G. ; Killeen , T. ; Lin , Z. ; Gimelshein , N. ; Antiga , L. ; Desmaison , A. ; Kpf , A. ; Yang , E. ; DeVito , Z. ; Raison , M. ; Tejani , A. ; Chilamkurthy , S. ; Steiner , B. ; Fang , L. ; Bai , J. ; Chintala , S.      . PyTorch : Imperative Style , High- Performance Deep Learning Library . NIPS ,     –     . Paulus , R. ; Xiong , C. ; Socher , R.      . Deep Rein- forced Model Abstractive Summarization . ICLR . Raffel , C. ; Shazeer , N. ; Roberts , A. ; Lee , K. ; Narang , S. ; Matena , M. ; Zhou , Y. ; Li , W. ; Liu , P. J .      . Explor- ing limits transfer learning uniﬁed text-to-text transformer . J. Mach . Learn . Res .    : –   :   . Rush , A. M. ; Chopra , S. ; Weston , J .      . Neural Attention Model Abstractive Sentence Summarization . EMNLP ,    –    . Sandhaus , E.      . new york times annotated corpus . Linguistic Data Consortium , Philadelphia . See , A. ; Liu , P. J. ; Manning , C. D.      . Get Point : Summarization Pointer-Generator Networks . ACL ,     –     .       Zhong , M. ; Wang , D. ; Liu , P. ; Qiu , X. ; Huang , X .     b . Closer Look Data Bias Neural Extractive Summariza- tion Models . arXiv preprint arXiv:    .      . Zhou , Q. ; Yang , N. ; Wei , F. ; Huang , S. ; Zhou , M. ; Zhao , T.      . Neural document summarization jointly learning score select sentences . ACL ,    –    . Sharma , E. ; Huang , L. ; Hu , Z. ; Wang , L.      . Entity-Driven Framework Abstractive Summarization . EMNLP ,     –     . Sun , S. ; Cheng , Y. ; Gan , Z. ; Liu , J .      . Patient Knowl- edge Distillation BERT Model Compression . EMNLP ,     –     . Tan , X. ; Ren , Y. ; , D. ; Qin , T. ; Zhao , Z. ; Liu , T.- .      . Multilingual Neural Machine Translation Knowledge Distillation . ICLR . OpenReview.net . URL https : //openreview.net/forum ? id=S gUsoR YX . Vaswani , A. ; Shazeer , N. ; Parmar , N. ; Uszkoreit , J. ; Jones , L. ; Gomez , A. N. ; Kaiser , Ł. ; Polosukhin , .      . Attention need . NIPS ,     –     . Wang , D. ; Liu , P. ; Zheng , Y. ; Qiu , X. ; Huang , X .      . Heterogeneous Graph Neural Networks Extractive Docu- ment Summarization . ACL ,     –     . Wang , D. ; Liu , P. ; Zhong , M. ; Fu , J. ; Qiu , X. ; Huang , X .      . Exploring Domain Shift Extractive Text Summa- rization . arXiv preprint arXiv:    .      . Xiao , W. ; Carenini , G.      . Extractive Summarization Long Documents Combining Global Local Context . EMNLP ,     –     . Xu , J. ; Durrett , G.      . Neural Extractive Text Summa- rization Syntactic Compression . EMNLP     –     . Xu , J. ; Gan , Z. ; Cheng , Y. ; Liu , J .      . Discourse-Aware Neural Extractive Text Summarization . ACL ,     –     . Yan , Y. ; Qi , W. ; Gong , Y. ; Liu , D. ; Duan , N. ; Chen , J. ; Zhang , R. ; Zhou , M.      . ProphetNet : Predicting Fu- ture N-gram Sequence-to-Sequence Pre-training . arXiv preprint arXiv:    .      ,     –     . Yang , Z. ; Yang , D. ; Dyer , C. ; , X. ; Smola , A. ; Hovy , E.      . Hierarchical Attention Networks Document Classiﬁcation . NAACL-HLT ,     –     . Zhang , H. ; Gong , Y. ; Yan , Y. ; Duan , N. ; Xu , J. ; Wang , J. ; Gong , M. ; Zhou , M.     a . Pretraining-Based Natural Language Generation Text Summarization . CoNLL ,    –    . Zhang , J. ; Zhao , Y. ; Saleh , M. ; Liu , P. J .     b . PEGA- SUS : Pre-training Extracted Gap-sentences Abstrac- tive Summarization . arXiv preprint arXiv:    .      ,      –      . Zhang , X. ; Lapata , M. ; Wei , F. ; Zhou , M.      . Neural EMNLP , Latent Extractive Document Summarization .    –    . Zhang , X. ; Wei , F. ; Zhou , M.      . HIBERT : Document Level Pre-training Hierarchical Bidirectional Transformers Document Summarization . ACL ,     –     . Zhong , M. ; Liu , P. ; Chen , Y. ; Wang , D. ; Qiu , X. ; Huang , X .      . Extractive Summarization Text Matching . ACL ,     –     . Zhong , M. ; Liu , P. ; Wang , D. ; Qiu , X. ; Huang , X .     a . Searching Effective Neural Extractive Summarization : Works Whats Next . ACL ,     –     .      