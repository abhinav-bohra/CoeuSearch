1 2 0 2 g u A 3 1 ] L C . s c [ 1 v 7 0 1 6 0 . 8 0 1 2 : v i X r a Aspect Sentiment Triplet Extraction Using Reinforcement Learning Samson Yu Bai Jian SUTD, Singapore samson_yu@sutd.edu.sg Tapas Nayak IIT KGP, India tnk02.05@gmail.com Navonil Majumder SUTD, Singapore navonil_majumder@sutd.edu.sg Soujanya Poria SUTD, Singapore sporia@sutd.edu.sg ABSTRACT Aspect Sentiment Triplet Extraction (ASTE) is the task of extracting triplets of aspect terms, their associated sentiments, and the opinion terms that provide evidence for the expressed sentiments. Previous approaches to ASTE usually simultaneously extract all three compo- nents or first identify the aspect and opinion terms, then pair them up to predict their sentiment polarities. In this work, we present a novel paradigm, ASTE-RL, by regarding the aspect and opinion terms as arguments of the expressed sentiment in a hierarchical reinforcement learning (RL) framework. We first focus on senti- ments expressed in a sentence, then identify the target aspect and opinion terms for that sentiment. This takes into account the mu- tual interactions among the triplet’s components while improving exploration and sample efficiency. Furthermore, this hierarchical RL setup enables us to deal with multiple and overlapping triplets. In our experiments, we evaluate our model on existing datasets from laptop and restaurant domains and show that it achieves state-of- the-art performance. The implementation of this work is publicly available at https://github.com/declare-lab/ASTE-RL. ACM Reference Format: Samson Yu Bai Jian, Tapas Nayak, Navonil Majumder, and Soujanya Poria. 2021. Aspect Sentiment Triplet Extraction Using Reinforcement Learning. In Proceedings of the 30th ACM Int’l Conf. on Information and Knowledge Management (CIKM ’21), November 1–5, 2021, Virtual Event, Australia. ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/XXXXXX.XXXXXX 1 INTRODUCTION Aspect-based sentiment analysis (ABSA) or target-based sentiment analysis (TBSA) is an important research area in natural language processing (NLP) [5, 15]. It consists of various fine-grained sen- timent analysis tasks [11–13], with the three most fundamental being aspect/target term extraction, opinion term extraction and as- pect/target term sentiment classification. Aspect Sentiment Triplet Extraction (ASTE) is a relatively new subtask of ABSA introduced by Li et al. [9], Peng et al. [14]. In ASTE, the task is to extract triplets containing aspect terms, their associated sentiment polarities, and the opinion terms that express those sentiments. A sentence may contain multiple such triplets where the aspect terms or opinion Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. CIKM ’21, November 1–5, 2021, Virtual Event, Australia. © 2021 Association for Computing Machinery. ACM ISBN 978-1-4503-8446-9/21/11. . . $15.00 https://doi.org/10.1145/XXXXXX.XXXXXX terms across triplets may overlap with each other. We include an example of such triplets in Table 1. Table 1: An Example of ASTE Triplets Present in a Sentence Sentence Triplets Appetizers are excellent ; you can make a great ( but slightly expensive ) meal out of them . [Aspect ; Opinion ; Sentiment] (1) Appetizer ; excellent ; positive (2) meal ; great ; positive (3) meal ; slightly expensive ; negative Existing methods, such as, CMLA+ [22], RINANTE+ [2], Li- unified-R [9], WhatHowWhy [14], OTE-MTL [27], GTS [25], JET [26], TOP [6] and BMRC [1] are mainly divided into simultaneous and sequential methods. Early works [2, 9, 14, 22, 25, 27] usually employ a two-staged approach where they simultaneously extract aspect terms with sentiments and opinion terms. These triplets are subsequently decoded through triplet classification or pairwise matching. Recent works [1, 6] have shifted towards a more multi- stage, restrictive and sequential process during the extraction stage that can potentially capture more mutual dependencies and corre- lations among the triplet’s components while forgoing the triplet decoding stage. In this work, we tackle the ASTE task using a novel paradigm ASTE-RL where we consider the aspect and opinion terms as arguments of the sentiments expressed in a sentence. Previous approaches usually simultaneously extract all three components or first identify the aspect and opinion terms, then pair them up to predict their sentiment polarities. Unlike previous approaches, we propose a hierarchical reinforcement learning (RL) framework [4, 21] where we first consider the sentiment polarities, then iden- tify their associated opinion and aspect terms using separate RL processes. This process is repeated to extract all triplets present in a sentence. With this hierarchical RL setup, the model handle multiple triplets and overlapping triplets, and model interactions between the three components effectively. Inspired by the recent success of the multi-turn machine reading comprehension (MRC) framework [1, 10], we incorporate ideas to further improve mutual interactions. 2 PROPOSED FRAMEWORK 2.1 Overview We divide our framework ASTE-RL into three components: 1) aspect-oriented sentiment classification, 2) opinion term extraction and 3) aspect term extraction. For the sentiment classification com- ponent, the sentiment is expressed towards the aspect term, and has four possible labels: 𝐿𝐶 = {none, positive, negative, neutral}. Our opinion and aspect extraction components are sequence la- beling models with a BIO tagging scheme [17]. With this BIO             scheme, we have three different labels to tag an input sequence for the opinion/aspect terms: 𝐿𝑂,𝐴 = {B, I, O}. For a given sen- tence 𝑋 = {𝑥1, 𝑥2, ..., 𝑥 𝐽 } with 𝐽 tokens, ASTE-RL aims to output a set of labels {(𝑌 𝑂, 𝑌 𝐴, 𝐶) |𝐾 | } where |𝐾 | is the number of labels, 𝑌 𝑂 = {𝑦𝑂 𝐽 } represents the tagging labels for the opin- 1 ion term in a predicted triplet, 𝑌 𝐴 = {𝑦𝐴 𝐽 } represents 1 the tagging labels for the aspect term, and 𝐶 = {𝑐} represents the sentiment polarity. , ..., 𝑦𝑂 , ..., 𝑦𝐴 , 𝑦𝑂 2 , 𝑦𝐴 2 The three components are structured in a two-level hierarchy [21]. In the higher level, we have the sentiment indicator. During the sequential scan of a sentence, an agent will decide at each position 𝑗 in a sentence at the token 𝑥 𝑗 if it has gathered sufficient information to mark the position as indicative of a sentiment that is expressed towards an aspect term. If not, the agent will mark it as none. Otherwise, it will mark it as either positive, negative or neutral. In the latter case, the agent launches two subtasks in the lower level for the opinion and aspect extractions to identify the terms as arguments of the sentiment and engages in sequence labeling. Upon completion, the agent will return to the high-level sentiment indication process and continue the sequential scan of the sentence. This process is well-suited to be formulated as a semi-Markov decision process [20]: 1) a high-level RL process that detects a sentiment indicator in a sentence; 2) two low-level RL processes that identify the opinion and aspect terms separately for the corresponding sentiment. 2.2 Aspect-Oriented Sentiment Classification with High-Level RL The high-level RL policy 𝜋𝑢 aims to detect the aspect-oriented sentiments in a sentence. This can be seen as a RL policy over options, where options are high-level actions [20]. Option: The option o𝑡 is selected from 𝐿𝐶 = {none, positive, negative, neutral} where none indicates no sentiment indicated towards any aspect term. State: The state s𝑢 𝑡 at each time step 𝑡 is represented by: 1) the current hidden state h𝑡 , 2) the current part-of-speech (POS) tag p𝑡 , 3) the sentiment polarity vector v𝑐 𝑡 , and 4) the high-level state for the previous time step s𝑢 𝑡 −1 . To obtain p𝑡 for each token in a sentence, we pass the sentence into the spaCy (https://spacy.io/) POS tagger. The sentiment polarity vector v𝑐 𝑡 is the embedding of the latest option o𝑡 ′ where o𝑡 ′ ≠ none. Both the POS tag and sentiment embeddings are learned parameters in the model. Hence, the state s𝑢 𝑡 is formally represented by: 𝑡 = 𝑓 𝑢 𝑢 s 𝑢 𝑐 𝑢 𝑡 −1]), 𝑠 [h𝑡 ; p𝑡 ; v 𝑠 (W 𝑡 ; s (1) where 𝑓 𝑢 𝑠 (·) is a non-linear function implemented by a MLP. The hidden state h𝑡 is obtained from a pre-trained BERT model [3] with Whole Word Masking and fine-tuned on the SQuAD v1.1 training set [16]. Specifically, we first combine the query "Which tokens indicate sentiments relating pairs of aspect spans and opinion spans?" and the review sentence 𝑋 into the BERT tokenizer to get a final input 𝐼 = {[𝐶𝐿𝑆], 𝑞𝑢 , [𝑆𝐸𝑃], 𝑥1, 𝑥2, ..., 𝑥 𝐽 }. We then pass this 1 input into the BERT model, and h𝑡 represents the output vector from the BERT model that corresponds to the token 𝑥𝑡 . The initial state s𝑢 0 is initialized as: s𝑢 , ..., 𝑞𝑢 |𝑞 | 0 = 0. Policy: The stochastic policy for sentiment detection 𝜋𝑢 speci- fies a probability distribution over the options: 𝑢 𝑢 𝑢 𝑡 ). 𝑡 ) = 𝑠𝑜 𝑓 𝑡𝑚𝑎𝑥 (W o𝑡 ∼ 𝜋𝑢 (o𝑡 |s 𝜋 s (2) Reward: At every time step, when o𝑡 is executed, the interme- diate reward r𝑢 𝑡 provided by the environment follows this: 𝑢 𝑡 = r 1, 0, −1,    if ot in 𝑋 if ot = none if ot not in 𝑋 . (3) If a sentiment that is expressed towards an aspect term is detected at a time step (i.e. o𝑡 ≠ none), the agent will launch two subtasks as low-level RL processes. When the subtasks are completed, the agent will return to the high-level RL process. Otherwise, the agent continues its sequential scan of 𝑋 until the last option o𝐽 about the last word 𝑥 𝐽 of 𝑋 is sampled. When all options are sampled (i.e. at the end of the combined hierarchical RL process), there is a final reward r𝑢 𝑓 𝑖𝑛𝑎𝑙 = 𝐹1 (𝑋 ) where 𝐹1 is the harmonic mean of the precision and recall in terms of the sentiment(s) in a sentence 𝑋 . 𝑓 𝑖𝑛𝑎𝑙 for the high-level process: r𝑢 2.3 Opinion and Aspect Extractions with Low-Level RL Every time the high-level policy detects an aspect-oriented senti- ment, two low-level policies 𝜋𝑂 , 𝜋𝐴 𝑙 will extract the corresponding 𝑙 opinion and aspect terms respectively and separately for the senti- ment. In this subsection, we will generalize the RL elements such that they apply for both low-level RL processes, unless otherwise stated. Action: The action at every time step 𝑡 is to assign a tag to the current word. The action a𝑡 is selected from 𝐿𝑂,𝐴 = {B, I, O}, following a BIO tagging scheme. The B/I symbols represent the beginning and inside of an opinion/aspect term respectively, while the O symbol represents the unmarked label. State: Similar to the high-level policy, the state s𝑙 𝑡 at each time step 𝑡 is represented by: 1) the current hidden state h𝑡 , 2) the current 𝑂,𝐴 POS tag p𝑡 , 3) the opinion/aspect tag vector v , 4) the low-level 𝑡 state for the previous time step s𝑙 𝑡 −1. To enhance the interactions between the sentiment and its associated opinion/aspect terms, we add a context vector v𝑐𝑡𝑥 to the state s𝑙 𝑡 at each time step 𝑡, using 𝑡 ′ the sentiment state representation assigned to the latest option o𝑡 ′: 𝑡 ′ = 𝑔𝑐𝑡𝑥 (W 𝑐𝑡𝑥 v 𝑢 𝑐𝑡𝑥 𝑡 ′). 𝑣𝑡′ s (4) We also add the output vector from the BERT model for the [CLS] token, while computing the output vectors for the hidden states. Hence, the state s𝑙 𝑡 is formally represented by: 𝑐𝑡𝑥 𝑡 = 𝑓 𝑙 𝑙 𝑙 𝑡 −1; v ; s s 𝑡 ′ 𝑂,𝐴 𝑙 𝑠 [h𝑡 ; p𝑡 ; v 𝑠 (W 𝑡 𝐶𝐿𝑆 ; h 𝑡 ]). (5) Note that the representations used to compute the first low-level states for the opinion and aspect extractions are different. The repre- sentation used to compute the first low-level state for opinion term 𝑡 ′). The extraction s representation used to compute the first low-level state for aspect term extraction s . These initializations help us capture interactions between the triplet’s components. 𝑓 𝑙 𝑠 (·) is a non-linear function implemented by a MLP, is initialized using s𝑢 0 = 𝑔𝑙 (W𝑙 is initialized using s 𝑙,𝐴 0 = s 𝑡 ′ as: s 𝑠0 s𝑢 𝑙,𝑂 1 𝑙,𝐴 1 as: s 𝑙,𝑂 𝐽 𝑙,𝑂 𝐽 𝑙,𝑂 while 𝑔𝑐𝑡𝑥 (·) and 𝑔𝑙 (·) are linear functions that are implemented by a single linear layer. The hidden state h𝑡 is obtained in the same way as in the high-level RL process. However, the queries are changed. The query is "What is the opinion span for the o𝑡 ′ sentiment indicated at 𝑥𝑡 ′?" for opinion term extraction and "What is the aspect span for the o𝑡 ′ sentiment indicated at 𝑥𝑡 ′?" for aspect term extraction. Policy: The stochastic policy for opinion/aspect extraction 𝜋𝑙 specifies a probability distribution over the actions given the low- level state s𝑙 𝑡 and the high-level option o𝑡 ′ that launches the current subtask: 𝑙 𝑙 𝑙 a𝑡 ∼ 𝜋𝑙 (a𝑡 |s 𝑡 ; o𝑡 ′) = 𝑠𝑜 𝑓 𝑡𝑚𝑎𝑥 (W 𝑡 ). 𝜋 [o𝑡 ′]s Reward: At every time step, when a𝑡 is executed, the interme- 𝑡 is computed as the prediction error over the gold (6) diate reward 𝑟𝑙 labels: 𝑟 𝑙 𝑡 = (cid:40)𝜆, −0.5, if tag 𝑦𝑡 is predicted correctly otherwise, (7) where 𝜆 ≥ 0 and 𝜆 depends on the aspect/opinion tag type. This enables the model to learn a policy that emphasizes the prediction of B and I tags and avoids only predicting O tags in a trivial manner. When all actions are sampled, there is a final reward 𝑟𝑙 𝑓 𝑖𝑛𝑎𝑙 for the low-level processes, represented by: 𝑟 𝑙 𝑓 𝑖𝑛𝑎𝑙 = (cid:40) 1, −1, if all tags are predicted correctly otherwise. (8) There will also be negative rewards in the cases where the low- level processes produce impossible predictions, namely cases where there are no or more than one B tag present, and no or more than one opinion/aspect term identified for each predicted triplet. Note that the low-level rewards are non-zero only in the case where the option o𝑡 ′ from the high-level process is correctly predicted. 2.4 Hierarchical Policy Learning We learn the high-level policy by maximizing the expected total re- ward at each time step 𝑡 as the agent samples trajectories following the high-level policies 𝜋𝑢 . Likewise, we learn the low-level policies by maximizing the expected total reward at each time step 𝑡 as the agent samples trajectories following the low-level policies 𝜋𝐴 , 𝜋𝑂 . 𝑙 𝑙 We then optimize all policies using policy gradient methods [19] with the REINFORCE algorithm [21, 23]. 2.5 Training Procedure We pre-train our ASTE-RL models for 40 epochs with a learning rate of 2e-5. During pre-training, we give our model the ground-truth options or actions at every time step to limit the exploration of the agent due to the high-dimensional state space in our setup. This prevents the agent from exploring too many unreasonable cases, e.g. an I tag preceding a B tag, and learning too slowly. We then fine-tune the best model (chosen based on the Dev 𝐹1 score) with RL policy for 15 epochs with a learning rate of 5e-6. We sample 5 trajectories for each data point during RL fine-tuning. We initialize the BERT parameters from pre-trained weights [3] and update them during training for this task. We set the dimension of sentiment polarity and opinion/aspect tag embeddings at 300. For POS embeddings, we set the dimension at 25. We randomly initialize these embeddings and update them during training. We set the state vector dimension for s𝑢 𝑡 at 300. We apply dropout [18] after 𝑡 and s𝑙 the non-linear activations in 𝑓 𝑢 𝑠 (·) and 𝑓 𝑙 𝑠 (·) during training and set the dropout rate at 0.5. We train our models in mini-batches of size 16 and optimize the model parameters using the Adam optimizer [7]. 3 EXPERIMENTS AND ANALYSIS 3.1 Datasets & Evaluation Metrics Table 2: ASTE-Data-V2 Dataset Statistics 14Lap 14Rest 15Rest 16Rest Train Dev Test Train Dev Test Train Dev Test Train Dev Test #sentence #positive #negative #neutral 906 817 517 126 219 169 141 36 328 364 116 63 1266 1692 480 166 310 404 119 54 492 773 155 66 605 783 205 25 148 185 53 11 322 317 143 25 857 1015 329 50 210 252 76 11 326 407 78 29 We use the ASTE-Data-V2 dataset1 curated by Xu et al. [26] to show the effectiveness of ASTE-RL in two different domains of English reviews, namely the laptop and restaurant domains. 14Rest, 15Rest, 16Rest are the datasets of the restaurant domain and 14Lap is of the laptop domain. We include the statistics of the four datasets in ASTE-Data-V2 in Table 2, where #sentence represents the number of sentences, and #positive, #negative, and #neutral represent the numbers of triplets with positive, negative, and neutral sentiment polarities respectively. We process the sentences with BERT’s WordPiece tokenizer [24] to make them work for ASTE-RL. Since the WordPiece tokenization may break down the tokens in the original dataset into subwords, we need to align the opinion/aspect term annotations and our BIO tagging scheme. We tag every token that corresponds to the opin- ion/aspect term tokens in the original annotations with I, except for the first token, which we tag with B. We follow the evaluation metrics of Xu et al. [26] for our ex- periments. An extracted triplet is correct if the entire aspect term, opinion term, and sentiment polarity match with a ground-truth triplet. We report precision, recall and F1 score based on this. 3.2 Baselines We compare the performance of ASTE-RL against the following baselines: (i) WhatHowWhy: Peng et al. [14] proposed a multi- layer LSTM neural architecture for co-extraction of aspect terms with sentiments, and opinion terms, with a Graph Convolutional Network [8] component to capture dependency information to enhance the co-extraction. (ii) OTE-MTL: Zhang et al. [27] pro- posed a multi-task learning framework to jointly extract aspect and opinion terms while parsing word-level sentiment dependen- cies, before conducting a triplet decoding process. We use results from Huang et al. [6] for OTE-MTL’s performance on ASTE-Data- V2. (iii) GTS: Wu et al. [25] proposed an end-to-end grid tagging framework and a grid inference strategy to exploit mutual indi- cation between opinion factors. We use results from Huang et al. [6] for GTS’ performance on ASTE-Data-V2, and report them for two variants: bidirectional LSTM (BiLSTM) and BERT. (iv) JET: Xu et al. [26] proposed a position-aware tagging scheme for triplet extraction. They encode information about sentiment polarities and distances between the start position of aspect term and the 1https://github.com/xuuuluuu/SemEval-Triplet-data Table 3: Results of ASTE-RL and Previous Methods on the ASTE-Data-V2 Dataset Model WhatHowWhy [14] OTE-MTL [27] GTSBiLSTM [25] JET𝑡 BiLSTM [26] JET𝑜 BiLSTM [26] GTSBERT [25] JET𝑡 BERT [26] JET𝑜 BERT [26] TOP [6] BMRC [1] ASTE-RL - Pre-training only 14Lap 14Rest 15Rest 16Rest Dev 𝐹1 Prec Rec 𝐹1 Dev 𝐹1 Prec Rec 𝐹1 Dev 𝐹1 Prec Rec 𝐹1 Dev 𝐹1 Prec Rec 𝐹1 - - - 48.26 45.83 - 50.40 48.84 - 56.08 58.14 57.35 37.38 54.26 58.02 54.84 55.98 57.12 53.53 55.39 57.84 65.91 64.80 62.00 50.38 41.07 40.11 34.44 35.36 53.42 43.28 47.33 59.33 52.15 54.99 55.84 42.87 46.75 47.43 42.31 43.34 55.21 47.86 51.04 58.58 58.18 59.50 58.73 - - - 53.14 53.54 - 56.00 56.89 - 62.83 64.40 64.50 43.24 63.07 71.41 66.76 61.50 71.76 63.44 70.56 63.59 72.17 70.60 69.70 63.66 58.25 53.00 49.09 55.13 59.09 54.12 55.94 73.44 65.43 68.65 69.23 51.46 60.56 60.84 56.58 58.14 64.81 58.41 62.40 68.16 68.64 69.61 69.47 - - - 55.06 60.97 - 59.86 64.78 - 72.47 74.01 72.84 48.07 60.88 64.57 59.77 64.37 54.71 68.20 64.45 54.53 62.48 65.45 63.31 57.51 42.68 44.33 42.27 44.33 55.05 42.89 51.96 63.30 55.55 60.29 61.61 52.32 50.18 52.57 49.52 52.50 54.88 52.66 57.53 58.59 58.79 62.72 62.44 - - - 58.45 60.90 - 60.67 63.75 - 70.91 72.11 71.50 46.96 65.65 70.17 63.59 70.94 65.89 65.28 70.42 63.57 69.87 67.21 64.76 64.24 54.28 55.95 50.97 57.00 66.27 51.95 58.37 71.98 65.68 69.69 70.74 54.21 59.42 62.26 56.59 63.21 66.08 57.85 63.83 67.52 67.35 68.41 67.57 opinion term’s start and end positions (JET𝑡 ) or vice versa (JET𝑜 ). We report the results for two variants: BiLSTM and BERT. (v) TOP: Huang et al. [6] proposed a two-stage method to enhance correla- tions between aspect and opinion terms. Aspect and opinion terms are first extracted with sequence labeling, and artificial tags are added to each pair to establish correlation. A sentiment polarity is then identified for each pair using the resulting representations. (vi) BMRC: Chen et al. [1] proposed a transformation of the ASTE task into a multi-turn MRC task and a bidirectional MRC framework to address it. They use non-restrictive, restrictive and sentiment classification queries in a three-turn process to extract triplets. We train and test BMRC on ASTE-Data-V2 over 5 runs with different random seeds. Table 4: 𝐹1 Scores for Multiple and Overlapping Triplets 14Lap 14Rest 15Rest 16Rest ASTE-RL BMRC ASTE-RL BMRC ASTE-RL BMRC ASTE-RL BMRC Single Multiple No Overlap Overlap 62.46 57.70 62.24 55.94 63.04 53.77 62.80 50.18 67.44 70.23 72.16 67.23 66.51 69.17 70.81 63.96 61.33 63.95 62.17 63.83 59.59 56.37 59.75 55.48 66.31 69.84 70.13 65.20 67.83 64.11 69.23 58.59 3.3 Experimental Results The experimental results are shown in Table 3. We observe that BERT-based models (their results are in the row above ASTE-RL’s results in Table 3) generally perform better than the non-BERT models. Hence, we only experiment with BERT for our ASTE-RL model. We select our best model for each dataset based on its Dev 𝐹1 score. For reproducibility, we report the testing results averaged over 5 runs with different random seeds. ASTE-RL outperforms existing baselines on all four datasets, and significantly outperforms existing baselines on the 15Rest dataset. When compared to the second-best performance for each dataset, we observe an average improvement of 1.68% 𝐹1 score across all four datasets, and an improvement of 3.93% on 15Rest. We also observe that our model strikes a balance between the TOP and BMRC models in terms of precision and recall, and hypothesize that this balance can be flexibly shifted depending on 𝛽 to fit dataset requirements, if we generalize r𝑢 𝑓 𝑖𝑛𝑎𝑙 = 𝐹𝛽 (𝑋 ), where 𝐹𝛽 (𝑋 ) is the weighted harmonic mean of precision and recall. as usual and after that we run for another 15 epochs with a learning rate of 5e-6 (as used in RL fine-tuning step). As compared to the RL fine-tuning setting with multinomial sampling, this setting has lower 𝐹1 scores with an average decrease of 0.51% over 5 runs with different random seeds. In this setting, our model achieves slightly higher recall, but precision is significantly lower across all four datasets. This might be because multinomial sampling encourages more exploration after the initial pre-training of 40 epochs. 3.5 Analysis on Multiple & Overlapping Triplet Extraction We show the results of ASTE-RL and BMRC in complex situations where there are multiple and overlapping triplets in a sentence in Table 4. For the multiple triplet scenario, we observe that there is a performance increase for 14Rest, 15Rest and 16Rest and a decrease for 14Lap as compared to the case where only one triplet is present in a sentence. For the overlapping triplet scenario, we observe a performance increase for for 15Rest and a decrease for 14Lap, 14Rest and 16Rest. In general, we observe that ASTE-RL can handle multiple and overlapping triplets in a sentence consistently well due to its hierar- chical RL setup, as compared to BMRC. There is a total 𝐹1 decrease of 4.76% for multiple triplet extraction for ASTE-RL across all four datasets as compared to 16.21% for BMRC, and a total 𝐹1 decrease for overlapping triplet extraction of 16.16% for ASTE-RL as compared to 34.38% for BMRC. 4 CONCLUSION In this work, we propose a novel ASTE-RL model based on hierar- chical reinforcement learning (RL) paradigm for aspect sentiment triplet extraction (ASTE). In this paradigm, we treat the aspect and opinion terms as arguments of the sentiment polarities. We decom- pose the ASTE task into a hierarchy of three subtasks: high-level sentiment polarity extraction, and low-level opinion and aspect term extractions. This approach is good at modeling the interactions between the three tasks and handling multiple and overlapping triplets. We incorporate the multi-turn MRC elements in our model to further improve these interactions. Our proposed model achieves state-of-the-art performance on four challenging datasets for the ASTE task. 3.4 Effect of RL Fine-tuning In Table 3, we report our results for ASTE-RL without the RL fine tuning step. In this setting, we pre-train our ASTE-RL for 40 epochs ACKNOWLEDGMENTS This project is supported by the DSO grant no. RTDST190702 awarded to SUTD titled Complex Question Answering. 2016. Google’s neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144 (2016). [25] Zhen Wu, Chengcan Ying, Fei Zhao, Zhifang Fan, Xinyu Dai, and Rui Xia. 2020. Grid Tagging Scheme for Aspect-oriented Fine-grained Opinion Extraction. arXiv preprint arXiv:2010.04640 (2020). [26] Lu Xu, Hao Li, Wei Lu, and Lidong Bing. 2020. Position-Aware Tagging for Aspect Sentiment Triplet Extraction. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. https://doi.org/10.18653/v1/2020.emnlp- main.183 [27] Chen Zhang, Qiuchi Li, Dawei Song, and Benyou Wang. 2020. A Multi-task Learn- ing Framework for Opinion Triplet Extraction. arXiv preprint arXiv:2010.01512 (2020). REFERENCES [1] Shaowei Chen, Yu Wang, Jie Liu, and Yuelin Wang. 2021. Bidirectional Machine Reading Comprehension for Aspect Sentiment Triplet Extraction. arXiv preprint arXiv:2103.07665 (2021). [2] Hongliang Dai and Yangqiu Song. 2019. Neural Aspect and Opinion Term Extrac- tion with Mined Rules as Weak Supervision. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. https://www.aclweb. org/anthology/P19-1520 [3] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018). [4] Jiafei Duan, Samson Yu, Hui Li Tan, and Cheston Tan. 2020. Actionet: An Inter- active End-To-End Platform For Task-Based Data Collection And Augmentation In 3D Environment. In 2020 IEEE International Conference on Image Processing (ICIP). IEEE, 1566–1570. [5] Jiafei Duan, Samson Yu, Hui Li Tan, Hongyuan Zhu, and Cheston Tan. 2021. A Survey of Embodied AI: From Simulators to Research Tasks. arXiv preprint arXiv:2103.04918 (2021). [6] Lianzhe Huang, Peiyi Wang, Sujian Li, Tianyu Liu, Xiaodong Zhang, Zhicong Cheng, Dawei Yin, and Houfeng Wang. 2021. First Target and Opinion then Polarity: Enhancing Target-opinion Correlation for Aspect Sentiment Triplet Extraction. arXiv preprint arXiv:2102.08549 (2021). [7] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti- mization. arXiv preprint arXiv:1412.6980 (2014). [8] Thomas N Kipf and Max Welling. 2016. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907 (2016). [9] Xin Li, Lidong Bing, Piji Li, and Wai Lam. 2019. A Unified Model for Opinion Target Extraction and Target Sentiment Prediction. In The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI. https://doi.org/10.1609/aaai.v33i01. 33016714 [10] Xiaoya Li, Fan Yin, Zijun Sun, Xiayu Li, Arianna Yuan, Duo Chai, Mingxin Zhou, and Jiwei Li. 2019. Entity-Relation Extraction as Multi-Turn Question Answering. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, Florence, Italy, 1340–1350. https://doi.org/10.18653/v1/P19-1129 [11] Bing Liu. 2012. Sentiment analysis and opinion mining. Synthesis lectures on human language technologies (2012). [12] Bing Liu et al. 2010. Sentiment analysis and subjectivity. Handbook of natural language processing (2010). [13] Tetsuya Nasukawa and Jeonghee Yi. 2003. Sentiment analysis: Capturing favora- bility using natural language processing. In Proceedings of the 2nd international conference on Knowledge capture. [14] Haiyun Peng, Lu Xu, Lidong Bing, Fei Huang, Wei Lu, and Luo Si. 2020. Knowing what, how and why: A near complete solution for aspect-based sentiment analysis. In Proceedings of the AAAI Conference on Artificial Intelligence. https://aaai.org/ ojs/index.php/AAAI/article/view/6383 [15] S. Poria, D. Hazarika, N. Majumder, and R. Mihalcea. 2020. Beneath the Tip of the Iceberg: Current Challenges and New Directions in Sentiment Analysis Research. IEEE Transactions on Affective Computing 01 (nov 2020), 1–29. https: //doi.org/10.1109/TAFFC.2020.3038167 [16] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text. arXiv preprint arXiv:1606.05250 (2016). [17] Lance A Ramshaw and Mitchell P Marcus. 1999. Text chunking using transformation-based learning. In Natural language processing using very large corpora. Springer, 157–176. [18] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research 15, 1 (2014), 1929–1958. [19] Richard S Sutton, David A McAllester, Satinder P Singh, Yishay Mansour, et al. 1999. Policy gradient methods for reinforcement learning with function approxi- mation.. In NIPs, Vol. 99. Citeseer, 1057–1063. [20] Richard S Sutton, Doina Precup, and Satinder Singh. 1999. Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning. Artificial intelligence (1999). [21] Ryuichi Takanobu, Tianyang Zhang, Jiexi Liu, and Minlie Huang. 2019. A hi- erarchical framework for relation extraction with reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence. https://doi.org/10. 1609/aaai.v33i01.33017072 [22] Wenya Wang, Sinno Jialin Pan, Daniel Dahlmeier, and Xiaokui Xiao. 2017. Cou- pled Multi-Layer Attentions for Co-Extraction of Aspect and Opinion Terms. In Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence. http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14441 [23] Ronald J Williams. 1992. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning 8, 3-4 (1992), 229–256. [24] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. 