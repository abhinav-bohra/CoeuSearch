Aspect Sentiment Classiﬁcation Towards Question-Answering with Reinforced Bidirectional Attention Network Jingjing Wang1, Changlong Sun2, Shoushan Li1,∗ , Xiaozhong Liu2, Min Zhang1, Luo Si2, Guodong Zhou1 1School of Computer Science and Technology, Soochow University, China 2Alibaba Group, China djingwang@gmail.com, {lishoushan, minzhang, gdzhou}@suda.edu.cn, {changlong.scl, xiaozhong.lxz, luo.si}@alibaba-inc.com Abstract Question-Answering (QA) Style Review In the literature, existing studies on aspect sen- timent classiﬁcation (ASC) focus on individ- ual non-interactive reviews. This paper ex- tends the research to interactive reviews and proposes a new research task, namely Aspect Sentiment Classiﬁcation towards Question- Answering (ASC-QA), for real-world appli- cations. This new task aims to predict sen- timent polarities for speciﬁc aspects from in- teractive QA style reviews. In particular, a high-quality annotated corpus is constructed for ASC-QA to facilitate corresponding re- search. On this basis, a Reinforced Bidirec- tional Attention Network (RBAN) approach is proposed to address two inherent challenges in ASC-QA, i.e., semantic matching between question and answer, and data noise. Experi- mental results demonstrate the great advantage of the proposed approach to ASC-QA against several state-of-the-art baselines. 1 Introduction As a ﬁne-grained sentiment analysis task, Aspect Sentiment Classiﬁcation (ASC) aims to predict sentiment polarities (e.g., positive, negative, neu- tral) towards given particular aspects from a text and has been drawing more and more interests in natural language processing and computational linguistics over the past few years (Jiang et al., 2011; Tang et al., 2016b; Wang et al., 2018a). However, most of the existing studies on ASC fo- cus on individual non-interactive reviews, such as customer reviews (Pontiki et al., 2014) and tweets (Mitchell et al., 2013; Vo and Zhang, 2015; Dong et al., 2014). For example, in a customer re- view “The food is delicious, but ambience is badly in need of improvement.”, the customer mentions two aspects, i.e., “food” and “ambience”, and ex- presses positive sentiment towards the former and negative sentiment towards the latter. ∗Corresponding author - Question: Is [battery life] durable? How about [oper- ating speed] of the phone? - Answer: Yes, very durable but quite slow and obtuse. Aspect Sentiment Classiﬁcation Towards QA - Input: QA text pair with given aspects - Output: [battery life]: Positive [operating speed]: Negative Figure 1: An example for illustrating the proposed task of Aspect Sentiment Classiﬁcation towards Question- Answering (ASC-QA). Recently, a new interactive reviewing form, namely “Customer Question-Answering (QA)”, has become increasingly popular and a large-scale of such QA style reviews (as shown in Figure 1) could be found in several famous e-commerce platforms (e.g., Amazon and Taobao). Compared to traditional non-interactive customer reviews, interactive QA style reviews are more reliable and convincing because answer providers are ran- domly selected from the real customers who have purchased the product (Shen et al., 2018a). To well automatically-understand the QA style reviews, it’s worthwhile to perform ASC on the QA style reviews. However, we believe that Aspect Sentiment Classiﬁcation towards QA (ASC-QA) is not easy work and this novel task faces at least two ma- jor challenges. On one hand, different from tra- ditional non-interactive reviews with a single se- quence structure, interactive QA style reviews consist of two parallel units, i.e., question and an- swer. Thus, it’s rather difﬁcult to infer the sen- timent polarity towards an aspect based on a sin- gle question or single answer. Take Figure 1 as an example. A well-behaved approach to ASC-QA should match each question and answer bidirec- tionally so as to correctly determine the sentiment polarity towards a speciﬁc aspect. Proceedingsofthe57thAnnualMeetingoftheAssociationforComputationalLinguistics,pages3548–3557Florence,Italy,July28-August2,2019.c(cid:13)2019AssociationforComputationalLinguistics3548On the other hand, different from common QA matching tasks such as question-answering (Shen et al., 2018a), ASC-QA focuses on extracting sen- timent information towards a speciﬁc aspect and may suffer from much aspect-irrelevant noisy in- formation. For instance, in Figure 1, although the words in the answer (e.g., “quite slow”, “ob- tuse”) and the question (e.g., “operating speed”) are relevant to aspect “operating speed”, they are noisy for the other aspect “battery life”. These noisy words might provide wrong signals and mis- lead the model into assigning a negative sentiment polarity to aspect “battery life” and vice versa. Therefore, a well-behaved approach to ASC-QA should alleviate the effects of noisy words for a speciﬁc aspect in both question and answer during model training. In this paper, we propose a reinforced bidirec- tional attention network approach to tackle the above two challenges. Speciﬁcally, we ﬁrst pro- pose a word selection model, namely Reinforced Aspect-relevant Word Selector (RAWS), to alle- viate the effects of noisy words for a speciﬁc as- pect through discarding noisy words and only se- lect aspect-relevant words in a word sequence. On the basis of RAWS, we then develop a Rein- forced Bidirectional Attention Network (RBAN) approach to ASC-QA, which employs two funda- mental RAWS modules to perform word selection over the question and answer text respectively. In this way, RBAN is capable of not only address- ing the semantic matching problem in the QA text pair, but also alleviating the effects of noisy words for a speciﬁc aspect in both the question and an- swer sides. Finally, we optimize RBAN via a rein- forcement learning algorithm, i.e., policy gradient (Williams, 1992; Sutton et al., 1999). The main contributions of this paper are in two folds: • We propose a new research task, i.e., As- pect Sentiment Classiﬁcation towards Question- Answering (ASC-QA), and construct a high- quality annotated benchmark corpus for this task. • We propose an innovative reinforced bidirec- tional attention network approach to ASC-QA and validate the effectiveness of this approach through extensive experiments. 2 Data Collection and Annotation We collect 150k QA style reviews from Taobao1, the most famous electronic business platform in 1http://www.taobao.com China. The QA style reviews consist of three dif- ferent domains: Bags, Cosmetics and Electron- ics. Since corpus annotation is labor-expensive and time-consuming, we randomly select 10k QA text pairs from each domain to perform annota- tion. Speciﬁcally, following Pontiki et al. (2014), we deﬁne an aspect at two levels of granularity, i.e., aspect term and aspect category. Besides, fol- lowing Pontiki et al. (2015), we deﬁne three senti- ment polarities, i.e., positive, negative and neutral (mildly positive or mildly negative) towards both aspect terms and categories. In this way, each QA text pair is annotated with two tuples, i.e., (aspect term, polarity), (aspect category, polarity). For Tuple (Aspect Term, Polarity), we anno- tate the single/multi-word terms together with its corresponding polarities inside each QA text pair according to four main guidelines as follows: (1) We only annotate the aspect term when the re- lated question and answer are matched. For exam- ple, the QA text pair in Figure 1 is annotated as (“battery life”, positive) and (“operating speed”, negative) due to words “durable”, “slow” and “ob- tuse”. However, in E1, the answer does not reply to the question correctly and thus the aspects of “macos” and “screen” will not be annotated. E1: Q: Is macos good? How about the screen? A: The shopkeeper is very warm-hearted. (2) We only annotate the aspect term towards which an opinion is expressed. For example, in E2, the answer conveys only objective informa- tion without expressing opinions towards “phone” and thus “phone” will not be annotated. However, “case” will be annotated and tagged as neutral. E2: Q: How is this phone? How about the case? A: I bought this phone yesterday. Case is okay nothing great. (3) We only annotate aspect terms which explic- itly name particular aspects. For example, in E3, “this”, “it” will not be annotated. E3: Q: Is this expensive? Did anybody buy one? A: Of course, it’s quite expensive. (4) When one aspect term has two different de- scriptions in both question and answer, the an- notated aspect term should be consistent with the question. For example, in E4, the annotated aspect term should be “battery life” instead of “battery”. E4: Q: Is battery life durable? A: Yes, this battery is very durable. 3549Domains Aspect Categories Bags Cosmetics Electronics Size, Price, Appearance, Quality, Weight, Certiﬁed Products, Smell, Accessories, Ma- terial, Life Timer, Style, Workmanship, Color, Stain Resistant, Practicality Price, Efﬁcacy, Moisturizing Performance, Certiﬁed Products, Adverse Reaction, Ex- foliator, Texture, Long Lasting, Smell, Ma- terial, Noticeable Color, Quality, Colour, Touch, Skin Whitening, Acne System Performance, Appearance, Battery, Computing (e.g., cpu, gpu, tpu etc.), Cer- tiﬁed Products, Quality, IO (e.g., keyboard, screen, etc.), Price, Storage, Function (e.g., touch id, waterproof etc.) Table 1: The deﬁned aspect categories in each domain. For Tuple (Aspect Category, Polarity), we ﬁrst deﬁne2 15, 16, 10 aspect categories (as shown in Table 1) for the domains of Bags, Cosmetics and Electronics respectively. Then, we annotate aspect categories (chosen from the above predeﬁned cat- egory list) discussed in each QA text pair accord- ing to similar guidelines for aspect term. For ex- ample, there are two aspect categories discussed in Figure 1, i.e., Battery and System Performance, and annotated as (Battery, positive) and (System Performance, negative) respectively. Finally, we discard the QA text pairs which have no annotated term and category. We assign two annotators to tag each QA text pair and the Kappa consistency check value of the annotation is 0.81. When two annotators cannot reach an agreement, an expert will make the ﬁnal decision, ensuring the quality of data annotation. Table 2 shows the statistics of the ﬁnal corpus. To motivate future investigations for this track of research, the annotated corpus consisting of three domains are released in github3. 3 Our Approach In this section, we ﬁrst introduce the word selec- tion model, i.e., Reinforced Aspect-relevant Word Selector (RAWS) as illustrated in Figure 2, which functions as a fundamental module of our ap- proach to alleviate the effects of noisy words (Sec- tion 3.1). On the basis of RAWS, we present the Reinforced Bidirectional Attention Network (RBAN) approach to ASC-QA as illustrated in Figure 3, which employs two RAWS modules to 2Aspect categories are deﬁned and summarized through preliminary annotation. 3https://github.com/jjwangnlp/ASC-QA Domains Bags Cosmetics Electronics Pos. 2503 2834 2742 Neg. Neu. All #Cat. 724 956 821 453 503 531 3680 4293 4094 15 16 10 Table 2: Corpus statistics (Pos., Neg. and Neu. denote the number of positive, negative and neutral for aspect . term; #Cat. denotes the number of aspect category). perform word selection over the question and an- swer text respectively (Section 3.2). Finally, we introduce our optimization strategy via policy gra- dient and back-propagation (Section 3.3). 3.1 Reinforced Aspect-relevant Word Selector (RAWS) Figure 2 shows the framework of the word selec- tion model, i.e., Reinforced Aspect-relevant Word Selector (RAWS). Given an input word sequence x = {x1, .., xE}, RAWS aims to discard noisy words and only select aspect-relevant words in- xaspect xaspect4. The output side x for a speciﬁc aspect xaspect of RAWS is an equal-length sequence of one-hot variables o = [o1, .., oE], where oi = 1 if the word xi is selected otherwise oi = 0. In this way, RAWS virtually functions as a “hard” attention mechanism and thus cannot be di- rectly optimized through back-propagation due to the non-differentiable problem as proposed in Xu et al. (2015) and Shen et al. (2018b). To address this issue, we employ the reinforcement learn- ing algorithm, i.e., policy gradient (Sutton et al., 1999), to model RAWS. In this fashion, RAWS plays as an agent which decides to select the word or not by following a policy network as follows. Policy Network. In this paper, we adopt a stochastic policy network pπ which can provide a conditional probability distribution pπ(o|·) over action sequence o = [o1, .., oE]. Here, o is exactly the output of RAWS and oi = 1 indicates that xi is selected otherwise oi = 0 indicates that xi is discarded. More speciﬁcally, we adopt LSTM (Graves, 2013) to construct the policy network pπ for performing word selection over word sequence x, denoted as LSTMp. In order to differentiate whether a word is selected or discarded, inspired by Lei et al. (2016), we incorporate the action re- sult oi into the input ˆvi of LSTMp at time-step i and compute hidden state hi ∈ Rd of word xi as: hi = LSTMp(ˆvi), ˆvi = vi ⊕ (oi ⊗ e) (1) 4The aspect denotes an aspect term or aspect category as introduced in Section 2. 35501 0 1 0 Action  sequence (cid:82) Softmax Decoder (cid:69)(cid:58)(cid:85) + =(cid:331) QA text pair vector (cid:85) Output RAWS Input (cid:51) (cid:50) (cid:51) (cid:50) Select (cid:51) (cid:50) Discard Aspect Vector (cid:89)(cid:68) Policy Network (cid:83)(cid:83)  (cid:86)1 (cid:75)1  (cid:86)2 (cid:75)2 (cid:3)(cid:86)(cid:76) (cid:75)(cid:76) (cid:3)(cid:86)(cid:40) State (cid:75)(cid:40) LSTM(cid:83) (cid:82)1 (cid:89)1 (cid:82)2 (cid:89)2 (cid:82)(cid:76) (cid:89)(cid:76) (cid:82)(cid:40) (cid:89)(cid:40) Word Sequence (cid:91) Action  Result Word Embedding ∑  A2Q (cid:3)(cid:86)(cid:84) 0 (cid:84)(cid:330)(cid:291) 0 ∑  0 0 (cid:70)(cid:54) 0 0 0 0 0 0 wise softmmax Column-wise softmax (cid:54)(cid:20)(cid:40) (cid:54)(cid:20)(cid:77) -∞  Reinforced  Bidirectional  Attention (cid:3)(cid:86)(cid:68) (cid:68)(cid:330)(cid:291) 0 ∑  0 0 0 0 0 0 ∑  Q2A (cid:85)(cid:54) 0 0 ow--wise s Row-wise softmax 1 (cid:3)(cid:82)(cid:84) -∞  -∞  (cid:54)(cid:76)(cid:40) -∞  (cid:54)(cid:76)(cid:77) -∞  (cid:76)(cid:77)(cid:54) 0 0 1 1 -∞  -∞  -∞  1 1 0 0 0 0 1 1 (cid:3)(cid:82)(cid:68) RAWS RAWS LSTM (cid:68) (cid:83) LSTM (cid:84) (cid:83) Word  Encoder (cid:84)(cid:82)1 (cid:84)(cid:89)1 (cid:84)(cid:82)2 (cid:84)(cid:89)2 (cid:84) (cid:76)(cid:82) (cid:84) (cid:76)(cid:89) Question (cid:84) (cid:40)(cid:82) (cid:84) (cid:40)(cid:89) (cid:84)(cid:91) (cid:68)(cid:82)1 (cid:68)(cid:89)1 (cid:68) (cid:68) (cid:40)(cid:82) (cid:77)(cid:82) (cid:68) (cid:68) (cid:77)(cid:89) (cid:40)(cid:89) Answer (cid:68)(cid:91) Action  Result W d Word  Embedding Figure 2: The framework of word selection model, i.e., Reinforced Aspect-relevant Word Selector (RAWS). where vi ∈ Rd is word embedding of word xi; ⊕ denotes vector concatenation and ⊗ denotes element-wise multiplication; oi ⊗ e = [oi; ..; oi], that is, oi is tiled d(cid:3) times across the row, where e ∈ Rd(cid:3) is a column vector with d(cid:3) 1s and d(cid:3) is set to be 50 tuned with development set; ˆvi ∈ Rd+d(cid:3) . In principle, the policy network pπ uses a Re- ward to guide the policy learning over word se- quence x. It samples an Action oi with the proba- bility pπ(oi|si; θr) at each State si. In this paper, state, action and reward are deﬁned as follows. • State. The state si at i-th time-step should provide adequate information for deciding to se- xaspect xaspect . Thus, the lect a word or not for aspect xaspect state si ∈ R4d is composed of four parts, i.e., hi−1, ci−1, vi and va, deﬁned as si = hi−1 ⊕ ci−1 ⊕ vi ⊕ va, where ci−1 is memory state of LSTMp; va ∈ Rd is aspect vector5 of xaspect xaspect xaspect. • Action. pπ samples action oi ∈ {0, 1} with conditional probability pπ(oi|si; θr), which could be cast as a binary classiﬁcation problem. Thus, we use a logistic function to compute pπ(oi|si; θr). oi ∼ pπ(oi|si; θr) = oi sigmoid(Wrsi + br) +(1 − oi)(1 − sigmoid(Wrsi + br)) (2) where θr = {Wr ∈ R1×4d, br ∈ R} is the param- eter to be learned. ∼ denotes the discrete action sampling operation. • Reward. In order to select aspect-relevant words inside word sequence x, we deﬁne an 5If aspect is a single word like “food”, aspect vector is word embedding, while aspect is multi-word expression like “operating speed” in Figure 1, aspect vector is an average of its constituting word embeddings as Tang et al. (2016b). Figure 3: The framework of our proposed Reinforced Bidirectional Attention Network (RBAN) approach. aspect-relevant reward R based on cosine simi- larity between aspect vector va ∈ Rd of xaspect xaspect xaspect and the last hidden state hE ∈ Rd of LSTMp after pπ ﬁnishes all actions, i.e., R = log cos(va, hE) xaspect)) − γE(cid:3)/E xaspect + log p(y|(P, xaspect (3) va·hE where log cos(va, hE) = log ||va|| ||hE || is a cosine delay reward. Besides, it’s worthwhile to mention xaspect that, we regard the loss log p(y|(P, xaspect xaspect) pre- sented in Eq.(10) from the classiﬁcation phase as another loss delay reward. This loss reward com- bining with the above cosine reward could pro- vide adequate supervision signals to guide RAWS to select aspect-relevant and also discriminative words (e.g., sentiment words “slow” and “ob- tuse” for aspect “operating speed”) for performing ASC-QA. γE(cid:3)/E is an additional term for limiting the number of selected words. E(cid:3) = i=1 oi de- notes the number of selected words. γ is a penalty weight (tuned to be 0.01 with development set). (cid:2)E 3.2 Reinforced Bidirectional Attention Network (RBAN) Figure 3 shows the overall framework of our pro- posed reinforced bidirectional attention network (RBAN) approach to ASC-QA, which consists of three parts: 1) Word Encoder. 2) Reinforced Bidi- rectional Attention. 3) Softmax Decoder. Word Encoder. Given a QA text pair P with xaspect, let xq = {xq i }, ∀i ∈ [1, Eq] xaspect an aspect xaspect denotes the word sequence in question text, and xa = {xa j }, ∀j ∈ [1, Ea] denotes the word se- quence in answer text. To alleviate the effects of π and pa xaspect noisy words for aspect xaspect xaspect in both the question and answer text, we make use of two RAWS mod- ules (as introduced in Section 3.1) to perform word selection over question xq and answer xa respec- tively. More speciﬁcally, we employ two LSTMp to construct policy networks pq π for sam- pling action oq over question xq and sampling ac- tion oa over answer xa. Here, the two LSTMp are denoted as LSTMq p respectively. Therefore, according to Eq.(1), the hidden states j ∈ Rd of words xq hq i , ha j are computed as: i ⊕ (oq hq i = LSTMq j = LSTMa ha j ⊕ (oa where vq i , va sented in Section 4.1) of the word xq j ∈ Rd are word embeddings (pre- i and xa i = vq j = va i ), ˆvq j ), ˆva p and LSTMa i ⊗ e) j ⊗ e) p(ˆvq p(ˆva i and xa j . (4) i , ..] and oa = [.., oa Reinforced Bidirectional Attention. Once their actions two RAWS modules ﬁnish all oq = [.., oq j , ..] over question xq and answer xa, we employ a positional mask ma- trix M ∈ REq×Ea to calculate the matching ma- trix S ∈ REq×Ea between question and answer as: (cid:3) oq i = oa 0 −∞ otherwise Mij = j = 1 (5) Sij = w(cid:4) tanh(W1hq i + W2ha j + b) + Mij (6) where Sij denotes the similarity between the i-th question word and the j-th answer word; Mij = −∞ leads to Sij = −∞ indicating that the i-th question word or the j-th answer word has been xaspect xaspect and thus dis- regarded as the noisy word for xaspect carded by RAWS; W1, W2 ∈ Rd×d, w, b ∈ Rd are the trainable parameters. In order to mine semantic matching informa- tion between question and answer, we employ S to compute attentions in both directions, which could be seen as a Question-to-Answer attention and an Answer-to-Question attention. Speciﬁcally, we ﬁrst employ the row/column-wise softmax oper- ation to get two normalized matrices Sr and Sc. i: = softmax([Si1, .., SiEa]), ∀i ∈ [1, Eq] Sr Sc :j = softmax([S1j, .., SEqj]), ∀j ∈ [1, Ea] where Sij = −∞ leads to Sr ij = 0 when the softmax operation is applied. This switches off the attentions between word xq j so as to ﬁl- ter the noisy word information and only mine the xaspect xaspect. matching information relevant to aspect xaspect i and xa Second, since each word xq i in question inter- acts all words in answer xa and vice versa, its im- portance can be measured as the summation of the ij, Sc (7) strengths of all these interactions, i.e., matching scores computed in Eq.(7). Therefore, we perform row/column-wise summation operation over the normalized matching matrices, i.e., ˆαa = i Sr i: j , ..] ∈ REa j Sc and ˆαq = and ˆαq = [.., ˆαq are matching score vectors. Finally, the bidirectional attention is com- puted as follows: :j, where ˆαa = [.., ˆαa i , ..] ∈ REq (cid:2) (cid:2) • Question-to-Answer Attention (Q2A). We ﬁrst perform softmax operation over ˆαa to com- pute the attention weight αa j of word xa j in an- exp( ˆαa j ) swer text as αa t ) . Then, the vec- t=1 exp( ˆαa tor sa ∈ Rd of the answer text is computed as a weighted sum of hidden state ha j based on the at- (cid:2)Ea j ha j=1 αa tention weight αa j . j , i.e., sa = j = (cid:2)Ea • Answer-to-Question Attention (A2Q). Sim- ilar to question-to-answer attention, the question vector sq ∈ Rd is computed based on attention i hq weight αq i . Subsequently, we concatenate the answer vector sa and question vector sq so as to obtain the vector representation r ∈ R2d of the QA text pair P, i.e., r = sa ⊕ sq. t ) , i.e., sq = exp( ˆαq i ) t=1 exp( ˆαq i=1 αq (cid:2)Eq i = (cid:2)Eq Softmax Decoder. To perform ASC-QA, we feed the vector r to a softmax classiﬁer, i.e., β = W r + b, where β ∈ RC is the output vector. Then, the probability of labeling sentence with sentiment polarity l ∈ [1, C] is computed by pθ = exp(βl) (cid:2)C c=1 exp(βc) . Finally, the label with the highest probability stands for the predicted sentiment po- xaspect xaspect. larity towards aspect xaspect 3.3 Optimization via Policy Gradient and Back-Propagation For θq r and θa r for policy networks pq The parameters in RBAN are divided into two groups: 1) θq π in two fundamental RAWS modules. 2) θ for the rest parts including word embeddings, LSTM, bidirec- tional attention and softmax decoder. π, pa r, we optimize it with policy gradient algo- rithm (Sutton et al., 1999). In detail, we ﬁrst obtain an aspect-relevant reward Rq according to Eq.(3) after pq π ﬁnishes all actions. Then, the policy gra- dient w.r.t. θq r is computed by differentiating the maximized expected reward J(θq r) as follows: ∇θq r J(θq r) = Eoq∼pq π [ Eq(cid:4) i=1 Rq∇θq r log pq π(oq i |sq i )] (8) 3552J(θq r where ∇θq r) is estimated by using Monte- Carlo simulation (Sutton et al., 1999) to sample some action sequences over question texts. Simi- larly, the policy gradient w.r.t. θa r is computed as: ∇θa r J(θa r ) = Eoa∼pa π [ Ea(cid:4) j=1 Ra∇θa r log pa π(oa j |sq j )] (9) For θ, we optimize it with back-propagation. In detail, the objective of learning θ is to minimize the cross-entropy loss function in the classiﬁcation phase as follows: xaspect xaspect))] xaspect,y)∼D[− log p(y|(P, xaspect J(θ) = E(P,xaspect xaspect (10) xaspect xaspect, y) denotes QA text pair P with where (P, xaspect xaspect xaspect from dataset D; y is ground- given aspect xaspect xaspect xaspect. truth sentiment polarity towards aspect xaspect r and θq Note that, during model training, θq r are not updated in early stage, and thus two RAWS modules select all words in question and answer. When θ is optimized until the loss over develop- ment set does not decrease signiﬁcantly, we then begin to optimize θ, θq r and θa r simultaneously. 4 Experimentation We systematically evaluate the performance of our proposed RBAN approach to ASC-QA on the cor- pus as described in Section 2. 4.1 Experimental Settings Data Settings. As introduced in Section 2, we have annotated QA text pairs from three different domains listed in Table 2. For each domain, we randomly split the annotated data into training, de- velopment, and testing sets with the ratio of 8:1:1. Word Embedding. We ﬁrst adopt FudanNLP (Qiu et al., 2013) to perform word segmentation over our collected 150k Chinese QA text pairs. Then, we employ these QA text pairs to pre-train 200-dimension word vectors with skip-gram6. Hyper-parameters. In all our experiments, word embeddings are optimized during training. The dimensions of LSTM hidden states are set to be 200. The other hyper-parameters are tuned ac- cording to the development set. Speciﬁcally, we adopt Adam optimizer (Kingma and Ba, 2014) with an initial learning rate of 0.01 for cross- entropy training and adopt the SGD optimizer with 6 https://github.com/dav/word2vec a learning rate of 0.002 for all policy gradients training. Regularization weight of parameters is 10−5, dropout rate is 0.25 and batch size is 32. Evaluation Metrics. The performance is eval- uated using Accuracy (Acc.) and Macro-F1 (F1) (Wang et al., 2018a). Moreover, t-test is used to evaluate the signiﬁcance (Yang and Liu, 1999). Task Deﬁnition. Our proposed ASC-QA con- sists of two sub-tasks: 1) Term-level ASC-QA. Given a set of pre-identiﬁed aspect terms, this sub- task is to determine the polarity towards each as- pect term inside a QA text pair. 2) Category-level ASC-QA. Given a set of pre-identiﬁed aspect cat- egories, this sub-task is to determine the polarity towards each aspect category discussed in a QA text pair. 4.2 Baselines For comparison, we implement several state-of- the-art approaches to ASC as baselines. Since the input of all these approaches should be a single se- quence, we concatenate question and answer text to generate a single sequence. Besides, we em- ploy some QA matching approaches to ASC-QA and implement several basic versions of RBAN as baselines. Note that, for fair comparison, all the above baselines adopt the same pre-trained word embeddings as RBAN. The baselines are listed as follows in detail: 1) LSTM (Wang et al., 2016). This approach only adopts a standard LSTM network to model the text without considering aspect information. 2) RAM (Chen et al., 2017). This is a state-of-the- art deep memory network approach to ASC. 3) GCAE (Xue and Li, 2018). This is a state-of- the-art approach to ASC which combines CNN and gating mechanisms to learn text representa- tion. 4) S-LSTM (Wang and Lu, 2018). This is a state-of-the-art approach to ASC which considers structural dependencies between targets and opin- ion terms. 5) BIDAF (Seo et al., 2016). This is a QA matching approach to reading comprehen- sion. We substitute its decoding layer with soft- max decoder to perform ASC-QA. 6) HMN (Shen et al., 2018a). This is a QA matching approach to coarse-grained sentiment classiﬁcation towards QA style reviews. 7) MAMC (Yin et al., 2017). This is a QA matching approach to ASC which proposes a hierarchical iterative attention to learn the aspect-speciﬁc text representation. 8) RBAN w/o RAWS. Our RBAN approach without using RAWS modules. 9) RBAN w/o Q2A. Our RBAN 3553Term-level ASC-QA Cosmetics Acc. F1 Category-level ASC-QA Cosmetics Acc. F1 F1 F1 Acc. Acc. Bags Bags Approaches Electronics Acc. F1 Electronics Acc. F1 0.571 0.757 0.582 0.771 0.534 0.756 0.528 0.773 0.493 0.739 0.522 0.752 LSTM (Wang et al., 2016) RAM (Chen et al., 2017) 0.605 0.782 0.614 0.805 0.557 0.788 0.561 0.795 0.519 0.762 0.579 0.792 GCAE (Xue and Li, 2018) 0.617 0.779 0.623 0.819 0.570 0.781 0.590 0.787 0.514 0.791 0.576 0.788 S-LSTM (Wang and Lu, 2018) 0.615 0.824 0.623 0.821 0.569 0.794 0.587 0.828 0.522 0.788 0.581 0.801 BIDAF (Seo et al., 2016) 0.613 0.815 0.618 0.813 0.558 0.809 0.592 0.830 0.515 0.788 0.571 0.787 HMN (Shen et al., 2018a) 0.607 0.817 0.615 0.821 0.561 0.802 0.606 0.827 0.512 0.798 0.579 0.804 MAMC (Yin et al., 2017) 0.621 0.825 0.629 0.823 0.562 0.815 0.612 0.837 0.524 0.794 0.582 0.805 0.623 0.826 0.633 0.827 0.578 0.817 0.616 0.839 0.532 0.804 0.591 0.813 RBAN w/o RAWS RBAN w/o Q2A 0.595 0.788 0.614 0.817 0.569 0.779 0.578 0.814 0.514 0.788 0.569 0.782 RBAN w/o A2Q 0.623 0.837 0.639 0.834 0.588 0.821 0.617 0.845 0.536 0.815 0.603 0.826 RBAN 0.648 0.856 0.662 0.855 0.616 0.833 0.634 0.869 0.557 0.833 0.625 0.839 Table 3: Performances of all the approaches to two sub-tasks, i.e., Term-level and Category-level ASC-QA. In each sub-task, all approaches are evaluated in three different domains, i.e., Bags, Cosmetics and Electronics. approach without using question-to-answer atten- tion. 10) RBAN w/o A2Q. Our RBAN approach without using answer-to-question attention. 4.3 Experimental Results Table 3 shows the performances of different ap- proaches to ASC-QA. From this table, we can see that all the three state-of-the-art ASC approaches, i.e., RAM, GCAE and S-LSTM, perform bet- ter than LSTM. This conﬁrms the usefulness of considering aspect information in ASC. Besides, both the attention based approaches RAM and S- LSTM achieve comparable or better performance than GCAE. This result demonstrates the useful- ness of a proper attention mechanism to model as- pect information. The two QA matching approaches, i.e., BIDAF and HMN could achieve comparable performance with the three state-of-the-art ASC approaches, and MAMC even beats all of them. This indi- cates the appropriateness of treating question and answer in a QA style review as two parallel units instead of a single sequence in ASC-QA. Furthermore, our RBAN w/o RAWS approach (i.e., without considering aspect information) per- forms consistently better than MAMC. This en- courages to employ bidirectional attention to learn the representation vectors of both the question and answer in order to capture the sentiment informa- tion therein. Besides, it’s interesting to notice that RBAN w/o A2Q (i.e., without question vector sq) performs much better than RBAN w/o Q2A (i.e., without answer vector sa). This is due to the fact that the main sentiment polarity towards aspect is usually expressed in the answer text. In comparison, when using RAWS, RBAN per- forms best and signiﬁcantly outperforms RBAN w/o RAWS (p-value < 0.05), which encourages to discard noisy words for a speciﬁc aspect in both Impressively, in the question and answer sides. the sub-task of Term-level ASC-QA, compared to LSTM, RBAN achieves average improvements of 7.97% (F1) and 8.67% (Acc.) in three do- In the sub-task of Category-level ASC- mains. QA, compared to LSTM, RBAN achieves aver- age improvements of 9.1% (F1) and 9.23% (Acc.). Signiﬁcance test shows that these improvements are all signiﬁcant (p-value < 0.05). These results encourage to incorporate both RAWS and bidirec- tional attentions to tackle ASC-QA. 5 Analysis and Discussion Case Study. We provide a qualitative analysis of our approach on the development set. Speciﬁ- cally, in Figure 4, we visualize the attention matrix Sr in RBAN towards aspect “operating speed” in two cases, i.e., not using RAWS (Figure 4(a)) and using RAWS (Figure 4(b)). In Figure 4(a), color blue denotes attention weight (the darker the more important), we can ﬁnd that both aspect “bat- tery life” and aspect “operating speed” in question have been successfully matched with their corre- sponding answer phrases, i.e., “very durable” and “quite slow and obtuse”. However, RBAN with- out RAWS can’t discard noisy words (e.g., “bat- tery life”, “durable”) for aspect “operating speed”. In Figure 4(b), color white denotes the word inside question or answer has been discarded, we can ﬁnd that RBAN is capable of effectively discard- ing noisy words such as “battery” and “durable” and highlighting those signiﬁcant words such as “slow” and “obtuse” for aspect “operating speed”. 3554(a) RBAN without RAWS (b) RBAN with RAWS Figure 4: Attention matrices for a QA text pair (each row is a question word and each column is an answer word). (a) and (b) show attention matrices of RBAN without RAWS and RBAN towards aspect term “operating speed”. Error Analysis. We randomly analyze 100 er- ror cases in the experiments, which can be roughly categorized into 5 types. 1) 27% errors are be- cause that the answer length is too short. An ex- ample is “Question: Is the screen good? Answer: No.”. 2) 24% errors are due to negation words. An example is “the case is not good”. Our approach fails to select the word “not” and incorrectly pre- dicts positive polarity. This inspires us to optimize our approach so as to capture the negation scope better in the future. 3) 19% errors are due to the wrong prediction on recognizing neutral instances. The shortage of neutral training examples makes the prediction of neutral instances very difﬁcult. 4) 16% errors are due to comparative opinions. An example is “macos is much better than Windows”. Our approach incorrectly predicts positive for as- pect “Windows”. 5) Finally, 14% errors are due to mistakes during Chinese word segmentation. An example is “好难看(very ugly)”. It’s incorrectly segmented into “好(good)|难(hard)|看(look)” and predicted as positive. This encourages to improve the performance of word segmentation on infor- mal customer reviews. 6 Related Work Existing studies on Aspect Sentiment Classiﬁca- tion (ASC) could be divided into two groups ac- cording to the different level of text, i.e., sentence- level ASC and document-level ASC. Sentence-level ASC is typically regarded as a sentence-level text classiﬁcation which aims to in- corporate aspect information into a model. Re- cently, Wang et al. (2016); Ma et al. (2017) pro- pose an attention based LSTM to ASC by explor- ing the connection between an aspect and the con- tent of a sentence. Tang et al. (2016b), Chen et al. (2017) and Wang et al. (2018b) employ memory networks to model the context and aspect. Wang and Lu (2018) propose a segmentation attention to capture structural dependency between target and opinion terms. Document-level ASC aims to predict sentiment ratings for aspects inside a long text. Traditional studies (Titov and McDonald, 2008; Wang et al., 2010; Pontiki et al., 2016) solve document-level ASC as a sub-problem by utilizing heuristic based methods or topic models. Recently, Lei et al. (2016) focus on extracting rationales for aspects in a document. Li et al. (2018) propose an user- aware attention approach to document-level ASC. Yin et al. (2017) model document-level ASC as a machine comprehension problem, of which the in- put is also a parallel unit, i.e., question and answer. However, their question texts are pseudo and arti- ﬁcially constructed. This disaccords with the fact that real-world question texts also possibly involve multi-aspect and sentiment information. Unlike all the above studies, this paper performs ASC on a different type of text, i.e., QA style re- views. To the best of our knowledge, this is the ﬁrst attempt to perform ASC on QA style reviews. 7 Conclusion In this paper, we propose a new task, i.e., Aspect Sentiment Classiﬁcation towards Question An- swering (ASC-QA). Speciﬁcally, we ﬁrst build a high-quality human annotated benchmark corpus. Then, we design a reinforced bidirectional atten- tion network (RBAN) approach to address ASC- QA. Empirical studies show that our proposed ap- proach signiﬁcantly outperforms several state-of- the-art baselines in the task of ASC-QA. In our fu- ture work, we would like to solve other challenges in ASC-QA such as data imbalance and negation detection to improve the performance. Further- more, we would like to explore the effectiveness of our approach to ASC-QA in other languages. 3555Acknowledgments We thank our anonymous reviewers for their help- ful comments. This work was supported by three NSFC grants, i.e., No.61672366, No.61702149 and No.61525205. This work was also supported by the joint research project of Alibaba Group and Soochow University. References Peng Chen, Zhongqian Sun, Lidong Bing, and Wei Yang. 2017. Recurrent attention network on mem- ory for aspect sentiment analysis. In Proceedings of EMNLP-2017, pages 452–461. Li Dong, Furu Wei, Chuanqi Tan, Duyu Tang, Ming Zhou, and Ke Xu. 2014. Adaptive recursive neural network for target-dependent twitter sentiment clas- siﬁcation. In Proceedings of ACL-2014, pages 49– 54. Alex Graves. 2013. Generating sequences with recur- rent neural networks. CoRR, abs/1308.0850. Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and Tiejun Zhao. 2011. Target-dependent twitter senti- In Proceedings of ACL-2011, ment classiﬁcation. pages 151–160. Diederik P. Kingma and Jimmy Ba. 2014. Adam: CoRR, A method for stochastic optimization. abs/1412.6980. Tao Lei, Regina Barzilay, and Tommi S. Jaakkola. 2016. Rationalizing neural predictions. In Proceed- ings of EMNLP-2016, pages 107–117. Junjie Li, Haitong Yang, and Chengqing Zong. 2018. Document-level multi-aspect sentiment classiﬁca- tion by jointly modeling users, aspects, and over- all ratings. In Proceedings of COLING-2018, pages 925–936. Dehong Ma, Sujian Li, Xiaodong Zhang, and Houfeng Interactive attention networks for In Proceed- Wang. 2017. aspect-level sentiment classiﬁcation. ings of IJCAI-2017, pages 4068–4074. Margaret Mitchell, Jacqui Aguilar, Theresa Wilson, and Benjamin Van Durme. 2013. Open domain tar- In Proceedings of EMNLP-2013, geted sentiment. pages 1643–1654. Maria Pontiki, Dimitris Galanis, Haris Papageor- giou, Ion Androutsopoulos, Suresh Manandhar, Mo- hammad Al-Smadi, Mahmoud Al-Ayyoub, Yanyan Zhao, Bing Qin, Orph´ee De Clercq, V´eronique Hoste, Marianna Apidianaki, Xavier Tannier, Na- talia V. Loukachevitch, Evgeniy V. Kotelnikov, N´uria Bel, Salud Mar´ıa Jim´enez Zafra, and G¨ulsen Eryigit. 2016. Semeval-2016 task 5: Aspect based sentiment analysis. In Proceedings of NAACL-2016, pages 19–30. Maria Pontiki, Dimitris Galanis, Haris Papageorgiou, Suresh Manandhar, and Ion Androutsopoulos. 2015. Semeval-2015 task 12: Aspect based sentiment anal- ysis. In Proceedings of NAACL-2015, pages 486– 495. Maria Pontiki, Dimitris Galanis, John Pavlopoulos, Harris Papageorgiou, Ion Androutsopoulos, and Suresh Manandhar. 2014. Semeval-2014 task 4: As- pect based sentiment analysis. In Proceedings of COLING-2014, pages 27–35. Xipeng Qiu, Qi Zhang, and Xuanjing Huang. 2013. Fudannlp: A toolkit for chinese natural language processing. In Proceedings of ACL-2013, pages 49– 54. Min Joon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. 2016. Bidirectional at- tention ﬂow for machine comprehension. CoRR, abs/1611.01603. Chenlin Shen, Changlong Sun, Jingjing Wang, Yangyang Kang, Shoushan Li, Xiaozhong Liu, Luo Si, Min Zhang, and Guodong Zhou. 2018a. Senti- ment classiﬁcation towards question-answering with hierarchical matching network. In Proceedings of EMNLP-2018, pages 3654–3663. Tao Shen, Tianyi Zhou, Guodong Long, Jing Jiang, Sen Wang, and Chengqi Zhang. 2018b. Reinforced self- attention network: a hybrid of hard and soft attention for sequence modeling. In Proceedings of IJCAI- 2018, pages 4345–4352. Richard S. Sutton, David A. McAllester, Satinder P. Singh, and Yishay Mansour. 1999. Policy gradi- ent methods for reinforcement learning with func- tion approximation. In Proceedings of NIPS-1999, pages 1057–1063. Duyu Tang, Bing Qin, and Ting Liu. 2016b. Aspect level sentiment classiﬁcation with deep memory net- work. In Proceedings of EMNLP-2016, pages 214– 224. Ivan Titov and Ryan T. McDonald. 2008. A joint model of text and aspect ratings for sentiment sum- marization. In Proceedings of ACL-2008, pages 308–316. Duy-Tin Vo and Yue Zhang. 2015. Target-dependent twitter sentiment classiﬁcation with rich automatic features. In Proceedings of IJCAI-2015, pages 1347–1353. Bailin Wang and Wei Lu. 2018. Learning latent opin- ions for aspect-level sentiment classiﬁcation. In Proceedings of AAAI-2018, pages 5537–5544. Hongning Wang, Yue Lu, and Chengxiang Zhai. 2010. Latent aspect rating analysis on review text data: a rating regression approach. In Proceedings of SIGKDD-2010, pages 783–792. 3556Jingjing Wang, Jie Li, Shoushan Li, Yangyang Kang, Min Zhang, Luo Si, and Guodong Zhou. 2018a. As- pect sentiment classiﬁcation with both word-level and clause-level attention networks. In Proceedings of IJCAI-2018, pages 4439–4445. Shuai Wang, Sahisnu Mazumder, Bing Liu, Mianwei Zhou, and Yi Chang. 2018b. Target-sensitive mem- ory networks for aspect sentiment classiﬁcation. In Proceedings of ACL-2018, pages 957–967. Yequan Wang, Minlie Huang, Xiaoyan Zhu, and Li Zhao. 2016. Attention-based LSTM for aspect- level sentiment classiﬁcation. In Proceedings of EMNLP-2016, pages 606–615. Ronald J. Williams. 1992. Simple statistical gradient- following algorithms for connectionist reinforce- ment learning. Machine Learning, 8:229–256. Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron C. Courville, Ruslan Salakhutdinov, Richard S. Zemel, and Yoshua Bengio. 2015. Show, attend and tell: Neural image caption generation with visual attention. In Proceedings of ICML-2015, pages 2048–2057. Wei Xue and Tao Li. 2018. Aspect based sentiment analysis with gated convolutional networks. In Pro- ceedings of ACL-2018, pages 2514–2523. Yiming Yang and Xin Liu. 1999. A re-examination In Proceedings of of text categorization methods. SIGIR-1999, pages 42–49. Yichun Yin, Yangqiu Song, and Ming Zhang. 2017. Document-level multi-aspect sentiment classiﬁca- tion as machine comprehension. In Proceedings of EMNLP-2017, pages 2044–2054. 3557