        c e    ] V C .  c [   v           .         : v  X r  VATT : Transformers Multimodal Self-Supervised Learning Raw Video , Audio Text Hassan Akbari∗ Columbia University ha     @ columbia.edu Liangzhe Yuan Google lzyuan @ google.com Rui Qian∗ Cornell University rq   @ cornell.edu Wei-Hong Chuang Google whchuang @ google.com Shih-Fu Chang Columbia University sc    @ columbia.edu Yin Cui Google yincui @ google.com Boqing Gong Google bgong @ google.com Abstract present framework learning multimodal representations unlabeled data using convolution-free Transformer architectures . Speciﬁcally , Video- Audio-Text Transformer ( VATT ) takes raw signals inputs extracts multi- modal representations rich enough beneﬁt variety downstream tasks . train VATT end-to-end scratch using multimodal contrastive losses evaluate performance downstream tasks video action recognition , audio event classiﬁcation , image classiﬁcation , text-to-video retrieval . Furthermore , study modality-agnostic , single-backbone Transformer sharing weights among three modalities . show convolution-free VATT outperforms state-of-the-art ConvNet-based architectures downstream tasks . Especially , VATT ’ vision Transformer achieves top-  accuracy   .  % Kinetics-    ,   .  % Kinetics-    ,   .  % Kinetics-    ,   .  % Moments Time , new records avoiding supervised pre-training . Transferring image classiﬁ- cation leads   .  % top-  accuracy ImageNet compared   .  % training Transformer scratch , showing generalizability model despite domain gap videos images . VATT ’ audio Transformer also sets new record waveform-based audio event recognition achieving mAP   .  % AudioSet without supervised pre-training . VATT ’ source code publicly available.    Introduction Convolutional neural networks ( CNNs ) [    ,    ] triumphed various computer vision tasks . inductive bias induced convolutions , namely translation invariance locality , proven effective visual data . meantime , however , witness natural language processing ( NLP ) community paradigm shift models strong inductive biases , recurrent neural networks [    ,   ] CNNs [     ,    ] , general architectures constructed upon self- attention . Particularly , Transformers [    ] become de facto model architecture NLP ∗Work done internship Google .  https : //github.com/google-research/google-research/tree/master/vatt   th Conference Neural Information Processing Systems ( NeurIPS      ) , Sydney , Australia .       Figure   : Overview VATT architecture self-supervised , multimodal learning strategy . VATT linearly projects modality feature vector feeds Transformer encoder . deﬁne semantically hierarchical common space account granularity different modalities employ Noise Contrastive Estimation ( NCE ) train model . tasks [    ,    ,    ,    ] . Pre-training Transformer large text corpora followed ﬁne-tuning gives rise state-of-the-art results different downstream tasks . view success attention mechanism NLP , rich line works exploring potential computer vision . Early work studied hybrid models consisting convolutions attention modules [    ,    ,    ,     ] . Recent studies showed convolution-free , specially designed all-attention models match CNNs ’ performance image recognition tasks [     ,    ,    ] . recently , [    ] achieved impressive performance several image recognition tasks , including ImageNet [    ] , using pre-trained Transformer minimal architecture changes . work delivered compelling message “ large scale ( supervised ) training trumps inductive bias ( image classiﬁcation ) . ” conclusion extended video recognition tasks [   ,   ] . However , large-scale supervised training Transformers essentially troubling two main reasons . First , rules much larger part “ big visual data , ” i.e , vast amount unlabeled , unstructured visual data . result , supervised training strategy could produce biased systems require even labeled data correct biases . Second , strategy fundamentally limits application scope Transformers computer vision costly extremely time-consuming collect enough labeled images videos training millions parameters , choosing hyper-parameters , validating expected generalization . Hence , work poses another pressing question Transformers take raw signals input . empower large-scale , unlabeled visual data ? answer question , draw insights NLP . BERT [    ] GPT [    ,    ,    ] use masked language modeling pre-training tasks . Natural languages organic supervision Transformers . sequentially place words , phrases , sentences context , granting semantics syntax . visual data , organic supervision arguably multimodal videos . abundantly available digital world , temporal , cross-modality regulation , therefore supervision , requires human annotation . extreme scale multimodal videos potentially capable teach Transformers necessary priors , opposed predeﬁned inductive biases , model visual world . end , study self-supervised , multimodal pre-training three Transformers [    ] , take input raw RGB frames internet videos , audio waveforms , text transcripts speech audio , respectively . call video , audio , text Transformers VATT . Figure   illustrates architecture . VATT borrows exact architecture BERT [    ] ViT [    ] except layer tokenization linear projection reserved modality separately . design shares spirit ViT make minimal changes architecture learned model transfer weights various frameworks tasks . Furthermore , self-supervised , multimodal learning strategy resonates spirit BERT GPT pre-training requires minimal human curated labels . evaluate pre-trained Transformers variety downstream tasks : image classiﬁcation , video action recognition , audio event classiﬁcation , zero-shot text-to-video retrieval . Fine-tuning   NCEloss “ Sled dogs running snow pulling sled. ” Input VideoInput Audio WaveformInput TextLinear Projection (  D RGB voxels ) Linear Projection (  D waveform ) Linear Projection (  -hot word vectors ) Transformer EncoderModality-Specific Modality-Agnostic···Extra Learnable [ AGG ] EmbeddingModality-Specific Patch + Position EmbeddingMultimodal Projection Head+Multi-Head AttentionNorm+NormMLPEmbeddingL ✕Transformer EncoderVATTMultimodalProjection Headvideo featureaudio featuretext featureMIL-NCEloss vision-modality Transformer ImageNet [    ] obtains top-  accuracy   .  % , comparable   .  % achieved ViT . result especially appealing considering domain gap videos images , ViT pre-trained using large-scale , human-curated image dataset . Furthermore , set new records Kinetics-    [    ] , Kinetics-    [    ] , Moments Time [    ] , AudioSet [    ] without supervised pre-training . VATT results , along others reported NLP tasks [    ,    ] , image recognition [    ] , semantic segmentation [     ] , point cloud classiﬁcation [     ] , action recoginition [   ] , demonstrate Transformer versatile general-purpose architecture different types data . move one step forward , challenge Transformers VATT seemingly strong constraint : sharing weights among video , audio , text modalities . idea test whether exists single , general-purpose model modalities — course , still layers tokenization linear projection . Preliminary results encouraging . modality-agnostic Transformer par three modality-speciﬁc ones slightly smaller sizes . Finally , another contribution work DropToken , simple yet effective technique reduce training complexity minor reduction end Transformers ’ performance . DropToken randomly drops portion video audio tokens input sequence training , al- lowing high-resolution inputs leveraging abundance . signiﬁcant Transformers computational complexity quadratic respect number input tokens .   Related work  .  Transformers Vision Transformer originally built NLP tasks [    ] design multi-head attention shows effectiveness modeling long-term correlation words . attempts made use Transformer vision tasks like image super-resolution [    ] , object detection [    ] multimodal video understanding [    ,    ,    ] . However methods still rely feature extracted CNNs . Recently , [    ] proposes set convolution-free vision Transformers directly work raw images obtain competitive performance CNNs . [    ] improves training data efﬁciency [    ] using stronger data augmentations knowledge distillation . Since , pure Transformer design adopted various vision tasks including semantic segmentation [     ] , point cloud classiﬁcation [     ] , action recoginition [   ,    ,   ] . best knowledge , VATT ﬁrst Transformer model raw multimodal inputs video , audio text .  .  Self-Supervised Learning Single vision modality . Early work self-supervised visual representation learning usually learns unlabeled images via manually speciﬁed pretext tasks , like auto-encoding [    ,     ,     ] , patch location prediction [    ] , solving jigsaw puzzles [    ] , image rotation prediction [    ] . [    ] propose novel instance discrimination objective . recent trend contrastive learning [    ,    ,     ,    ,    ,    ] integrates data augmentations instance discrimination maintaining relative consistency representations image augmented view . Clustering also provide effective addition [    ] . Recently , [    ] conduct contrastive learning using ViT [    ] achieve impressive results . video domain , natural exploit temporal signals pretext task . Examples include predicting future frame [    ] , motion appearance statistics [    ] , speed [   ,    ] encodings [    ,    ,    ] , sorting frames video clips [    ,    ,    ,    ] . Recently , [    ] apply contrastive learning videos temporal sampling strategy temporally consistent spatial augmentation . Multimodal video . Video natural source multimodal data . Multimodal self-supervised learning achieved predicting whether video correspondence audio stream [   ,   ,    ,    ] , cross-modality clustering [   ] , evolving losses [    ] . Recently , [   ] use contrastive loss learn video , audio text ; [    ] learn predict broad view spans longer temporal context narrow view . VATT serves ﬁrst work combining strength convolution-free Transformer multimodal contrastive learning .     Approach section , introduce convolution-free VATT architecture elaborate self- supervised multimodal objectives training VATT scratch . Figure   overview architecture . feed modality tokenization layer , raw input projected embedding vector followed Transformer . two major settings :   ) backbone Transformers separate speciﬁc weights modality ,   ) Transformers share weights , namely , single backbone Transformer applied modalities . either setting , backbone extracts modality-speciﬁc representations , mapped common spaces compared contrastive losses . describe module following .  .  Tokenization Positional Encoding VATT operates raw signals . vision-modality input consists  -channel RGB pixels video frames , audio input form air density amplitudes ( waveforms ) , text input sequence words . ﬁrst deﬁne modality-speciﬁc tokenization layer takes input raw signals returns sequence vectors fed Transformers . Besides , modality positional encoding , injects order tokens Transformers [    ] . partition entire video clip size × H × W sequence ( cid:    ) /t ( cid:    ) · ( cid:    ) H/h ( cid:    ) · ( cid:    ) W/w ( cid:    ) patches , patch contains × h × w ×   voxels . apply linear projection entire voxels patch get d-dimensional vector representation . projection performed learnable weight Wvp ∈ Rt·h·w· ×d . seen  D extension patching mechanism proposed [    ] . encode position patches , deﬁne dimension-speciﬁc sequence learnable embeddings follows : ei , j , k = eTemporali+eHorizontalj + eVerticalk , ETemporal ∈ R ( cid:    ) /t ( cid:    ) ×d , EHorizontal ∈ R ( cid:    ) H/h ( cid:    ) ×d , EVertical ∈ R ( cid:    ) W/w ( cid:    ) ×d (   ) ei i-th row E. scheme allows us use ( cid:    ) /t ( cid:    ) + ( cid:    ) H/h ( cid:    ) + ( cid:    ) W/w ( cid:    ) positional embeddings encode ( cid:    ) /t ( cid:    ) · ( cid:    ) H/h ( cid:    ) · ( cid:    ) W/w ( cid:    ) patches video clip . raw audio waveform  D input length ( cid:   ) , partition ( cid:    ) ( cid:   ) /t ( cid:   ) ( cid:    ) segments containing ( cid:   ) waveform amplitudes . Similar video , apply linear projection learnable weight Wap ∈ Rt ( cid:   ) ×d elements patch get d-dimensional vector representation . use ( cid:    ) ( cid:   ) /t ( cid:   ) ( cid:    ) learnable embeddings encode position waveform segment . text , ﬁrst construct vocabulary size v words training dataset . input text sequence , map word v-dimensional one-hot vector followed linear projection learnable weight Wtp ∈ Rv×d . equivalent embedding dictionary lookup , widely used natural language understanding [    ] .  . .  DropToken introduce DropToken , simple yet effective strategy reduce computational complexity training . get token sequence video audio modality , randomly sample portion tokens feed sampled sequence , complete set tokens , Transformer . crucial reducing computational cost Transformer ’ computation complexity quadratic , ( N   ) , N number tokens input sequence . effort reducing input length would reduce number FLOPs quadratically . immediate impact wall clock time training models makes possible host large models limited hardware . argue instead reducing resolution dimension raw inputs , better keep high-ﬁdelity input randomly sample tokens via DropToken . DropToken appealing especially raw video audio inputs , may contain high redundancies .  .  Transformer Architecture simplicity , adopt established Transformer architecture [    ] , widely used NLP . Similar ViT [    ] , tweak architecture weights easily transferred standard Transformer implementation . brieﬂy elaborate pipeline ( also illustrated Figure   middle panel ) refer reader [    ,    ] details   standard Transformer architecture . sequence input tokens Transformer follows formulation : zin = [ xAGG ; x WP ; x WP ; . . . ; xN WP ] + ePOS (   ) xn input patches sequence xAGG learnable embedding special aggregation token whose corresponding output Transformer ( z  ) used aggregated representation entire input sequence . later used classiﬁcation common space mapping . use standard self-attention [    ] Multi-Head-Attention ( MHA ) module , GeLU [    ] activation MLP layer . also use Layer Normalization [   ] MHA MLP modules . text model , remove position encoding ePOS add learnable relative bias attention score ﬁrst layer MHA module . simple change makes text model ’ weights directly transferable state-of-the-art text model T  [    ] .  .  Common Space Projection use common space projection contrastive learning common space train networks . speciﬁcally , given video-audio-text triplet , deﬁne semantically hierarchical common space mapping enables us directly compare video-audio pairs well video-text pairs cosine similarity . argued [   ] , comparison feasible assume different levels semantic granularity modalities . achieve , deﬁne multi-level projections follows : zv , va = gv→va ( zvideo ) , zt , vt = gt→vt ( ztext ) , za , va = ga→va ( zaudio ) zv , vt = gv→vt ( zv , va ) (   ) gv→va ga→va projection heads respectively map video audio Trans- formers ’ outputs video-audio common space Sva . Moreover , gt→vt gv→vt project text Transformer ’ outputs video embedding Sva space video-text common space , Svt . multi-level common space projection depicted Figure   ( rightmost panel ) . main intuition behind hierarchy different modalities different levels semantic granularity , impose inductive bias common space projection . Similar [   ] , use linear projection ga→va ( . ) , gt→vt ( . ) , gv→vt ( . ) , two-layer projection ReLU gv→va ( . ) . ease training , batch normalization used linear layer .  .  Multimodal Contrastive Learning Inspired [   ,   ,    ] , use Noise Contrastive Estimation ( NCE ) align video-audio pairs Multiple Instance Learning NCE ( MIL-NCE ) align video-text pairs . pairs composed different temporal locations video-audio-text stream . Positive pairs two modalities constructed sampling corresponding streams location video , negative pairs constructed sampling non-matching locations video [   ] . Concretely , given common space speciﬁed Section   , loss objectives written follows : NCE ( zv , va , za , va ) = − log ( cid:   ) exp ( z ( cid:   ) v , vaza , va/τ ) + ( cid:   ) exp ( z ( cid:   ) v , vaza , va/τ ) z ( cid:   ) ∈N exp ( z ( cid:   ) ( cid:   ) v , vaz ( cid:   ) ( cid:   ) , , va/τ ) MIL-NCE ( zv , vt , { zt , vt } ) = − log ( cid:   ) ( cid:   ) zt , vt∈P exp ( z ( cid:   ) ( cid:   ) zt , vt∈P exp ( z ( cid:   ) v , vtzt , vt/τ ) + ( cid:   ) v , vtzt , vt/τ ) z ( cid:   ) ∈N exp ( z ( cid:   ) ( cid:   ) v , vtz ( cid:   ) , vt/τ ) (   ) N contains non-matching pairs batch . Equation   , P contains ﬁve text clips nearest neighbors video clip time . τ temperature adjust softness objectives distinguishing positive pairs negative pairs . overall per-sample objective training entire VATT model end-to-end follows : L = NCE ( zv , va , za , va ) + λMIL-NCE ( zv , vt , { zt , vt } ) , (   ) λ balances two losses . model optimized based back-propagation average loss calculated batch samples .   (   ) ( cid:   ) ,   Experiments section , ﬁrst brieﬂy describe experimental setup pre-training downstream evaluation , present results analytic interpretation VATT different tasks . refer reader Appendix detailed description experimental settings .  .  Experimental Setup Pre-train : use combination AudioSet [    ] HowTo   M [    ] datasets pre-train VATT— use subset HowTo   M dataset compliance Youtube ’ policies . Following [   ] , use video-audio-text triplets HowTo   M clips using video-audio pairs AudioSet . sample    frames    fps spatial size     ×     following random crop , horizontal ﬂip color augmentation ( details A. .  ) . Accordingly , sample audio waveforms sync   kHz . video audio normalized [ - ,  ] . use patch sizes   ×    ×        video raw waveform tokenization , respectively ( ablation A.  ) . use one-hot vectors encode text sequences ( capped    tokens ) vocabulary size     . pre-training experiments , use DropToken drop rate    % . train models using Adam optimizer [    ] quarter-period cosine scheduled learning rate  e-   e-    k warmup steps . Optimization performed totally    k steps batch size      (     exploration experiments ) . Following previously established practice [   ] projection common spaces Sva Svt , use dva =     dvt =     . also use temperature τ =  .   weight λ =   loss Equation   . use   network sizes experiments ( details A. .  ) . use Medium model (    M parameters ) modality-agnostic variant ( VATT-MA ) ,   variants modality-speciﬁc video-audio-text backbones : Base-Base-Small ( BBS ;    M ) , Medium-Base-Small ( MBS ;    M ) , Large-Base-Small ( LBS ;    M ) . Pre-training MBS VATT batch size          TPUs ( v  ) takes less   days . Pre-training batch size     takes less   day . Downstream : evaluate pre-trained VATT models   major downstream tasks using total    datasets . use UCF    [    ] , HMDB   [    ] , Kinetics-    [    ] , Kinetics-    [    ] , Moments Time [    ] video action recognition . use ESC   [    ] AudioSet [    ] audio event classiﬁcation , evaluate quality video-text common space representations zero-shot text-to-video retrieval YouCook  [     ] MSR-VTT [    ] . Finally , evaluate transferability vision backbone ﬁne-tuning ImageNet classiﬁcation [    ] . Since HMDB   , UCF    , ESC   small datasets compared size networks , use train linear classiﬁer top frozen pre-trained backbones . exploration experiments , report linear classiﬁcation accuracy zero-shot video retrieval metrics . refer Appendix detailed description datasets experimental setup .  .  Results  . .  Fine-tuning video action recognition ﬁne-tune VATT ’ vision Transformer Kinetics-    , Kinetics-    , Moments Time , three arguably established large-scale datasets video action recognition . use ﬁnal checkpoints four pre-train settings experiments : three modality-speciﬁc variations ( LBS , MBS , BBS ) , one modality-agnostic ( Medium ) . Table   shows results compared state-of-the-art video models . three datasets , achieve higher accuracy previous works including TimeSFormer [   ] , recent effort ﬁne-tuning ViT checkpoints obtained supervised pre-training . contrast , pre-training rely labels curated humans . best knowledge , VATT provides ﬁrst vision Transformer backbone pre-trained scratch using self-supervision multimodal videos achieves state-of-the-art results video action recognition . also worth mentioning ﬁne-tuning VATT recent Kinetics-    dataset results top-  accuracy   .  % , outperforms state-of-the-art top-  accuracy   .  % [    ] . quantify much multimodal self-supervised pre-training helps achieving numbers , train variant scratch without pre-training observe top-  top-  accuracies   .  %   .  % Kinetics-    , respectively . low accuracies verify efﬁcacy pre-training strategy VATT . Finally , ﬁnd VATT-MA-Medium , modality-agnostic   METHOD I D [    ] R (  +  ) [    ] bLVNet [    ] S D-G [    ] Oct-I D+NL [    ] D D [    ] I D+NL [    ] ip-CSN-    [    ] AttentionNAS [    ] AssembleNet-    [    ] MoViNet-A  [    ] LGD- D-    [    ] SlowFast-R   -NL [    ] X D-XL [    ] X D-XXL [    ] TimeSFormer-L [   ] VATT-Base VATT-Medium VATT-Large VATT-MA-Medium Kinetics-    Kinetics-    TOP-  TOP-  TOP-  TOP-  Moments Time TOP-  TOP-  TFLOPS   .    .    .    .    .    .    .    .  - -   .    .    .    .    .    .    .    .    .    .    .    .    .    .  - -   .    .  - - -   .    .    .    .    .    .    .    .    .    .  - - -   .    .  - -   .  -   .    .    .    .  -   .    .    .    .    .    .  - - - - - - -   .  - -   .    .    .  -   .    .    .    .    .    .  -   .  - - - - -   .    .    .  - - - - -   .    .    .    .    .  -   .  - - - - -   .    .  - - - - - -   .    .    .    .  -   .   .   -  .   -   .   .   .  -  .   -  .   .   .   .    .     .     .     .   Table   : Video action recognition accuracy Kinetics-    , Kinetics-    , Moments Time . backbone shared video , audio , text modalities , par modality-speciﬁc VATT- Base ﬁne-tuned video action recognition . result encouraging indicates potential unifying three data modalities single Transformer backbone .  . .  Fine-tuning audio event classiﬁcation ﬁne-tune VATT ’ audio Transformer AudioSet , benchmarks task multi-label audio event classiﬁcation . use ﬁnal checkpoints two pre-train settings : one modality-speciﬁc ( BBS ) , one modality-agnostic ( Medium ) . Table   shows results compared state-of-the-art models . Following common practice [    ,    ] , report mean Average Precision ( mAP ) , Area Curve ( AUC ) , d-prime ( based AUC ) [    ] . audio Transformer consistently outperforms existing CNN-based models metrics . interestingly , ﬁne-tuning modality-agnostic backbone ( VATT-MA-Medium ) par ﬁne-tuning modality-speciﬁc one ( VATT-Base ) . best knowledge , VATT ﬁrst Transformer outperforms CNN-based models audio event recognition . VATT operates raw waveforms utilize handcrafted features .  . .  Fine-tuning image classiﬁcation section , show pipeline capable transferring learned knowledge another domain performing image classiﬁcation task , even though models pre-trained multimodal video domain . ﬁne-tune vision Transformer VATT-BBS ImageNet without modiﬁcation backbone architecture . Instead , satisfy voxel-to-patch layer replicate input image   times feed network . network sees input single-frame video clip performs spatial self-attention . Table   shows results ﬁne-tuning vision Transformer end-to-end ImageNet . see pre-training leads signiﬁcant boost accuracy compared training scratch . also observe even though self-supervised pre-training happens video domain , still achieve competitive results supervised pre-training using large-scale image data [    ] .  . .  Zero-shot text-to-video retrieval feed video-text pairs VATT-MBS , extract representations Svt space . calculate similarity video-text pair YouCook  MSR-VTT . Given text query , rank videos based similarities text . measure recall   METHOD mAP AUC d-prime   .    .  DaiNet [    ]   .    .  LeeNet   [    ]   .    .  LeeNet   [    ]   .    .  Res dNet   [    ] Res dNet   [    ]   .    .  Wavegram-CNN [    ]   .    .   .     .     .     .     .     .    VATT-Base   .    .   .    VATT-MA-Medium   .    .   .    Table   : Finetuning results AudioSet event classiﬁcation . METHOD PRE-TRAINING DATA TOP-  TOP-  iGPT-L [    ] ViT-Base [    ] VATT-Base VATT-Base ImageNet JFT - HowTo   M   .    .    .    .  - -   .    .  Table   : Finetuning results ImageNet classiﬁcation . METHOD MIL-NCE [    ] MMV [   ] VATT-MBS VATT-MA-Medium      YouCook  MSR-VTT BATCH EPOCH R @    MedR R @    MedR                           .    .    .    .                .    .    .    .              Table   : Zero-shot text-to-video retrieval . correct video top-   videos . also measure median rank correct video . Table   compares video retrieval results two baselines . experiments observe zero-shot retrieval results heavily affected batch size number epochs , conﬁrming observation made [   ] . said , model still delivers comparable results MMV [   ] pre-trained half number epochs half batch size . also experiment larger batch size      longer pre-training   epochs , arriving exactly results MIL-NCE [    ] YouCook  R @      .  MedR    MSR-VTT . also notice , probably due noisy nature text transcripts , sophisticated language model like underrated . shown [   ] , using simple linear projection would still perform reasonably well . worth exploring , higher-quality text sources future work .  . .  Feature visualization take modality-speciﬁc modality-agnostic VATT ﬁne-tuned Kinetics-    visual- ize output feature representations using t-SNE . comparison , also include feature visualization vision Transformer trained scratch Kinetics-    . Figure   , observe ﬁne-tuned VATT yields much better separation model trained scratch . Furthermore , worth noting clear difference modality-agnostic features modality-speciﬁc ones . investigate VATT backbones without ﬁne-tuning . randomly choose  k video clips YouCook  dataset store representations two points pre-trained VATT model . One tokenization layer ( input space Transformer ) , common space projection ( output space ) , loss computed . Figure  -top visualizes representations , comparing modality-speciﬁc VATT modality-agnostic VATT . Interestingly , observe representations slightly mixed together modality-agnostic setting compared modality-speciﬁc ones , implying modality-agnostic backbone sees different modalities different symbols describing concept . analogous uniﬁed language model NLP supports multiple languages . see well VATT distinguishes positive video-text pairs randomly sampled pairs , calculate pair-wise similarities possible pairs perform Kernel Density Estimation ( KDE ) visualize distributions similarities positive pairs vs. negative pairs . perform procedure input output spaces modality-speciﬁc modality-agnostic backbones . Figure  -bottom shows KDE curves similarities . see VATT settings separates positive negative pairs output space . veriﬁes VATT ’ efﬁcacy learning semantic common space different modalities , even share backbone across modalities .  . .  Model Activations measure average activation modality-agnostic VATT full multimodal input fed model . speciﬁcally , sample    k short video clips test split HowTo   M along corresponding audio text feed model separately .   Figure   : t-SNE visualization feature representations extracted vision Transformer different training settings . better visualization , show     random classes Kinetics-    . Figure   : t-SNE visualization distribution pair-wise similarities input space vs. output space modality-speciﬁc modality-agnostic backbones different modalities fed . modality , calculate average activation node output MLP module , residual addition ( Figure  -Transformer Encoder ) . Figure   shows average activations across nodes Medium-size model . observe earlier nodes model activated text inputs , middle-to-later nodes activated video audio modalities . However , nodes last layers network activated modalities almost equally . might suggest model allocates different nodes certain modalities reaching level semantic perception modalities later layers . observation encourages studies possibility utilizing Mixture-of-Experts [    ,    ,    ] increase model ’ capacity simultaneous multimodal perception . leave direction research future work .  . .  Effect DropToken introduced new method reduce redundancy high-resolution data . study effect proposed DropToken method downstream applications pre-training computation , perform pre-training randomly dropping    % ,    % ,    % ,   % ( drop ) tokens video audio inputs . Table   shows accuracy linear classiﬁcation HMDB   , UCF    , ESC   R @    YouCook  MSR-VTT vs. drop rate along GFLOPs forward call . choose    % sampling rate large-scale pre-training offers good trade-off accuracy computational costs . take ﬁnal checkpoint pre-trained VATT    % DropToken rate perform ﬁne-tuning Kinetics-    different DropToken rates different spatial temporal resolutions see high-resolution inputs coupled DropToken compare low-resolution inputs tokens dropped ﬁne-tuning . Table   shows top-  accuracy Kinetics-    . argue using low-resolution inputs , common approach reduce computational cost training . Instead , suggest using high-resolution inputs DropToken , whose accuracy training cost comparable better low-resolution counterparts .   t-SNE-          t-SNE-        t-SNE-          From Scratcht-SNE-          t-SNE-        t-SNE-          Modality-Specifict-SNE-          t-SNE-        t-SNE-          Modality-Agnostic         t-SNE-            t-SNE- Input / M-SpecificModalitiesVideoText           t-SNE-            t-SNE- Output / M-SpecificModalitiesVideoText       t-SNE-            t-SNE- Input / M-AgnosticModalitiesVideoText           t-SNE-          t-SNE- Output / M-AgnosticModalitiesVideoText .  .  .  .  .  .  . Similarities .  .  .  .  . DensityPairsPositiveNegative .  .  .  .  .  . Similarities .  .  .  .  .  .  . DensityPairsPositiveNegative .  .  .  .  .  .  . Similarities .   .   .   .   .   .   .   .   .  DensityPairsPositiveNegative .  .  .  .  .  . Similarities .  .  .  .  .  .  . DensityPairsPositiveNegative Figure   : average node activation across Modality-Agnostic-Medium VATT feeding multimodal video-audio-text triplet model . DropToken Drop Rate    %    %    %   % Multimodal GFLOPs    .     .     .     .  HMDB   UCF    ESC   YouCookII MSR-VTT   .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .  Resolution/ FLOPs DropToken Drop Rate    %    %    %   %    ×     ×     Inference ( GFLOPs )    ×     ×     Inference ( GFLOPs ) - - - - - - - - - - - -   .     .    .      .     ×     ×       .  Inference ( GFLOPs )    .     .     .      .    .    .    .  Table   : Top-  accuracy linear classiﬁca- tion R @    video retrieval vs. drop rate vs. inference GFLOPs VATT-MBS . Table   : Top-  accuracy video action recogni- tion Kinetics    using high-resolution inputs coupled DropToken vs. low-resolution inputs .   Conclusion Discussion paper , present self-supervised multimodal representation learning framework based Transformers . study suggests Transformers effective learning semantic video/audio/text representations — even one model shared across modalities — multi- modal self-supervised pre-training promising reducing dependency large-scale labeled data . show DropToken signiﬁcantly reduce pre-training complexity video audio modalities minor impact models ’ generalization . report new records results video action recognition audio event classiﬁcation competitive performance image classiﬁcation video retrieval . results , still see limitations work . Firstly , videos organic audio speech , approach depends meaningful multimodal correspondences . Besides , text modality currently consists speech transcripts , noisy sometimes sparse . Potential negative Societal Impacts mainly concerned applications . models could biased one applies approach multimodal videos representative enough . Finally , method still demanding computation , though managed avoid need human labels . Future work improve upon limitations . Acknowledgments Disclosure Funding would like thank Min-Hsuan Tsai , Jean-Baptise Alayrac , Andrew Audibert , Yeqing Li , Vidush Mukund , TensorFlow team help codes , infrastructure , insightful discussions .    Early Activation TextLate Activation Video & AudioModality-Agnostic ActivationNode # Input ModalitiesVideoAudioTextVideoAudioTextModality                                              References [   ] Jean-Baptiste Alayrac , Adrià Recasens , Rosalia Schneider , Relja Arandjelovi´c , Jason Rama- puram , Jeffrey De Fauw , Lucas Smaira , Sander Dieleman , Andrew Zisserman . Self- supervised multimodal versatile networks . NeurIPS ,      .   ,   ,   ,   ,    ,    ,    ,    [   ] Humam Alwassel , Dhruv Mahajan , Bruno Korbar , Lorenzo Torresani , Bernard Ghanem , Du Tran . Self-supervised learning cross-modal audio-video clustering . arXiv preprint arXiv:    .      ,      .   ,    [   ] Relja Arandjelovic Andrew Zisserman . Look , listen learn . CVPR ,      .   ,   [   ] Relja Arandjelovic Andrew Zisserman . Objects sound . ECCV ,      .   [   ] Anurag Arnab , Mostafa Dehghani , Georg Heigold , Chen Sun , Mario Luˇci´c , Cordelia Schmid . Vivit : video vision transformer . arXiv preprint arXiv:    .      ,      .   ,   [   ] Jimmy Lei Ba , Jamie Ryan Kiros , Geoffrey E Hinton . Layer normalization . arXiv preprint arXiv:    .      ,      .   [   ] Dzmitry Bahdanau , Kyunghyun Cho , Yoshua Bengio . Neural machine translation jointly learning align translate . ICLR ,      .   [   ] Sagie Benaim , Ariel Ephrat , Oran Lang , Inbar Mosseri , William Freeman , Michael Rubin- stein , Michal Irani , Tali Dekel . Speednet : Learning speediness videos . CVPR ,      .   [   ] Gedas Bertasius , Heng Wang , Lorenzo Torresani . space-time attention need video understanding ? arXiv preprint arXiv:    .      ,      .   ,   ,   ,   [    ] Tom B Brown , Benjamin Mann , Nick Ryder , Melanie Subbiah , Jared Kaplan , Prafulla Dhari- wal , Arvind Neelakantan , Pranav Shyam , Girish Sastry , Amanda Askell , et al . Language models few-shot learners . arXiv preprint arXiv:    .      ,      .   ,   [    ] Nicolas Carion , Francisco Massa , Gabriel Synnaeve , Nicolas Usunier , Alexander Kirillov , Sergey Zagoruyko . End-to-end object detection transformers . ECCV ,      .   [    ] Mathilde Caron , Ishan Misra , Julien Mairal , Priya Goyal , Piotr Bojanowski , Armand Joulin . Unsupervised learning visual features contrasting cluster assignments . NeurIPS ,      .   [    ] Joao Carreira Andrew Zisserman . Quo vadis , action recognition ? new model kinetics dataset . CVPR ,      .   [    ] Joao Carreira Andrew Zisserman . Quo vadis , action recognition ? new model kinetics dataset . CVPR ,      .   ,   ,    [    ] Joao Carreira , Eric Noland , Andras Banki-Horvath , Chloe Hillier , Andrew Zisserman . short note kinetics-    . arXiv preprint arXiv:    .      ,      .   ,   ,    [    ] Mark Chen , Alec Radford , Rewon Child , Jeffrey Wu , Heewoo Jun , David Luan , Ilya Sutskever . Generative pretraining pixels . ICML ,      .   [    ] Ting Chen , Simon Kornblith , Mohammad Norouzi , Geoffrey Hinton . simple framework contrastive learning visual representations . ICML ,      .   [    ] Xinlei Chen , Saining Xie , Kaiming . empirical study training self-supervised visual transformers . arXiv preprint arXiv:    .      ,      .   [    ] Yen-Chun Chen , Linjie Li , Licheng Yu , Ahmed El Kholy , Faisal Ahmed , Zhe Gan , Yu Cheng , Jingjing Liu . Uniter : Universal image-text representation learning . ECCV ,      .   [    ] Yunpeng Chen , Haoqi Fan , Bing Xu , Zhicheng Yan , Yannis Kalantidis , Marcus Rohrbach , Shuicheng Yan , Jiashi Feng . Drop octave : Reducing spatial redundancy convolutional neural networks octave convolution . ICCV ,      .      [    ] Wei Dai , Chia Dai , Shuhui Qu , Juncheng Li , Samarjit Das . deep convolutional neural networks raw waveforms . ICASSP ,      .   [    ] Jia Deng , Wei Dong , Richard Socher , Li-Jia Li , Kai Li , Li Fei-Fei . Imagenet : large-scale hierarchical image database . CVPR ,      .   ,   ,   ,    [    ] Jacob Devlin , Ming-Wei Chang , Kenton Lee , Kristina Toutanova . Bert : Pre-training deep bidirectional transformers language understanding . NAACL ,      .   ,   ,   [    ] Carl Doersch , Abhinav Gupta , Alexei Efros . Unsupervised visual representation learning context prediction . ICCV ,      .   [    ] Alexey Dosovitskiy , Lucas Beyer , Alexander Kolesnikov , Dirk Weissenborn , Xiaohua Zhai , Thomas Unterthiner , Mostafa Dehghani , Matthias Minderer , Georg Heigold , Sylvain Gelly , Jakob Uszkoreit , Neil Houlsby . image worth   x   words : Transformers image recognition scale . ICLR ,      .   ,   ,   ,   ,   [    ] Heng Wang Du Tran , Lorenzo Torresani , Jamie Ray , Yann LeCun , Manohar Paluri . closer look spatiotemporal convolutions action recognition .      ieee . CVPR ,      .   [    ] Quanfu Fan , Chun-Fu ( Ricarhd ) Chen , Hilde Kuehne , Marco Pistoia , David Cox . Less : Learning Efﬁcient Video Representations Temporal Aggregation Modules . NeurIPS .      .   [    ] William Fedus , Barret Zoph , Noam Shazeer . Switch transformers : Scaling trillion parameter models simple efﬁcient sparsity . arXiv preprint arXiv:    .      ,      .   [    ] Christoph Feichtenhofer . X d : Expanding architectures efﬁcient video recognition . CVPR ,      .   [    ] Christoph Feichtenhofer , Haoqi Fan , Jitendra Malik , Kaiming . Slowfast networks video recognition . ICCV ,      .   ,    [    ] Basura Fernando , Hakan Bilen , Efstratios Gavves , Stephen Gould . Self-supervised video representation learning odd-one-out networks . CVPR ,      .   [    ] Jonas Gehring , Michael Auli , David Grangier , Denis Yarats , Yann N Dauphin . Convolu- tional sequence sequence learning . ICML ,      .   [    ] Jort F. Gemmeke , Daniel P. W. Ellis , Dylan Freedman , Jansen , Wade Lawrence , R. Chan- ning Moore , Manoj Plakal , Marvin Ritter . Audio set : ontology human-labeled dataset audio events . ICASSP ,      .   ,   ,    [    ] Jort F Gemmeke , Daniel PW Ellis , Dylan Freedman , Jansen , Wade Lawrence , R Channing Moore , Manoj Plakal , Marvin Ritter . Audio set : ontology human-labeled dataset audio events . ICASSP ,      .   [    ] Spyros Gidaris , Praveer Singh , Nikos Komodakis . Unsupervised representation learning predicting image rotations . ICLR ,      .   [    ] Rohit Girdhar Deva Ramanan . Attentional pooling action recognition . NeurIPS ,      .   [    ] Jean-Bastien Grill , Florian Strub , Florent Altché , Corentin Tallec , Pierre H Richemond , Elena Buchatskaya , Carl Doersch , Bernardo Avila Pires , Zhaohan Daniel Guo , Mohammad Ghesh- laghi Azar , et al . Bootstrap latent : new approach self-supervised learning . NeurIPS ,      .   [    ] Tengda Han , Weidi Xie , Andrew Zisserman . Video representation learning dense predictive coding . ICCV Workshops ,      .   [    ] Tengda Han , Weidi Xie , Andrew Zisserman . Memory-augmented dense predictive coding video representation learning . ECCV ,      .      [    ] Kaiming , Haoqi Fan , Yuxin Wu , Saining Xie , Ross Girshick . Momentum contrast unsupervised visual representation learning . CVPR ,      .   [    ] Olivier J Hénaff , Aravind Srinivas , Jeffrey De Fauw , Ali Razavi , Carl Doersch , SM Eslami , Aaron van den Oord . Data-efﬁcient image recognition contrastive predictive coding . arXiv preprint arXiv:    .      ,      .   [    ] Dan Hendrycks Kevin Gimpel . Gaussian error linear units ( gelus ) . arXiv preprint arXiv:    .      ,      .   [    ] Sepp Hochreiter Jürgen Schmidhuber . Long short-term memory . Neural computation ,      .   [    ] Han Hu , Zheng Zhang , Zhenda Xie , Stephen Lin . Local relation networks image recognition . ICCV ,      .   [    ] Dahun Kim , Donghyeon Cho , Kweon . Self-supervised video representation learning space-time cubic puzzles . AAAI ,      .   [    ] Diederik P Kingma Jimmy Ba . Adam : method stochastic optimization . arXiv preprint arXiv:    .     ,      .   ,    [    ] Dan Kondratyuk , Liangzhe Yuan , Yandong Li , Li Zhang , Mingxing Tan , Matthew Brown , Boqing Gong . Movinets : Mobile video networks efﬁcient video recognition . CVPR ,      .   ,   [    ] Qiuqiang Kong , Changsong Yu , Yong Xu , Turab Iqbal , Wenwu Wang , Mark Plumbley . Weakly labelled audioset tagging attention neural networks . TASLP ,      .   [    ] Qiuqiang Kong , Yin Cao , Turab Iqbal , Yuxuan Wang , Wenwu Wang , Mark Plumbley . Panns : Large-scale pretrained audio neural networks audio pattern recognition . TASLP ,      .   ,    [    ] Bruno Korbar , Du Tran , Lorenzo Torresani . Cooperative learning audio video models self-supervised synchronization . NeurIPS ,      .   ,    [    ] Alex Krizhevsky , Ilya Sutskever , Geoffrey E Hinton . Imagenet classiﬁcation deep convolutional neural networks . NeurIPS ,      .   [    ] H. Kuehne , H. Jhuang , E. Garrote , T. Poggio , T. Serre . HMDB : large video database human motion recognition . ICCV ,      .   ,    [    ] Yann LeCun , Léon Bottou , Yoshua Bengio , Patrick Haffner . Gradient-based learning applied document recognition . Proceedings IEEE ,      .   [    ] Hsin-Ying Lee , Jia-Bin Huang , Maneesh Singh , Ming-Hsuan Yang . Unsupervised repre- sentation learning sorting sequences . ICCV ,      .   [    ] Jongpil Lee , Jiyoung Park , Keunhyoung Luke Kim , Juhan Nam . Sample-level deep convolutional neural networks music auto-tagging using raw waveforms . arXiv preprint arXiv:    .      ,      .   [    ] William Lotter , Gabriel Kreiman , David Cox . Deep predictive coding networks video prediction unsupervised learning . arXiv preprint arXiv:    .      ,      .   [    ] Huaishao Luo , Lei Ji , Botian Shi , Haoyang Huang , Nan Duan , Tianrui Li , Xilin Chen , Ming Zhou . Univilm : uniﬁed video language pre-training model multimodal understanding generation . arXiv preprint arXiv:    .      ,      .   [    ] Antoine Miech , Dimitri Zhukov , Jean-Baptiste Alayrac , Makarand Tapaswi , Ivan Laptev , Josef Sivic . Howto   m : Learning text-video embedding watching hundred million narrated video clips . ICCV ,      .   ,       [    ] Antoine Miech , Jean-Baptiste Alayrac , Lucas Smaira , Ivan Laptev , Josef Sivic , Andrew Zisserman . End-to-end learning visual representations uncurated instructional videos . CVPR ,      .   ,   ,    ,    [    ] Tomas Mikolov , Kai Chen , Greg Corrado , Jeffrey Dean . Efﬁcient estimation word representations vector space . arXiv preprint arXiv:    .     ,      .   ,    [    ] Mathew Monfort , Alex Andonian , Bolei Zhou , Kandan Ramakrishnan , Sarah Adel Bargal , Tom Yan , Lisa Brown , Quanfu Fan , Dan Gutfreund , Carl Vondrick , et al . Moments time dataset : one million videos event understanding . TPAMI ,      .   ,   ,    [    ] Pedro Morgado , Nuno Vasconcelos , Ishan Misra . Audio-visual instance discrimination cross-modal agreement . arXiv preprint arXiv:    .      ,      .   [    ] Mehdi Noroozi Paolo Favaro . Unsupervised learning visual representations solving jigsaw puzzles . ECCV ,      .   [    ] Deepak Pathak , Philipp Krahenbuhl , Jeff Donahue , Trevor Darrell , Alexei Efros . Context encoders : Feature learning inpainting . CVPR ,      .   [    ] Mandela Patrick , Yuki Asano , Polina Kuznetsova , Ruth Fong , João F Henriques , Geoffrey Zweig , Andrea Vedaldi . Multi-modal self-supervision generalized data transforma- tions . arXiv preprint arXiv:    .      ,      .    [    ] Karol J. Piczak . ESC : Dataset Environmental Sound Classiﬁcation . ACM MM ,      .   ,    [    ] AJ Piergiovanni , Anelia Angelova , Michael Ryoo . Evolving losses unsupervised video representation learning . CVPR ,      .   ,    [    ] Rui Qian , Tianjian Meng , Boqing Gong , Ming-Hsuan Yang , Huisheng Wang , Serge Belongie , Yin Cui . Spatiotemporal contrastive video representation learning . CVPR ,      .   [    ] Zhaofan Qiu , Ting Yao , Chong-Wah Ngo , Xinmei Tian , Tao Mei . Learning spatio-temporal representation local global diffusion . CVPR ,      .   [    ] Alec Radford , Karthik Narasimhan , Tim Salimans , Ilya Sutskever . Improving language understanding generative pre-training .      .   [    ] Alec Radford , Jeffrey Wu , Rewon Child , David Luan , Dario Amodei , Ilya Sutskever . Language models unsupervised multitask learners . OpenAI blog ,      .   [    ] Colin Raffel , Noam Shazeer , Adam Roberts , Katherine Lee , Sharan Narang , Michael Matena , Yanqi Zhou , Wei Li , Peter J Liu . Exploring limits transfer learning uniﬁed text-to-text transformer . JMLR ,      .   [    ] Prajit Ramachandran , Niki Parmar , Ashish Vaswani , Irwan Bello , Anselm Levskaya , Jonathon Shlens . Stand-alone self-attention vision models . arXiv preprint arXiv:    .      ,      .   [    ] Adrià Recasens , Pauline Luc , Jean-Baptiste Alayrac , Luyu Wang , Florian Strub , Corentin Tallec , Mateusz Malinowski , Viorica Patraucean , Florent Altché , Michal Valko , et al . Broaden views self-supervised video learning . arXiv preprint arXiv:    .      ,      .   [    ] Steffen Rendle . Factorization machines . ICDM ,      .    [    ] Carlos Riquelme , Joan Puigcerver , Basil Mustafa , Maxim Neumann , Rodolphe Jenatton , André Susano Pinto , Daniel Keysers , Neil Houlsby . Scaling vision sparse mixture experts . arXiv preprint arXiv:    .      ,      .   [    ] Michael Ryoo , AJ Piergiovanni , Mingxing Tan , Anelia Angelova . Assemblenet : arXiv preprint Searching multi-stream neural connectivity video architectures . arXiv:    .      ,      .      [    ] Gilad Sharir , Asaf Noy , Lihi Zelnik-Manor . image worth   x   words , video worth ? arXiv preprint arXiv:    .      ,      .   [    ] Noam Shazeer , Azalia Mirhoseini , Krzysztof Maziarz , Andy Davis , Quoc Le , Geoffrey Hinton , Jeff Dean . Outrageously large neural networks : sparsely-gated mixture-of-experts layer . arXiv preprint arXiv:    .      ,      .   [    ] Laura Smith , Nikita Dhawan , Marvin Zhang , Pieter Abbeel , Sergey Levine . Avid : Learning multi-stage tasks via pixel-level translation human videos . arXiv preprint arXiv:    .      ,      .    [    ] Khurram Soomro , Amir Roshan Zamir , Mubarak Shah . Ucf    : dataset     human actions classes videos wild . arXiv preprint arXiv:    .     ,      .   ,    [    ] Nitish Srivastava , Elman Mansimov , Ruslan Salakhudinov . Unsupervised learning video representations using lstms . ICML ,      .   [    ] Jonathan Stroud , David Ross , Chen Sun , Jia Deng , Rahul Sukthankar . D d : Distilled  d networks video action recognition . WACV ,      .   [    ] Chen Sun , Fabien Baradel , Kevin Murphy , Cordelia Schmid . Learning video represen- tations using contrastive bidirectional transformer . arXiv preprint arXiv:    .      ,      .   [    ] Yonglong Tian , Dilip Krishnan , Phillip Isola . Contrastive multiview coding . ECCV ,      .   [    ] Hugo Touvron , Matthieu Cord , Matthijs Douze , Francisco Massa , Alexandre Sablayrolles , Hervé Jégou . Training data-efﬁcient image transformers & distillation attention . arXiv preprint arXiv:    .      ,      .   [    ] Du Tran , Heng Wang , Lorenzo Torresani , Matt Feiszli . Video classiﬁcation channel- separated convolutional networks . ICCV ,      .   [    ] Ashish Vaswani , Noam Shazeer , Niki Parmar , Jakob Uszkoreit , Llion Jones , Aidan N Gomez , Lukasz Kaiser , Illia Polosukhin . Attention need . arXiv preprint arXiv:    .      ,      .   ,   ,   ,   ,   [    ] Fei Wang , Mengqing Jiang , Chen Qian , Shuo Yang , Cheng Li , Honggang Zhang , Xiaogang Wang , Xiaoou Tang . Residual attention network image classiﬁcation . CVPR ,      .   [    ] Jiangliu Wang , Jianbo Jiao , Linchao Bao , Shengfeng , Yunhui Liu , Wei Liu . Self- supervised spatio-temporal representation learning videos predicting motion ap- pearance statistics . CVPR ,      .   [    ] Jiangliu Wang , Jianbo Jiao , Yun-Hui Liu . Self-supervised video representation learning pace prediction . ECCV ,      .   [    ] Xiaofang Wang , Xuehan Xiong , Maxim Neumann , AJ Piergiovanni , Michael Ryoo , Anelia Angelova , Kris Kitani , Wei Hua . Attentionnas : Spatiotemporal attention cell search video classiﬁcation . ECCV ,      .   [    ] Xiaolong Wang , Ross Girshick , Abhinav Gupta , Kaiming . Non-local neural networks . CVPR ,      .   [    ] Sanghyun Woo , Jongchan Park , Joon-Young Lee , Kweon . Cbam : Convolutional block attention module . ECCV ,      .   [    ] Zhirong Wu , Yuanjun Xiong , Stella X Yu , Dahua Lin . Unsupervised feature learning via non-parametric instance discrimination . CVPR ,      .   [    ] Saining Xie , Chen Sun , Jonathan Huang , Zhuowen Tu , Kevin Murphy . Rethinking spatiotemporal feature learning : Speed-accuracy trade-offs video classiﬁcation . ECCV ,      .      [    ] Dejing Xu , Jun Xiao , Zhou Zhao , Jian Shao , Di Xie , Yueting Zhuang . Self-supervised spatiotemporal learning via video clip order prediction . CVPR ,      .   [    ] Jun Xu , Tao Mei , Ting Yao , Yong Rui . Msr-vtt : large video description dataset bridging video language . CVPR ,      .   ,    [    ] Fuzhi Yang , Huan Yang , Jianlong Fu , Hongtao Lu , Baining Guo . Learning texture transformer network image super-resolution . CVPR ,      .   [     ] Mang Ye , Xu Zhang , Pong C Yuen , Shih-Fu Chang . Unsupervised embedding learning via invariant spreading instance feature . CVPR ,      .   [     ] Hongyi Zhang , Moustapha Cisse , Yann N Dauphin , David Lopez-Paz . mixup : Beyond empirical risk minimization . arXiv preprint arXiv:    .      ,      .    [     ] Richard Zhang , Phillip Isola , Alexei Efros . Colorful image colorization . ECCV ,      .   [     ] Richard Zhang , Phillip Isola , Alexei Efros . Split-brain autoencoders : Unsupervised learning cross-channel prediction . CVPR ,      .   [     ] Xiang Zhang , Junbo Zhao , Yann Lecun . Character-level convolutional networks text classiﬁcation . NeurIPS ,      .   [     ] Zhang , K Li , K Li , B Zhong , Fu . Residual non-local attention networks image restoration . ICLR ,      .   [     ] Hengshuang Zhao , Jiaya Jia , Vladlen Koltun . Exploring self-attention image recogni- tion . CVPR ,      .   [     ] Hengshuang Zhao , Li Jiang , Jiaya Jia , Philip Torr , Vladlen Koltun . Point transformer . arXiv preprint arXiv:    .      ,      .   [     ] Sixiao Zheng , Jiachen Lu , Hengshuang Zhao , Xiatian Zhu , Zekun Luo , Yabiao Wang , Yanwei Fu , Jianfeng Feng , Tao Xiang , Philip HS Torr , et al . Rethinking semantic segmentation sequence-to-sequence perspective transformers . arXiv preprint arXiv:    .      ,      .   [     ] Luowei Zhou , Chenliang Xu , Jason J Corso . Towards automatic learning procedures web instructional videos . AAAI ,      .   ,       Appendix Appendix contains detailed explanations datasets ( A.  ) experimental setup ( A.  ) pre-training downstream tasks . also cover linear evaluation results compared state-of-the-art ( A.  ) ablation study input parameters ( A.  ) . A.  Datasets A. .  Pre-training Following [   ,    ] , use HowTo   M [    ] AudioSet [    ] pre-train VATT . former contains  . M unique videos , providing multiple clips audio narration scripts resulting    M video-audio-text triplets total . narration scripts extracted speech audio using off-the-shelf ASR . use subset HowTo   M comply Youtube ’ policies , results almost  M unique videos less    M clips . AudioSet consists   -second clips sampled two million videos YouTube . dataset contains variety audio events corresponding video without narration , text input dataset . use labels datasets . uniformly sample clips datasets ; mini-batch pre-training contains samples datasets . order ﬁll empty text AudioSet , feed sequence zeros text Transformer exclude samples MIL-NCE loss . A. .  Downstream evaluate pre-trained VATT set diverse , representative downstream tasks test different aspects learned representations . Video action recognition : evaluate visual representations UCF    [    ] (     classes ,   ,    videos ) , HMDB   [    ] (    classes ,  ,    videos ) , Kinetics-    [    ] (     classes ,    ,    videos ) , Kinetics-    [    ] (     classes ,    ,    videos ) , Moments Time [    ] (     classes ,    ,    videos ) . Since UCF    HMDB   small datasets compared size model , freeze vision backbone use outputs train linear classiﬁer . use split #   results two datasets reference design exploration . Kinetics-    , Kinetics-    , Moments Time , ﬁne-tune vision backbone initialized pre-trained checkpoint . Audio event classiﬁcation : use ESC   [    ] (    classes ,      audio clips ) AudioSet [    ] (     classes , ∼ M audio clips ) evaluate audio Transformer audio event classiﬁcation . use ESC   train linear classiﬁer top frozen audio Transformer . use split #   results dataset reference design exploration . also use AudioSet ﬁne-tune audio backbone initialized pre-trained checkpoint . Zero-shot video retrieval : evaluate quality video-text common space represen- tations zero-shot text-to-video retrieval two established datasets area : YouCook  [     ] MSR-VTT [    ]  . k  k video-text pairs , respectively . follow evaluation pipeline described [   ] report Recall    ( R @    ) . Image classiﬁcation : Although exists domain gap images video datasets used pre-training VATT , test learned vision Transformer image domain . ﬁne- tune last checkpoint vision Transformer ImageNet [    ] modiﬁcation architecture tokenization pipeline . elaborate sequel . A.  Experimental Setup A. .  Inputs pre-training , sample    frames    fps pre-training datasets . frames , randomly crop temporally consistent spatial region whose relative area range [  .   ,   ] aspect ratio [  .  ,   ] . crops resized     ×     , followed horizontal ﬂip color augmentation . color augmentation follows [   ] randomizes brightness ( max delta =   /    ) , saturation ( max delta =  .  ) , contrast ( max delta= .  ) , hue ( max delta= .  ) .    clip values ensure RGB [   ,   ] . audio waveforms sampled sync video frames   kHz . video audio inputs normalized [ -  ,   ] numerical stability . use patch sizes   ×    ×        video raw waveform tokenization , respectively . use one-hot vectors encode text sequences vocabulary size     , word vec [    ] . resulting sequence retains maximum    words either clipping padding . use DropToken drop rate    % pre-training . video ﬁne-tuning evaluation ,    frames temporal stride   sampled    fps (  .   seconds ) crop size     ×     ( similar video augmentation pre-training ) , drop tokens . change input size audio text evaluation . A. .  Network setup VATT use Transformer architecture described main paper various sizes shown Table   . use Medium model modality-agnostic variant ( VATT-MA ) . experiments modality-speciﬁc Transformers , use Small Base models text audio modalities , respectively , varying model sizes video modality . results   variants modality-speciﬁc video-audio-text backbones : Base-Base-Small ( BBS ) , Medium-Base-Small ( MBS ) , Large-Base-Small ( LBS ) . Model Layers Hidden Size MLP Size Heads Params   Small Base    Medium       Large                                                    .    .     .     .  Table   : Details Transformer architectures VATT . A. .  Projection heads contrastive losses use dva =     dvt =     projection common spaces Sva Svt , respectively . normalize vectors calculating NCE MIL-NCE objectives use tempera- ture τ =  .   weight λ =   loss deﬁned paper . choose values following previously established practice [   ] ; may achieve better results varying hyper-parameters . A. .  Pre-training setup pre-train VATT scratch using Adam [    ] initial learning rate  e-  ,   k warmup steps ,    k steps total , batch size      , quarter-period cosine schedule anneal learning rate  e-   e-  . exploration experiments , use batch size     keeping rest training parameters . pipeline implemented Tensorﬂow ( v .  ) , models trained   days using     TPUs ( v  ) . A. .  Video ﬁne-tuning setup video action recognition , use SGD momentum  .  initial learning rate  .    ,  . k warmup steps , batch size    ,    k steps total , half-period cosine schedule anneal learning rate   . use label smoothing smoothing factor α =  .  . video frame resolution     ×     , results increase number positional encoding weights . increase due fact , pre-train time ,  +  +   positional encoding buckets ,  +  +   positional buckets required completely encode    /   horizontal    /   vertical locations ﬁne-tune . generate new positional embeddings , create new set positional encoding buckets bi-cubic interpolation original buckets . step , ﬁne-tune entire network , including positional encoding buckets , end-to-end . tried ﬁxed positional embeddings ( solely based interpolation missing locations ) observe signiﬁcant improvements . uniformly sample   clips cover entire    seconds video apply standard  -crop evaluation following [    ] . average logits across resulting    views ﬁnal class predictions .    A.  Audio ﬁne-tuning setup audio event classiﬁcation , use SGD momentum  .  , initial learning rate  .  ,  k warmup steps , batch size      ,   k steps total , half-period cosine schedule anneal learning rate   . observe increasing effective receptive ﬁeld improves overall performance . suggest might due fact AudioSet annotations multi-label event might occur different temporal positions . Hence , employ duration  . s   kHz sampling rate (    . k total input samples ) . Similar [    ] , use mixup [     ] input-label ( x-y ) pairs mini-batch : x = αx  + (   − α ) x  , = αy  + (   − α ) y  , input-label pairs randomly sampled mini-batch , mixing rate α sampled Beta (   ,   ) distribution . also perform data balancing penalizing loss value sample inverse per-batch number repetitive labels carries . crucial avoiding over-ﬁtting since AudioSet long-tailed distribution , dominant classes may disrupt training [    ] . A. .  Image ﬁne-tuning setup ﬁnetune pre-trained VATT ImageNet    epochs     ×     input resolution ,     batch size , SGD momentum  .  , cosine learning rate decay initial learning rate  e-  , label smoothing  .  . weight decay used . A. .  Linear evaluation setup use linear classiﬁer ﬁxed backbones across datasets tasks . observe using matrix factorization classiﬁer weight [    ] leads stable result across experiments . speciﬁcally , use factorized weight C = U V ∈ Rd×c , U ∈ Rd×n V ∈ Rn×c learnable weights . training classiﬁer , randomly choose subset n components U V , hence leading low-rank classiﬁer weight , C. classiﬁer weight , C , trained using Adam optimizer learning rate  e-  , batch size    , total   k training steps , sampling rate    % n =     components . A. .  Zero-shot retrieval setup zero-shot text-to-video retrieval , use  k split MSR-VTT entire test split YouCook  pool retrieval . use     ×     central crops    frames temporal stride   sampled    fps . Since input clip covers  .   seconds , full clip length    seconds , average embeddings   uniformly sampled clips calculating similarity text query ’ embedding . ( cid:   )  -normalize vector assure dot product results cosine similarity . A.  Linear evaluation frozen VATT also test VATT ’ ability generalize datasets entire backbone frozen . setting , focus video audio modalities train linear classiﬁer outputs frozen backbones . addition low-rank classiﬁer ( LRC ) described Section A.  , also report results SVM classiﬁer following pipeline [   ] . Table   shows performance model three datasets . observe VATT outperform best CNN counterparts [   ] , achieves comparable numbers baselines . could suggest VATT ’ backbones learn less-linearly-separable feature , especially given contrastive estimation head includes non-linear projections . A.  Ablation study input parameters Since VATT takes raw multimodal signals inputs , choice input size patched signiﬁcant impact ﬁnal performance . First , alter frame crop size number sampled frames video clip keeping patch size ﬁxed   ×    ×    . Table   shows using small frame crop size larger number frames hurts video-related results , signiﬁcantly change audio classiﬁcation numbers .    METHOD MIL-NCE [    ] AVTS [    ] XDC [   ] ELo [    ] AVID [    ] GDT [    ] MMV [   ] VATT-Medium + SVM VATT-Medium + LRC VATT-MA-Medium + LRC UCF    HMDB   ESC     .  - - - - -   .    .    .    .    .  - -   .  - -   .    .    .    .  -   .    .  -   .    .    .    .    .    .  Table   : Linear evaluation results video action recognition UCF    HMDB   audio event classiﬁcation ESC   . refers Modality-Agnostic backbone . Frame Size Patch Size UCF HMDB YC  MSRVTT ESC   ×   ×     ×  ×     .    ×   ×      ×   ×      ×   ×      ×   ×      ×   ×     ×  ×    ×  ×    ×  ×    ×  ×    ×  ×     .     .     .     .     .     .    .     .    .     .     .     .     .     .     .     .     .     .     .     .     .    .     .        .        .         Table   : Effect video frame patch size downstream results . , keep best frame size (    ×     ×     ) vary video patch size . ﬁnd going beyond   ×    ×    along either time spatial dimensions helpful . avoid patches smaller   ×    ×    signiﬁcantly increaseed wall clock time experiments . Finally , compare different audio patch sizes perform experiment using spectrograms , opposed raw waveforms , audio input . goal see raw waveforms compare handcrafted spectrograms . use MEL spectrogram    bins , STFT length    ms , STFT step    ms following similar setup [   ] . Tables    summarize results , observe patch size     gives rise best waveform-based results , using spectrogram lead conclusive improvement . experiment spectrograms demonstrates VATT able learn semantic representations raw audios . best knowledge , ﬁrst time raw audio waveforms used multimodal self-supervised learning . Input Patch Size UCF HMDB YC  MSRVTT ESC Waveform Waveform Waveform Waveform                    .     .     .     .     .     .    .     .     .     .     .     .     .     .     .     .     .     .     .    .  Spectrogram    ×     .    .     .     .      Table    : Effect audio input type patch size downstream results .    