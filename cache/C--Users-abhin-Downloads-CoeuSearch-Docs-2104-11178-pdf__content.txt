1 2 0 2 c e D 7 ] V C . s c [ 3 v 8 7 1 1 1 . 4 0 1 2 : v i X r a VATT: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text Hassan Akbari∗ Columbia University ha2436@columbia.edu Liangzhe Yuan Google lzyuan@google.com Rui Qian∗ Cornell University rq49@cornell.edu Wei-Hong Chuang Google whchuang@google.com Shih-Fu Chang Columbia University sc250@columbia.edu Yin Cui Google yincui@google.com Boqing Gong Google bgong@google.com Abstract We present a framework for learning multimodal representations from unlabeled data using convolution-free Transformer architectures. Speciﬁcally, our Video- Audio-Text Transformer (VATT) takes raw signals as inputs and extracts multi- modal representations that are rich enough to beneﬁt a variety of downstream tasks. We train VATT end-to-end from scratch using multimodal contrastive losses and evaluate its performance by the downstream tasks of video action recognition, audio event classiﬁcation, image classiﬁcation, and text-to-video retrieval. Furthermore, we study a modality-agnostic, single-backbone Transformer by sharing weights among the three modalities. We show that the convolution-free VATT outperforms state-of-the-art ConvNet-based architectures in the downstream tasks. Especially, VATT’s vision Transformer achieves the top-1 accuracy of 82.1% on Kinetics-400, 83.6% on Kinetics-600, 72.7% on Kinetics-700, and 41.1% on Moments in Time, new records while avoiding supervised pre-training. Transferring to image classiﬁ- cation leads to 78.7% top-1 accuracy on ImageNet compared to 64.7% by training the same Transformer from scratch, showing the generalizability of our model despite the domain gap between videos and images. VATT’s audio Transformer also sets a new record on waveform-based audio event recognition by achieving the mAP of 39.4% on AudioSet without any supervised pre-training. VATT’s source code is publicly available.2 1 Introduction Convolutional neural networks (CNNs) [53, 51] have triumphed over various computer vision tasks. The inductive bias induced by convolutions, namely translation invariance and locality, are proven effective for the visual data. In the meantime, however, we witness in the natural language processing (NLP) community a paradigm shift from the models with strong inductive biases, such as recurrent neural networks [43, 7] and CNNs [104, 32], to more general architectures constructed upon self- attention. Particularly, Transformers [88] have become the de facto model architecture for NLP ∗Work done during an internship at Google. 2https://github.com/google-research/google-research/tree/master/vatt 35th Conference on Neural Information Processing Systems (NeurIPS 2021), Sydney, Australia.             Figure 1: Overview of the VATT architecture and the self-supervised, multimodal learning strategy. VATT linearly projects each modality into a feature vector and feeds it into a Transformer encoder. We deﬁne a semantically hierarchical common space to account for the granularity of different modalities and employ the Noise Contrastive Estimation (NCE) to train the model. tasks [23, 70, 71, 10]. Pre-training a Transformer on large text corpora followed by ﬁne-tuning gives rise to state-of-the-art results for different downstream tasks. In view of the success of the attention mechanism in NLP, there has been a rich line of works exploring its potential in computer vision. Early work studied hybrid models consisting of both convolutions and attention modules [89, 94, 36, 105]. Recent studies showed that convolution-free, specially designed all-attention models can match CNNs’ performance on image recognition tasks [106, 44, 73]. Most recently, [25] achieved impressive performance on several image recognition tasks, including ImageNet [22], using a pre-trained Transformer with minimal architecture changes. Their work delivered a compelling message that “large scale (supervised) training trumps inductive bias (for image classiﬁcation).” This conclusion was further extended to video recognition tasks by [9, 5]. However, the large-scale supervised training of Transformers is essentially troubling for two main reasons. First, it rules out the much larger other part of “big visual data,” i.e, the vast amount of unlabeled, unstructured visual data. As a result, the supervised training strategy could produce biased systems that require even more labeled data to correct their biases. Second, this strategy fundamentally limits the application scope of Transformers in computer vision because it is costly and extremely time-consuming to collect enough labeled images or videos for training the millions of parameters, choosing hyper-parameters, and validating their expected generalization. Hence, this work poses another pressing question about the Transformers that take raw signals as input. How to empower them with large-scale, unlabeled visual data? To answer this question, we draw insights from NLP. BERT [23] and GPT [70, 71, 10] use masked language modeling as their pre-training tasks. Natural languages are organic supervision for Transformers. They sequentially place words, phrases, and sentences into context, granting them semantics and syntax. For visual data, the most organic supervision is arguably the multimodal videos. They are abundantly available in the digital world, and their temporal, cross-modality regulation, and therefore supervision, requires no human annotation. The extreme scale of multimodal videos is potentially capable to teach Transformers necessary priors, as opposed to predeﬁned inductive biases, to model the visual world. To this end, we study self-supervised, multimodal pre-training of three Transformers [88], which take as input the raw RGB frames of internet videos, audio waveforms, and text transcripts of the speech audio, respectively. We call the video, audio, text Transformers VATT. Figure 1 illustrates the architecture. VATT borrows the exact architecture from BERT [23] and ViT [25] except the layer of tokenization and linear projection reserved for each modality separately. This design shares the same spirit as ViT that we make the minimal changes to the architecture so that the learned model can transfer its weights to various frameworks and tasks. Furthermore, the self-supervised, multimodal learning strategy resonates the spirit of BERT and GPT that the pre-training requires minimal human curated labels. We evaluate the pre-trained Transformers on a variety of downstream tasks: image classiﬁcation, video action recognition, audio event classiﬁcation, and zero-shot text-to-video retrieval. Fine-tuning 2 NCEloss“Sled dogs running on the snow pulling the sled.”Input VideoInput Audio WaveformInput TextLinear Projection (3D RGB voxels)Linear Projection (1D waveform)Linear Projection (1-hot word vectors)Transformer EncoderModality-Specific OR Modality-Agnostic···Extra Learnable [AGG] EmbeddingModality-Specific Patch + Position EmbeddingMultimodal Projection Head+Multi-Head AttentionNorm+NormMLPEmbeddingL ✕Transformer EncoderVATTMultimodalProjection Headvideo featureaudio featuretext featureMIL-NCElossthe vision-modality Transformer on ImageNet [22] obtains the top-1 accuracy of 78.7%, which is comparable to 79.9% achieved by ViT. This result is especially appealing considering the domain gap between videos and images, and that ViT is pre-trained using a large-scale, human-curated image dataset. Furthermore, we set new records on Kinetics-400 [14], Kinetics-600 [15], Moments in Time [61], and AudioSet [33] without supervised pre-training. Our VATT results, along with others reported for NLP tasks [23, 10], image recognition [25], semantic segmentation [108], point cloud classiﬁcation [107], and action recoginition [9], demonstrate that Transformer is a versatile general-purpose architecture for different types of data. To move one step forward, we challenge the Transformers in VATT by a seemingly too strong constraint: sharing weights among the video, audio, and text modalities. The idea is to test whether there exists a single, general-purpose model for all the modalities — of course, they still have their own layers of tokenization and linear projection. Preliminary results are encouraging. This modality-agnostic Transformer is on par with three modality-speciﬁc ones of slightly smaller sizes. Finally, another contribution of this work is DropToken, a simple and yet effective technique to reduce the training complexity with a minor reduction of the end Transformers’ performance. DropToken randomly drops a portion of the video and audio tokens from each input sequence during training, al- lowing for high-resolution inputs and leveraging their abundance. This is signiﬁcant for Transformers because their computational complexity is quadratic with respect to the number of input tokens. 2 Related work 2.1 Transformers in Vision Transformer was originally built for NLP tasks [88] and the design of multi-head attention shows its effectiveness on modeling long-term correlation of words. A few attempts have been made to use Transformer for vision tasks like image super-resolution [99], object detection [11] and multimodal video understanding [84, 19, 57]. However these methods still rely on the feature extracted by CNNs. Recently, [25] proposes a set of convolution-free vision Transformers which directly work on raw images and obtain competitive performance with CNNs. [86] improves the training data efﬁciency of [25] by using stronger data augmentations and knowledge distillation. Since then, the pure Transformer design has been adopted to various vision tasks including semantic segmentation [108], point cloud classiﬁcation [107], action recoginition [9, 78, 5]. To the best of our knowledge, our VATT is the ﬁrst Transformer model on raw multimodal inputs of video, audio and text. 2.2 Self-Supervised Learning Single vision modality. Early work of self-supervised visual representation learning usually learns from unlabeled images via manually speciﬁed pretext tasks, like auto-encoding [64, 102, 103], patch location prediction [24], solving jigsaw puzzles [63], and image rotation prediction [35]. [95] propose a novel instance discrimination objective. The recent trend of contrastive learning [40, 17, 100, 37, 41, 85] integrates data augmentations and instance discrimination by maintaining relative consistency between representations of an image and its augmented view. Clustering can also provide an effective addition [12]. Recently, [18] conduct contrastive learning using ViT [25] and achieve impressive results. As for the video domain, it is natural to exploit the temporal signals as the pretext task. Examples include predicting the future frame [82], motion and appearance statistics [90], speed [8, 91] and encodings [56, 38, 39], sorting frames or video clips [54, 97, 45, 31]. Recently, [68] apply contrastive learning to videos with a temporal sampling strategy and temporally consistent spatial augmentation. Multimodal video. Video is a natural source of multimodal data. Multimodal self-supervised learning can be achieved by predicting whether a video has correspondence with an audio stream [3, 4, 62, 50], cross-modality clustering [2], and evolving losses [67]. Recently, [1] use contrastive loss to learn from video, audio and text; [74] learn to predict a broad view that spans a longer temporal context from a narrow view. VATT serves as a ﬁrst work combining the strength of convolution-free Transformer and multimodal contrastive learning. 3 3 Approach In this section, we introduce our convolution-free VATT architecture and elaborate on the self- supervised multimodal objectives for training VATT from scratch. Figure 1 is an overview of the architecture. We feed each modality to a tokenization layer, where the raw input is projected to an embedding vector followed by a Transformer. There are two major settings: 1) The backbone Transformers are separate and have speciﬁc weights for each modality, and 2) The Transformers share weights, namely, there is a single backbone Transformer applied to any of the modalities. In either setting, the backbone extracts modality-speciﬁc representations, which are then mapped to common spaces to be compared with each other by contrastive losses. We describe each module in the following. 3.1 Tokenization and Positional Encoding VATT operates on raw signals. The vision-modality input consists of 3-channel RGB pixels of video frames, the audio input is in the form of air density amplitudes (waveforms), and the text input is a sequence of words. We ﬁrst deﬁne a modality-speciﬁc tokenization layer that takes as input the raw signals and returns a sequence of vectors to be fed to the Transformers. Besides, each modality has its own positional encoding, which injects the order of tokens into Transformers [88]. We partition an entire video clip of size T × H × W to a sequence of (cid:100)T /t(cid:101) · (cid:100)H/h(cid:101) · (cid:100)W/w(cid:101) patches, where each patch contains t × h × w × 3 voxels. We apply a linear projection on the entire voxels in each patch to get a d-dimensional vector representation. This projection is performed by a learnable weight Wvp ∈ Rt·h·w·3×d. This can be seen as a 3D extension of the patching mechanism proposed in [25]. To encode the position of these patches, we deﬁne a dimension-speciﬁc sequence of learnable embeddings as follows: ei,j,k = eTemporali+eHorizontalj + eVerticalk, ETemporal ∈ R(cid:100)T /t(cid:101)×d, EHorizontal ∈ R(cid:100)H/h(cid:101)×d, EVertical ∈ R(cid:100)W/w(cid:101)×d (1) where ei is the i-th row of E. This scheme allows us to use (cid:100)T /t(cid:101) + (cid:100)H/h(cid:101) + (cid:100)W/w(cid:101) positional embeddings to encode all the (cid:100)T /t(cid:101) · (cid:100)H/h(cid:101) · (cid:100)W/w(cid:101) patches in a video clip. The raw audio waveform is a 1D input with length T (cid:48), and we partition it to (cid:100)T (cid:48)/t(cid:48)(cid:101) segments each containing t(cid:48) waveform amplitudes. Similar to video, we apply a linear projection with a learnable weight Wap ∈ Rt(cid:48)×d to all elements in a patch to get a d-dimensional vector representation. We use (cid:100)T (cid:48)/t(cid:48)(cid:101) learnable embeddings to encode the position of each waveform segment. For text, we ﬁrst construct a vocabulary of size v out of all words in our training dataset. For an input text sequence, we then map each word to a v-dimensional one-hot vector followed by a linear projection with a learnable weight Wtp ∈ Rv×d. This is equivalent to an embedding dictionary lookup, which has been widely used in natural language understanding [60]. 3.1.1 DropToken We introduce DropToken, a simple and yet effective strategy to reduce the computational complexity during training. Once we get the token sequence for the video or audio modality, we randomly sample a portion of the tokens and then feed the sampled sequence, not the complete set of tokens, to the Transformer. This is crucial for reducing the computational cost because a Transformer’s computation complexity is quadratic, O(N 2), where N is number of tokens in the input sequence. Any effort on reducing the input length would reduce the number of FLOPs quadratically. This has an immediate impact on the wall clock time for training these models and makes it possible to host large models in limited hardware. We argue that instead of reducing the resolution or dimension of the raw inputs, it is better to keep a high-ﬁdelity input and randomly sample the tokens via DropToken. DropToken is appealing especially with the raw video and audio inputs, which may contain high redundancies. 3.2 The Transformer Architecture For simplicity, we adopt the most established Transformer architecture [23], which has been widely used in NLP. Similar to ViT [25], we do not tweak the architecture so that our weights can be easily transferred to any standard Transformer implementation. We will brieﬂy elaborate on the pipeline (also illustrated in Figure 1 middle panel) and refer the reader to [25, 23] for more details of the 4 standard Transformer architecture. The sequence of input tokens to the Transformer follows the below formulation: zin = [xAGG; x0WP ; x1WP ; . . . ; xN WP ] + ePOS (2) where xn is the input patches sequence and xAGG is the learnable embedding of a special aggregation token whose corresponding output in the Transformer (z0 out) is used as the aggregated representation for the entire input sequence. This will be later used for classiﬁcation and common space mapping. We use a standard self-attention [88] as the Multi-Head-Attention (MHA) module, and GeLU [42] as the activation in the MLP layer. We also use Layer Normalization [6] before the MHA and MLP modules. In our text model, we remove the position encoding ePOS and add a learnable relative bias to each attention score of the ﬁrst layer in the MHA module. This simple change makes our text model’s weights directly transferable to the state-of-the-art text model T5 [72]. 3.3 Common Space Projection We use common space projection and contrastive learning in that common space to train our networks. More speciﬁcally, given a video-audio-text triplet, we deﬁne a semantically hierarchical common space mapping that enables us to directly compare video-audio pairs as well as video-text pairs by the cosine similarity. As argued in [1], such comparison is more feasible if we assume there are different levels of semantic granularity for these modalities. To achieve this, we deﬁne multi-level projections as follows: zv,va = gv→va(zvideo out ), zt,vt = gt→vt(ztext out ), za,va = ga→va(zaudio out ) zv,vt = gv→vt(zv,va) (3) where gv→va and ga→va are the projection heads to respectively map the video and audio Trans- formers’ outputs to the video-audio common space Sva. Moreover, gt→vt and gv→vt project the text Transformer’s outputs and the video embedding in the Sva space to video-text common space, Svt. This multi-level common space projection is depicted in Figure 1 (the rightmost panel). The main intuition behind this hierarchy is that different modalities have different levels of semantic granularity, so we should impose this as an inductive bias in the common space projection. Similar to [1], we use a linear projection for ga→va(.), gt→vt(.), and gv→vt(.), and a two-layer projection with ReLU in between for gv→va(.). To ease the training, a batch normalization is used after each linear layer. 3.4 Multimodal Contrastive Learning Inspired by [1, 3, 59], we use Noise Contrastive Estimation (NCE) to align video-audio pairs and Multiple Instance Learning NCE (MIL-NCE) to align video-text pairs. The pairs are composed from different temporal locations in the video-audio-text stream. Positive pairs from two modalities are constructed by sampling their corresponding streams from the same location in the video, and negative pairs are constructed by sampling from any non-matching locations in the video [1]. Concretely, given the common space speciﬁed in Section 3, the loss objectives can be written as follows: NCE(zv,va, za,va) = − log (cid:32) exp(z(cid:62) v,vaza,va/τ ) + (cid:80) exp(z(cid:62) v,vaza,va/τ ) z(cid:48)∈N exp(z(cid:48)(cid:62) v,vaz(cid:48) (cid:33) , a,va/τ ) MIL-NCE(zv,vt, {zt,vt}) = − log (cid:32) (cid:80) zt,vt∈P exp(z(cid:62) (cid:80) zt,vt∈P exp(z(cid:62) v,vtzt,vt/τ ) + (cid:80) v,vtzt,vt/τ ) z(cid:48)∈N exp(z(cid:48)(cid:62) v,vtz(cid:48) t,vt/τ ) (5) where N contains all non-matching pairs in a batch. In Equation 5, P contains ﬁve text clips that are nearest neighbors to the video clip in time. τ is a temperature to adjust the softness of the objectives in distinguishing the positive pairs from the negative pairs. The overall per-sample objective for training the entire VATT model end-to-end is as follows: L = NCE(zv,va, za,va) + λMIL-NCE(zv,vt, {zt,vt}), (6) where λ balances the two losses. The model is optimized based on the back-propagation of the average loss calculated over a batch of samples. 5 (4) (cid:33) , 4 Experiments In this section, we ﬁrst brieﬂy describe the experimental setup for the pre-training and downstream evaluation, and then present the results and analytic interpretation of VATT in different tasks. We refer the reader to the Appendix for a more detailed description of all experimental settings. 4.1 Experimental Setup Pre-train: we use a combination of AudioSet [33] and HowTo100M [58] datasets to pre-train VATT— we use only a subset of the HowTo100M dataset in compliance with Youtube’s policies. Following [1], we use video-audio-text triplets from HowTo100M clips while only using video-audio pairs from AudioSet. We sample 32 frames at 10 fps with a spatial size of 224 × 224 following a random crop, horizontal ﬂip and color augmentation (details in A.2.1). Accordingly, we sample audio waveforms in sync at 48kHz. Both video and audio are normalized between [-1,1]. We use patch sizes of 4 × 16 × 16 and 128 for video and raw waveform tokenization, respectively (ablation in A.5). We use one-hot vectors to encode text sequences (capped to 16 tokens) with the vocabulary size of 216. In all pre-training experiments, we use DropToken with drop rate 50%. We train our models using the Adam optimizer [46] with a quarter-period cosine scheduled learning rate from 1e-4 to 5e-5 and 10k warmup steps. Optimization is performed on totally 500k steps with batch size 2048 (512 in exploration experiments). Following the previously established practice [1] for the projection to the common spaces Sva and Svt, we use dva = 512 and dvt = 256. We also use the temperature of τ = 0.07 and the weight of λ = 1 in the loss in Equation 6. We use 4 network sizes in our experiments (details in A.2.2). We use the Medium model (155M parameters) for our modality-agnostic variant (VATT-MA), and 3 variants for the modality-speciﬁc video-audio-text backbones: Base-Base-Small (BBS; 197M), Medium-Base-Small (MBS; 264M), and Large-Base-Small (LBS; 415M). Pre-training an MBS VATT with batch size 2048 on 256 TPUs (v3) takes less than 3 days. Pre-training with batch size 512 takes less than 1 day. Downstream: we evaluate the pre-trained VATT models on 4 major downstream tasks using a total of 10 datasets. We use UCF101 [81], HMDB51 [52], Kinetics-400 [14], Kinetics-600 [15], and Moments in Time [61] for video action recognition. We use ESC50 [66] and AudioSet [33] for audio event classiﬁcation, and we evaluate the quality of our video-text common space representations by zero-shot text-to-video retrieval on YouCook2 [109] and MSR-VTT [98]. Finally, we evaluate the transferability of the vision backbone by ﬁne-tuning it on ImageNet classiﬁcation [22]. Since HMDB51, UCF101, and ESC50 are very small datasets compared to the size of our networks, we only use them to train a linear classiﬁer on top of the frozen pre-trained backbones. In our exploration experiments, we report linear classiﬁcation accuracy and zero-shot video retrieval metrics. We refer to the Appendix for a detailed description of the datasets and the experimental setup. 4.2 Results 4.2.1 Fine-tuning for video action recognition We ﬁne-tune VATT’s vision Transformer on Kinetics-400, Kinetics-600, and Moments in Time, three of the arguably most established large-scale datasets for video action recognition. We use the ﬁnal checkpoints of four pre-train settings for these experiments: three modality-speciﬁc variations (LBS, MBS, BBS), and one modality-agnostic (Medium). Table 1 shows the results compared with the state-of-the-art video models. On all three datasets, we achieve higher accuracy than previous works including TimeSFormer [9], a recent effort in ﬁne-tuning the ViT checkpoints obtained by supervised pre-training. In contrast, our pre-training does not rely on any labels curated by humans. To the best of our knowledge, VATT provides the ﬁrst vision Transformer backbone that is pre-trained from scratch using self-supervision on multimodal videos and achieves state-of-the-art results on video action recognition. It is also worth mentioning that ﬁne-tuning VATT on the most recent Kinetics-700 dataset results in a top-1 accuracy of 72.7%, which outperforms the state-of-the-art top-1 accuracy of 72.4% in [47]. To further quantify how much the multimodal self-supervised pre-training helps in achieving these numbers, we train a variant from scratch without any pre-training and observe the top-1 and top-5 accuracies of 26.4% and 51.8% on Kinetics-400, respectively. The low accuracies verify the efﬁcacy of our pre-training strategy for VATT. Finally, we ﬁnd that VATT-MA-Medium, the modality-agnostic 6 METHOD I3D [13] R(2+1)D [26] bLVNet [27] S3D-G [96] Oct-I3D+NL [20] D3D [83] I3D+NL [93] ip-CSN-152 [87] AttentionNAS [92] AssembleNet-101 [77] MoViNet-A5 [47] LGD-3D-101 [69] SlowFast-R101-NL [30] X3D-XL [29] X3D-XXL [29] TimeSFormer-L [9] VATT-Base VATT-Medium VATT-Large VATT-MA-Medium Kinetics-400 Kinetics-600 TOP-1 TOP-5 TOP-1 TOP-5 Moments in Time TOP-5 TOP-1 TFLOPS 71.1 72.0 73.5 74.7 75.7 75.9 77.7 77.8 - - 78.2 79.4 79.8 79.1 80.4 80.7 79.6 81.1 82.1 79.9 89.3 90.0 91.2 93.4 - - 93.3 92.8 - - - 94.4 93.9 93.9 94.6 94.7 94.9 95.6 95.5 94.9 71.9 - - - 76.0 77.9 - - 79.8 - 82.7 81.5 81.8 81.9 - 82.2 80.5 82.4 83.6 80.8 90.1 - - - - - - - 94.4 - - 95.6 95.1 95.5 - 95.6 95.5 96.1 96.6 95.5 29.5 - 31.4 - - - - - 32.5 34.3 39.1 - - - - - 38.7 39.5 41.1 37.8 56.1 - 59.3 - - - - - 60.3 62.7 - - - - - - 67.5 68.2 67.7 65.9 - 17.5 0.84 - 0.84 - 10.8 3.3 1.0 - 0.29 - 7.0 1.5 5.8 7.14 9.09 15.02 29.80 15.02 Table 1: Video action recognition accuracy on Kinetics-400, Kinetics-600, and Moments in Time. backbone shared by the video, audio, and text modalities, is on par with the modality-speciﬁc VATT- Base when ﬁne-tuned for the video action recognition. This result is encouraging as it indicates the potential of unifying three data modalities by a single Transformer backbone. 4.2.2 Fine-tuning for audio event classiﬁcation We ﬁne-tune VATT’s audio Transformer on AudioSet, which benchmarks the task of multi-label audio event classiﬁcation. We use the ﬁnal checkpoints of two pre-train settings: one modality-speciﬁc (BBS), and one modality-agnostic (Medium). Table 2 shows the results compared to state-of-the-art models. Following common practice [34, 48], we report mean Average Precision (mAP), Area Under Curve (AUC), and d-prime (based on AUC) [34]. Our audio Transformer consistently outperforms the existing CNN-based models in all metrics. More interestingly, ﬁne-tuning the modality-agnostic backbone (VATT-MA-Medium) is on par with ﬁne-tuning the modality-speciﬁc one (VATT-Base). To the best of our knowledge, VATT is the ﬁrst Transformer that outperforms CNN-based models in audio event recognition. VATT operates on raw waveforms and does not utilize any handcrafted features. 4.2.3 Fine-tuning for image classiﬁcation In this section, we show that our pipeline is capable of transferring the learned knowledge into another domain by performing the image classiﬁcation task, even though the models are pre-trained in the multimodal video domain. We ﬁne-tune the vision Transformer in VATT-BBS on ImageNet without any modiﬁcation to the backbone architecture. Instead, to satisfy the voxel-to-patch layer we replicate the input image 4 times and feed it to the network. The network sees the input as a single-frame video clip and performs spatial self-attention. Table 3 shows the results for ﬁne-tuning the vision Transformer end-to-end on ImageNet. We can see that our pre-training leads to a signiﬁcant boost in the accuracy compared to training from scratch. We also observe that even though the self-supervised pre-training happens in the video domain, we still achieve competitive results to the supervised pre-training using large-scale image data [25]. 4.2.4 Zero-shot text-to-video retrieval We feed video-text pairs to VATT-MBS, and extract representations in the Svt space. We then calculate the similarity between each video-text pair from YouCook2 and MSR-VTT. Given a text query, we rank the videos based on their similarities to the text. We then measure the recall for the 7 METHOD mAP AUC d-prime 29.5 95.8 DaiNet [21] 26.6 95.3 LeeNet11 [55] 33.6 96.3 LeeNet24 [55] 36.5 95.8 Res1dNet31 [49] Res1dNet51 [49] 35.5 94.8 Wavegram-CNN [49] 38.9 96.8 2.437 2.371 2.525 2.444 2.295 2.612 VATT-Base 39.4 97.1 2.895 VATT-MA-Medium 39.3 97.0 2.884 Table 2: Finetuning results for AudioSet event classiﬁcation. METHOD PRE-TRAINING DATA TOP-1 TOP-5 iGPT-L [16] ViT-Base [25] VATT-Base VATT-Base ImageNet JFT - HowTo100M 72.6 79.9 64.7 78.7 - - 83.9 93.9 Table 3: Finetuning results for ImageNet classiﬁcation. METHOD MIL-NCE [59] MMV [1] VATT-MBS VATT-MA-Medium 2048 YouCook2 MSR-VTT BATCH EPOCH R@10 MedR R@10 MedR 8192 4096 2048 27 8 4 4 51.2 45.4 45.5 40.6 10 13 13 17 32.4 31.1 29.7 23.6 30 38 49 67 Table 4: Zero-shot text-to-video retrieval. correct video in the top-10 videos. We also measure the median of the rank of the correct video. Table 4 compares our video retrieval results to two baselines. In our experiments we observe that the zero-shot retrieval results are heavily affected by the batch size and number of epochs, conﬁrming the observation made in [1]. That said, our model still delivers comparable results to MMV [1] while being pre-trained with a half number of epochs and a half batch size of theirs. We also experiment with a larger batch size 8192 and longer pre-training for 6 epochs, arriving at exactly the same results as MIL-NCE [59] on YouCook2 and the R@10 of 29.2 and MedR of 42 on MSR-VTT. We also notice that, probably due to the noisy nature of text transcripts, a sophisticated language model like ours is underrated. As shown in [1], using a simple linear projection would still perform reasonably well. It is worth exploring other, higher-quality text sources in future work. 4.2.5 Feature visualization We take our modality-speciﬁc and modality-agnostic VATT ﬁne-tuned on Kinetics-400 and visual- ize their output feature representations using t-SNE. For comparison, we also include the feature visualization of the vision Transformer trained from scratch on Kinetics-400. From Figure 2, we observe that the ﬁne-tuned VATT yields a much better separation than the model trained from scratch. Furthermore, it is worth noting that there is no clear difference between the modality-agnostic features and the modality-speciﬁc ones. We further investigate the VATT backbones without any ﬁne-tuning. We randomly choose 1k video clips from the YouCook2 dataset and store the representations from two points of a pre-trained VATT model. One is after the tokenization layer (input space of the Transformer), and the other is after the common space projection (output space), where the loss is computed. Figure 3-top visualizes the representations, comparing modality-speciﬁc VATT to modality-agnostic VATT. Interestingly, we observe that the representations are slightly more mixed together in the modality-agnostic setting compared to the modality-speciﬁc ones, implying that the modality-agnostic backbone sees different modalities as different symbols describing the same concept. This is analogous to a uniﬁed language model in NLP that supports multiple languages. To see how well VATT distinguishes positive video-text pairs from randomly sampled pairs, we calculate pair-wise similarities for all possible pairs and perform a Kernel Density Estimation (KDE) to visualize the distributions of the similarities of the positive pairs vs. negative pairs. We perform this procedure for both input and output spaces of the modality-speciﬁc and modality-agnostic backbones. Figure 3-bottom shows the KDE curves of these similarities. We can see that VATT in both settings separates the positive and negative pairs in its output space. This veriﬁes VATT’s efﬁcacy in learning a semantic common space for different modalities, even if we share the backbone across modalities. 4.2.6 Model Activations We measure the average activation of the modality-agnostic VATT when a full multimodal input is fed to the model. More speciﬁcally, we sample 100k short video clips from the test split of HowTo100M along with their corresponding audio and text and feed them to the model separately. For each 8 Figure 2: t-SNE visualization of the feature representations extracted by the vision Transformer in different training settings. For better visualization, we show 100 random classes from Kinetics-400. Figure 3: t-SNE visualization and distribution of pair-wise similarities of the input space vs. output space for modality-speciﬁc and modality-agnostic backbones when different modalities are fed. modality, we calculate the average activation of each node at the output of the MLP module, before the residual addition (Figure 1-Transformer Encoder). Figure 4 shows the average activations across all nodes in a Medium-size model. We observe that earlier nodes in the model are activated with the text inputs, while the middle-to-later nodes are activated with video and audio modalities. However, the nodes in the last layers of the network are activated with all modalities almost equally. This might suggest that the model allocates different nodes to certain modalities while reaching the same level of semantic perception for all modalities in the later layers. Such observation encourages further studies on the possibility of utilizing Mixture-of-Experts [79, 28, 76] to increase the model’s capacity for simultaneous multimodal perception. We leave this direction of research for future work. 4.2.7 Effect of DropToken We introduced a new method to reduce the redundancy in high-resolution data. To study the effect of the proposed DropToken method on downstream applications and the pre-training computation, we perform pre-training by randomly dropping 75%, 50%, 25%, and 0% (no drop) of the tokens from the video and audio inputs. Table 5 shows the accuracy of linear classiﬁcation on HMDB51, UCF101, ESC50 and R@10 on YouCook2 and MSR-VTT vs. the drop rate along with GFLOPs during a forward call. We choose 50% sampling rate for our large-scale pre-training as it offers a good trade-off between accuracy and computational costs. We then take the ﬁnal checkpoint of the pre-trained VATT with 50% DropToken rate and perform ﬁne-tuning on Kinetics-400 at different DropToken rates and at different spatial and temporal resolutions to see how high-resolution inputs coupled with DropToken compare to low-resolution inputs with no tokens dropped during ﬁne-tuning. Table 6 shows the top-1 accuracy on Kinetics-400. We argue against using low-resolution inputs, which is the most common approach to reduce the computational cost during training. Instead, we suggest using high-resolution inputs with DropToken, whose accuracy and training cost are comparable to or better than low-resolution counterparts. 9 t-SNE-1864202468t-SNE-26420246t-SNE-3864202468From Scratcht-SNE-1864202468t-SNE-26420246t-SNE-3864202468Modality-Specifict-SNE-1864202468t-SNE-26420246t-SNE-3864202468Modality-Agnostic151050510t-SNE-115105051015t-SNE-2Input / M-SpecificModalitiesVideoText15105051015t-SNE-115105051015t-SNE-2Output / M-SpecificModalitiesVideoText1050510t-SNE-115105051015t-SNE-2Input / M-AgnosticModalitiesVideoText15105051015t-SNE-1151050510t-SNE-2Output / M-AgnosticModalitiesVideoText0.20.00.20.40.60.81.0Similarities0.00.51.01.52.0DensityPairsPositiveNegative0.00.20.40.60.81.0Similarities0.00.51.01.52.02.53.0DensityPairsPositiveNegative0.20.00.20.40.60.81.0Similarities0.000.250.500.751.001.251.501.752.00DensityPairsPositiveNegative0.00.20.40.60.81.0Similarities0.00.51.01.52.02.53.0DensityPairsPositiveNegativeFigure 4: The average node activation across the Modality-Agnostic-Medium VATT while feeding a multimodal video-audio-text triplet to the model. DropToken Drop Rate 75% 50% 25% 0% Multimodal GFLOPs 188.1 375.4 574.2 784.8 HMDB51 UCF101 ESC50 YouCookII MSR-VTT 62.5 84.0 78.9 17.9 14.1 64.8 85.5 84.1 20.7 14.6 65.6 87.2 84.6 24.2 15.1 66.4 87.6 84.9 23.1 15.2 Resolution/ FLOPs DropToken Drop Rate 75% 50% 25% 0% 32 × 224 × 224 Inference (GFLOPs) 64 × 224 × 224 Inference (GFLOPs) - - - - - - - - - - - - 79.9 548.1 80.8 1222.1 32 × 320 × 320 79.3 Inference (GFLOPs) 279.8 572.5 898.9 1252.3 81.1 80.7 80.2 Table 5: Top-1 accuracy of linear classiﬁca- tion and R@10 of video retrieval vs. drop rate vs. inference GFLOPs in the VATT-MBS. Table 6: Top-1 accuracy of video action recogni- tion on Kinetics400 using high-resolution inputs coupled with DropToken vs. low-resolution inputs. 5 Conclusion and Discussion In this paper, we present a self-supervised multimodal representation learning framework based on Transformers. Our study suggests that Transformers are effective for learning semantic video/audio/text representations — even if one model is shared across modalities — and multi- modal self-supervised pre-training is promising for reducing their dependency on large-scale labeled data. We show that DropToken can signiﬁcantly reduce the pre-training complexity with video and audio modalities and have minor impact on the models’ generalization. We report new records of results on video action recognition and audio event classiﬁcation and competitive performance on image classiﬁcation and video retrieval. Having these results, we still see some limitations in our work. Firstly, not all videos have organic audio or speech, while our approach depends on meaningful multimodal correspondences. Besides, the text modality currently consists of speech transcripts, which are noisy and sometimes sparse. Potential negative Societal Impacts are mainly concerned with applications. The models could be biased if one applies our approach to the multimodal videos that are not representative enough. Finally, our method is still demanding in computation, though we managed to avoid the need for human labels. Future work can improve upon these limitations. Acknowledgments and Disclosure of Funding We would like to thank Min-Hsuan Tsai, Jean-Baptise Alayrac, Andrew Audibert, Yeqing Li, Vidush Mukund, and the TensorFlow team for their help with codes, infrastructure, and insightful discussions. 10 Early Activation for TextLate Activation for Video & AudioModality-Agnostic ActivationNode #Input ModalitiesVideoAudioTextVideoAudioTextModality0 2000 4000 6000 8000 10000 12000 14000 16000References [1] Jean-Baptiste Alayrac, Adrià Recasens, Rosalia Schneider, Relja Arandjelovi´c, Jason Rama- puram, Jeffrey De Fauw, Lucas Smaira, Sander Dieleman, and Andrew Zisserman. Self- supervised multimodal versatile networks. In NeurIPS, 2020. 3, 5, 6, 8, 17, 18, 19, 20 [2] Humam Alwassel, Dhruv Mahajan, Bruno Korbar, Lorenzo Torresani, Bernard Ghanem, and Du Tran. Self-supervised learning by cross-modal audio-video clustering. arXiv preprint arXiv:1911.12667, 2019. 3, 20 [3] Relja Arandjelovic and Andrew Zisserman. Look, listen and learn. In CVPR, 2017. 3, 5 [4] Relja Arandjelovic and Andrew Zisserman. Objects that sound. In ECCV, 2018. 3 [5] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Luˇci´c, and Cordelia Schmid. Vivit: A video vision transformer. arXiv preprint arXiv:2103.15691, 2021. 2, 3 [6] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. 5 [7] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In ICLR, 2015. 1 [8] Sagie Benaim, Ariel Ephrat, Oran Lang, Inbar Mosseri, William T Freeman, Michael Rubin- stein, Michal Irani, and Tali Dekel. Speednet: Learning the speediness in videos. In CVPR, 2020. 3 [9] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is space-time attention all you need for video understanding? arXiv preprint arXiv:2102.05095, 2021. 2, 3, 6, 7 [10] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari- wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020. 2, 3 [11] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In ECCV, 2020. 3 [12] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. In NeurIPS, 2020. 3 [13] Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the kinetics dataset. In CVPR, 2017. 7 [14] Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the kinetics dataset. In CVPR, 2017. 3, 6, 17 [15] Joao Carreira, Eric Noland, Andras Banki-Horvath, Chloe Hillier, and Andrew Zisserman. A short note about kinetics-600. arXiv preprint arXiv:1808.01340, 2018. 3, 6, 17 [16] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. Generative pretraining from pixels. In ICML, 2020. 8 [17] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In ICML, 2020. 3 [18] Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised visual transformers. arXiv preprint arXiv:2104.02057, 2021. 3 [19] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. Uniter: Universal image-text representation learning. In ECCV, 2020. 3 [20] Yunpeng Chen, Haoqi Fan, Bing Xu, Zhicheng Yan, Yannis Kalantidis, Marcus Rohrbach, Shuicheng Yan, and Jiashi Feng. Drop an octave: Reducing spatial redundancy in convolutional neural networks with octave convolution. In ICCV, 2019. 7 11 [21] Wei Dai, Chia Dai, Shuhui Qu, Juncheng Li, and Samarjit Das. Very deep convolutional neural networks for raw waveforms. In ICASSP, 2017. 8 [22] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR, 2009. 2, 3, 6, 17 [23] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In NAACL, 2019. 2, 3, 4 [24] Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsupervised visual representation learning by context prediction. In ICCV, 2015. 3 [25] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. 2, 3, 4, 7, 8 [26] Heng Wang Du Tran, Lorenzo Torresani, Jamie Ray, Yann LeCun, and Manohar Paluri. A closer look at spatiotemporal convolutions for action recognition. 2018 ieee. In CVPR, 2017. 7 [27] Quanfu Fan, Chun-Fu (Ricarhd) Chen, Hilde Kuehne, Marco Pistoia, and David Cox. More Is Less: Learning Efﬁcient Video Representations by Temporal Aggregation Modules. In NeurIPS. 2019. 7 [28] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efﬁcient sparsity. arXiv preprint arXiv:2101.03961, 2021. 9 [29] Christoph Feichtenhofer. X3d: Expanding architectures for efﬁcient video recognition. In CVPR, 2020. 7 [30] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. Slowfast networks for video recognition. In ICCV, 2019. 7, 18 [31] Basura Fernando, Hakan Bilen, Efstratios Gavves, and Stephen Gould. Self-supervised video representation learning with odd-one-out networks. In CVPR, 2017. 3 [32] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin. Convolu- tional sequence to sequence learning. In ICML, 2017. 1 [33] Jort F. Gemmeke, Daniel P. W. Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R. Chan- ning Moore, Manoj Plakal, and Marvin Ritter. Audio set: An ontology and human-labeled dataset for audio events. In ICASSP, 2017. 3, 6, 17 [34] Jort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Channing Moore, Manoj Plakal, and Marvin Ritter. Audio set: An ontology and human-labeled dataset for audio events. In ICASSP, 2017. 7 [35] Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by predicting image rotations. In ICLR, 2018. 3 [36] Rohit Girdhar and Deva Ramanan. Attentional pooling for action recognition. In NeurIPS, 2017. 2 [37] Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre H Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Ghesh- laghi Azar, et al. Bootstrap your own latent: A new approach to self-supervised learning. In NeurIPS, 2020. 3 [38] Tengda Han, Weidi Xie, and Andrew Zisserman. Video representation learning by dense predictive coding. In ICCV Workshops, 2019. 3 [39] Tengda Han, Weidi Xie, and Andrew Zisserman. Memory-augmented dense predictive coding for video representation learning. In ECCV, 2020. 3 12 [40] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In CVPR, 2020. 3 [41] Olivier J Hénaff, Aravind Srinivas, Jeffrey De Fauw, Ali Razavi, Carl Doersch, SM Eslami, and Aaron van den Oord. Data-efﬁcient image recognition with contrastive predictive coding. arXiv preprint arXiv:1905.09272, 2019. 3 [42] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016. 5 [43] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 1997. 1 [44] Han Hu, Zheng Zhang, Zhenda Xie, and Stephen Lin. Local relation networks for image recognition. In ICCV, 2019. 2 [45] Dahun Kim, Donghyeon Cho, and In So Kweon. Self-supervised video representation learning with space-time cubic puzzles. In AAAI, 2019. 3 [46] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 6, 18 [47] Dan Kondratyuk, Liangzhe Yuan, Yandong Li, Li Zhang, Mingxing Tan, Matthew Brown, and Boqing Gong. Movinets: Mobile video networks for efﬁcient video recognition. In CVPR, 2021. 6, 7 [48] Qiuqiang Kong, Changsong Yu, Yong Xu, Turab Iqbal, Wenwu Wang, and Mark D Plumbley. Weakly labelled audioset tagging with attention neural networks. TASLP, 2019. 7 [49] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang, Wenwu Wang, and Mark D Plumbley. Panns: Large-scale pretrained audio neural networks for audio pattern recognition. TASLP, 2020. 8, 19 [50] Bruno Korbar, Du Tran, and Lorenzo Torresani. Cooperative learning of audio and video models from self-supervised synchronization. NeurIPS, 2018. 3, 20 [51] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convolutional neural networks. In NeurIPS, 2012. 1 [52] H. Kuehne, H. Jhuang, E. Garrote, T. Poggio, and T. Serre. HMDB: a large video database for human motion recognition. In ICCV, 2011. 6, 17 [53] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 1998. 1 [54] Hsin-Ying Lee, Jia-Bin Huang, Maneesh Singh, and Ming-Hsuan Yang. Unsupervised repre- sentation learning by sorting sequences. In ICCV, 2017. 3 [55] Jongpil Lee, Jiyoung Park, Keunhyoung Luke Kim, and Juhan Nam. Sample-level deep convolutional neural networks for music auto-tagging using raw waveforms. arXiv preprint arXiv:1703.01789, 2017. 8 [56] William Lotter, Gabriel Kreiman, and David Cox. Deep predictive coding networks for video prediction and unsupervised learning. arXiv preprint arXiv:1605.08104, 2016. 3 [57] Huaishao Luo, Lei Ji, Botian Shi, Haoyang Huang, Nan Duan, Tianrui Li, Xilin Chen, and Ming Zhou. Univilm: A uniﬁed video and language pre-training model for multimodal understanding and generation. arXiv preprint arXiv:2002.06353, 2020. 3 [58] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic. Howto100m: Learning a text-video embedding by watching hundred million narrated video clips. In ICCV, 2019. 6, 17 13 [59] Antoine Miech, Jean-Baptiste Alayrac, Lucas Smaira, Ivan Laptev, Josef Sivic, and Andrew Zisserman. End-to-end learning of visual representations from uncurated instructional videos. In CVPR, 2020. 5, 8, 17, 20 [60] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efﬁcient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013. 4, 18 [61] Mathew Monfort, Alex Andonian, Bolei Zhou, Kandan Ramakrishnan, Sarah Adel Bargal, Tom Yan, Lisa Brown, Quanfu Fan, Dan Gutfreund, Carl Vondrick, et al. Moments in time dataset: one million videos for event understanding. TPAMI, 2019. 3, 6, 17 [62] Pedro Morgado, Nuno Vasconcelos, and Ishan Misra. Audio-visual instance discrimination with cross-modal agreement. arXiv preprint arXiv:2004.12943, 2020. 3 [63] Mehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving jigsaw puzzles. In ECCV, 2016. 3 [64] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A Efros. Context encoders: Feature learning by inpainting. In CVPR, 2016. 3 [65] Mandela Patrick, Yuki M Asano, Polina Kuznetsova, Ruth Fong, João F Henriques, Geoffrey Zweig, and Andrea Vedaldi. Multi-modal self-supervision from generalized data transforma- tions. arXiv preprint arXiv:2003.04298, 2020. 20 [66] Karol J. Piczak. ESC: Dataset for Environmental Sound Classiﬁcation. In ACM MM, 2015. 6, 17 [67] AJ Piergiovanni, Anelia Angelova, and Michael S Ryoo. Evolving losses for unsupervised video representation learning. In CVPR, 2020. 3, 20 [68] Rui Qian, Tianjian Meng, Boqing Gong, Ming-Hsuan Yang, Huisheng Wang, Serge Belongie, and Yin Cui. Spatiotemporal contrastive video representation learning. In CVPR, 2021. 3 [69] Zhaofan Qiu, Ting Yao, Chong-Wah Ngo, Xinmei Tian, and Tao Mei. Learning spatio-temporal representation with local and global diffusion. In CVPR, 2019. 7 [70] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. 2018. 2 [71] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. OpenAI blog, 2019. 2 [72] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer. JMLR, 2020. 5 [73] Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya, and Jonathon Shlens. Stand-alone self-attention in vision models. arXiv preprint arXiv:1906.05909, 2019. 2 [74] Adrià Recasens, Pauline Luc, Jean-Baptiste Alayrac, Luyu Wang, Florian Strub, Corentin Tallec, Mateusz Malinowski, Viorica Patraucean, Florent Altché, Michal Valko, et al. Broaden your views for self-supervised video learning. arXiv preprint arXiv:2103.16559, 2021. 3 [75] Steffen Rendle. Factorization machines. In ICDM, 2010. 19 [76] Carlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, André Susano Pinto, Daniel Keysers, and Neil Houlsby. Scaling vision with sparse mixture of experts. arXiv preprint arXiv:2106.05974, 2021. 9 [77] Michael S Ryoo, AJ Piergiovanni, Mingxing Tan, and Anelia Angelova. Assemblenet: arXiv preprint Searching for multi-stream neural connectivity in video architectures. arXiv:1905.13209, 2019. 7 14 [78] Gilad Sharir, Asaf Noy, and Lihi Zelnik-Manor. An image is worth 16x16 words, what is a video worth? arXiv preprint arXiv:2103.13915, 2021. 3 [79] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017. 9 [80] Laura Smith, Nikita Dhawan, Marvin Zhang, Pieter Abbeel, and Sergey Levine. Avid: Learning multi-stage tasks via pixel-level translation of human videos. arXiv preprint arXiv:1912.04443, 2019. 20 [81] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012. 6, 17 [82] Nitish Srivastava, Elman Mansimov, and Ruslan Salakhudinov. Unsupervised learning of video representations using lstms. In ICML, 2015. 3 [83] Jonathan Stroud, David Ross, Chen Sun, Jia Deng, and Rahul Sukthankar. D3d: Distilled 3d networks for video action recognition. In WACV, 2020. 7 [84] Chen Sun, Fabien Baradel, Kevin Murphy, and Cordelia Schmid. Learning video represen- tations using contrastive bidirectional transformer. arXiv preprint arXiv:1906.05743, 2019. 3 [85] Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. In ECCV, 2020. 3 [86] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Hervé Jégou. Training data-efﬁcient image transformers & distillation through attention. arXiv preprint arXiv:2012.12877, 2020. 3 [87] Du Tran, Heng Wang, Lorenzo Torresani, and Matt Feiszli. Video classiﬁcation with channel- separated convolutional networks. In ICCV, 2019. 7 [88] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762, 2017. 1, 2, 3, 4, 5 [89] Fei Wang, Mengqing Jiang, Chen Qian, Shuo Yang, Cheng Li, Honggang Zhang, Xiaogang Wang, and Xiaoou Tang. Residual attention network for image classiﬁcation. In CVPR, 2017. 2 [90] Jiangliu Wang, Jianbo Jiao, Linchao Bao, Shengfeng He, Yunhui Liu, and Wei Liu. Self- supervised spatio-temporal representation learning for videos by predicting motion and ap- pearance statistics. In CVPR, 2019. 3 [91] Jiangliu Wang, Jianbo Jiao, and Yun-Hui Liu. Self-supervised video representation learning by pace prediction. In ECCV, 2020. 3 [92] Xiaofang Wang, Xuehan Xiong, Maxim Neumann, AJ Piergiovanni, Michael S Ryoo, Anelia Angelova, Kris M Kitani, and Wei Hua. Attentionnas: Spatiotemporal attention cell search for video classiﬁcation. In ECCV, 2020. 7 [93] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In CVPR, 2018. 7 [94] Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In So Kweon. Cbam: Convolutional block attention module. In ECCV, 2018. 2 [95] Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via non-parametric instance discrimination. In CVPR, 2018. 3 [96] Saining Xie, Chen Sun, Jonathan Huang, Zhuowen Tu, and Kevin Murphy. Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classiﬁcation. In ECCV, 2018. 7 15 [97] Dejing Xu, Jun Xiao, Zhou Zhao, Jian Shao, Di Xie, and Yueting Zhuang. Self-supervised spatiotemporal learning via video clip order prediction. In CVPR, 2019. 3 [98] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large video description dataset for bridging video and language. In CVPR, 2016. 6, 17 [99] Fuzhi Yang, Huan Yang, Jianlong Fu, Hongtao Lu, and Baining Guo. Learning texture transformer network for image super-resolution. In CVPR, 2020. 3 [100] Mang Ye, Xu Zhang, Pong C Yuen, and Shih-Fu Chang. Unsupervised embedding learning via invariant and spreading instance feature. In CVPR, 2019. 3 [101] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. arXiv preprint arXiv:1710.09412, 2017. 19 [102] Richard Zhang, Phillip Isola, and Alexei A Efros. Colorful image colorization. In ECCV, 2016. 3 [103] Richard Zhang, Phillip Isola, and Alexei A Efros. Split-brain autoencoders: Unsupervised learning by cross-channel prediction. In CVPR, 2017. 3 [104] Xiang Zhang, Junbo Zhao, and Yann Lecun. Character-level convolutional networks for text classiﬁcation. NeurIPS, 2015. 1 [105] Y Zhang, K Li, K Li, B Zhong, and Y Fu. Residual non-local attention networks for image restoration. In ICLR, 2019. 2 [106] Hengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Exploring self-attention for image recogni- tion. In CVPR, 2020. 2 [107] Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip Torr, and Vladlen Koltun. Point transformer. arXiv preprint arXiv:2012.09164, 2020. 3 [108] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xiang, Philip HS Torr, et al. Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers. arXiv preprint arXiv:2012.15840, 2020. 3 [109] Luowei Zhou, Chenliang Xu, and Jason J Corso. Towards automatic learning of procedures from web instructional videos. In AAAI, 2018. 6, 17 16 A Appendix Appendix contains more detailed explanations about datasets (A.1) and the experimental setup (A.2) for both pre-training and downstream tasks. We also cover linear evaluation results compared to state-of-the-art (A.4) and an ablation study on the input parameters (A.5). A.1 Datasets A.1.1 Pre-training Following [1, 59], we use HowTo100M [58] and AudioSet [33] to pre-train VATT. The former contains 1.2M unique videos, each providing multiple clips with audio and narration scripts resulting in 136M video-audio-text triplets in total. The narration scripts are extracted from speech audio using an off-the-shelf ASR. We use a subset of HowTo100M to comply with Youtube’s policies, which results in having almost 1M unique videos and less than 100M clips. AudioSet consists of 10-second clips sampled from two million videos from YouTube. The dataset contains a variety of audio events with their corresponding video without any narration, so we do not have any text input from this dataset. We do not use any labels from the datasets. We uniformly sample clips from these datasets; a mini-batch in the pre-training contains samples from both datasets. In order to ﬁll in the empty text in AudioSet, we feed a sequence of zeros to the text Transformer and exclude those samples from the MIL-NCE loss. A.1.2 Downstream We evaluate the pre-trained VATT on a set of diverse, representative downstream tasks to test different aspects of the learned representations. Video action recognition: We evaluate the visual representations on UCF101 [81] (101 classes, 13,320 videos), HMDB51 [52] (51 classes, 6,766 videos), Kinetics-400 [14] (400 classes, 234,584 videos), Kinetics-600 [15] (600 classes, 366,016 videos), and Moments in Time [61] (339 classes, 791,297 videos). Since UCF101 and HMDB51 are small datasets compared to the size of our model, we freeze the vision backbone and use its outputs to train a linear classiﬁer. We use the split #1 results of the two datasets as a reference in our design exploration. For Kinetics-400, Kinetics-600, and Moments in Time, we ﬁne-tune our vision backbone initialized from the pre-trained checkpoint. Audio event classiﬁcation: We use ESC50 [66] (50 classes, 2000 audio clips) and AudioSet [33] (527 classes, ∼2M audio clips) to evaluate our audio Transformer on audio event classiﬁcation. We use ESC50 to train a linear classiﬁer on top of the frozen audio Transformer. We use the split #1 results of this dataset as a reference in our design exploration. We also use AudioSet to ﬁne-tune our audio backbone initialized from the pre-trained checkpoint. Zero-shot video retrieval: We evaluate the quality of our video-text common space represen- tations by zero-shot text-to-video retrieval on two of the most established datasets in this area: YouCook2 [109] and MSR-VTT [98] with 3.1k and 1k video-text pairs, respectively. We follow the same evaluation pipeline described in [1] and report the Recall at 10 (R@10). Image classiﬁcation: Although there exists a domain gap between images and the video datasets used for pre-training VATT, we test the learned vision Transformer in the image domain. We ﬁne- tune the last checkpoint of the vision Transformer on ImageNet [22] with no modiﬁcation to our architecture or the tokenization pipeline. We will elaborate on this in the sequel. A.2 Experimental Setup A.2.1 Inputs During pre-training, we sample 32 frames at 10 fps for both pre-training datasets. For these frames, we randomly crop a temporally consistent spatial region whose relative area is in the range of [0.08, 1] and its aspect ratio in [0.5, 2]. These crops are then resized to 224 × 224, followed by a horizontal ﬂip and color augmentation. The color augmentation follows [1] and randomizes brightness (max delta = 32/255), saturation (max delta = 0.4), contrast (max delta=0.4), and hue (max delta=0.2). We 17 clip values to ensure the RGB is in [0, 1]. The audio waveforms are sampled in sync with the video frames at 48kHz. Both video and audio inputs are normalized between [-1, 1] for numerical stability. We use patch sizes of 4 × 16 × 16 and 128 for video and raw waveform tokenization, respectively. We use one-hot vectors to encode text sequences with the vocabulary size of 216, which is the same as word2vec [60]. The resulting sequence retains a maximum of 16 words by either clipping or padding. We use DropToken with a drop rate of 50% during pre-training. For video ﬁne-tuning and evaluation, 32 frames with a temporal stride of 2 are sampled at 25 fps (2.56 seconds) with a crop size of 320 × 320 (with similar video augmentation during pre-training), and we do not drop any tokens. We do not change the input size for audio and text during evaluation. A.2.2 Network setup in VATT We use the same Transformer architecture described in the main paper with various sizes shown in Table 7. We use the Medium model for our modality-agnostic variant (VATT-MA). For the experiments with modality-speciﬁc Transformers, we use the Small and Base models for the text and audio modalities, respectively, while varying the model sizes for the video modality. This results in 3 variants for the modality-speciﬁc video-audio-text backbones: Base-Base-Small (BBS), Medium-Base-Small (MBS), and Large-Base-Small (LBS). Model Layers Hidden Size MLP Size Heads Params 6 Small Base 12 Medium 12 24 Large 512 768 1024 1024 2048 3072 4096 4096 8 12 16 16 20.9 M 87.9 M 155.0 M 306.1 M Table 7: Details of the Transformer architectures in VATT. A.2.3 Projection heads and contrastive losses We use dva = 512 and dvt = 256 for the projection to the common spaces Sva and Svt, respectively. We normalize the vectors before calculating the NCE and MIL-NCE objectives and use the tempera- ture of τ = 0.07 and the weight of λ = 1 in the loss deﬁned in the paper. We choose these values following the previously established practice [1]; we may achieve better results by varying these hyper-parameters. A.2.4 Pre-training setup We pre-train VATT from scratch using Adam [46] with an initial learning rate of 1e-4, 10k warmup steps, 500k steps in total, a batch size of 2048, and a quarter-period cosine schedule to anneal the learning rate from 1e-4 to 5e-5. In the exploration experiments, we use a batch size of 512 while keeping the rest of the training parameters the same. Our pipeline is implemented in Tensorﬂow (v2.4), and our models are trained for 3 days using 256 TPUs (v3). A.2.5 Video ﬁne-tuning setup For video action recognition, we use the SGD with a momentum of 0.9 and an initial learning rate of 0.005, 2.5k warmup steps, a batch size of 64, 100k steps in total, and a half-period cosine schedule to anneal the learning rate to 0. We use label smoothing with smoothing factor α = 0.1. The video frame resolution is 320 × 320, which results in an increase in the number of positional encoding weights. This increase is due to the fact that, in the pre-train time, we have 8+14+14 positional encoding buckets, while 8+20+20 positional buckets are required to completely encode 320/16 horizontal and 320/16 vertical locations in ﬁne-tune. To generate the new positional embeddings, we create a new set of positional encoding buckets by bi-cubic interpolation from the original buckets. After this step, we ﬁne-tune the entire network, including the positional encoding buckets, end-to-end. We tried ﬁxed positional embeddings (solely based on interpolation for the missing locations) and did not observe signiﬁcant improvements. We uniformly sample 4 clips to cover the entire 10 seconds of the video and apply a standard 3-crop evaluation following [30]. We average the logits across the resulting 12 views before having the ﬁnal class predictions. 18 A.3 Audio ﬁne-tuning setup For audio event classiﬁcation, we use the SGD with a momentum of 0.9, an initial learning rate of 0.2, 5k warmup steps, a batch size of 1024, 50k steps in total, and a half-period cosine schedule to anneal the learning rate to 0. We observe that increasing the effective receptive ﬁeld improves the overall performance. We suggest that this might be due to the fact that the AudioSet annotations are multi-label and each event might occur in different temporal positions. Hence, we employ the duration of 6.4s with 24kHz sampling rate (153.6k total input samples). Similar to [49], we use mixup [101] on input-label (x-y) pairs in a mini-batch as below: x = αx1 + (1 − α)x2, y = αy1 + (1 − α)y2, where the input-label pairs are randomly sampled from a mini-batch, and the mixing rate α is sampled from a Beta(5, 5) distribution. We also perform data balancing by penalizing the loss value of a sample with the inverse of the per-batch number of repetitive labels it carries. This is crucial for avoiding over-ﬁtting since AudioSet has a long-tailed distribution, and a few dominant classes may disrupt the training [49]. A.3.1 Image ﬁne-tuning setup We ﬁnetune the pre-trained VATT on ImageNet for 50 epochs with 384 × 384 input resolution, 512 batch size, SGD with momentum of 0.9, cosine learning rate decay with an initial learning rate of 8e-2, and label smoothing of 0.1. No weight decay is used. A.3.2 Linear evaluation setup We use a linear classiﬁer with ﬁxed backbones across all datasets and tasks. We observe that using matrix factorization on the classiﬁer weight [75] leads to a more stable result across experiments. More speciﬁcally, we use a factorized weight C = U V ∈ Rd×c, where U ∈ Rd×n and V ∈ Rn×c are learnable weights. During training this classiﬁer, we randomly choose a subset of the n components in U and V , hence leading to a low-rank classiﬁer weight, C. The classiﬁer weight, C, is trained using the Adam optimizer with a learning rate of 5e-4, a batch size of 64, a total of 50k training steps, and a sampling rate of 10% on its n = 128 components. A.3.3 Zero-shot retrieval setup For zero-shot text-to-video retrieval, we use the 1k split of MSR-VTT and the entire test split of YouCook2 as the pool for retrieval. We use 224 × 224 central crops for 32 frames with a temporal stride of 2 sampled at 25 fps. Since each input clip covers 2.56 seconds, and the full clip length is 10 seconds, we average the embeddings over 4 uniformly sampled clips before calculating the similarity with a text query’s embedding. We (cid:96)2-normalize each vector to assure that a dot product results in the cosine similarity. A.4 Linear evaluation on frozen VATT We also test VATT’s ability to generalize to other datasets when the entire backbone is frozen. In this setting, we focus on the video and audio modalities and train a linear classiﬁer on the outputs of the frozen backbones. In addition to the low-rank classiﬁer (LRC) described in Section A.2, we also report the results of a SVM classiﬁer following the same pipeline as [1]. Table 8 shows the performance of our model on three datasets. We observe that VATT does not outperform the best CNN counterparts in [1], and achieves comparable numbers to other baselines. This could suggest that VATT’s backbones learn less-linearly-separable feature, especially given that the contrastive estimation head includes non-linear projections. A.5 Ablation study on input parameters Since VATT takes raw multimodal signals as inputs, the choice of input size and how they are patched has a signiﬁcant impact on the ﬁnal performance. First, we alter the frame crop size and the number of sampled frames from each video clip while keeping the patch size ﬁxed to 5 × 16 × 16. Table 9 shows that using a small frame crop size and a larger number of frames hurts the video-related results, but it does not signiﬁcantly change the audio classiﬁcation numbers. 19 METHOD MIL-NCE [59] AVTS [50] XDC [2] ELo [67] AVID [80] GDT [65] MMV [1] VATT-Medium + SVM VATT-Medium + LRC VATT-MA-Medium + LRC UCF101 HMDB51 ESC50 83.4 - - - - - 91.8 89.2 89.6 84.4 54.8 - - 64.5 - - 67.1 63.3 65.2 63.1 - 82.3 84.8 - 89.2 88.5 88.9 82.5 84.7 81.2 Table 8: Linear evaluation results for video action recognition on UCF101 and HMDB51 and audio event classiﬁcation on ESC50. MA refers to the Modality-Agnostic backbone. Frame Size Patch Size UCF HMDB YC2 MSRVTT ESC 32×224×224 4×16×16 87.8 32×200×200 32×224×224 64×224×224 32×224×224 32×224×224 5×16×16 5×16×16 5×16×16 8×16×16 8×32×32 87.16 87.74 86.57 86.52 82.68 67.7 67.08 67.6 63.09 65.64 60.73 27.53 23.98 27.47 18.52 23.43 15.27 17.99 17.84 17.96 12.5 16.14 13.79 87 86.25 87 86.25 84 87 Table 9: Effect of video frame and patch size on downstream results. Then, we keep the best frame size (32 × 224 × 224) and vary the video patch size. We ﬁnd going beyond 4 × 16 × 16 along either the time or spatial dimensions is not helpful. We avoid patches that are smaller than 4 × 16 × 16 because of the signiﬁcantly increaseed wall clock time in experiments. Finally, we compare different audio patch sizes and perform an experiment using spectrograms, as opposed to the raw waveforms, as audio input. The goal is to see how the raw waveforms compare to the handcrafted spectrograms. We use the MEL spectrogram with 80 bins, the STFT length of 42 ms, and the STFT step of 21 ms following a similar setup in [1]. Tables 10 summarize the results, in which we observe that the patch size of 128 gives rise to the best waveform-based results, and using spectrogram does not lead to any conclusive improvement. The experiment with the spectrograms demonstrates that VATT is able to learn semantic representations from raw audios. To the best of our knowledge, this is the ﬁrst time that raw audio waveforms are used for multimodal self-supervised learning. Input Patch Size UCF HMDB YC2 MSRVTT ESC Waveform Waveform Waveform Waveform 128 256 512 1024 88.14 87.74 87.21 86.41 68.13 66.1 67.34 66.36 25.72 24.19 26.11 24.46 17.31 16.55 16.91 16.38 87.75 83.75 82.5 82.5 Spectrogram 16 × 5 88.3 67.52 26.62 16.86 88 Table 10: Effect of the audio input type and patch size on downstream results. 20 