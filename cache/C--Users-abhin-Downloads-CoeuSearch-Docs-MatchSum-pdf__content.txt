Extractive Summarization as Text Matching Ming Zhong∗, Pengfei Liu∗, Yiran Chen, Danqing Wang, Xipeng Qiu†, Xuanjing Huang Shanghai Key Laboratory of Intelligent Information Processing, Fudan University School of Computer Science, Fudan University 825 Zhangheng Road, Shanghai, China {mzhong18,pfliu14,yrchen19,dqwang18,xpqiu,xjhuang}@fudan.edu.cn 0 2 0 2 r p A 9 1 ] L C . s c [ 1 v 5 9 7 8 0 . 4 0 0 2 : v i X r a Abstract This paper creates a paradigm shift with regard to the way we build neural extractive summa- rization systems. Instead of following the com- monly used framework of extracting sentences individually and modeling the relationship be- tween sentences, we formulate the extractive summarization task as a semantic text match- ing problem, in which a source document and candidate summaries will be (extracted from the original text) matched in a semantic space. Notably, this paradigm shift to seman- tic matching framework is well-grounded in our comprehensive analysis of the inherent gap between sentence-level and summary-level ex- tractors based on the property of the dataset. Besides, even instantiating the framework with a simple form of a matching model, we have driven the state-of-the-art extractive re- sult on CNN/DailyMail to a new level (44.41 in ROUGE-1). Experiments on the other ﬁve datasets also show the effectiveness of the matching framework. We believe the power of this matching-based summarization frame- work has not been fully exploited. To encour- age more instantiations in the future, we have released our codes, processed dataset, as well as generated summaries in https://github. com/maszhongming/MatchSum. 1 Introduction The task of automatic text summarization aims to compress a textual document to a shorter highlight while keeping salient information on the original text. In this paper, we focus on extractive summa- rization since it usually generates semantically and grammatically correct sentences (Dong et al., 2018; Nallapati et al., 2017) and computes faster. Currently, most of the neural extractive summa- rization systems score and extract sentences (or smaller semantic unit (Xu et al., 2019)) one by ∗These two authors contributed equally. † Corresponding author. Figure 1: MATCHSUM framework. We match the con- textual representations of the document with gold sum- mary and candidate summaries (extracted from the doc- ument). Intuitively, better candidate summaries should be semantically closer to the document, while the gold summary should be the closest. one from the original text, model the relationship between the sentences, and then select several sen- tences to form a summary. Cheng and Lapata (2016); Nallapati et al. (2017) formulate the ex- tractive summarization task as a sequence label- ing problem and solve it with an encoder-decoder framework. These models make independent bi- nary decisions for each sentence, resulting in high redundancy. A natural way to address the above problem is to introduce an auto-regressive decoder (Chen and Bansal, 2018; Jadhav and Rajan, 2018; Zhou et al., 2018), allowing the scoring operations of different sentences to inﬂuence on each other. Trigram Blocking (Paulus et al., 2017; Liu and La- pata, 2019), as a more popular method recently, has the same motivation. At the stage of selecting sen- tences to form a summary, it will skip the sentence that has trigram overlapping with the previously se- lected sentences. Surprisingly, this simple method of removing duplication brings a remarkable per- formance improvement on CNN/DailyMail. The above systems of modeling the relationship between sentences are essentially sentence-level extractors, rather than considering the semantics DocumentCandidate SummaryGold SummaryextractSemantic SpaceBERTBERTBERT            of the entire summary. This makes them more inclined to select highly generalized sentences while ignoring the coupling of multiple sentences. Narayan et al. (2018b); Bae et al. (2019) utilize reinforcement learning (RL) to achieve summary- level scoring, but still limited to the architecture of sentence-level summarizers. To better understand the advantages and limi- tations of sentence-level and summary-level ap- proaches, we conduct an analysis on six benchmark datasets (in Section 3) to explore the characteristics of these two methods. We ﬁnd that there is indeed an inherent gap between the two approaches across these datasets, which motivates us to propose the following summary-level method. In this paper, we propose a novel summary-level framework (MATCHSUM, Figure 1) and conceptu- alize extractive summarization as a semantic text matching problem. The principle idea is that a good summary should be more semantically similar as a whole to the source document than the unqualiﬁed summaries. Semantic text matching is an important research problem to estimate semantic similarity between a source and a target text fragment, which has been applied in many ﬁelds, such as informa- tion retrieval (Mitra et al., 2017), question answer- ing (Yih et al., 2013; Severyn and Moschitti, 2015), natural language inference (Wang and Jiang, 2016; Wang et al., 2017) and so on. One of the most con- ventional approaches to semantic text matching is to learn a vector representation for each text frag- ment, and then apply typical similarity metrics to compute the matching scores. Speciﬁc to extractive summarization, we pro- pose a Siamese-BERT architecture to compute the similarity between the source document and the candidate summary. Siamese BERT leverages the pre-trained BERT (Devlin et al., 2019) in a Siamese network structure (Bromley et al., 1994; Hoffer and Ailon, 2015; Reimers and Gurevych, 2019) to de- rive semantically meaningful text embeddings that can be compared using cosine-similarity. A good summary has the highest similarity among a set of candidate summaries. We evaluate the proposed matching framework and perform signiﬁcance testing on a range of benchmark datasets. Our model outperforms strong baselines signiﬁcantly in all cases and improve the state-of-the-art extractive result on CNN/DailyMail. Besides, we design experiments to observe the gains brought by our framework. We summarize our contributions as follows: 1) Instead of scoring and extracting sentences one by one to form a summary, we formulate ex- tractive summarization as a semantic text match- ing problem and propose a novel summary-level framework. Our approach bypasses the difﬁculty of summary-level optimization by contrastive learn- ing, that is, a good summary should be more se- mantically similar to the source document than the unqualiﬁed summaries. 2) We conduct an analysis to investigate whether extractive models must do summary-level extrac- tion based on the property of dataset, and attempt to quantify the inherent gap between sentence-level and summary-level methods. 3) Our proposed framework has achieved supe- rior performance compared with strong baselines on six benchmark datasets. Notably, we obtain a state-of-the-art extractive result on CNN/DailyMail (44.41 in ROUGE-1) by only using the base version of BERT. Moreover, we seek to observe where the performance gain of our model comes from. 2 Related Work 2.1 Extractive Summarization Recent research work on extractive summarization spans a large range of approaches. These work usu- ally instantiate their encoder-decoder framework by choosing RNN (Zhou et al., 2018), Transformer (Zhong et al., 2019b; Wang et al., 2019) or GNN (Wang et al., 2020) as encoder, non-auto-regressive (Narayan et al., 2018b; Arumae and Liu, 2018) or auto-regressive decoders (Jadhav and Rajan, 2018; Liu and Lapata, 2019). Despite the effectiveness, these models are essentially sentence-level extrac- tors with individual scoring process favor the high- est scoring sentence, which probably is not the optimal one to form summary1. The application of RL provides a means of summary-level scoring and brings improvement (Narayan et al., 2018b; Bae et al., 2019). However, these efforts are still limited to auto-regressive or non-auto-regressive architectures. Besides, in the non-neural approaches, the Integer Linear Program- ming (ILP) method can also be used for summary- level scoring (Wan et al., 2015). In addition, there is some work to solve extrac- tive summarization from a semantic perspective be- fore this paper, such as concept coverage (Gillick 1We will quantify this phenomenon in Section 3. and Favre, 2009), reconstruction (Miao and Blun- som, 2016) and maximize semantic volume (Yo- gatama et al., 2015). 2.2 Two-stage Summarization Recent studies (Alyguliyev, 2009; Galanis and An- droutsopoulos, 2010; Zhang et al., 2019a) have attempted to build two-stage document summariza- tion systems. Speciﬁc to extractive summarization, the ﬁrst stage is usually to extract some fragments of the original text, and the second stage is to select or modify on the basis of these fragments. Chen and Bansal (2018) and Bae et al. (2019) follow a hybrid extract-then-rewrite architecture, with policy-based RL to bridge the two networks together. Lebanoff et al. (2019); Xu and Durrett (2019); Mendes et al. (2019) focus on the extract- then-compress learning paradigm, namely compres- sive summarization, which will ﬁrst train an extrac- tor for content selection. Our model can be viewed as an extract-then-match framework, which also employs a sentence extractor to prune unnecessary information. 3 Sentence-Level or Summary-Level? A Dataset-dependent Analysis Although previous work has pointed out the weak- ness of sentence-level extractors, there is no sys- tematic analysis towards the following questions: 1) For extractive summarization, is the summary- level extractor better than the sentence-level extrac- tor? 2) Given a dataset, which extractor should we choose based on the characteristics of the data, and what is the inherent gap between these two extractors? In this section, we investigate the gap between sentence-level and summary-level methods on six benchmark datasets, which can instruct us to search for an effective learning framework. It is worth not- ing that the sentence-level extractor we use here doesn’t include a redundancy removal process so that we can estimate the effect of the summary- level extractor on redundancy elimination. Notably, the analysis method to estimate the theoretical ef- fectiveness presented in this section is generalized and can be applicable to any summary-level ap- proach. 3.1 Deﬁnition We refer to D = {s1, · · · , sn} as a single document consisting of n sentences, and C = {s1, · · · , sk, |si ∈ D} as a candidate summary in- cluding k (k ≤ n) sentences extracted from a docu- ment. Given a document D with its gold summary C∗, we measure a candidate summary C by cal- culating the ROUGE (Lin and Hovy, 2003) value between C and C∗ in two levels: 1) Sentence-Level Score: gsen(C) = 1 |C| (cid:88) s∈C R(s, C∗), (1) where s is the sentence in C and |C| represents the number of sentences. R(·) denotes the average ROUGE score2. Thus, gsen(C) indicates the aver- age overlaps between each sentence in C and the gold summary C∗. 2) Summary-Level Score: gsum(C) = R(C, C∗), (2) where gsum(C) considers sentences in C as a whole and then calculates the ROUGE score with the gold summary C∗. Pearl-Summary We deﬁne the pearl-summary to be the summary that has a lower sentence-level score but a higher summary-level score. Deﬁnition 1 A candidate summary C is deﬁned as a pearl-summary if there exists another can- didate summary C(cid:48) that satisﬁes the inequality: gsen(C(cid:48)) > gsen(C) while gsum(C(cid:48)) < gsum(C). Clearly, if a candidate summary is a pearl-summary, it is challenging for sentence-level summarizers to extract it. Best-Summary The best-summary refers to a summary has highest summary-level score among all the candidate summaries. Deﬁnition 2 A summary ˆC is deﬁned as the best- summary when it satisﬁes: ˆC = argmax gsum(C), where C denotes all the candidate summaries of the document. C∈C 3.2 Ranking of Best-Summary For each document, we sort all candidate sum- maries3 in descending order based on the sentence- level score, and then deﬁne z as the rank index of the best-summary ˆC. 2Here we use mean F1 of ROUGE-1, ROUGE-2 and ROUGE-L. 3We use an approximate method here: take #Ext (see Table 1) of ten highest-scoring sentences to form candidate sum- maries. Datasets Source Reddit XSum CNN/DM WikiHow PubMed Multi-News Social Media News News Knowledge Base Scientiﬁc Paper News Type SDS SDS SDS SDS SDS MDS Train 41,675 203,028 287,084 168,126 83,233 44,972 # Pairs Valid 645 11,273 13,367 6,000 4,946 5,622 # Tokens Test 645 11,332 11,489 6,000 5,025 5,622 Doc. 482.2 430.2 766.1 580.8 444.0 487.3 Sum. 28.0 23.3 58.2 62.6 209.5 262.0 # Ext 2 2 3 4 6 9 Table 1: Datasets overview. SDS represents single-document summarization and MDS represents multi-document summarization. The data in Doc. and Sum. indicates the average length of document and summary in the test set respectively. # Ext denotes the number of sentences should extract in different datasets. Since the appearance of the pearl-summary will bring challenges to sentence-level extractors, we attempt to investigate the proportion of pearl- summary in different datasets on six benchmark datasets. A detailed description of these datasets is displayed in Table 1. As demonstrated in Figure 2, we can observe that for all datasets, most of the best-summaries are not made up of the highest-scoring sentences. Speciﬁ- cally, for CNN/DM, only 18.9% of best-summaries are not pearl-summary, indicating sentence-level extractors will easily fall into a local optimization, missing better candidate summaries. Different from CNN/DM, PubMed is most suit- able for sentence-level summarizers, because most of best-summary sets are not pearl-summary. Ad- ditionally, it is challenging to achieve good perfor- mance on WikiHow and Multi-News without a summary-level learning process, as these two datasets are most evenly distributed, that is, the appearance of pearl-summary makes the selection of the best-summary more complicated. In conclusion, the proportion of the pearl- summaries in all the best-summaries is a prop- erty to characterize a dataset, which will affect our choices of summarization extractors. 3.3 Inherent Gap between Sentence-Level and Summary-Level Extractors Above analysis has explicated that the summary- level method is better than the sentence-level method because it can pick out pearl-summaries, but how much improvement can it bring given a speciﬁc dataset? Based on the deﬁnition of Eq. (1) and (2), we can characterize the upper bound of the sentence- level and summary-level summarization systems for a document D as: (a) Reddit (b) XSum (c) CNN/DM (d) WikiHow (e) PubMed (f) Multi-News Figure 2: Distribution of z(%) on six datasets. Because the number of candidate summaries for each document is different (short text may have relatively few candi- dates), we use z / number of candidate summaries as the X-axis. The Y-axis represents the proportion of the best-summaries with this rank in the test set. Intuitively, 1) if z = 1 ( ˆC comes ﬁrst), it means that the best-summary is composed of sentences with the highest score; 2) If z > 1, then the best- summary is a pearl-summary. And as z increases ( ˆC gets lower rankings), we could ﬁnd more can- didate summaries whose sentence-level score is higher than best-summary, which leads to the learn- ing difﬁculty for sentence-level extractors. inherently unaware of pearl-summary, so obtain- ing the best-summary is difﬁcult. To better utilize the above characteristics of the data, we propose a summary-level framework which could score and extract a summary directly. Speciﬁcally, we formulate the extractive summa- rization task as a semantic text matching problem, in which a source document and candidate sum- maries will be (extracted from the original text) matched in a semantic space. The following section will detail how we instantiate our proposed match- ing summarization framework by using a simple siamese-based architecture. 4.1 Siamese-BERT Inspired by siamese network structure (Bromley et al., 1994), we construct a Siamese-BERT archi- tecture to match the document D and the candidate summary C. Our Siamese-BERT consists of two BERTs with tied-weights and a cosine-similarity layer during the inference phase. Unlike the modiﬁed BERT used in (Liu, 2019; Bae et al., 2019), we directly use the original BERT to derive the semantically meaningful embeddings from document D and candidate summary C since we need not obtain the sentence-level representa- tion. Thus, we use the vector of the ‘[CLS]’ token from the top BERT layer as the representation of a document or summary. Let rD and rC denote the embeddings of the document D and candidate summary C. Their similarity score is measured by f (D, C) = cosine(rD, rC). In order to ﬁne-tune Siamese-BERT, we use a margin-based triplet loss to update the weights. In- tuitively, the gold summary C∗ should be semanti- cally closest to the source document, which is the ﬁrst principle our loss should follow: L1 = max(0, f (D, C) − f (D, C∗) + γ1), (7) where C is the candidate summary in D and γ1 is a margin value. Besides, we also design a pairwise margin loss for all the candidate summaries. We sort all candidate summaries in descending order of ROUGE scores with the gold summary. Naturally, the candidate pair with a larger ranking gap should have a larger margin, which is the second principle to design our loss function: Figure 3: ∆(D) for different datasets. αsen(D) = max C∈CD αsum(D) = max C∈CD gsen(C), gsum(C), (3) (4) where CD is the set of candidate summaries ex- tracted from D. Then, we quantify the potential gain for a doc- ument D by calculating the difference between αsen(D) and αsum(D): ∆(D) = αsum(D) − αsen(D). (5) Finally, a dataset-level potential gain can be ob- tained as: ∆(D) = 1 |D| (cid:88) D∈D ∆(D), (6) where D represents a speciﬁc dataset and |D| is the number of documents in this dataset. We can see from Figure 3, the performance gain of the summary-level method varies with the dataset and has an improvement at a max- imum 4.7 on CNN/DM. From Figure 3 and Ta- ble 1, we can ﬁnd the performance gain is re- lated to the length of reference summary for dif- ferent datasets. In the case of short summaries (Reddit and XSum), the perfect identiﬁcation of pearl-summaries does not lead to much improve- ment. Similarly, multiple sentences in a long sum- mary (PubMed and Multi-News) already have a large degree of semantic overlap, making the improvement of the summary-level method rela- tively small. But for a medium-length summary (CNN/DM and WikiHow, about 60 words), the summary-level learning process is rewarding. We will discuss this performance gain with speciﬁc models in Section 5.4. 4 Summarization as Matching The above quantitative analysis suggests that for most of the datasets, sentence-level extractors are L2 = max(0, f (D, Cj) − f (D, Ci) (i < j), + (j − i) ∗ γ2) (8) RedditXSumCNN/DMWikiHowPubMedMulti-News012345∆(D)where Ci represents the candidate summary ranked i and γ2 is a hyperparameter used to distinguish be- tween good and bad candidate summaries. Finally, our margin-based triplet loss can be written as: Reddit XSum CNN/DM Wiki PubMed M-News Ext Sel Size 5 1, 2 15 5 1, 2 15 5 2, 3 20 5 3, 4, 5 16 7 6 7 10 9 9 L = L1 + L2. (9) The basic idea is to let the gold summary have the highest matching score, and at the same time, a bet- ter candidate summary should obtain a higher score compared with the unqualiﬁed candidate summary. Figure 1 illustrate this idea. In the inference phase, we formulate extractive summarization as a task to search for the best sum- mary among all the candidates C extracted from the document D. ˆC = arg max f (D, C). (10) C∈C 4.2 Candidates Pruning Curse of Combination The matching idea is more intuitive while it suffers from combinatorial explosion problems. For example, how could we determine the size of the candidate summary set or should we score all possible candidates? To allevi- ate these difﬁculties, we propose a simple candidate pruning strategy. ext|s(cid:48) i ∈ D}. Concretely, we introduce a content selection module to pre-select salient sentences. The mod- ule learns to assign each sentence a salience score and prunes sentences irrelevant with the current document, resulting in a pruned document D(cid:48) = {s(cid:48) 1, · · · , s(cid:48) Similar to much previous work on two-stage summarization, our content selection module is a parameterized neural network. In this paper, we use BERTSUM (Liu and Lapata, 2019) without tri- gram blocking (we call it BERTEXT) to score each sentence. Then, we use a simple rule to obtain the candidates: generating all combinations of sel sentences subject to the pruned document, and re- organize the order of sentences according to the original position in the document to form candidate summaries. Therefore, we have a total of (cid:0)ext (cid:1) sel candidate sets. 5 Experiment 5.1 Datasets In order to verify the effectiveness of our frame- work and obtain more convicing explanations, we perform experiments on six divergent mainstream datasets as follows. Table 2: Details about the candidate summary for dif- ferent datasets. Ext denotes the number of sentences after we prune the original document, Sel denotes the number of sentences to form a candidate summary and Size is the number of ﬁnal candidate summaries. CNN/DailyMail (Hermann et al., 2015) is a commonly used summarization dataset modiﬁed by Nallapati et al. (2016), which contains news ar- ticles and associated highlights as summaries. In this paper, we use the non-anonymized version. PubMed (Cohan et al., 2018) is collected from scientiﬁc papers and thus consists of long docu- ments. We modify this dataset by using the intro- duction section as the document and the abstract section as the corresponding summary. WikiHow (Koupaee and Wang, 2018) is a di- verse dataset extracted from an online knowledge base. Articles in it span a wide range of topics. XSum (Narayan et al., 2018a) is a one-sentence summary dataset to answer the question “What is the article about?”. All summaries are profession- ally written, typically by the authors of documents in this dataset. Multi-News (Fabbri et al., 2019) is a multi- document news summarization dataset with a rela- tively long summary, we use the truncated version and concatenate the source documents as a single input in all experiments. Reddit (Kim et al., 2019) is a highly abstractive dataset collected from social media platform. We only use the TIFU-long version of Reddit, which regards the body text of a post as the document and the TL;DR as the summary. 5.2 Implementation Details We use the base version of BERT to implement our models in all experiments. Adam optimizer (Kingma and Ba, 2014) with warming-up is used and our learning rate schedule follows Vaswani et al. (2017) as: lr = 2e−3 · min(step−0.5, step · wm−1.5), (11) where each step is a batch size of 32 and wm denotes warmup steps of 10,000. We choose γ1 = 0 and γ2 = 0.01. When γ1<0.05 and R-1 R-2 R-L Model R-1 R-2 R-L Model LEAD ORACLE MATCH-ORACLE 40.43 17.62 36.67 52.59 31.23 48.87 51.08 26.94 47.22 BANDITSUM (Dong et al., 2018) NEUSUM (Zhou et al., 2018) JECS (Xu and Durrett, 2019) HIBERT (Zhang et al., 2019b) PNBERT (Zhong et al., 2019a) PNBERT + RL BERTEXT BERTEXT BERTEXT (Liu, 2019) BERTEXT + Tri-Blocking BERTSUM 41.50 18.70 37.60 41.59 19.01 37.98 41.70 18.50 37.90 42.37 19.95 38.83 42.39 19.51 38.69 42.69 19.60 38.85 42.29 19.38 38.63 42.76 19.87 39.11 42.57 19.96 39.04 43.23 20.22 39.60 ∗ (Liu and Lapata, 2019) 43.85 20.34 39.90 † (Bae et al., 2019) † + RL Reddit BERTEXT (Num = 1) BERTEXT (Num = 2) MATCHSUM (Sel = 1) MATCHSUM (Sel = 2) MATCHSUM (Sel = 1, 2) 21.99 23.86 22.87 24.90 25.09 XSum BERTEXT (Num = 1) BERTEXT (Num = 2) MATCHSUM (Sel = 1) MATCHSUM (Sel = 2) MATCHSUM (Sel = 1, 2) 22.53 22.86 23.35 24.48 24.86 5.21 5.85 5.15 5.91 6.17 4.36 4.48 4.46 4.58 4.66 16.99 19.11 17.40 20.03 20.13 16.23 17.16 16.71 18.31 18.41 BERTEXT (Ours) BERTEXT + Tri-Blocking (Ours) MATCHSUM (BERT-base) MATCHSUM (RoBERTa-base) 42.73 20.13 39.20 43.18 20.16 39.56 44.22 20.62 40.38 44.41 20.86 40.55 Table 4: Results on test sets of Reddit and XSum. N um indicates how many sentences BERTEXT ex- tracts as a summary and Sel indicates the number of sentences we choose to form a candidate summary. Table 3: Results on CNN/DM test set. The model with ∗ indicates that the large version of BERT is used. BERTEXT† add an additional Pointer Network com- pared to other BERTEXT in this table. 0.005<γ2<0.05 they have little effect on perfor- mance, otherwise they will cause performance degradation. We use the validation set to save three best checkpoints during training, and record the performance of the best checkpoints on the test set. Importantly, all the experimental results listed in this paper are the average of three runs. To obtain a Siamese-BERT model on CNN/DM, we use 8 Tesla- V100-16G GPUs for about 30 hours of training. For datasets, we remove samples with empty document or summary and truncate the document to 512 tokens, therefore ORACLE in this paper is calculated on the truncated datasets. Details of candidate summary for the different datasets can be found in Table 2. 5.3 Experimental Results Results on CNN/DM As shown in Table 3, we list strong baselines with different learning ap- proaches. The ﬁrst section contains LEAD, OR- ACLE and MATCH-ORACLE4. Because we prune documents before matching, MATCH-ORACLE is relatively low. 4LEAD and ORACLE are common baselines in the sum- marization task. The former means extracting the ﬁrst sev- eral sentences of a document as a summary, the latter is the groundtruth used in extractive models training. MATCH- ORACLE is the groundtruth used to train MATCHSUM. We can see from the second section, although RL can score the entire summary, it does not lead to much performance improvement. This is prob- ably because it still relies on the sentence-level summarizers such as Pointer network or sequence labeling models, which select sentences one by one, rather than distinguishing the semantics of differ- ent summaries as a whole. Trigram Blocking is a simple yet effective heuristic on CNN/DM, even better than all redundancy removal methods based on neural models. Compared with these models, our proposed MATCHSUM has outperformed all competitors by a large margin. For example, it beats BERTEXT by 1.51 ROUGE-1 score when using BERT-base as the encoder. Additionally, even compared with the baseline with BERT-large pre-trained encoder, our model MATCHSUM (BERT-base) still perform better. Furthermore, when we change the encoder to RoBERTa-base (Liu et al., 2019), the perfor- mance can be further improved. We think the im- provement here is because RoBERTa introduced 63 million English news articles during pretraining. The superior performance on this dataset demon- strates the effectiveness of our proposed matching framework. Results on Datasets with Short Summaries Reddit and XSum have been heavily evaluated by abstractive summarizer due to their short sum- maries. Here, we evaluate our model on these two datasets to investigate whether MATCHSUM could achieve improvement when dealing with Model LEAD ORACLE MATCH-ORACLE BERTEXT + 3gram-Blocking + 4gram-Blocking MATCHSUM (BERT-base) WikiHow R-2 5.83 12.98 10.55 8.71 8.45 8.67 8.98 R-L 23.24 32.68 32.87 28.24 28.28 28.32 29.58 R-1 24.97 35.59 35.22 30.31 30.37 30.40 31.85 PubMed R-2 12.22 20.33 15.42 14.88 13.62 14.37 14.91 R-1 37.58 45.12 42.21 41.05 38.81 40.29 41.21 R-L 33.44 40.19 37.67 36.57 34.52 35.88 36.75 R-1 43.08 49.06 47.45 45.80 44.94 45.86 46.20 Multi-News R-2 14.27 21.54 17.41 16.42 15.47 16.23 16.51 R-L 38.97 44.27 43.14 41.53 40.63 41.57 41.89 Table 5: Results on test sets of WikiHow, PubMed and Multi-News. MATCHSUM beats the state-of-the-art BERT model with Ngram Blocking on all different domain datasets. summaries containing fewer sentences compared with other typical extractive models. When taking just one sentence to match the orig- inal document, MATCHSUM degenerates into a re-ranking of sentences. Table 4 illustrates that this degradation can still bring a small improve- ment (compared to BERTEXT (Num = 1), 0.88 ∆R-1 on Reddit, 0.82 ∆R-1 on XSum). How- ever, when the number of sentences increases to two and summary-level semantics need to be taken into account, MATCHSUM can obtain a more re- markable improvement (compared to BERTEXT (Num = 2), 1.04 ∆R-1 on Reddit, 1.62 ∆R-1 on XSum). In addition, our model maps candidate summary as a whole into semantic space, so it can ﬂexibly choose any number of sentences, while most other methods can only extract a ﬁxed number of sen- tences. From Table 4, we can see this advantage leads to further performance improvement. Results on Datasets with Long Summaries When the summary is relatively long, summary- level matching becomes more complicated and is harder to learn. We aim to compare the difference between Trigram Blocking and our model when dealing with long summaries. Table 5 presents that although Trigram Blocking works well on CNN/DM, it does not always main- tain a stable improvement. Ngram Blocking has little effect on WikiHow and Multi-News, and it causes a large performance drop on PubMed. We think the reason is that Ngram Blocking can- not really understand the semantics of sentences or summaries, just restricts the presence of entities with many words to only once, which is obviously not suitable for the scientiﬁc domain where entities may often appear multiple times. On the contrary, our proposed method does not have these strong constraints, but aligns the original document with the summary from semantic space. Experiment results display that our model is robust on all domains, especially on WikiHow, MATCH- SUM beats the state-of-the-art BERT model by 1.54 ROUGE-1 score. 5.4 Analysis In the following, our analysis is driven by two ques- tions: 1) Whether the beneﬁts of MATCHSUM are con- sistent with the property of the dataset analyzed in Section 3? 2) Why have our model achieved different per- formance gains on diverse datasets? Dataset Splitting Testing Typically, we choose three datasets (XSum, CNN/DM and WikiHow) with the largest performance gain for this exper- iment. We split each test set into roughly equal numbers of ﬁve parts according to z described in Section 3.2, and then experiment with each subset. Figure 4 shows that the performance gap be- tween MATCHSUM and BERTEXT is always the smallest when the best-summary is not a pearl- summary (z = 1). The phenomenon is in line with our understanding, in these samples, the ability of the summary-level extractor to discover pearl- summaries does not bring advantages. As z increases, the performance gap gener- ally tends to increase. Speciﬁcally, the beneﬁt of MATCHSUM on CNN/DM is highly consistent with the appearance of pearl-summary. It can only bring an improvement of 0.49 in the subset with the smallest z, but it rises sharply to 1.57 when z reaches its maximum value. WikiHow is similar to CNN/DM, when best-summary consists entirely of highest-scoring sentences, the performance gap is obviously smaller than in other samples. XSum (a) XSum (b) CNN/DM (c) WikiHow Figure 4: Datasets splitting experiment. We split test sets into ﬁve parts according to z described in Section 3.2. The X-axis from left to right indicates the subsets of the test set with the value of z from small to large, and the Y-axis represents the ROUGE improvement of MATCHSUM over BERTEXT on this subset. BERTEXT on dataset D. Moreover, compared with the inherent gap between sentence-level and summary-level extractors, we deﬁne the ratio that MATCHSUM can learn on dataset D as: ψ(D) = ∆(D)∗/∆(D), (14) where ∆(D) is the inherent gap between sentence- level and summary-level extractos. It is clear from Figure 5, the value of ψ(D) de- pends on z (see Figure 2) and the length of the gold summary (see Table 1). As the gold summaries get longer, the upper bound of summary-level ap- proaches becomes more difﬁcult for our model to reach. MATCHSUM can achieve 0.64 ψ(D) on XSum (23.3 words summary), however, ψ(D) is less than 0.2 in PubMed and Multi-News whose summary length exceeds 200. From another per- spective, when the summary length are similar, our model performs better on datasets with more pearl- summaries. For instance, z is evenly distributed in Multi-News (see Figure 2), so higher ψ(D) (0.18) can be obtained than PubMed (0.09), which has the least pearl-summaries. A better understanding of the dataset allows us to get a clear awareness of the strengths and lim- itations of our framework, and we also hope that the above analysis could provide useful clues for future research on extractive summarization. 6 Conclusion We formulate the extractive summarization task as a semantic text matching problem and propose a novel summary-level framework to match the source document and candidate summaries in the semantic space. We conduct an analysis to show how our model could better ﬁt the characteristic of the data. Experimental results show MATCHSUM Figure 5: ψ of different datasets. Reddit is excluded because it has too few samples in the test set. is slightly different, although the trend remains the same, our model does not perform well in the samples with the largest z, which needs further improvement and exploration. From the above comparison, we can see that the performance improvement of MATCHSUM is concentrated in the samples with more pearl- summaries, which illustrates our semantic-based summary-level model can capture sentences that are not particularly good when viewed individually, thereby forming a better summary. Intuitively, Comparison Across Datasets im- provements brought by MATCHSUM framework should be associated with inherent gaps presented in Section 3.3. To better understand their relation, we introduce ∆(D)∗ as follows: ∆(D)∗ = gsum(CM S) − gsum(CBE), ∆(D)∗ = 1 |D| (cid:88) D∈D ∆(D)∗, (12) (13) where CM S and CBE represent the candidate sum- mary selected by MATCHSUM and BERTEXT in the document D, respectively. Therefore, ∆(D)∗ can indicate the improvement by MATCHSUM over 123451.051.11.151.21.251.3z:Small=⇒Large∆R123450.40.60.811.21.41.6z:Small=⇒Large∆R123450.811.2z:Small=⇒Large∆RXSumCNN/DMWikiHowPubMedMulti-News00.10.20.30.40.50.60.7ψ(D)outperforms the current state-of-the-art extractive model on six benchmark datasets, which demon- strates the effectiveness of our method. We believe the power of this matching-based summarization In the framework has not been fully exploited. future, more forms of matching models can be ex- plored to instantiated the proposed framework. Acknowledgment We would like to thank the anonymous reviewers for their valuable comments. This work is sup- ported by the National Key Research and Develop- ment Program of China (No. 2018YFC0831103), National Natural Science Foundation of China (No. U1936214 and 61672162), Shanghai Mu- nicipal Science and Technology Major Project (No. 2018SHZDZX01) and ZJLab. References RM Alyguliyev. 2009. The two-stage unsupervised ap- proach to multidocument summarization. Automatic Control and Computer Sciences, 43(5):276. Kristjan Arumae and Fei Liu. 2018. Reinforced extrac- tive summarization with question-focused rewards. In Proceedings of ACL 2018, Student Research Workshop, pages 105–111. Sanghwan Bae, Taeuk Kim, Jihoon Kim, and Sang- goo Lee. 2019. Summary level training of sentence rewriting for abstractive summarization. In Proceed- ings of the 2nd Workshop on New Frontiers in Sum- marization, pages 10–20. Jane Bromley, Isabelle Guyon, Yann LeCun, Eduard S¨ackinger, and Roopak Shah. 1994. Signature veriﬁ- cation using a” siamese” time delay neural network. In Advances in neural information processing sys- tems, pages 737–744. Yen-Chun Chen and Mohit Bansal. 2018. Fast abstrac- tive summarization with reinforce-selected sentence rewriting. In Proceedings of the 56th Annual Meet- ing of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pages 675–686. Jianpeng Cheng and Mirella Lapata. 2016. Neural sum- In marization by extracting sentences and words. Proceedings of the 54th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1: Long Papers), volume 1, pages 484–494. Arman Cohan, Franck Dernoncourt, Doo Soon Kim, Trung Bui, Seokhwan Kim, Walter Chang, and Nazli Goharian. 2018. A discourse-aware attention model for abstractive summarization of long documents. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies, Volume 2 (Short Papers), volume 2, pages 615–621. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language under- standing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies, Volume 1 (Long and Short Papers), pages 4171–4186. Yue Dong, Yikang Shen, Eric Crawford, Herke van Hoof, and Jackie Chi Kit Cheung. 2018. Bandit- sum: Extractive summarization as a contextual ban- dit. In Proceedings of the 2018 Conference on Em- pirical Methods in Natural Language Processing, pages 3739–3748. Alexander Richard Fabbri, Irene Li, Tianwei She, Suyi Li, and Dragomir R. Radev. 2019. Multi-news: A large-scale multi-document summarization dataset In ACL (1), and abstractive hierarchical model. pages 1074–1084. Association for Computational Linguistics. Dimitrios Galanis and Ion Androutsopoulos. 2010. An extractive supervised two-stage method for sentence In Human Language Technologies: compression. The 2010 Annual Conference of the North American Chapter of the Association for Computational Lin- guistics, pages 885–893. Association for Computa- tional Linguistics. Dan Gillick and Benoit Favre. 2009. A scalable global In Proceedings of the model for summarization. Workshop on Integer Linear Programming for Nat- ural Language Processing, pages 10–18. Karl Moritz Hermann, Tomas Kocisky, Edward Grefen- stette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching machines to read In Advances in Neural Informa- and comprehend. tion Processing Systems, pages 1684–1692. Elad Hoffer and Nir Ailon. 2015. Deep metric learning using triplet network. In International Workshop on Similarity-Based Pattern Recognition, pages 84–92. Springer. Aishwarya Jadhav and Vaibhav Rajan. 2018. Extrac- tive summarization with swap-net: Sentences and In Pro- words from alternating pointer networks. ceedings of the 56th Annual Meeting of the Associa- tion for Computational Linguistics (Volume 1: Long Papers), volume 1, pages 142–151. Byeongchang Kim, Hyunwoo Kim, and Gunhee Kim. 2019. Abstractive summarization of reddit posts In Proceed- with multi-level memory networks. ings of the 2019 Conference of the North American Chapter of the Association for Computational Lin- guistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2519–2531. Diederik Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980. Mahnaz Koupaee and William Yang Wang. 2018. Wik- ihow: A large scale text summarization dataset. arXiv preprint arXiv:1810.09305. Logan Lebanoff, Kaiqiang Song, Franck Dernoncourt, Doo Soon Kim, Seokhwan Kim, Walter Chang, and Scoring sentence singletons and Fei Liu. 2019. pairs for abstractive summarization. arXiv preprint arXiv:1906.00077. Chin-Yew Lin and Eduard Hovy. 2003. Auto- matic evaluation of summaries using n-gram co- occurrence statistics. In Proceedings of the 2003 Hu- man Language Technology Conference of the North American Chapter of the Association for Computa- tional Linguistics, pages 150–157. Yang Liu. 2019. Fine-tune bert for extractive summa- rization. arXiv preprint arXiv:1903.10318. Yang Liu and Mirella Lapata. 2019. Text summariza- In Proceedings of tion with pretrained encoders. the 2019 Conference on Empirical Methods in Nat- ural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3721–3731. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man- dar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining ap- proach. arXiv preprint arXiv:1907.11692. Alfonso Mendes, Shashi Narayan, Sebasti˜ao Miranda, Zita Marinho, Andr´e FT Martins, and Shay B Co- hen. 2019. Jointly extracting and compressing doc- uments with summary state representations. In Pro- ceedings of the 2019 Conference of the North Amer- ican Chapter of the Association for Computational Linguistics: Human Language Technologies, Vol- ume 1 (Long and Short Papers), pages 3955–3966. Yishu Miao and Phil Blunsom. 2016. Language as a latent variable: Discrete generative models for sen- tence compression. In Proceedings of the 2016 Con- ference on Empirical Methods in Natural Language Processing, pages 319–328. Bhaskar Mitra, Fernando Diaz, and Nick Craswell. 2017. Learning to match using local and distributed representations of text for web search. In Proceed- ings of the 26th International Conference on World Wide Web, pages 1291–1299. International World Wide Web Conferences Steering Committee. Ramesh Nallapati, Feifei Zhai, and Bowen Zhou. 2017. Summarunner: A recurrent neural network based se- quence model for extractive summarization of docu- ments. In Thirty-First AAAI Conference on Artiﬁcial Intelligence. Ramesh Nallapati, Bowen Zhou, Cicero dos Santos, C¸ a glar Gulc¸ehre, and Bing Xiang. 2016. Abstrac- tive text summarization using sequence-to-sequence rnns and beyond. CoNLL 2016, page 280. Shashi Narayan, Shay B Cohen, and Mirella Lapata. 2018a. Dont give me the details, just the summary! topic-aware convolutional neural networks for ex- In Proceedings of the 2018 treme summarization. Conference on Empirical Methods in Natural Lan- guage Processing, pages 1797–1807. Shashi Narayan, Shay B Cohen, and Mirella Lapata. 2018b. Ranking sentences for extractive summariza- tion with reinforcement learning. In Proceedings of the 2018 Conference of the North American Chap- ter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Pa- pers), volume 1, pages 1747–1759. Romain Paulus, Caiming Xiong, and Richard Socher. 2017. A deep reinforced model for abstractive sum- marization. arXiv preprint arXiv:1705.04304. Nils Reimers and Iryna Gurevych. 2019. Sentence- bert: Sentence embeddings using siamese bert- networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu- ral Language Processing (EMNLP-IJCNLP), pages 3973–3983. Aliaksei Severyn and Alessandro Moschitti. 2015. Learning to rank short text pairs with convolutional deep neural networks. In Proceedings of the 38th in- ternational ACM SIGIR conference on research and development in information retrieval, pages 373– 382. ACM. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Pro- cessing Systems, pages 5998–6008. Xiaojun Wan, Ziqiang Cao, Furu Wei, Sujian Li, and Ming Zhou. 2015. Multi-document summariza- tion via discriminative summary reranking. arXiv preprint arXiv:1507.02062. Danqing Wang, Pengfei Liu, Yining Zheng, Xipeng Qiu, and Xuan-Jing Huang. 2020. Heterogeneous graph neural networks for extractive document sum- marization. In Proceedings of the 58th Conference of the Association for Computational Linguistics. Danqing Wang, Pengfei Liu, Ming Zhong, Jie Fu, Xipeng Qiu, and Xuanjing Huang. 2019. Exploring domain shift in extractive text summarization. arXiv preprint arXiv:1908.11664. Shuohang Wang and Jing Jiang. 2016. Learning natu- ral language inference with lstm. In Proceedings of the 2016 Conference of the North American Chap- ter of the Association for Computational Linguistics: Human Language Technologies, pages 1442–1451. Zhiguo Wang, Wael Hamza, and Radu Florian. 2017. Bilateral multi-perspective matching for natural lan- guage sentences. In Proceedings of the 26th Inter- national Joint Conference on Artiﬁcial Intelligence, pages 4144–4150. AAAI Press. Jiacheng Xu and Greg Durrett. 2019. Neural extrac- tive text summarization with syntactic compression. In Proceedings of the 2019 Conference on Empiri- cal Methods in Natural Language Processing, Hong Kong, China. Association for Computational Lin- guistics. Jiacheng Xu, Zhe Gan, Yu Cheng, and Jingjing Discourse-aware neural extractive arXiv preprint Liu. 2019. model for text summarization. arXiv:1910.14142. Wen-tau Yih, Ming-Wei Chang, Christopher Meek, and Andrzej Pastusiak. 2013. Question answering using enhanced lexical semantic models. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1744–1753. Dani Yogatama, Fei Liu, and Noah A Smith. 2015. Ex- tractive summarization by maximizing semantic vol- In Proceedings of the 2015 Conference on ume. Empirical Methods in Natural Language Processing, pages 1961–1966. Haoyu Zhang, Yeyun Gong, Yu Yan, Nan Duan, Jian- jun Xu, Ji Wang, Ming Gong, and Ming Zhou. language gen- 2019a. arXiv preprint eration for text summarization. arXiv:1902.09243. Pretraining-based natural Xingxing Zhang, Furu Wei, and Ming Zhou. 2019b. Hibert: Document level pre-training of hierarchical bidirectional transformers for document summariza- tion. In ACL. Ming Zhong, Pengfei Liu, Danqing Wang, Xipeng Qiu, and Xuan-Jing Huang. 2019a. Searching for effec- tive neural extractive summarization: What works and whats next. In Proceedings of the 57th Confer- ence of the Association for Computational Linguis- tics, pages 1049–1058. Ming Zhong, Danqing Wang, Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. 2019b. A closer look at data bias in neural extractive summarization models. EMNLP-IJCNLP 2019, page 80. Qingyu Zhou, Nan Yang, Furu Wei, Shaohan Huang, Ming Zhou, and Tiejun Zhao. 2018. Neural docu- ment summarization by jointly learning to score and select sentences. In Proceedings of the 56th Annual Meeting of the Association for Computational Lin- guistics (Volume 1: Long Papers), volume 1, pages 654–663. 