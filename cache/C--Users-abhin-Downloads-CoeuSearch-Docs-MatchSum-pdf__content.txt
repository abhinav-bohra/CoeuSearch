Extractive Summarization Text Matching Ming Zhong∗ , Pengfei Liu∗ , Yiran Chen , Danqing Wang , Xipeng Qiu† , Xuanjing Huang Shanghai Key Laboratory Intelligent Information Processing , Fudan University School Computer Science , Fudan University     Zhangheng Road , Shanghai , China { mzhong   , pfliu   , yrchen   , dqwang   , xpqiu , xjhuang } @ fudan.edu.cn         r p      ] L C .  c [   v           .         : v  X r  Abstract paper creates paradigm shift regard way build neural extractive summa- rization systems . Instead following com- monly used framework extracting sentences individually modeling relationship be- tween sentences , formulate extractive summarization task semantic text match- ing problem , source document candidate summaries ( extracted original text ) matched semantic space . Notably , paradigm shift seman- tic matching framework well-grounded comprehensive analysis inherent gap sentence-level summary-level ex- tractors based property dataset . Besides , even instantiating framework simple form matching model , driven state-of-the-art extractive re- sult CNN/DailyMail new level (   .   ROUGE-  ) . Experiments ﬁve datasets also show effectiveness matching framework . believe power matching-based summarization frame- work fully exploited . encour- age instantiations future , released codes , processed dataset , well generated summaries https : //github . com/maszhongming/MatchSum .   Introduction task automatic text summarization aims compress textual document shorter highlight keeping salient information original text . paper , focus extractive summa- rization since usually generates semantically grammatically correct sentences ( Dong et al. ,      ; Nallapati et al. ,      ) computes faster . Currently , neural extractive summa- rization systems score extract sentences ( smaller semantic unit ( Xu et al. ,      ) ) one ∗These two authors contributed equally . † Corresponding author . Figure   : MATCHSUM framework . match con- textual representations document gold sum- mary candidate summaries ( extracted doc- ument ) . Intuitively , better candidate summaries semantically closer document , gold summary closest . one original text , model relationship sentences , select several sen- tences form summary . Cheng Lapata (      ) ; Nallapati et al . (      ) formulate ex- tractive summarization task sequence label- ing problem solve encoder-decoder framework . models make independent bi- nary decisions sentence , resulting high redundancy . natural way address problem introduce auto-regressive decoder ( Chen Bansal ,      ; Jadhav Rajan ,      ; Zhou et al. ,      ) , allowing scoring operations different sentences inﬂuence . Trigram Blocking ( Paulus et al. ,      ; Liu La- pata ,      ) , popular method recently , motivation . stage selecting sen- tences form summary , skip sentence trigram overlapping previously se- lected sentences . Surprisingly , simple method removing duplication brings remarkable per- formance improvement CNN/DailyMail . systems modeling relationship sentences essentially sentence-level extractors , rather considering semantics DocumentCandidate SummaryGold SummaryextractSemantic SpaceBERTBERTBERT      entire summary . makes inclined select highly generalized sentences ignoring coupling multiple sentences . Narayan et al . (     b ) ; Bae et al . (      ) utilize reinforcement learning ( RL ) achieve summary- level scoring , still limited architecture sentence-level summarizers . better understand advantages limi- tations sentence-level summary-level ap- proaches , conduct analysis six benchmark datasets ( Section   ) explore characteristics two methods . ﬁnd indeed inherent gap two approaches across datasets , motivates us propose following summary-level method . paper , propose novel summary-level framework ( MATCHSUM , Figure   ) conceptu- alize extractive summarization semantic text matching problem . principle idea good summary semantically similar whole source document unqualiﬁed summaries . Semantic text matching important research problem estimate semantic similarity source target text fragment , applied many ﬁelds , informa- tion retrieval ( Mitra et al. ,      ) , question answer- ing ( Yih et al. ,      ; Severyn Moschitti ,      ) , natural language inference ( Wang Jiang ,      ; Wang et al. ,      ) . One con- ventional approaches semantic text matching learn vector representation text frag- ment , apply typical similarity metrics compute matching scores . Speciﬁc extractive summarization , pro- pose Siamese-BERT architecture compute similarity source document candidate summary . Siamese BERT leverages pre-trained BERT ( Devlin et al. ,      ) Siamese network structure ( Bromley et al. ,      ; Hoffer Ailon ,      ; Reimers Gurevych ,      ) de- rive semantically meaningful text embeddings compared using cosine-similarity . good summary highest similarity among set candidate summaries . evaluate proposed matching framework perform signiﬁcance testing range benchmark datasets . model outperforms strong baselines signiﬁcantly cases improve state-of-the-art extractive result CNN/DailyMail . Besides , design experiments observe gains brought framework . summarize contributions follows :   ) Instead scoring extracting sentences one one form summary , formulate ex- tractive summarization semantic text match- ing problem propose novel summary-level framework . approach bypasses difﬁculty summary-level optimization contrastive learn- ing , , good summary se- mantically similar source document unqualiﬁed summaries .   ) conduct analysis investigate whether extractive models must summary-level extrac- tion based property dataset , attempt quantify inherent gap sentence-level summary-level methods .   ) proposed framework achieved supe- rior performance compared strong baselines six benchmark datasets . Notably , obtain state-of-the-art extractive result CNN/DailyMail (   .   ROUGE-  ) using base version BERT . Moreover , seek observe performance gain model comes .   Related Work  .  Extractive Summarization Recent research work extractive summarization spans large range approaches . work usu- ally instantiate encoder-decoder framework choosing RNN ( Zhou et al. ,      ) , Transformer ( Zhong et al. ,     b ; Wang et al. ,      ) GNN ( Wang et al. ,      ) encoder , non-auto-regressive ( Narayan et al. ,     b ; Arumae Liu ,      ) auto-regressive decoders ( Jadhav Rajan ,      ; Liu Lapata ,      ) . Despite effectiveness , models essentially sentence-level extrac- tors individual scoring process favor high- est scoring sentence , probably optimal one form summary  . application RL provides means summary-level scoring brings improvement ( Narayan et al. ,     b ; Bae et al. ,      ) . However , efforts still limited auto-regressive non-auto-regressive architectures . Besides , non-neural approaches , Integer Linear Program- ming ( ILP ) method also used summary- level scoring ( Wan et al. ,      ) . addition , work solve extrac- tive summarization semantic perspective be- fore paper , concept coverage ( Gillick  We quantify phenomenon Section   . Favre ,      ) , reconstruction ( Miao Blun- som ,      ) maximize semantic volume ( Yo- gatama et al. ,      ) .  .  Two-stage Summarization Recent studies ( Alyguliyev ,      ; Galanis An- droutsopoulos ,      ; Zhang et al. ,     a ) attempted build two-stage document summariza- tion systems . Speciﬁc extractive summarization , ﬁrst stage usually extract fragments original text , second stage select modify basis fragments . Chen Bansal (      ) Bae et al . (      ) follow hybrid extract-then-rewrite architecture , policy-based RL bridge two networks together . Lebanoff et al . (      ) ; Xu Durrett (      ) ; Mendes et al . (      ) focus extract- then-compress learning paradigm , namely compres- sive summarization , ﬁrst train extrac- tor content selection . model viewed extract-then-match framework , also employs sentence extractor prune unnecessary information .   Sentence-Level Summary-Level ? Dataset-dependent Analysis Although previous work pointed weak- ness sentence-level extractors , sys- tematic analysis towards following questions :   ) extractive summarization , summary- level extractor better sentence-level extrac- tor ?   ) Given dataset , extractor choose based characteristics data , inherent gap two extractors ? section , investigate gap sentence-level summary-level methods six benchmark datasets , instruct us search effective learning framework . worth not- ing sentence-level extractor use ’ include redundancy removal process estimate effect summary- level extractor redundancy elimination . Notably , analysis method estimate theoretical ef- fectiveness presented section generalized applicable summary-level ap- proach .  .  Deﬁnition refer = { s  , · · · , sn } single document consisting n sentences , C = { s  , · · · , sk , |si ∈ } candidate summary in- cluding k ( k ≤ n ) sentences extracted docu- ment . Given document gold summary C∗ , measure candidate summary C cal- culating ROUGE ( Lin Hovy ,      ) value C C∗ two levels :   ) Sentence-Level Score : gsen ( C ) =   |C| ( cid:   ) s∈C R ( , C∗ ) , (   ) sentence C |C| represents number sentences . R ( · ) denotes average ROUGE score  . Thus , gsen ( C ) indicates aver- age overlaps sentence C gold summary C∗ .   ) Summary-Level Score : gsum ( C ) = R ( C , C∗ ) , (   ) gsum ( C ) considers sentences C whole calculates ROUGE score gold summary C∗ . Pearl-Summary deﬁne pearl-summary summary lower sentence-level score higher summary-level score . Deﬁnition   candidate summary C deﬁned pearl-summary exists another can- didate summary C ( cid:   ) satisﬁes inequality : gsen ( C ( cid:   ) ) > gsen ( C ) gsum ( C ( cid:   ) ) < gsum ( C ) . Clearly , candidate summary pearl-summary , challenging sentence-level summarizers extract . Best-Summary best-summary refers summary highest summary-level score among candidate summaries . Deﬁnition   summary ˆC deﬁned best- summary satisﬁes : ˆC = argmax gsum ( C ) , C denotes candidate summaries document . C∈C  .  Ranking Best-Summary document , sort candidate sum- maries  descending order based sentence- level score , deﬁne z rank index best-summary ˆC .  Here use mean F  ROUGE-  , ROUGE-  ROUGE-L .  We use approximate method : take # Ext ( see Table   ) ten highest-scoring sentences form candidate sum- maries . Datasets Source Reddit XSum CNN/DM WikiHow PubMed Multi-News Social Media News News Knowledge Base Scientiﬁc Paper News Type SDS SDS SDS SDS SDS MDS Train   ,       ,       ,       ,      ,      ,    # Pairs Valid       ,      ,     ,     ,     ,    # Tokens Test       ,      ,     ,     ,     ,    Doc .    .     .     .     .     .     .  Sum .   .    .    .    .     .     .  # Ext             Table   : Datasets overview . SDS represents single-document summarization MDS represents multi-document summarization . data Doc . Sum . indicates average length document summary test set respectively . # Ext denotes number sentences extract different datasets . Since appearance pearl-summary bring challenges sentence-level extractors , attempt investigate proportion pearl- summary different datasets six benchmark datasets . detailed description datasets displayed Table   . demonstrated Figure   , observe datasets , best-summaries made highest-scoring sentences . Speciﬁ- cally , CNN/DM ,   .  % best-summaries pearl-summary , indicating sentence-level extractors easily fall local optimization , missing better candidate summaries . Different CNN/DM , PubMed suit- able sentence-level summarizers , best-summary sets pearl-summary . Ad- ditionally , challenging achieve good perfor- mance WikiHow Multi-News without summary-level learning process , two datasets evenly distributed , , appearance pearl-summary makes selection best-summary complicated . conclusion , proportion pearl- summaries best-summaries prop- erty characterize dataset , affect choices summarization extractors .  .  Inherent Gap Sentence-Level Summary-Level Extractors analysis explicated summary- level method better sentence-level method pick pearl-summaries , much improvement bring given speciﬁc dataset ? Based deﬁnition Eq . (   ) (   ) , characterize upper bound sentence- level summary-level summarization systems document : ( ) Reddit ( b ) XSum ( c ) CNN/DM ( ) WikiHow ( e ) PubMed ( f ) Multi-News Figure   : Distribution z ( % ) six datasets . number candidate summaries document different ( short text may relatively candi- dates ) , use z / number candidate summaries X-axis . Y-axis represents proportion best-summaries rank test set . Intuitively ,   ) z =   ( ˆC comes ﬁrst ) , means best-summary composed sentences highest score ;   ) z >   , best- summary pearl-summary . z increases ( ˆC gets lower rankings ) , could ﬁnd can- didate summaries whose sentence-level score higher best-summary , leads learn- ing difﬁculty sentence-level extractors . inherently unaware pearl-summary , obtain- ing best-summary difﬁcult . better utilize characteristics data , propose summary-level framework could score extract summary directly . Speciﬁcally , formulate extractive summa- rization task semantic text matching problem , source document candidate sum- maries ( extracted original text ) matched semantic space . following section detail instantiate proposed match- ing summarization framework using simple siamese-based architecture .  .  Siamese-BERT Inspired siamese network structure ( Bromley et al. ,      ) , construct Siamese-BERT archi- tecture match document candidate summary C. Siamese-BERT consists two BERTs tied-weights cosine-similarity layer inference phase . Unlike modiﬁed BERT used ( Liu ,      ; Bae et al. ,      ) , directly use original BERT derive semantically meaningful embeddings document candidate summary C since need obtain sentence-level representa- tion . Thus , use vector ‘ [ CLS ] ’ token top BERT layer representation document summary . Let rD rC denote embeddings document candidate summary C. similarity score measured f ( , C ) = cosine ( rD , rC ) . order ﬁne-tune Siamese-BERT , use margin-based triplet loss update weights . In- tuitively , gold summary C∗ semanti- cally closest source document , ﬁrst principle loss follow : L  = max (   , f ( , C ) − f ( , C∗ ) + γ  ) , (   ) C candidate summary γ  margin value . Besides , also design pairwise margin loss candidate summaries . sort candidate summaries descending order ROUGE scores gold summary . Naturally , candidate pair larger ranking gap larger margin , second principle design loss function : Figure   : ∆ ( ) different datasets . αsen ( ) = max C∈CD αsum ( ) = max C∈CD gsen ( C ) , gsum ( C ) , (   ) (   ) CD set candidate summaries ex- tracted . , quantify potential gain doc- ument calculating difference αsen ( ) αsum ( ) : ∆ ( ) = αsum ( ) − αsen ( ) . (   ) Finally , dataset-level potential gain ob- tained : ∆ ( ) =   |D| ( cid:   ) D∈D ∆ ( ) , (   ) represents speciﬁc dataset |D| number documents dataset . see Figure   , performance gain summary-level method varies dataset improvement max- imum  .  CNN/DM . Figure   Ta- ble   , ﬁnd performance gain re- lated length reference summary dif- ferent datasets . case short summaries ( Reddit XSum ) , perfect identiﬁcation pearl-summaries lead much improve- ment . Similarly , multiple sentences long sum- mary ( PubMed Multi-News ) already large degree semantic overlap , making improvement summary-level method rela- tively small . medium-length summary ( CNN/DM WikiHow ,    words ) , summary-level learning process rewarding . discuss performance gain speciﬁc models Section  .  .   Summarization Matching quantitative analysis suggests datasets , sentence-level extractors L  = max (   , f ( , Cj ) − f ( , Ci ) ( < j ) , + ( j − ) ∗ γ  ) (   ) RedditXSumCNN/DMWikiHowPubMedMulti-News      ∆ ( ) Ci represents candidate summary ranked γ  hyperparameter used distinguish be- tween good bad candidate summaries . Finally , margin-based triplet loss written : Reddit XSum CNN/DM Wiki PubMed M-News Ext Sel Size     ,          ,          ,          ,   ,                   L = L  + L  . (   ) basic idea let gold summary highest matching score , time , bet- ter candidate summary obtain higher score compared unqualiﬁed candidate summary . Figure   illustrate idea . inference phase , formulate extractive summarization task search best sum- mary among candidates C extracted document . ˆC = arg max f ( , C ) . (    ) C∈C  .  Candidates Pruning Curse Combination matching idea intuitive suffers combinatorial explosion problems . example , could determine size candidate summary set score possible candidates ? allevi- ate difﬁculties , propose simple candidate pruning strategy . ext|s ( cid:   ) ∈ } . Concretely , introduce content selection module pre-select salient sentences . mod- ule learns assign sentence salience score prunes sentences irrelevant current document , resulting pruned document ( cid:   ) = { ( cid:   )   , · · · , ( cid:   ) Similar much previous work two-stage summarization , content selection module parameterized neural network . paper , use BERTSUM ( Liu Lapata ,      ) without tri- gram blocking ( call BERTEXT ) score sentence . , use simple rule obtain candidates : generating combinations sel sentences subject pruned document , re- organize order sentences according original position document form candidate summaries . Therefore , total ( cid:  ) ext ( cid:  ) sel candidate sets .   Experiment  .  Datasets order verify effectiveness frame- work obtain convicing explanations , perform experiments six divergent mainstream datasets follows . Table   : Details candidate summary dif- ferent datasets . Ext denotes number sentences prune original document , Sel denotes number sentences form candidate summary Size number ﬁnal candidate summaries . CNN/DailyMail ( Hermann et al. ,      ) commonly used summarization dataset modiﬁed Nallapati et al . (      ) , contains news ar- ticles associated highlights summaries . paper , use non-anonymized version . PubMed ( Cohan et al. ,      ) collected scientiﬁc papers thus consists long docu- ments . modify dataset using intro- duction section document abstract section corresponding summary . WikiHow ( Koupaee Wang ,      ) di- verse dataset extracted online knowledge base . Articles span wide range topics . XSum ( Narayan et al. ,     a ) one-sentence summary dataset answer question “ article ? ” . summaries profession- ally written , typically authors documents dataset . Multi-News ( Fabbri et al. ,      ) multi- document news summarization dataset rela- tively long summary , use truncated version concatenate source documents single input experiments . Reddit ( Kim et al. ,      ) highly abstractive dataset collected social media platform . use TIFU-long version Reddit , regards body text post document TL ; DR summary .  .  Implementation Details use base version BERT implement models experiments . Adam optimizer ( Kingma Ba ,      ) warming-up used learning rate schedule follows Vaswani et al . (      ) : lr =  e−  · min ( step− .  , step · wm− .  ) , (    ) step batch size    wm denotes warmup steps   ,    . choose γ  =   γ  =  .   . γ  <  .   R-  R-  R-L Model R-  R-  R-L Model LEAD ORACLE MATCH-ORACLE   .     .     .     .     .     .     .     .     .   BANDITSUM ( Dong et al. ,      ) NEUSUM ( Zhou et al. ,      ) JECS ( Xu Durrett ,      ) HIBERT ( Zhang et al. ,     b ) PNBERT ( Zhong et al. ,     a ) PNBERT + RL BERTEXT BERTEXT BERTEXT ( Liu ,      ) BERTEXT + Tri-Blocking BERTSUM   .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .   ∗ ( Liu Lapata ,      )   .     .     .   † ( Bae et al. ,      ) † + RL Reddit BERTEXT ( Num =   ) BERTEXT ( Num =   ) MATCHSUM ( Sel =   ) MATCHSUM ( Sel =   ) MATCHSUM ( Sel =   ,   )   .     .     .     .     .   XSum BERTEXT ( Num =   ) BERTEXT ( Num =   ) MATCHSUM ( Sel =   ) MATCHSUM ( Sel =   ) MATCHSUM ( Sel =   ,   )   .     .     .     .     .    .    .    .    .    .    .    .    .    .    .     .     .     .     .     .     .     .     .     .     .   BERTEXT ( ) BERTEXT + Tri-Blocking ( ) MATCHSUM ( BERT-base ) MATCHSUM ( RoBERTa-base )   .     .     .     .     .     .     .     .     .     .     .     .   Table   : Results test sets Reddit XSum . N um indicates many sentences BERTEXT ex- tracts summary Sel indicates number sentences choose form candidate summary . Table   : Results CNN/DM test set . model ∗ indicates large version BERT used . BERTEXT† add additional Pointer Network com- pared BERTEXT table .  .    < γ  <  .   little effect perfor- mance , otherwise cause performance degradation . use validation set save three best checkpoints training , record performance best checkpoints test set . Importantly , experimental results listed paper average three runs . obtain Siamese-BERT model CNN/DM , use   Tesla- V   -  G GPUs    hours training . datasets , remove samples empty document summary truncate document     tokens , therefore ORACLE paper calculated truncated datasets . Details candidate summary different datasets found Table   .  .  Experimental Results Results CNN/DM shown Table   , list strong baselines different learning ap- proaches . ﬁrst section contains LEAD , OR- ACLE MATCH-ORACLE  . prune documents matching , MATCH-ORACLE relatively low .  LEAD ORACLE common baselines sum- marization task . former means extracting ﬁrst sev- eral sentences document summary , latter groundtruth used extractive models training . MATCH- ORACLE groundtruth used train MATCHSUM . see second section , although RL score entire summary , lead much performance improvement . prob- ably still relies sentence-level summarizers Pointer network sequence labeling models , select sentences one one , rather distinguishing semantics differ- ent summaries whole . Trigram Blocking simple yet effective heuristic CNN/DM , even better redundancy removal methods based neural models . Compared models , proposed MATCHSUM outperformed competitors large margin . example , beats BERTEXT  .   ROUGE-  score using BERT-base encoder . Additionally , even compared baseline BERT-large pre-trained encoder , model MATCHSUM ( BERT-base ) still perform better . Furthermore , change encoder RoBERTa-base ( Liu et al. ,      ) , perfor- mance improved . think im- provement RoBERTa introduced    million English news articles pretraining . superior performance dataset demon- strates effectiveness proposed matching framework . Results Datasets Short Summaries Reddit XSum heavily evaluated abstractive summarizer due short sum- maries . , evaluate model two datasets investigate whether MATCHSUM could achieve improvement dealing Model LEAD ORACLE MATCH-ORACLE BERTEXT +  gram-Blocking +  gram-Blocking MATCHSUM ( BERT-base ) WikiHow R-   .     .     .    .    .    .    .   R-L   .     .     .     .     .     .     .   R-    .     .     .     .     .     .     .   PubMed R-    .     .     .     .     .     .     .   R-    .     .     .     .     .     .     .   R-L   .     .     .     .     .     .     .   R-    .     .     .     .     .     .     .   Multi-News R-    .     .     .     .     .     .     .   R-L   .     .     .     .     .     .     .   Table   : Results test sets WikiHow , PubMed Multi-News . MATCHSUM beats state-of-the-art BERT model Ngram Blocking different domain datasets . summaries containing fewer sentences compared typical extractive models . taking one sentence match orig- inal document , MATCHSUM degenerates re-ranking sentences . Table   illustrates degradation still bring small improve- ment ( compared BERTEXT ( Num =   ) ,  .   ∆R-  Reddit ,  .   ∆R-  XSum ) . How- ever , number sentences increases two summary-level semantics need taken account , MATCHSUM obtain re- markable improvement ( compared BERTEXT ( Num =   ) ,  .   ∆R-  Reddit ,  .   ∆R-  XSum ) . addition , model maps candidate summary whole semantic space , ﬂexibly choose number sentences , methods extract ﬁxed number sen- tences . Table   , see advantage leads performance improvement . Results Datasets Long Summaries summary relatively long , summary- level matching becomes complicated harder learn . aim compare difference Trigram Blocking model dealing long summaries . Table   presents although Trigram Blocking works well CNN/DM , always main- tain stable improvement . Ngram Blocking little effect WikiHow Multi-News , causes large performance drop PubMed . think reason Ngram Blocking can- really understand semantics sentences summaries , restricts presence entities many words , obviously suitable scientiﬁc domain entities may often appear multiple times . contrary , proposed method strong constraints , aligns original document summary semantic space . Experiment results display model robust domains , especially WikiHow , MATCH- SUM beats state-of-the-art BERT model  .   ROUGE-  score .  .  Analysis following , analysis driven two ques- tions :   ) Whether beneﬁts MATCHSUM con- sistent property dataset analyzed Section   ?   ) model achieved different per- formance gains diverse datasets ? Dataset Splitting Testing Typically , choose three datasets ( XSum , CNN/DM WikiHow ) largest performance gain exper- iment . split test set roughly equal numbers ﬁve parts according z described Section  .  , experiment subset . Figure   shows performance gap be- tween MATCHSUM BERTEXT always smallest best-summary pearl- summary ( z =   ) . phenomenon line understanding , samples , ability summary-level extractor discover pearl- summaries bring advantages . z increases , performance gap gener- ally tends increase . Speciﬁcally , beneﬁt MATCHSUM CNN/DM highly consistent appearance pearl-summary . bring improvement  .   subset smallest z , rises sharply  .   z reaches maximum value . WikiHow similar CNN/DM , best-summary consists entirely highest-scoring sentences , performance gap obviously smaller samples . XSum ( ) XSum ( b ) CNN/DM ( c ) WikiHow Figure   : Datasets splitting experiment . split test sets ﬁve parts according z described Section  .  . X-axis left right indicates subsets test set value z small large , Y-axis represents ROUGE improvement MATCHSUM BERTEXT subset . BERTEXT dataset D. Moreover , compared inherent gap sentence-level summary-level extractors , deﬁne ratio MATCHSUM learn dataset : ψ ( ) = ∆ ( ) ∗/∆ ( ) , (    ) ∆ ( ) inherent gap sentence- level summary-level extractos . clear Figure   , value ψ ( ) de- pends z ( see Figure   ) length gold summary ( see Table   ) . gold summaries get longer , upper bound summary-level ap- proaches becomes difﬁcult model reach . MATCHSUM achieve  .   ψ ( ) XSum (   .  words summary ) , however , ψ ( ) less  .  PubMed Multi-News whose summary length exceeds     . another per- spective , summary length similar , model performs better datasets pearl- summaries . instance , z evenly distributed Multi-News ( see Figure   ) , higher ψ ( ) (  .   ) obtained PubMed (  .   ) , least pearl-summaries . better understanding dataset allows us get clear awareness strengths lim- itations framework , also hope analysis could provide useful clues future research extractive summarization .   Conclusion formulate extractive summarization task semantic text matching problem propose novel summary-level framework match source document candidate summaries semantic space . conduct analysis show model could better ﬁt characteristic data . Experimental results show MATCHSUM Figure   : ψ different datasets . Reddit excluded samples test set . slightly different , although trend remains , model perform well samples largest z , needs improvement exploration . comparison , see performance improvement MATCHSUM concentrated samples pearl- summaries , illustrates semantic-based summary-level model capture sentences particularly good viewed individually , thereby forming better summary . Intuitively , Comparison Across Datasets im- provements brought MATCHSUM framework associated inherent gaps presented Section  .  . better understand relation , introduce ∆ ( ) ∗ follows : ∆ ( ) ∗ = gsum ( CM ) − gsum ( CBE ) , ∆ ( ) ∗ =   |D| ( cid:   ) D∈D ∆ ( ) ∗ , (    ) (    ) CM CBE represent candidate sum- mary selected MATCHSUM BERTEXT document , respectively . Therefore , ∆ ( ) ∗ indicate improvement MATCHSUM       .   .  .   .  .   . z : Small=⇒Large∆R      .  .  .   .  .  . z : Small=⇒Large∆R      .   . z : Small=⇒Large∆RXSumCNN/DMWikiHowPubMedMulti-News  .  .  .  .  .  .  . ψ ( ) outperforms current state-of-the-art extractive model six benchmark datasets , demon- strates effectiveness method . believe power matching-based summarization  framework fully exploited . future , forms matching models ex- plored instantiated proposed framework . Acknowledgment would like thank anonymous reviewers valuable comments . work sup- ported National Key Research Develop- ment Program China ( .     YFC        ) , National Natural Science Foundation China ( . U                 ) , Shanghai Mu- nicipal Science Technology Major Project ( .     SHZDZX   ) ZJLab . References RM Alyguliyev .      . two-stage unsupervised ap- proach multidocument summarization . Automatic Control Computer Sciences ,    (   ) :    . Kristjan Arumae Fei Liu .      . Reinforced extrac- tive summarization question-focused rewards . Proceedings ACL      , Student Research Workshop , pages    –    . Sanghwan Bae , Taeuk Kim , Jihoon Kim , Sang- goo Lee .      . Summary level training sentence rewriting abstractive summarization . Proceed- ings  nd Workshop New Frontiers Sum- marization , pages   –   . Jane Bromley , Isabelle Guyon , Yann LeCun , Eduard S¨ackinger , Roopak Shah .      . Signature veriﬁ- cation using ” siamese ” time delay neural network . Advances neural information processing sys- tems , pages    –    . Yen-Chun Chen Mohit Bansal .      . Fast abstrac- tive summarization reinforce-selected sentence rewriting . Proceedings   th Annual Meet- ing Association Computational Linguistics ( Volume   : Long Papers ) , volume   , pages    –    . Jianpeng Cheng Mirella Lapata .      . Neural sum-  marization extracting sentences words . Proceedings   th Annual Meeting As- sociation Computational Linguistics ( Volume   : Long Papers ) , volume   , pages    –    . Arman Cohan , Franck Dernoncourt , Doo Soon Kim , Trung Bui , Seokhwan Kim , Walter Chang , Nazli Goharian .      . discourse-aware attention model abstractive summarization long documents . Proceedings      Conference North American Chapter Association Computa- tional Linguistics : Human Language Technologies , Volume   ( Short Papers ) , volume   , pages    –    . Jacob Devlin , Ming-Wei Chang , Kenton Lee , Kristina Toutanova .      . Bert : Pre-training deep bidirectional transformers language under- standing . Proceedings      Conference North American Chapter Association Computational Linguistics : Human Language Tech- nologies , Volume   ( Long Short Papers ) , pages     –     . Yue Dong , Yikang Shen , Eric Crawford , Herke van Hoof , Jackie Chi Kit Cheung .      . Bandit- sum : Extractive summarization contextual ban- dit . Proceedings      Conference Em- pirical Methods Natural Language Processing , pages     –     . Alexander Richard Fabbri , Irene Li , Tianwei , Suyi Li , Dragomir R. Radev .      . Multi-news : large-scale multi-document summarization dataset ACL (   ) , abstractive hierarchical model . pages     –     . Association Computational Linguistics . Dimitrios Galanis Ion Androutsopoulos .      . extractive supervised two-stage method sentence Human Language Technologies : compression .      Annual Conference North American Chapter Association Computational Lin- guistics , pages    –    . Association Computa- tional Linguistics . Dan Gillick Benoit Favre .      . scalable global Proceedings model summarization . Workshop Integer Linear Programming Nat- ural Language Processing , pages   –   . Karl Moritz Hermann , Tomas Kocisky , Edward Grefen- stette , Lasse Espeholt , Kay , Mustafa Suleyman , Phil Blunsom .      . Teaching machines read Advances Neural Informa- comprehend . tion Processing Systems , pages     –     . Elad Hoffer Nir Ailon .      . Deep metric learning using triplet network . International Workshop Similarity-Based Pattern Recognition , pages   –   . Springer . Aishwarya Jadhav Vaibhav Rajan .      . Extrac- tive summarization swap-net : Sentences Pro- words alternating pointer networks . ceedings   th Annual Meeting Associa- tion Computational Linguistics ( Volume   : Long Papers ) , volume   , pages    –    . Byeongchang Kim , Hyunwoo Kim , Gunhee Kim .      . Abstractive summarization reddit posts Proceed- multi-level memory networks . ings      Conference North American Chapter Association Computational Lin- guistics : Human Language Technologies , Volume   ( Long Short Papers ) , pages     –     . Diederik Kingma Jimmy Ba .      . Adam : method stochastic optimization . arXiv preprint arXiv:    .     . Mahnaz Koupaee William Yang Wang .      . Wik- ihow : large scale text summarization dataset . arXiv preprint arXiv:    .      . Logan Lebanoff , Kaiqiang Song , Franck Dernoncourt , Doo Soon Kim , Seokhwan Kim , Walter Chang , Scoring sentence singletons Fei Liu .      . pairs abstractive summarization . arXiv preprint arXiv:    .      . Chin-Yew Lin Eduard Hovy .      . Auto- matic evaluation summaries using n-gram co- occurrence statistics . Proceedings      Hu- man Language Technology Conference North American Chapter Association Computa- tional Linguistics , pages    –    . Yang Liu .      . Fine-tune bert extractive summa- rization . arXiv preprint arXiv:    .      . Yang Liu Mirella Lapata .      . Text summariza- Proceedings tion pretrained encoders .      Conference Empirical Methods Nat- ural Language Processing  th International Joint Conference Natural Language Processing ( EMNLP-IJCNLP ) , pages     –     . Yinhan Liu , Myle Ott , Naman Goyal , Jingfei Du , Man- dar Joshi , Danqi Chen , Omer Levy , Mike Lewis , Luke Zettlemoyer , Veselin Stoyanov .      . Roberta : robustly optimized bert pretraining ap- proach . arXiv preprint arXiv:    .      . Alfonso Mendes , Shashi Narayan , Sebasti˜ao Miranda , Zita Marinho , Andr´e FT Martins , Shay B Co- hen .      . Jointly extracting compressing doc- uments summary state representations . Pro- ceedings      Conference North Amer- ican Chapter Association Computational Linguistics : Human Language Technologies , Vol- ume   ( Long Short Papers ) , pages     –     . Yishu Miao Phil Blunsom .      . Language latent variable : Discrete generative models sen- tence compression . Proceedings      Con- ference Empirical Methods Natural Language Processing , pages    –    . Bhaskar Mitra , Fernando Diaz , Nick Craswell .      . Learning match using local distributed representations text web search . Proceed- ings   th International Conference World Wide Web , pages     –     . International World Wide Web Conferences Steering Committee . Ramesh Nallapati , Feifei Zhai , Bowen Zhou .      . Summarunner : recurrent neural network based se- quence model extractive summarization docu- ments . Thirty-First AAAI Conference Artiﬁcial Intelligence . Ramesh Nallapati , Bowen Zhou , Cicero dos Santos , C¸ glar Gulc¸ehre , Bing Xiang .      . Abstrac- tive text summarization using sequence-to-sequence rnns beyond . CoNLL      , page     . Shashi Narayan , Shay B Cohen , Mirella Lapata .     a . Dont give details , summary ! topic-aware convolutional neural networks ex- Proceedings      treme summarization . Conference Empirical Methods Natural Lan- guage Processing , pages     –     . Shashi Narayan , Shay B Cohen , Mirella Lapata .     b . Ranking sentences extractive summariza- tion reinforcement learning . Proceedings      Conference North American Chap- ter Association Computational Linguistics : Human Language Technologies , Volume   ( Long Pa- pers ) , volume   , pages     –     . Romain Paulus , Caiming Xiong , Richard Socher .      . deep reinforced model abstractive sum- marization . arXiv preprint arXiv:    .      . Nils Reimers Iryna Gurevych .      . Sentence- bert : Sentence embeddings using siamese bert- networks . Proceedings      Conference Empirical Methods Natural Language Processing  th International Joint Conference Natu- ral Language Processing ( EMNLP-IJCNLP ) , pages     –     . Aliaksei Severyn Alessandro Moschitti .      . Learning rank short text pairs convolutional deep neural networks . Proceedings   th in- ternational ACM SIGIR conference research development information retrieval , pages    –     . ACM . Ashish Vaswani , Noam Shazeer , Niki Parmar , Jakob Uszkoreit , Llion Jones , Aidan N Gomez , Łukasz Kaiser , Illia Polosukhin .      . Attention need . Advances Neural Information Pro- cessing Systems , pages     –     . Xiaojun Wan , Ziqiang Cao , Furu Wei , Sujian Li , Ming Zhou .      . Multi-document summariza- tion via discriminative summary reranking . arXiv preprint arXiv:    .      . Danqing Wang , Pengfei Liu , Yining Zheng , Xipeng Qiu , Xuan-Jing Huang .      . Heterogeneous graph neural networks extractive document sum- marization . Proceedings   th Conference Association Computational Linguistics . Danqing Wang , Pengfei Liu , Ming Zhong , Jie Fu , Xipeng Qiu , Xuanjing Huang .      . Exploring domain shift extractive text summarization . arXiv preprint arXiv:    .      . Shuohang Wang Jing Jiang .      . Learning natu- ral language inference lstm . Proceedings      Conference North American Chap- ter Association Computational Linguistics : Human Language Technologies , pages     –     . Zhiguo Wang , Wael Hamza , Radu Florian .      . Bilateral multi-perspective matching natural lan- guage sentences . Proceedings   th Inter- national Joint Conference Artiﬁcial Intelligence , pages     –     . AAAI Press . Jiacheng Xu Greg Durrett .      . Neural extrac- tive text summarization syntactic compression . Proceedings      Conference Empiri- cal Methods Natural Language Processing , Hong Kong , China . Association Computational Lin- guistics . Jiacheng Xu , Zhe Gan , Yu Cheng , Jingjing Discourse-aware neural extractive arXiv preprint Liu .      . model text summarization . arXiv:    .      . Wen-tau Yih , Ming-Wei Chang , Christopher Meek , Andrzej Pastusiak .      . Question answering using enhanced lexical semantic models . Proceedings   st Annual Meeting Association Computational Linguistics ( Volume   : Long Papers ) , pages     –     . Dani Yogatama , Fei Liu , Noah Smith .      . Ex- tractive summarization maximizing semantic vol- Proceedings      Conference ume . Empirical Methods Natural Language Processing , pages     –     . Haoyu Zhang , Yeyun Gong , Yu Yan , Nan Duan , Jian- jun Xu , Ji Wang , Ming Gong , Ming Zhou . language gen-     a . arXiv preprint eration text summarization . arXiv:    .      . Pretraining-based natural Xingxing Zhang , Furu Wei , Ming Zhou .     b . Hibert : Document level pre-training hierarchical bidirectional transformers document summariza- tion . ACL . Ming Zhong , Pengfei Liu , Danqing Wang , Xipeng Qiu , Xuan-Jing Huang .     a . Searching effec- tive neural extractive summarization : works whats next . Proceedings   th Confer- ence Association Computational Linguis- tics , pages     –     . Ming Zhong , Danqing Wang , Pengfei Liu , Xipeng Qiu , Xuanjing Huang .     b . closer look data bias neural extractive summarization models . EMNLP-IJCNLP      , page    . Qingyu Zhou , Nan Yang , Furu Wei , Shaohan Huang , Ming Zhou , Tiejun Zhao .      . Neural docu- ment summarization jointly learning score select sentences . Proceedings   th Annual Meeting Association Computational Lin- guistics ( Volume   : Long Papers ) , volume   , pages    –    . 