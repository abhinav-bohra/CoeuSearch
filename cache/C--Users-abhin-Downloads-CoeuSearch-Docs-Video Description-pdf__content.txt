        r     ] V C .  c [   v           .         : v  X r    Video Description : Survey Methods , Datasets Evaluation Metrics Nayyer Aafaq , Ajmal Mian , Wei Liu , Syed Zulqarnain Gilani , Mubarak Shah Abstract—Video description automatic generation natural language sentences describe contents given video . applications human-robot interaction , helping visually impaired video subtitling . past years seen surge research area due unprecedented success deep learning computer vision natural language processing . Numerous methods , datasets evaluation metrics proposed literature , calling need comprehensive survey focus research efforts ﬂourishing new direction . paper ﬁlls gap surveying state art approaches focus deep learning models ; comparing benchmark datasets terms domains , number classes , repository size ; identifying pros cons various evaluation metrics like SPICE , CIDEr , ROUGE , BLEU , METEOR , WMD . Classical video description approaches combined subject , object verb detection template based language models generate sentences . However , release large datasets revealed methods cope diversity unconstrained open domain videos . Classical approaches followed short era statistical methods soon replaced deep learning , current state art video description . survey shows despite fast-paced developments , video description research still infancy due following reasons . Analysis video description models challenging difﬁcult ascertain contributions , towards accuracy errors , visual features adopted language model ﬁnal description . Existing datasets neither contain adequate visual diversity complexity linguistic structures . Finally , current evaluation metrics fall short measuring agreement machine generated descriptions humans . conclude survey listing promising future research directions . Index Terms—Deep learning , video description , video captioning , video text , language vision , video captioning datasets , video captioning evaluation metrics , BLEU , METEOR , ROUGE , CIDEr , SPICE , WMD . ( cid:   )   INTRODUCTION ESCRIBING short video natural language trivial task people , challenging one machines . Automatic video description involves understanding many entities detection oc- currences video employing computer vision techniques . entities include background scene , humans , objects , hu- man actions , human-object interactions , human-human interac- tions , events , order events occur . information must articulated using compre- hensible grammatically correct text employing Natural Language Processing ( NLP ) techniques . past years , two traditionally independent ﬁelds , Computer Vision ( CV ) Natural Language Processing ( NLP ) joined forces address upsurge research interests understanding describing images videos . Special issues journals published focusing language vision [   ] workshops uniting two areas also held regularly NLP CV conferences [    ] , [    ] , [    ] , [     ] . Automatic video description many applications human-robot interaction , automatic video subtitling video surveillance . used help visually impaired generating verbal descriptions surroundings • N. Aafaq , S. Z. Gilani , W. Liu A. Mian Department Computer Science Software Engineering ( CSSE ) , University Western Australia ( UWA ) , WA ,      . • M. Shah University Central Florida ( UCF ) , FL , USA E-mail : nayyer.aafaq @ research.uwa.edu.au , [ zulqarnain.gilani , wei.liu , aj- mal.mian ] @ uwa.edu.au , shah @ crcv.ucf.edu speech synthesis , automatically generating reading ﬁlm descriptions . Currently , achieved costly time-consuming manual processes . Another application description sign language videos natural language . Video description also gen- erate written procedures human service robots automatically converting actions demonstration video simple instructions , example , assembling furni- ture , installing CD-ROM , making coffee changing ﬂat tyre [    ] , [    ] . advancement video description opens enor- mous opportunities many application domains . en- visaged near future , would able interact robots manner humans [     ] . video description advanced stage able comprehend events unfolding real world render spoken words , Service Robots Smart phone Apps able understand human actions events converse humans much meaningful coherent manner . example , could answer user ’ question left wallet discuss cook dinner . industry settings , could potentially remind worker actions/procedures missing routine operation . recent release dialogue dataset , Talk Walk [     ] , introduced yet another interesting application natural language dialogue guide tourist helps tourist reach previously unseen location map using perception , action interaction modeling . Leveraging recent developments deep neural net-         Fig .   : basic framework deep learning based video captioning . visual model encodes video frames vector space . language model takes input visual vector word embeddings generate sentence describes input visual content . works NLP CV , increased availability large multi-modal datasets , automatically generating stories pixels longer science ﬁction . growing body work mainly originated robotics community labeled broadly language grounded meaning vision robotic perception [     ] . Related research ar- eas include , connecting words pictures [    ] , [    ] , [    ] , narrating images natural language sentences [    ] , [    ] , [    ] understanding natural language instructions robotic applications [    ] , [     ] , [     ] . Another closely related ﬁeld Visual Information Retrieval ( VIR ) , takes visual ( image , drawing sketch ) , text ( tags , keywords com- plete sentence ) mixed visual text query perform content based search . Thanks release benchmark datasets MS COCO [    ] Flicker  k [     ] , research image captioning retrieval [    ] , [    ] , [    ] , [     ] , image question answering [    ] , [     ] , [     ] , [     ] also become active . Automatically generating natural language sentences describing video content two components ; under- standing visual content describing grammati- cally correct natural language sentences . Figure   shows simple deep learning based video captioning framework . task video description relatively challenging , compared image captioning , objects video relevant description de- tected objects play role observed activity [    ] . Moreover , video description methods must ad- ditionally capture speed , direction relevant objects well causality among events , actions , objects . Finally , events videos varying lengths may even result possible overlap events [    ] . See Figure   example . event piano recitals spanned almost entire duration video , however , applause short event takes place end . example illustrates differences three related areas research , namely , image captioning , video captioning dense video captioning . example , image captioning techniques recognize event mere clapping whereas actually applause resulted previous event - piano playing . Figure   summarizes related research um- brella Visual Description . classiﬁcation based whether input still images ( Image Captioning ) multi- frame short videos ( Video Captioning ) . Note , however , short video captioning different video auto- transcription audio speeches main focus . Video captioning concerns mainly visual content opposed audio signals . particular , Video Description extends video captioning aim provide detailed account visual contents video . deﬁne terminologies used paper . • Visual Description : unifying concept encompass- ing ( see Fig .   ) automatic generation single multiple natural language sentences convey information still images video clips . • Video Captioning : Conveying information video clip whole single automatically generated natural language sentence based premise short video clips usually contain one main event [    ] , [    ] , [    ] , [     ] , [     ] , [     ] . • Video Description : Automatically generating multiple natural language sentences provide narrative relatively longer video clip . descriptions detailed may form paragraphs . Video description sometimes also referred story telling paragraph generation [     ] , [     ] . • Dense Video Captioning : Detection conveying in- formation , possibly overlapping , events dif- ferent lengths video using natural language sentence per event . illustrated Fig .   , dense video captioning localizes events time [    ] , [     ] , [     ] , [     ] generates sentences necessarily coherent . hand video de- scription gives detailed account one events video clip using multiple coher- ent sentences without localize individual events . Video captioning research started classical tem- plate based approaches Subject ( ) , Verb ( V ) , Object ( ) detected separately joined using sentence template . approaches referred SVO-Triplets [    ] , [    ] . However , advent deep learning tremendous advancements CV NLP equally affected area video captioning . Hence , latest approaches follow deep learning based architectures [     ] , [     ] encode visual features  D/ D-CNN use LSTM/GRU learn sequence . output approaches either single sentence [     ] , [     ] , mul- tiple sentences [    ] , [    ] , [    ] , [     ] , [     ] , [     ] per video   Fig .   : Illustration differences image captioning , video captioning dense video captioning . Image ( video frame ) captioning describes frame single sentence . Video captioning describes complete video one sentence . dense video captioning , event video temporally detected described single sentence eventually resulting multiple sentences localized time necessarily coherent . clip . Early research video description mostly focused domain speciﬁc short video clips limited vocabularies objects activities [    ] , [    ] , [    ] , [    ] , [     ] , [     ] . Description open domain relatively longer videos remains challenge , needs large vocabularies train- ing data . Methods follow CNN-LSTM/GRU framework mainly differ different types CNNs language models ( vanilla RNN , LSTM , GRUs ) employ well pass extracted visual features language model ( ﬁrst time step time steps ) . Later methods progressed introducing additional transformations top standard encoder- decoder framework . transformations include atten- tion mechanism [     ] model learns part video focus , sequence learning [     ] models sequence video frames sequence words corresponding sentence , semantic attributes [    ] , [     ] exploits visual semantics addition CNN features , joint modeling visual content compositional text [     ] . recently , video based visual description problem evolved towards dense video captioning video story telling . New datasets also introduced progress along lines . Fig .   : Classiﬁcation visual content description . sur- vey focuses video images . comes performance comparison , quantitative evaluation video description systems straight- forward . Currently , automatic evaluations typically performed using machine translation image caption- including Bilingual Evaluation Understudy ing metrics , ( BLEU ) [     ] , Recall Oriented Understudy Gisting Eval- uation ( ROUGE ) [    ] , Metric Evaluation Translation Explicit Ordering ( METEOR ) [    ] , Consensus based Image Description Evaluation ( CIDEr ) [     ] , re- cently proposed Semantic Propositional Image Caption- ing Evaluation ( SPICE ) [    ] Word Mover ’ Distance ( WMD ) [    ] metrics . Section  .  presents measures . , give brief overview establish motivation survey . BLEU precision-based metric , ac- counts precise matching n-grams generated ground truth references . METEOR , hand , ﬁrst creates alignment two sentences compar- ing exact tokens , stemmed tokens paraphrases . also takes consideration semantically similar matches using WordNet synonyms . ROUGE , similar BLEU , different n-grams based versions computes recall generated sentences reference sentences . CIDEr human-consensus-based evaluation metric , de- veloped speciﬁcally evaluating image captioning meth- ods also used video description tasks . WMD makes use word embeddings ( semantically meaningful vector representations words ) compares two texts using Earth Mover ’ Distance ( EMD ) . metric relatively less sensitive word order synonym changes sentence , like CIDEr METEOR , provides high correlation human judgments . Lastly , SPICE recent metric correlates human judgment semantic quality compared previously reported met- rics . compares semantic information two sentences matching content dependency parse trees . metrics capture different performance measures method perfectly aligned human judgments . Also , due hand engineered nature metrics , scores unstable candidate sentence perturbed synonyms , word order , length redundancy . Hence , need evaluation metric learned training data score harmony human judgments describing videos diverse content . current literature lacks comprehensive system- atic survey covers different aspects video description research including methods , dataset characteristics , evalua- tion measures , benchmark results related competitions video Q & challenges . ﬁll gap present comprehensive survey literature . ﬁrst highlight important applications major trends video de- scription Section   classify automatic video description methods three groups , giving overview models group Section   . Section   , elaborate available video description datasets used benchmarking . Section   , present details video competitions challenges . Furthermore , review evaluation metrics used quantitative analysis generated descriptions Section   . Section   , benchmark results achieved aforementioned methods compared discussed . Section   , discuss possible future directions ﬁnally Section   concludes survey discusses insights ﬁndings .   VIDEO DESCRIPTION METHODS Video description literature divided three main phases . classical methods phase , pioneering vi- sual description research employed classical CV NLP methods ﬁrst detect entities ( objects , actions , scenes ) videos ﬁt standard sentence templates . statistical methods phase , employed statistical methods deal relatively larger datasets . phase lasted relatively short time . Finally , deep learning phase , current state art believed potential solve open domain automatic video description problem . , give detailed survey methods category .  .  Classical Methods SVO ( Subject , Object , Verb ) tuples based methods among ﬁrst successful methods used speciﬁcally video description . However , research efforts made long describe visual content natural lan- guage , albeit explicitly captioning description . ﬁrst ever attempt goes back Koller et al . [    ]      , developed system able char- acterize motion vehicles real trafﬁc scenes using natural language verbs . Later      , Brand et al . [    ] dubbed ” Inverse Hollywood Problem ” ( since Hollywood script ( description ) converted video , problem opposite ) , described series actions semantic tag summaries order develop sto- ryboard instructional videos . also developed system , “ video gister ” , able heuristically parse videos series key actions generate script describes actions detected video . also generated key frames depicting detected causal events deﬁned series events semantics rep- resentation e.g . Add enter , motion , detach remove attach , move , leave . Video gister limited one human arm ( actor ) interacting non liquid objects able understand ﬁve actions ( touch , put , get , add , remove ) .   Getting back SVO tuple based methods , tackle video description generation task two stages . ﬁrst stage known content identiﬁcation focuses visual recognition classiﬁcation main objects video clip . typically include performer actor , action object action . second stage involves sentence generation maps objects identiﬁed ﬁrst stage Subject , Verb Object ( hence name SVO ) , ﬁlling handcrafted templates grammati- cally sound sentences . templates created using grammar rule-based systems , effective constrained environments , i.e . short clips videos limited number objects actions . Numerous method proposed detecting objects , humans , actions , events videos . summarize recognition techniques used Stage SVO tuples based approaches . • Object Recognition : Object recognition SVO ap- proaches performed typically using conven- tional methods , including model-based shape match- ing edge detection color matching [    ] , HAAR features matching [     ] , context-based ob- ject recognition [     ] , Scale Invariant Feature Trans- form ( SIFT ) [     ] , discriminatively trained part- based models [    ] Deformable Parts Model ( DPM ) [    ] , [    ] . • • Human Activity Detection : Human detection methods employed features Histograms Oriented Gradient ( HOG ) [    ] followed SVM . activity detection , features like Spatiotemporal Interest Points Histogram Oriented Optical Flow ( HOOF ) [    ] , Bayesian Networks ( BN ) [    ] , Dynamic Bayesian Networks ( DBNs ) [    ] , Hidden Markov Models ( HMM ) [    ] , state machines [    ] , PNF Networks [     ] used SVO approaches . Integrated Approaches : Instead detecting description-relevant entities separately , Stochastic Attribute Image Grammar ( SAIG ) [     ] Stochas- tic Context Free Grammars ( SCFG ) [     ] , allow compositional representation visual entities present video , image scene based spatial functional relations . Using visual grammar , content image ﬁrst extracted parse graph . parsing algorithm used ﬁnd best scoring entities describe video . words , entities present video equal relevance , distinct feature class methods compared aforementioned approaches . Stage II , sentence generation , variety meth- ods proposed including HALogen repre- sentation [    ] , Head-driven Phrase Structure Grammar ( HPSG ) [     ] , planner surface realizer [     ] . pri- mary common task methods deﬁne templates . template user-deﬁned language structure containing placeholders . order function properly , template com- prises three parts named lexicons , grammar template rules . Lexicon represents vocabulary describes high level video features . Template rules user-deﬁned rules guiding   table door ” , represented : [ PRED : walk , AG : person , GO-LOC : ( door ) , SO-LOC : front ( table ) ] , PRED predicate action , AG agent actor , GO-LOC goal location SO-LOC source location . list semantic primitives deﬁned movements , organized using body action state transitions . example , moving detected speed fast , activity state transitioned moving running . also distinguish durative actions ( e.g . walk ) instantaneous actions ( e.g . stand ) . major drawback approach easily extended complex scenarios multiple actors , incorporating temporal information , capturing causal relationship events . heavy reliance correctness manually created activity concept hierarchy state transition model also prevents used practical situations . Hakeem et . al . [    ] addressed shortcomings Ko- jima et . al ’ [    ] work proposed extended case framework ( CASEE ) using hierarchical CASE representa- tions . incorporated multiple agent events , temporal information , causal relationship events describe events natural language . introduced case-list incorporate multiple agents AG , [ PRED : move , AG : { person  , person  } , ... ] . Moreover , incorpo- rated temporal information CASE using temporal logic encode relationship sub-events . events conditional events , also captured causal relationship events . example , sen- tence ” man played piano crowd applauded ” , applaud occurred piano played . [ CAUSE : [ PRED : play , : crowed , FAC : applaud ] ] . Khan et al . [    ] introduced framework describe human related contents actions ( limited ﬁve ) emotions videos using natural language sentences . implemented suite conventional image processing techniques , including face detection [    ] , emotion detec- tion [     ] , action detection [    ] , non-human object detec- tion [     ] scene classiﬁcation [    ] , extract high level entities interest video frames . include humans , objects , actions , gender , position emotion . Since approach encapsulates human related actions , human rendered Subject objects upon action performed rendered Object . template based approach adopted generate natural language sentences based detected entities . evaluated method dataset    snippets , spanning      seconds duration .    ,    snippets human close-ups    showed human activities stand , walk , sit , run wave . primary focus research activities involving human interacting objects . Hence , method generate description human detected video . method identify actions subtle movements ( smoking drinking ) interactions among humans . (   ) Action Object Focused : Lee et al . [    ] proposed method semantically annotating visual content three sequential stages namely , image parsing , event inference language generation . “ image parsing engine ” using Fig .   : example various templates used sentence generation videos . Subject , verb , object used ﬁll template . Verb obtained action/activity detection methods using spatio-temporal features whereas subject object obtained object detection meth- ods using spatial features . selection appropriate lexicons sentence generation . Grammar deﬁnes linguistic rules describe structure expressions language , ensuring generated sentence syntactically correct . Using production rules , Grammar generate large number various conﬁg- urations relatively small vocabulary . template based approaches , sentence generated ﬁtting important entities categories required template , e.g . subject , verb , object , place . Entities actions recognized content identiﬁcation stage used lexicons . Correctness generated sen- tence ensured Grammar . Figure   presents examples popular templates used sentence generation template based approaches . Figure   gives timeline classical methods evolved time whereas provide survey SVO methods grouping three categories namely , subject ( human ) focused , action object focused methods use SVO approach open domain videos . Note division boundaries frequently blurred categories . (   ) Subject ( Human ) Focused :      , Kojima et al . [    ] proposed one earliest methods designed speciﬁcally video captioning . method focuses primarily describing videos one person performing one action . detect humans scene , calculated probability pixel coming background skin region using values distributions pixel chromaticity . human ’ head hands detected , human posture estimated considering three kinds geometric information i.e . position head hands direction head . example , obtain head direction , detected head image compared list pre- collected head models threshold used decide matching head direction . object detection , applied two-way matching , i.e . shape-based matching pixel based color matching list predeﬁned known objects . Actions detected related object handling difference image used detect actions putting object lifting object . generate description sentences , pre-deﬁned case frames verb patterns proposed Nishida et al . [     ] , [     ] used . Case frame type frame expression used representing relationship cases , classiﬁed   categories . frequently used ones agent , object , locus . example , “ person walks   Fig .   : Evolution classical methods time . general focus methods moved subjects ( humans ) actions objects open domain videos containing three SVO categories . stochastic attribute image grammar ( SAIG ) [     ] em- ployed produce visual vocabulary i.e . list visual entities present frame along relationships . output fed “ event inference engine ” , extracts semantic contextual information vi- sual events , along relationships . Video Event Markup Language ( VEML ) [     ] used represent se- mantic information . ﬁnal stage , head-driven phrase structure grammar ( HPSG ) [     ] used generate text description semantic representation . Compared Kojima et al . [    ] , grammar-based methods infer annotate wider range scenes events . Ten streams urban trafﬁc maritime scenes period     minutes , containing     moving objects used evaluation . detected events include “ entering scene , moving , stopping , turning , approaching trafﬁc inter- section , watercraft approaching maritime markers land areas scenarios one object follows ” [    ] . Recall Precision rates employed evaluate accuracy events detected respect manually labeled ground truth . Due poor estimation motion direction low number perspective views , method perform well “ turning ” events . Hanckmann et al . [    ] proposed method automat- ically describe events involving multiple actions (   av- erage ) , performed one individuals . Unlike Khan et al . [    ] , human-human interactions taken account addition human-object interactions . Bag-of-features (    total ) collected action detectors [    ] detecting classifying actions video . description generator subsequently describes verbs relating actions scene entities . ﬁnds appropriate actors among objects persons connects appropriate verbs . contrast Khan et al . [    ] assume subject al- ways person , Hanckmann et al . [    ] generalizes subjects include vehicles well . Furthermore , number human actions much richer . Compared ﬁve verbs Khan et al . [    ] ) ,    verbs capturing diverse range actions approach , arrive , bounce , carry , catch etc . Barbu et al . [    ] generated sentence descriptions short videos highly constrained domains consisting    object classes ,    action classes vocabulary     words . rendered detected object action noun verb respectively . Adjectives used object properties prepositions used spatial relationships . approach comprises three steps . ﬁrst step , object detection [    ] carried frame limiting    detections per frame avoid detections . Second , object tracking [     ] , [     ] performed increase preci- sion . Third , using dynamic programming optimal set detections chosen . Verb labels corresponding actions videos produced using Hidden Markov Models ( HMMs ) . getting verb , tracks merged generate template based sentences comply grammar rules . Despite reasonably accurate lingual descriptions gen- erated videos constrained environments , afore- mentioned methods trouble scaling accommodate increased number objects actions open domain large video corpora . incorporate relevant concepts , methods require customized detectors entity . Furthermore , texts generated existing methods time mostly form putting together lists keywords using grammars templates without semantic veriﬁcation . address issue lacking seman- tic veriﬁcation , Das et . al [    ] proposed hybrid method produces content high relevance compared simple keyword annotation methods . borrowed ideas image captioning techniques . hybrid model comprises three steps hierarchical manner . First , bottom approach , keywords predicted using low level video features . approach ﬁrst ﬁnd proposal distri- bution training set vocabulary using multimodal latent topic models . using grammar rules parts speech ( POS ) tagging , probable subjects , objects verbs selected . Second , top approach , set concepts detected stitched together . tripartite graph template used converting stitched concepts natural language description . Finally , semantic ver- iﬁcation , produced ranked set natural language sentences comparing predicted keywords   Fig .   : Example Subject-Verb-Object-Place ( SVOP ) [     ] approach conﬁdences obtained integrating probabilities visual recognition system , statistics domain English text corpora determine likely SVOP tuple . red block shows low probability given correct object visual system rectiﬁed high probability linguistic model . detected concepts . Quantitative evaluation hybrid method shows able generate relevant content compared predecessors [    ] , [    ] . (   ) SVO Methods Open Domain Videos : prior mentioned works restricted constrained domains , Krishnamoorthy et al . [    ] lead early works describing open domain videos . used selected open domain YouTube videos , however , subjects objects limited    entities available classiﬁer training set . main contribution intro- duction text-mining using web-scale text corpora aid selection best SVO tuple improve sentence coherence . addition focusing open domain videos utilizing web scaled text corpora , Guadarrama et al . [    ] Thomason et al . [     ] started dealing relatively larger vocabularies . Compared Krishnamoorthy et al . [    ] , in- stead using    objects PASCAL dataset [    ] , videos YouTube corpora used detec- tion     objects ,    subjects ,     verbs . describe short YouTube videos , Guadarrama et al . [    ] proposed novel language driven approach . introduced “ zero- shot ” verb recognition selecting unseen verbs subject “ person ” , object training set . example , refers “ car ” model-predicted verb “ move ” , suitable verb would “ drive ” . Thomason et al . [     ] used visual recognition techniques YouTube videos probabilistic estimations subjects , verbs , objects . approach illustrated Figure   . object action classiﬁers trained ImageNet [     ] . addition detecting subjects , verbs objects , places (    scenes ) actions performed , e.g . kitchen play ground also identiﬁed . improve accuracy assigning visually detected entities right category , probabilities using language statistics obtained four “ domain ” English text corpora : English Gigaword , British National Corpus ( BNC ) , ukWac WaCkypedia EN used enhance conﬁdence word-category alignment sentence generation . small “ domain ” cor- pus comprising human-annotated sentences video description dataset also constructed incorporated sentence generation stage . Co-occurring bi-gram ( SV , VO , OP ) statistics candidate SVOP tuples calculated using “ domain ” “ domain ” corpus , used Factor Graph Model ( FGM ) predict probable SVO place combi- nation . Finally , detected SVOP tuple used generate English sentence template based approach . Classical methods focused mainly detection pre-deﬁned entities events separately . methods tried describe detected entities events us- ing template based sentences . However , describe open domain videos events entities , classical methods must employ object action detection techniques entity unrealistic due computational complexity . Moreover , template based de- scriptions insufﬁcient describe possible events videos given linguistic complexity diversity . Consequently , methods failed describe semantically rich videos .  .  Statistical Methods Na¨ıve SVO tuple rule-based engineering approaches in- deed inadequate describe open domain videos large datasets , YouTubeClips [    ] , TACoS-MultiLevel [     ] , MPII-MD [     ] , M-VAD [     ] . datasets contain large vocabularies well tens hours videos . three important differences open domain previous datasets . Firstly , open domain videos contain unforeseeable diverse set subjects , objects , activities places . Secondly , due sophisticated na- ture human languages , datasets often annotated multiple viable meaningful descriptions . Thirdly , videos described often long , potentially stretching many hours . Descriptions videos mul- tiple sentences even paragraphs become desirable . avoid tedious efforts required rule-based engi- neering methods , Rohrbach et . al . [     ] proposed machine learning method convert visual content natural lan- guage . used parallel corpora videos associated annotations . method follows two step approach . First , learns represent video intermediate se- mantic labels using maximum posterior estimate ( MAP ) . , translates semantic labels natural language   Fig .   : Deep learning based video description techniques literature comprise two main stages . ﬁrst stage involves visual content extraction represented either ﬁxed length vector dynamic vectors . second stage takes input visual representation vectors ﬁrst stage text generation generates single/multiple sentence ( ) . sentences using techniques borrowed Statistical Machine Translation ( SMT ) [    ] . machine translation approach , intermediate semantic label representation source expected annotations regarded target language . object activity recognition stages , re- search moved earlier threshold-based detection [    ] manual feature engineering traditional classiﬁers [    ] , [    ] , [    ] , [     ] . sentence generation stage , uptake machine learning methods observed recent years address issue large vocabulary . also evidenced trend recent methods use models lexical entries learned weakly supervised [     ] , [     ] , [     ] , [     ] fully supervised [    ] , [    ] , [    ] , [     ] fashion . However , separation two stages makes camp methods incapable capturing interplay visual features linguistic patterns , let alone learning transferable state space visual artifacts linguistic representations . next section , review deep learning methods discuss address scalability , language complexity domain transfer- ability issues faced open domain video description .  .  Deep Learning Models whirlwind success deep learning almost sub- ﬁelds computer vision , also revolutionized video description approaches . particular , Convolutional Neural Networks ( CNNs ) [    ] state art modeling visual data excel tasks object recognition [    ] , [     ] , [     ] . Long Short-Term Memory ( LSTMs ) [    ] general deep Recurrent Neural Networks ( RNNs ) , hand , dominating area sequence modeling , setting new benchmarks machine translation [    ] , [     ] , speech recognition [    ] closely related task image captioning [    ] , [     ] . conventional methods struggle cope large-scale , complex diverse datasets video description , researchers combined deep nets various conﬁgurations promising performances . shown Figure   , deep learning approaches video description also divided two sequential stages , namely , visual content extraction text genera- tion . However , contrast SVO Tuple Methods Section  .  , lexical word tokens generated result ﬁrst stage visual content extraction , visual features represented ﬁxed dynamic real-valued vectors produced instead . often referred video encoding stage . CNN , RNN Long Short-Term Memory ( LSTM ) used encoding stage learn visual features , used second stage text generation , also known decoding stage . decoding , different ﬂavours RNNs used , deep RNN , Bi-directional RNN , LSTM Gated Recurrent Units ( GRU ) . resulting description single sentence multiple sentences . Figure   illustrates typical end-to-end video description system encoder-decoder stages . encoding part followed transformations mean pooling , temporal encoding attention mechanisms represent visual content . methods apply sequence- to-sequence learning and/or semantic attributes learning frameworks . aforementioned mechanisms used different combinations contemporary meth- ods . group literature based different com- binations deep learning architectures encoding decoding stages , namely : • CNN - RNN Video Description , convolution architectures used visual encoding re- current structures used decoding . common architecture employed deep learning based video description methods ; • RNN - RNN Video Description , recurrent networks used stages ; • Deep reinforcement networks , relatively new re- search area video description .  . .  CNN-RNN Video Description Given success computer vision simplicity , CNN still far popular network structure used   Fig .   : Summary deep learning based video description methods . methods employ mean pooling frame representations represent video . advanced methods use attention mechanisms , semantic attribute learning , and/or employ sequence-to-sequence approach . methods differ whether visual features fed ﬁrst time step time steps language model . visual encoding . encoding process broadly categorized ﬁxed-size variable-size video encoding . Donahue et al . [    ] ﬁrst use deep neural networks solve video captioning problem . pro- posed three architectures video description . model based assumption CRF based predictions subjects , objects , verbs full pass complete video . allows architecture observe complete video time step . ﬁrst architecture , LSTM encoder- decoder CRF max , motivated statistical ma- chine translation ( SMT ) based video description approach Rohrbach et al . [     ] mentioned earlier Section  .  . Recognizing state art machine translation perfor- mance LSTMs , SMT module [     ] replaced stacked LSTM comprising two layers encoding decoding . Similar [     ] , ﬁrst LSTM layer encodes one-hot vector input sentence allowing variable- length inputs . ﬁnal hidden representation ﬁrst encoder stage fed decoder stage generate sentence producing one word per time step . Another variant architecture , LSTM decoder CRF max , incorporates max predictions . architecture encodes semantic representation ﬁxed length vector . Similar image description , LSTM able see whole visual content every time step . advantage LSTM able incorporate probability vectors training well testing . virtue LSTM exploited third variant architecture , LSTM decoder CRF probabilities . Instead using max predication like sec- ond variant ( LSTM decoder CRF max ) , architecture incorporates probability distributions . Although LSTM outperformed SMT based approach [     ] , still trainable end-to-end fashion . contrast work Donahue et al . [    ] , intermediate role representation adopted , Venugopalan et al . [     ] presented ﬁrst end-to-end trainable network architecture generating natural language description videos . model able simultaneously learn semantic well grammatical structure associated language . Moreover , Donahue et al . [    ] presented results domain speciﬁc cooking videos comprising pre-deﬁned ob- jects actors . hand , Venugopalan et al . [     ] reported results open domain YouTube Clips [    ] . avoid supervised intermediate representations , con- nected LSTM directly output CNN . CNN extracts visual features whereas LSTM models sequence dynamics . transformed short video ﬁxed length visual input using CNN model [    ] slightly different AlexNet [    ] . CNN model [    ] learned using ILSVRC-     object classiﬁcation dataset ( comprising  . M images ) , subset Ima- geNet [     ] . provides robust efﬁcient way without manual feature selection initialization object recognition videos . sampled every tenth frame video extracted features sample frames fc  layer CNN . Furthermore , represented complete video averaging extracted frame-wise feature vectors single vector . feature vectors fed two-layered LSTM [    ] . feature vectors CNN form input ﬁrst layer LSTM . second LSTM layer stacked top ﬁrst LSTM layer , hidden state ﬁrst LSTM layer becomes input second LSTM unit caption generation . essence , transforming multiple frame-based feature vectors single aggregated video-based vector , reduces video description problem image captioning one . end-to-end model performed better previous video description systems time able effectively generate sequence without templates . However , result simple averaging , valuable temporal information video , order appearances two objects , lost . Therefore , approach suitable generating captions short clips single major action clip . Open domain videos rich complex interactions among actors objects . Representation videos us- ing temporally averaged single feature vector , therefore , prone produce clutter . Consequently , descriptions produced bound inadequate valuable temporal ordering information events captured representation . success C D [     ] capturing spatio-temporal action dynamics videos , Li et al . [     ] proposed novel  D-CNN model spatio- temporal information videos .  D-CNN based GoogLeNet [     ] pre-trained activity recognition dataset . captures local ﬁne motion information consecutive frames . local motion information subsequently summarized preserved higher- level representations modeling video  D spatio- temporal cuboid . represented concatenation HoG , HoF , MbH [    ] , [     ] . transformations help capture local motion features also reduce computation subsequent  D CNN . global tempo- ral structure , temporal attention mechanism proposed adapted soft attention [    ] . Using  D CNN attention mechanisms RNN , able improve results . Recently , GRU-EVE [    ] proposed ef- fective computationally efﬁcient technique video captioning . GRU-EVE uses standard GRU language modeling Enriched Visual Encoding follows . applies Short Fourier Transform  D/ D-CNN features hierarchical manner encapsulate spatio- temporal video dynamics . visual features enriched high level semantics detected objects actions video . Interestingly , enriched features obtained applying Short Fourier Transform  D-CNN features alone [    ] , outperform C D [     ] features . Unlike ﬁxed video representation models discussed , variable visual representation models able directly map input videos comprising different number frames variable length words sentences ( outputs ) , successful modeling various complex temporal dynam- ics . Venugopalan et al . [     ] proposed architecture address variable representation problem input ( video frames ) output ( sentence ) stage . purpose used two-layered LSTM framework , sequence video frames input ﬁrst layer LSTM . hidden state ﬁrst LSTM layer forms input second layer LSTM . output second LSTM layer associated caption . LSTM parameters shared stages . Although sequence- to-sequence learning previously used machine translation [     ] , ﬁrst method [     ] use sequence-to-sequence approach video captioning . Later methods adopted similar framework , minor variations including attention mechanisms [     ] , making common visual-semantic-embedding [     ] using domain knowledge either language models [     ] visual classiﬁers [     ] . deep learning achieved much better results compared previously used classiﬁer based approaches , methods aimed producing one sentence video clip containing one major event . real-world applications , videos generally contain single event . Description multi-events semantically rich videos one sentence ends overly sim- pliﬁed , hence , uninformative . example , instead    saying “ someone sliced potatoes knife , chopped onions pieces put onions potatoes pot ” , single sentence generation method would prob- ably say “ someone cooking ” . Yu et al . [     ] proposed hierarchical recurrent neural network ( h-RNN ) applies attention mechanisms temporal spatial aspects . focused sentence decoder intro- duced hierarchical framework comprises sentence generator top paragraph generator . First , Gated Recurrent Unit ( GRU ) layer takes video features input generates single short sentence . recurrent layer generates paragraphs using context sentence vectors obtained sentence generator . paragraph generator thus captures dependencies be- tween sentences generates paragraph sentences related . Recently , Krishna et al . [    ] introduced con- cept dense-captioning events video employed action detection techniques predict temporal intervals . proposed model extract multiple events one single pass video , attempting describe detected events simultaneously . ﬁrst work kind detecting describing multiple overlapping events video . However , model achieve signiﬁcant improvement captioning benchmark .  . .  RNN - RNN Video Description Although popular CNN-RNN framework , another approach also encode visual information using RNNs . Srivastava et al . [     ] use one LSTM extract features video frames ( i.e . encoding ) pass feature vector another LSTM decoding . also introduced variants models predicted future sequences previous frames . au- thors adopted machine translation model [     ] visual recognition could achieve signiﬁcant improvement classiﬁcation accuracy . Yu et al . [     ] proposed similar approach used two RNN structures video description task . conﬁguration hierarchical decoder multiple Gated Recurrent Units ( GRU ) sentence generation . output decoder fed paragraph generator models time dependencies sentences focusing linguistic aspects . authors improved state-of-the-art results video description , however , method inefﬁcient videos involving ﬁne-grained activ- ities small interactive objects .  . .  Deep Reinforcement Learning Models Deep Reinforcement Learning ( DRL ) out-performed hu- mans many real-word games . DRL , artiﬁcial intelligent agents learn environment trial error adjust learning policies purely environmental rewards punishments . DRL approaches popularized Google Deep Mind [     ] , [     ] since      . Due absence straight forward cost function , learning mech- anisms approach considerably harder devise compared traditional supervised techniques . Two dis- tinct challenges evident reinforcement learning compared conventional supervised approaches : (   ) model full access function opti- mized . query function interaction . (   ) interaction environment state based present input depends previous actions . choice reinforcement learning algorithms depends scope problem hand . example , variants Hi- erarchical Reinforcement Learning ( HRL ) framework applied Atari games [    ] , [     ] . Similarly , different variants DRL used meet challenging requirements image captioning [     ] well video description [    ] , [    ] , [     ] , [     ] , [     ] . Xwang et al . [     ] proposed fully-differentiable neu- ral network architecture using reinforcement learning video description . method follows general encoder- decoder framework . encoding stage captures video frame features using ResNet-    [    ] . frame level fea- tures processed two stage encoder i.e . low level LSTM [     ] followed high level LSTM [    ] . decoding , employed HRL generate word word natural language descriptions . HRL agent comprises three components , low level worker accomplishes tasks set manager , high level manager sets goals internal critic ascertain whether task accomplished informs manager accordingly help manager update goals . process iterates till reaching end sentence token . method demonstrated capable capturing details video content thus generating ﬁne-grained descriptions . However , method shown little improvement existing baseline methods .      , Chen et al . [    ] proposed RL based model selecting key informative frames represent complete video , attempt minimize noise unnecessary computa- tions . Key frames selected maximize vi- sual diversity minimize textual discrepancy . Hence , compact subset  -  frames average represented full video . Evaluated several popular benchmarks , demonstrated video captions produced without performance degradation signiﬁcantly re- duced computational cost . method use motion features encoding , design trade-off speed accuracy . DRL based methods gaining popularity shown comparable results video description . Due uncon- ventional learning methodology , DRL methods unlikely suffer paucity labelled training data , hardware constraints overﬁtting problems . Therefore , meth- ods expected ﬂourish .   DATASETS availability labeled datasets video description main driving forces behind fast advance- ment research area . survey , summarize characteristics datasets give overview Table   . datasets categorized four main classes namely Cooking , Movies , Videos Wild Social Media . datasets , single caption per video assigned except datasets contain multiple sentences even paragraphs per video snippet .     .  Cooking  . .  MP-II Cooking Informatics ( MP-II ) Cooking Max Plank Institute dataset [     ] comprises    ﬁne grained cooking activities , performed    participants preparing    dishes fruit salad cake etc . data recorded kitchen camera installed ceiling .    cooking activities include “ wash hands ” , “ put bowl ” , “ cut apart ” , “ take drawer ” etc . person scene    frames ( one second ) performing activity annotated , “ background activity ” generated . ﬁne grained activities , example “ cut slices ” , “ pour ” , “ spice ” differentiated movements low inter-class high intra-class variability . total , dataset comprises    videos (    ,    frames ) , average length per clip approximately     seconds . dataset spans total   hours play length videos ,  ,    annotations .  . .  YouCook YouCook dataset [    ] consists    YouTube cooking videos different people cooking various recipes . back- ground ( kitchen/scene ) different videos . dataset represents challenging visual problem MP-II Cooking [     ] dataset recorded ﬁxed camera view point kitchen background . dataset divided six different cooking styles , example grilling , baking etc . machine learning , training set contains    videos test set contains    videos . Frame wise annotations objects actions also provided training videos . object categories dataset include “ utensils ” , “ bowls ” “ food ” etc . Amazon Mechanical Turk ( AMT ) employed human generated multiple natural language descrip- tions video . AMT worker provided least three sentences per video description , average   descriptions collected per video . See Figure   ( b ) example clips descriptions .  . .  TACoS Textually Annotated Cooking Scenes ( TACoS ) subset MP-II Composites [     ] . TACoS processed provide coherent textual descriptions high quality videos . Note MP-II Composites contain videos less activities MP-II Cooking [     ] . contains     high resolution videos    cooking activities . Videos MP-II Composites dataset span different lengths ranging  -   minutes average length  .  minutes . TACoS dataset constructed ﬁltering MP-II Composites , restricting activities involve manipulation cooking ingredients , least   videos activity . result , TACoS contains    ﬁne grained cooking activities     videos . AMT workers employed align sentences associated videos example : “ preparing carrots ” , “ cutting cucumber ” “ separating eggs ” etc . video ,    different textual descriptions collected . dataset comprises   ,    sentences containing   ,    actions descriptions . total    ,    words used dataset . Almost    % words i.e .   ,    describe TABLE   : Standard datasets benchmarking video description methods . Dataset Domain # classes # videos avg len # clips # sent # words vocab len ( hrs )    MSVD [    ] MPII Cooking [     ] YouCook [    ] TACoS [     ] TACos-MLevel [     ] MPII-MD [     ] M-VAD [     ] MSR-VTT [     ] Charades [     ] VTW [     ] YouCook II [     ] ActyNet Cap [    ] ANet-Entities [     ] VideoStory [    ] open cooking cooking cooking cooking movie movie open human open cooking open social media social media               - -        -    - - -                           ,     ,      ,     ,      ,      ,      k -    ,      ,       sec     sec -  ,      ,    - -  ,    -   ,     ,     ,    Nil    ,      ,        sec  ,      ,     ,        sec   ,      ,       ,      ,     .  sec   ,      ,     .  sec   ,      ,       ,      ,       sec   ,       ,     ,   ,      ,      ,       sec   ,       sec   . k     sec   . k     sec    ,     ,   ,        sec - - -  ,    - - - -   k    k -    k - - - - - - -  .   .   .    .    .    .    .    .    .      .     .     .  -    .  content example nouns , verbs , adjectives etc . words includes vocabulary size   ,    verb tokens . dataset also provides alignment sentences describing activities obtaining approximate time stamps activity starts ends . Figure   ( ) shows example clips descriptions .  . .  TACoS-MultiLevel TACoS Multilevel [     ] corpus annotations also col- lected via AMT workers TACoS corpus [     ] . video TACoS corpus , three levels descrip- tions collected include : (   ) detailed description video    sentences per video ; (   ) short description comprises  -  sentences per video ; ﬁnally (   ) single sentence description video . Annotation data provided form tuples object , activity , tool , source target person always subject . See Figure   ( e ) example clips descriptions .  . .  YouCook II YouCook-II Dataset [     ] consists      videos uniformly distributed    recipes . cooking videos sourced YouTube offer challenges open domain videos variations camera position , camera motion changing backgrounds . complete dataset spans total play time    .  hrs vocabulary      words . videos divided  -   segments per video average  .  segments per video elab- orating procedural steps . Individual segment length varies       seconds . segments temporally localized annotated . average length video     seconds reaching maximum     seconds . dataset randomly split train , validation test sets ratio    % :   % :   % respectively .  .  Movies  . .  MPII-MD MPII-Movie Description Corpus [     ] contains transcribed audio descriptions extracted    Hollywood movies . movies subdivided   ,    clips average length  .  seconds paired   ,    sentences amounting almost one sentence per clip . Every clip paired one sentence extracted script movie audio description data . Audio De- scriptions ( ADs ) collected ﬁrst retrieving audio streams movie using online services MakeMkV   Subtitle Edit   . audio streams tran- scribed using crowd sourced transcription service [   ] . transcribed texts aligned associated spoken sentences using time stamps . order remove misalignments audio content visual content , sentence also manually aligned corresponding video clip . manual alignment process , sentences describing content present video clip also ﬁltered . audio descriptions track added feature dataset tying describe visual content help visually impaired persons . total time span dataset videos almost   .  hours vocabulary size    ,    . Example clips descriptions shown Figure   ( f ) .  . .  M-VAD Montreal Video Annotation Dataset ( M-VAD ) [     ] based Descriptive Video Service ( DVS ) contains   ,    video clips    different movies . clip spanned  .  seconds average entire time complete dataset   .  hours . total number sen- tences   ,    , clips associated one sentence . vocabulary dataset spans   ,    words ( Nouns- ,    : Verbs- ,    : Adjectives- ,    : Adverbs-    ) . dataset split consists   ,    ,  ,     ,    video clips training , validation testing respec- tively . See Figure   ( g ) example clips descriptions .  .  Social Media  . .  VideoStory VideoStory [    ] multi sentence description dataset com- prising   k social media videos . dataset aimed address story narration description generation long  . https : //www.makemkv.com/  . http : //www.nikse.dk/SubtitleEdit/    Fig .   : Example video frames (   non-consecutive frames per clip ) captions various benchmark video description datasets . C -C  represent associated ( exemplary ) captions dataset . videos may sufﬁciently illustrated single sentence . video paired least one paragraph . average number temporally localized sentences per paragraph  .   . total       paragraphs dataset comprising    k sentences average   .   words per sentence . average , paragraph covers   .  % video content . dataset contains    % temporal overlap co-occurring events . dataset training , validation test split       ,     ,      videos respectively also proposes blind test set comprising      videos . training video accompa- nied one paragraph , however , videos validation test sets three paragraphs evaluation . Annotations blind test released available server benchmarking different methods .  . .  ActivityNet Entities ActivityNet Entities dataset ( ANet-Entities ) [     ] ﬁrst video dataset entities grounding annota- tions . dataset build training validation splits ActivityNet Captions dataset [    ] , different captions . dataset , noun phrases ( NPs ) video descriptions grounded bounding boxes video frames . dataset comprises       anno- tated videos ,   k video segments least one noun phrase annotated per segment    k bounding boxes annotations . dataset employs training set (   k ) similar ActivityNet Captions . However , validation set ActivityNet Captions randomly evenly split ANet-Entities validation (  . k ) testing (  . k ) sets .  .  Videos Wild  . .  MSVD Microsoft Video Description ( MSVD ) dataset [    ] comprises  ,    YouTube clips human annotated sentences . dataset also annotated AMT workers . audio muted clips avoid bias lexical choices descriptions . Furthermore , videos containing subtitles overlaid text removed quality control process dataset formulation . Finally , manual ﬁlter- ing carried submitted videos ensure video met prescribed criteria free inappropriate ambiguous content . duration video dataset typically       seconds mainly showing one activity . dataset comprises multilingual ( Chinese , English , German etc ) human generated descriptions . average ,    single sen- tence descriptions per clip . dataset frequently used research community detailed Results Section   . Almost research groups split dataset training , validation testing partitions      ,         videos respectively . Figure   ( ) shows example clips descriptions MSVD dataset .    training ,     validation      test videos . video comprises    reference captions annotated AMT workers . terms number clips multiple associated sen- tences , one largest video captioning datasets . addition video content , dataset also contains audio information potentially used multimodal research .  . .  Charades dataset [     ] contains      videos daily indoor household activities . videos recorded     AMT workers three different continents . given scripts describing actions objects required follow scripts perform actions speciﬁed objects . objects actions used scripts ﬁxed vocabulary . Videos recorded    different indoor scenes restricted use    objects     action classes . dataset comprises       annotations describing     actions . also provides       labels    object classes . Moreover , contains       descriptions covering videos . videos dataset depict daily life activities average duration    seconds . dataset split           videos training test purposes respectively .  . .  VTW Video Titles Wild ( VTW ) [     ] contains       video clips average  .  minutes duration per clip . clip described one sentence . However , incorporates diverse vocabulary , average one word appears two sentences across whole dataset . Besides single sentence per video , dataset also provides accompanying descriptions ( known augmented sentences ) describe information present visual content clip . dataset proposed video title generation opposed video content descrip- tion also used language-level understanding tasks including video question answering .  . .  ActivityNet Captions ActivityNet Captions dataset [    ] contains    k dense nat- ural language descriptions   k videos Activ- ityNet [     ] correspond approximately     hours . average , description composed   .   words covers    seconds video . multiple descriptions every video combined together , descriptions cover   .  % content present entire video . addition ,    % temporal overlap makes dataset especially interesting challenging studying multiple events occurring time . example dataset given Figure   ( h ) .  . .  MSR-VTT MSR-Video Text ( MSR-VTT ) [     ] contains wide vari- ety open domain videos video captioning task . comprises      videos subdivided   ,    clips . clips grouped    different categories . example shown Figure   ( c ) . dataset divided        VIDEO DESCRIPTION COMPETITIONS Another major driving force fast-paced develop- ment video description research comes many competitions challenges organized companies conferences recent years . major competitions listed . TABLE   : LSMDC Dataset Statistics . Dataset split LSMDC Training LSMDC Validation LSMDC Public Test LSMDC Blind Test LSMDC ( Total ) # movies                  # clips   ,     ,      ,     ,       ,    # words    ,      ,      ,      ,     ,   ,    # sent   ,     ,      ,     ,       ,    avg len ( sec )  .   .   .   .   .  tot len ( hrs )    .    .     .     .      .       .  LSMDC Large Scale Movie Description Challenge ( LSMDC ) [   ] started      conjunction ICCV      , ECCV workshop      . Challenge comprises test set released publicly blind test set withheld . server provided automatically evaluate [    ] results . challenge consists three primary tasks i.e . Movie Description , Annotation/Retrieval Fill-in-the-Blank . Since      , MovieQA challenge also included LSMDC addition previous three tasks . dataset challenge ﬁrst introduced ICCV      workshop [   ] . LSMDC dataset basically combines two benchmark datasets , M-VAD [     ] MPII- MD [     ] initially collected independently ( see Section  .  ) . two datasets merged Chal- lenge , overlaps removed avoid repetition movie test training sets . , man- ual alignments performed MPII-MD also removed validation test sets . dataset augmented clips ( without aligned annotations )    additional movies make blind test Challenge . additional clips added evaluation . ﬁnal LSMDC dataset    ,    video clips extracted     unique movies . approximately one sentence per clip . Names characters reference captions replaced token word “ SOMEONE ” . dataset split       training clips ,      validations clips ,       public test clips blind ( with- held ) test set      clips . average clip length ap- proximately  .  seconds . training set captions consists   ,    unique words . summary LSMDC dataset found Table   . survey benchmark results video description ( Section-  ) shows LSMDC emerged challenging dataset , evident poor performances several models . mentioned dataset section ( Sec- tion  .  ) , natural language descriptions movie clips typically sourced movie scripts audio descriptions , misalignments captions videos often occur text refer objects appeared cutting point clip . Misalignment certainly key contributing factor poor performances observed dataset . Submission protocol challenge similar MSCOCO Image Captioning Challenge [    ] , uses protocol automatic evaluation . Human evalu- ation used select ﬁnal winner . latest results automatic evaluation LSMDC publicly available [    ] .  .  MSR-VTT      , motivate challenge academic tech industry research community , Microsoft started Microsoft Research - Video Text ( MSR-VTT ) [   ] competi- tion aiming bringing together computer vision lan- guage researchers . dataset used competition MSR-VTT [     ] described dataset section ( Section  .  ) . participants competition asked develop video text model using MSR-VTT dataset . External datasets , either public private used help better object , action , scene , event detection , long external data used explicitly cited explained submission ﬁle . Unlike LSMDC , MSR-VTT challenge focuses video text task . challenge requires competing algorithm automatically generate least one natural language sentence describes informative part video . Accuracy benchmarked human gen- erated captions evaluation stage . evaluation based automatically computed score using multiple common metrics BLEU @   , METEOR , ROUGE-L , CIDEr-D . Details metrics given Section-   . Like LSMDC , human evaluations also used rank generated sentences .  .  TRECVID Text Retrieval Conference ( TREC ) series workshops emphasizing various subareas Information Retrieval ( IR ) research . particular , TREC Video Retrieval Evaluation ( TRECVID ) [   ] workshops , started      , dedicated research efforts content-based exploitation digital videos . primary areas interests include “ semantic indexing , video summarization , video copy detection , mul- timedia event detection ad-hoc video search ” [   ] . Since TREC-     , Video Text Description ( VTT ) [    ] using natural language also included challenge tasks . TRECVID-     VTT task used dataset   K automatically collected Twitter Vine videos , clip spans approximately   seconds . task performed manually annotated selected subset consists  ,    Twitter Vine videos . dataset divided four groups , G  , G  , G  G  , based number descriptions (     ) per videos . Furthermore , video tagged easy hard according difﬁculty level describing . Example frames VTT dataset show Figure    . TRECVID uses metrics METEOR , BLEU CIDEr ( details Section-   ) automatic evaluation , addition newly introduced metric , referred Se- mantic Text Similarity ( STS ) [    ] . name suggests , STS measures semantic similarity generated ref- erence descriptions . Human evaluations also employed gauge quality automatically generated descrip- tions following Direct Assessment ( DA ) [    ] method . Due high reliability , DA employed ofﬁcial ranking method machine translation benchmark evalua- tions [    ] . per DA based video description evaluation ,    Fig .    : Example video frames TRECVID-VTT dataset . ( ) Frames Easy-Video category ( b ) frames Hard-Video category . human assessors shown video-sentence pairs rate well sentence describes events video scale   −     [    ] .  .  ActivityNet Challenge ActivityNet Dense-Captioning Events Videos [   ] ﬁrst introduced      task ActivityNet Large Scale Activity Recognition Challenge [   ] , [    ] , running CVPR Workshop since      . task studies detection description multiple events video . Activ- ityNet Captions Dataset , multiple descriptions along time-stamps provided video clip , description covers unique portion clip . Together , multiple events clip covered narrated using set sentences . events may variable durations ( long short ) even overlap . Details dataset given Section  . .  Table   . Server based evaluations [   ] performed challenge . precision captions generated measured using BLEU , METEOR CIDEr metrics . latest results challenge also publicly available found online [   ] .   EVALUATION METRICS Evaluations performed machine generated cap- tions/descriptions videos divided Automatic Evaluations Human Evaluations . Automatic evaluations performed using six different metrics orig- inally designed machine translation image cap- tioning . metrics BLEU [     ] , ROUGEL [    ] , ME- TEOR [    ] , CIDEr [     ] , WMD [    ] , SPICE [    ] . , discuss metrics detail well limi- tations reliability . Human Evaluations performed unsatisfactory performance automatic metrics given numerous different ways correctly describe video .  .  Automatic Sentence Generation Evaluation Evaluation video descriptions , automatically manually generated , challenging speciﬁc ground truth “ right answer ” , taken reference benchmarking accuracy . video cor- rectly described wide variety sentences , may differ syntactically also terms semantic content . Consider sample MSVD dataset shown Figure    instance , several ground truth captions available video clip . Note caption describes clip equally valid , different way varied attentions levels details clip , ranging “ jet ” , “ commercial airplane ” “ South African jet ” “ ﬂying ” , “ soaring ” “ banking ” lastly “ air ” , “ blue sky ” “ clear sky ” . automatic evaluation , comparing gener- ated sentences ground truth descriptions , three evalua- tion metrics borrowed machine translation , namely , Bilingual Evaluation Understudy ( BLEU ) [     ] , Recall Ori- ented Understudy Gisting Evaluation ( ROUGE ) [    ] Metric Evaluation Translation Explicit Order- ing ( METEOR ) [    ] . Consensus based Image Description Evaluation ( CIDEr ) [     ] Semantic Propositional Im- age Captioning Evaluation ( SPICE ) [    ] two re- cently introduced metrics speciﬁcally designed image captioning tasks , also used automatic evaluation video description . Table   gives overview metrics included survey . addition automatic evaluation metrics , human evaluations also employed determine performance automated video description algorithms .  . .  Bilingual Evaluation Understudy ( BLEU ,      ) BLEU [     ] popular metric used quantify quality machine generated text . quality measures corre- spondence machine human outputs . BLEU scores take account overlap predicted uni– grams ( single word ) higher order n–gram ( sequence n adjacent words ) set one candidate reference sentences . According BLEU , high-scoring description match ground truth sentence length i.e . exact match words well order . BLEU evaluation score   exact match . Note number reference sentences ground truth per video , chances higher BLEU score . primarily designed evaluate text corpus level , therefore , use evaluation metric individual sentences may fair . BLEU calculated , log BLEU = min (   − lr lc ,   ) + N ( cid:   ) n=  wn log pn . equation , lr/lc ratio lengths corresponding reference corpus candidate description , wn positive weights , pn geometric average modiﬁed n-gram precisions . second term computes actual match score , ﬁrst term brevity penalty penalizes descriptions shorter reference description .    Fig .    : example MSVD [    ] dataset associated ground truth captions . Note video clip described differently . caption describes activity wholly partially different way .  . .  Recall Oriented Understudy Gisting Evaluation ( ROUGE ,      ) ROUGE [    ] metric proposed      evaluate text summaries . calculates recall score generated sentences corresponding reference sentences using n–grams . Similar BLEU , ROUGE also computed varying n–gram count . However , unlike BLEU based precision , ROUGE based recall values . Moreover , n–gram variants ROUGEn , versions known , ROUGEL ( Longest Common Subsequence ) , ROUGEW ( Weighted Longest Common Sub- sequence ) , ROUGES ( Skip-Bigram Co-Occurrences Statis- tics ) , ROUGESU ( extension ROUGES ) . refer reader original paper details . version used image video captioning evaluation ROUGEL , computes recall precision scores longest common subsequences ( LCS ) generated refer- ence sentence . metric compares common subsequences words candidate reference sentences . intu- ition behind longer LCS candidate reference sentences corresponds higher similarity two summaries . words need consecutive sequence . ROUGE-N computed ROUGE-N = ( cid:   ) S∈RSum ( cid:   ) S∈RSum ( cid:   ) gn∈S ( cid:   ) gn∈S Cm ( gn ) , C ( gn ) n n-gram length , gn , Cm ( gn ) represents highest number n-grams present candidate well ground truth summaries RSum stands reference summaries . LCS-based F-measure score computed ﬁnd similar summary length summary B length n. sentence ground truth summary B sentence candidate generated summary . recall Rlcs , precision Plcs f-score Flcs calculated  Rlcs = LCS ( , B )  , Plcs = LCS ( , B ) n , Flcs = (   + β  ) RlcsPlcs Rlcs + β Plcs , LCS ( , B ) length longest common subse- quence B , β = Plcs/Rlcs . LCS-based F-measure score computed equation Flcs known ROUGEL score . ROUGEL   = B , zero case B commonalities i.e . LCS ( , B ) =   . One advantages ROUGEL consider successive matches words employs in- sequence matches within sentence . Moreover , pre-deﬁning n-gram length also required automati- cally incorporated LCS .  . .  Metric Evaluation Translation Explicit Or- dering ( METEOR ,      ) METEOR [    ] proposed address shortcomings BLEU [     ] . Instead exact lexical match required BLEU , METEOR introduced semantic matching . METEOR takes WordNet [    ] , lexical database English lan- guage account various match levels , including exact words matches , stemmed words matches , synonymy match- ing paraphrase matching . METEOR score computation based well generated reference sentences aligned . sen- tence taken set unigrams alignment done mapping unigrams candidate reference sentences . mapping , unigram candidate sentence ( refer- ence sentence ) either map unigram reference sentence ( candidate sentence ) zero . case multiple options available alignments two sentences , alignment conﬁguration less number crossings preferred . ﬁnalizing alignment process , METEOR score calculated . Initially , unigram based precision score P calculated using P = mcr/mct relationship . mcr represents number unigrams co-occurring candidate , well reference sentences mct corresponds total number unigrams candidate sentences . unigram based recall score R calculated using R = mcr/mrt . mcr represents number unigrams co-occurring candidate well reference sentences . However , mrt number unigrams reference sentences . , precision recall scores used compute F-score using following equation : Fmean =   P R R +  P . precision , recall F-score measures account unigram based congruity cater n–grams . n–gram based similarities used calculate penalty p alignment candidate reference sentences . penalty takes account non-adjacent mappings two sentences . penalty calculated grouping unigrams minimum number chunks . chunk includes unigrams adjacent candidate TABLE   : Summary metrics used video description evaluation .    Machine translation Document summarization n-gram recall Methodology n-gram precision Metric Name Designed BLEU [     ] ROUGE [    ] METEOR [    ] Machine translation CIDEr [     ] SPICE [    ] WMD [    ] Image captioning Image captioning Document similarity n-gram synonym matching tf-idf weighted n-gram similarity Scene-graph synonym matching Earth mover distance word vec well reference sentences . generated sentence exact match reference sentence one chunk . penalty computed p =     ( Nc Nu )   , gn ( sij ) . , CIDEr uses higher order n-grams ( higher order , longer sequence words ) capture grammatical properties richer semantics text . matter , combines scores different n-grams using following equation : Nc represents number chunks Nu corresponds number unigrams grouped together . METEOR score sentence computed : CIDEr ( ci , Si ) = N ( cid:   ) n=  wnCIDErn ( ci , Si ) . = Fmean (   − p ) . Corpus level score computed using equa- tion using aggregated values arguments i.e . P , R p. case multiple reference sentences , maximum METEOR score generated reference sentence taken . date , correlation METEOR score human judgments better BLEU score . Moreover , Elliot et al . [    ] also found METEOR better evaluation metric compared contemporary met- rics . conclusion based Spearman ’ correlation computation automatic evaluation metrics human judgments .  . .  Consensus based Image Description Evaluation ( CIDEr ,      ) CIDEr [     ] recently introduced evaluation metric image captioning task . evaluates consensus predicted sentence ci reference sentences corre- sponding image . performs stemming converts words candidate well reference sentences root forms e.g . stems , stemmer , stemming , stemmed root word stem . CIDEr treats sentence set n–grams containing     words . encode con- sensus predicted sentence reference sentence , measures co-existence frequency n-grams sentences . Finally , n–grams common among reference sentences images given lower weight , likely less informative image content , biased towards lexical structure sentences . weight n–gram computed using Term Frequency Inverse Document Frequency ( TF- IDF ) [     ] . term TF puts higher weightage frequently occurring n–grams reference sentence image , whereas IDF puts lower weightage commonly appearing n–grams across whole dataset . Finally , CIDErn score computed CIDErn ( ci , Si ) =    ( cid:   ) j gn ( ci ) .gn ( sij ) ( cid:    ) gn ( ci ) ( cid:    ) . ( cid:    ) gn ( sij ) ( cid:    ) , gn ( ci ) vector representing n–grams length n ( cid:    ) gn ( ci ) ( cid:    ) depicts magnitude gn ( ci ) . true popular version CIDEr image video description evaluation CIDEr-D , incorporates modiﬁcations originally proposed CIDEr prevent higher scores captions badly fail human judgments . Firstly , proposed removal stemming ensure correct form words used . Otherwise , multiple forms verbs ( singular , plural etc ) mapped token producing high score incorrect sentences . Secondly , ensure words high conﬁdence repeated sentence high score produced original CIDEr produces even sentence make sense . done introducing Gaussian penalty length differences candidate reference sentences clipping n–grams count equal number occurrences reference sentence . latter ensures desired sentence length achieved repetition high conﬁdence words get high score . aforementioned changes makes metric robust ensures high correlation score [     ] .  . .  Word Mover ’ Distance ( WMD ,      ) WMD [    ] makes use word embeddings semantically meaningful vector representations words learnt text corpora . WMD distance measures dis- similarity two text documents . Two captions different words may still semantic meanings . hand , possible multiple captions Fig .    : Components WMD metric query D  two sentences D  D  BOW distance . D  less distance  .   matches query D  D  distance  .   . arrows show ﬂow two words labeled distance contribution . Figure adapted [    ] .    TABLE   : Variations automatic evaluation metric scores four types changes made candidate sentence i.e . words replaced synonyms , added redundancy sentence , changing word order , shortening sentence length . ﬁrst row shows upper bound scores BLEU-  , METEOR , ROUGE , CIDEr represented B , , R , C respectively . Variation reference candidate synonyms redundancy word order short length Description elderly man playing piano front crowd anteroom elderly man showing play piano front crowd hall room old man demonstrating play piano front crowd hall room elderly man showing play piano front crowd hall room woman elderly man front crowd showing play piano hall room man playing piano B      R   C     .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .   attributes , objects relations still different meanings . WMD proposed address problem . word embeddings good capturing semantic meanings easier compute WordNet thanks distributed vector representations words . distance two texts casted Earth Mover ’ Distance ( EMD ) [     ] , typically used transportation calculate travel cost using word vec embeddings [     ] . metric , caption description represented bag-of-words histogram includes start stop words . magnitude bag-of-words his- togram normalized . account semantic similar- ities exist pairs words , WMD metric uses Euclidean distance word vec embedding space . distance two documents captions deﬁned cost required move words captions . Figure    illustrates example WMD calculation process . WMD modelled special case EMD [     ] solved linear optimization . Compared BLUE , ROUGE CIDEr , WMD less sensitive words order synonym swapping . , similar CIDEr METEOR , gives high correlation human judgments .  . .  Semantic Propositional Image Captioning Evaluation ( SPICE ,      ) SPICE [    ] latest proposed evaluation metric im- age video descriptions . SPICE measures similarity scene graph tuples parsed machine generated descriptions ground truth . semantic scene graph encodes objects , attributes relation- ships dependency parse tree . scene graph tuple G ( c ) caption c consists semantic tokens object classes ( c ) , relation types R ( c ) attribute types ( c ) , G ( c ) = ( cid:    ) ( c ) , R ( c ) , ( c ) ( cid:    ) . SPICE computed based F -score tuples machine generated descriptions ground truth . Like METEOR , SPICE also uses WordNet ﬁnd treat synonyms positive matches . Although , current literature , SPICE score employed much one obvious limiting factor performance could quality parsing . instance , sentence ‘ ‘ white dog swimming river ” , failure case could word “ swimming ” parsed “ object ” word “ dog ” parsed “ attribute ” resulting bad score .  .  Human Evaluations Given lack reference captions low correlation human judgments automated evaluation metrics , human evaluations also often used judge quality machine generated captions . Human evaluations may either crowd-sourced , AMT workers specialist judges competitions . human evaluations structured using measurements Rele- vance Grammar Correctness . relevance based evaluation , video content relevance given subjective scores , highest score given “ Relevant ” minimum score “ Least Relevant ” . score two sentences can- unless identical . approaches grammar correctness measured , sentences graded based grammatical correctness without showing video content evaluators case , one sentence may score .  .  Limitations Evaluation Metrics Like video description , evaluation machine generated sentences equally difﬁcult task . metric speciﬁcally designed evaluating video description , in- stead machine translation image captioning metrics extended task . automatic metrics compute score given reference candidate sentences . paradigm serious problem several different ways describe video , correct time , depending upon “ described ” ( content selection ) “ described ” ( realiza- tion ) . metrics fail incorporate variations , therefore , far perfect . Various stud- ies [    ] , [     ] examined metric scores behave different conditions . Table   , perform similar experiments [    ] additional variation short length . First , original caption evaluated analyze maximum possible score achievable metric ( ﬁrst row Table   ) . Next , minor modiﬁcations introduced candidate sentences measure evaluation metrics behave . observed metric scores reduced , BLEU CIDEr affected , words replaced synonyms . apparently due failure match synonyms . experiments revealed metrics generally stable sentence perturbed additional words . However , changing word order sentence found alter scores n-gram based metrics like BLEU , ROUGE CIDEr signiﬁcantly ROUGE extent . hand , WMD SPICE found robust word order changes [    ] . Lastly , reducing sentence length signiﬁcantly affected BLEU , METEOR ROUGE scores little effect CIDEr score i.e . scores reduced    % ,    % ,    %   % respectively .  .  Reliability Evaluation Metrics good method evaluate video descriptions compare machine generated descriptions ground truth descriptions annotated humans . However , shown Figure    , reference captions vary within represent samples valid samples video clip . reference sample captions create better solution space hence lead reliable evaluation . Another aspect evaluation problem syntactic variations candidate sentences . problem also exists well studied ﬁeld machine translation . case , sentence source language translated various sentences target language . Syntactically different sentences may still semantic content . nutshell , evaluation metrics assess suitability caption visual input comparing well candidate caption matches reference caption ( ) . agreement metric scores human judgments ( i.e . gold standard ) improves increased number reference captions [     ] . Numerous studies [     ] , [     ] , [     ] , [     ] , [     ] also found CIDEr , WMD , SPICE METEOR higher correlations human judgments regarded superior amongst contemporary metrics . WMD SPICE recent automatic caption evaluation metrics studied extensively literature time survey .   BENCHMARK RESULTS summarize benchmark results various techniques video description dataset . group methods based dataset reported results order chronologically . Moreover , multiple variants model , best reported results reported . detailed analysis method variants , original paper consulted . addition , multiple n–gram scores reported BLEU metric , chosen BLEU @   results closest human evaluations . Table   , see methods reported results MSVD dataset , followed MSR-VTT , M-VAD , MPII-MD , ActivityNet Captions . popularity MSVD attributed diverse nature YouTube videos large number reference captioning . MPII-MD , M-VAD , MSR-VTT ActivityNet Captions popular size inclusion competitions ( see Section   ) .    Another key observation earlier works mainly reported results terms subject , verb , object ( SVO ) cases place ( scene ) detection accuracies video , whereas recent works started report sentence level matches using automatic evaluation met- rics . Considering diverse nature datasets limitations automatic evaluation metrics , analyze results different methods using four popular metrics namely BLEU , METEOR , CIDEr ROUGE . Table   summarizes results MSVD dataset . GRU- EVE [    ] achieves best performance METEOR ROUGEL metrics second best CIDEr metric whereas LSTM-TSA [     ] M -IC [     ] report best BLEU scores . RecNetlocal [     ] best CIDEr score second best BLEU score . shown Table   , TACoS Multilevel dataset , h-RNN [     ] best results reported metrics i.e . BLEU , METEOR CIDEr . method provide ROUGE score . challenging M-VAD dataset , overall reported results ( Table   ) poor , however , within presented results see far Temporal-Attention [     ] , HRNE [     ] reported results using BLEU metric BLEU score  .  . papers using dataset report METEOR results far BAE [    ] produced best METEOR score followed LSTM-TSA [     ] . HRNE [     ] Glove+Deep Fusion Ensemble [     ] share third place METEOR score . MPII-MD another challenging dataset still low benchmark results , shown Table   , similar M-VAD dataset . BAE [    ] reported BLEU score dataset . LSTM-TSA [     ] achieved best METEOR score followed LSTM-E [     ] S VT [     ] second third place respectively . paper using dataset reported CIDEr ROUGE score except BAE [    ] . Results another popular dataset , MSR-VTT , over- better M-VAD MPII-II datasets . shown Table   , CST-GT-None [     ] reported highest score four metrics i.e . BLEU , METEOR , CIDEr ROUGE . DenseVidCap [     ] HRL [     ] respectively report second third best scores BLEU metric . GRU-EVE [    ] reports third best score METEOR CIDEr metrics . Results another recent popular ActivityNet Cap- tions dataset presented Table    . dataset primarily introduced dense video captioning gaining popularity quickly . dataset , Dense- Cap Model [    ] stands top terms BLEU score . Best METEOR score reported LSTM-A+PG+R [     ] . Highest scores CIDEr ROUGE metrics achieved methods DVC [    ] JEDDi-Net [     ] respectively . Finally , Table    , report two results LSMDC Charades one result YouCook-II datasets . YouCook-II also recent dataset reported much literature . summarize best reporting methods dataset along published scores . tables group methods used dataset ( ) . Hence , one infer difﬁculty level datasets comparing intra dataset scores methods popularity particu- lar dataset number methods reported results . TABLE   : Performance video captioning methods MSVD dataset . Higher scores better metrics . best score metric shown bold .    Techniques / Models / Methods RBS+RBS & RF-TP+RBS [    ] SVO-LM ( ) [    ] FGM [     ] LSTM-YT [     ] TA [     ] S VT [     ] h-RNN [     ] MM-VDN [     ] Glove + Deep Fusion Ensble [     ] S FT [     ] HRNE [     ] GRU-RCN [    ] LSTM-E [     ] SCN-LSTM [    ] LSTM-TSA [     ] TDDF [     ] BAE [    ] PickNet [    ] M  − IC [     ] RecNetlocal [     ] TSA-ED [     ] GRU-EVE [    ] Yr                                                                                                               Dataset MSVD MSVD MSVD MSVD MSVD MSVD MSVD MSVD MSVD MSVD MSVD MSVD MSVD MSVD MSVD MSVD MSVD MSVD MSVD MSVD MSVD MSVD BLEU METEOR CIDEr ROUGE Results  .  +  .    .  +  .   SVO Accuracy   .    .  -   .    .    .  -   .    .    .    .    .    .    .    .    .    .    .    .  SVOP Accuracy   .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .  -   .   -   .  - - - -   .  -   .    .    .    .    .  -   .    .    .  - - - - - - - - - - - -   .  -   .  -   .  -   .  TABLE   : Performance video captioning methods TACoS-MLevel dataset . Higher scores better metrics . best score metric shown bold . Techniques / Models / Methods SMT ( SR ) + Prob I/P [     ] CRF + LSTM-Decoder [    ] h-RNN [     ] JEDDi-Net [     ] Yr                     Dataset TACoS MLevel TACoS MLevel TACoS MLevel TACoS MLevel Results BLEU   .    .    .    .  METEOR - -   .    .   CIDEr - -    .     .   ROUGE - - -   .   TABLE   : Performance video captioning methods M-VAD dataset . Techniques / Models / Methods Temporal-Attention ( TA ) [     ] S VT [     ] Visual-Labels [     ] HRNE [     ] Glove + Deep Fusion Ensemble [     ] LSTM-E [     ] LSTM-TSA [     ] BAE [    ] Yr                                         Dataset M-VAD M-VAD M-VAD M-VAD M-VAD M-VAD M-VAD M-VAD BLEU  .  - -  .  - - - - Results METEOR  .   .   .   .   .   .   .   .  CIDEr  .  - - - - - - - ROUGE - - - - - - - - TABLE   : Performance video captioning methods MPII-MD dataset . Techniques / Models / Methods S VT [     ] Visual-Labels [     ] SMT [     ] Glove + Deep Fusion Ensemble [     ] LSTM-E [     ] LSTM-TSA [     ] BAE [    ] Yr                                    Dataset MPII-MD MPII-MD MPII-MD MPII-MD MPII-MD MPII-MD MPII-MD Results BLEU - - - - - -  .  METEOR  .   .   .   .   .   .   .  CIDEr - - - - - -   .  ROUGE - - - - - -   .  TABLE   : Performance video captioning methods MSR-VTT dataset ..    Techniques / Models / Methods Alto [     ] VideoLab [     ] RUC-UVA [    ] v t-navigator [    ] TDDF [     ] DenseVidCap [     ] CST-GT-None [     ] PickNet [    ] HRL [     ] M  − V C [     ] RecNetlocal [     ] GRU-EVE [    ] Yr                                                             Dataset MSR-VTT MSR-VTT MSR-VTT MSR-VTT MSR-VTT MSR-VTT MSR-VTT MSR-VTT MSR-VTT MSR-VTT MSR-VTT MSR-VTT BLEU   .    .    .    .    .    .    .    .    .    .    .    .  Results METEOR   .    .    .    .    .    .    .    .    .    .    .    .  CIDEr   .    .    .    .    .    .    .    .    .  -   .    .  ROUGE   .    .    .    .    .    .    .    .    .  -   .    .  TABLE    : Performance video captioning methods ActivityNet Captions dataset . Techniques / Models / Methods Dense-Cap Model [    ] LSTM-A+PG+R [     ] TAC [     ] JEDDi-Net [     ] DVC [    ] Bi-SST [     ] Masked Transformer [     ] Yr                                    Dataset ActivityNet Cap ActivityNet Cap ActivityNet Cap ActivityNet Cap ActivityNet Cap ActivityNet Cap ActivityNet Cap BLEU  .   - -  .    .    .    .   Results METEOR  .    .    .    .     .    .    .   CIDEr   .  - -   .     .     .   - ROUGE - - -   .   -   .   - TABLE    : Performance video captioning methods various benchmark datasets . Techniques / Models / Methods CT-SAN [     ] GEAN [     ] HRL [     ] TSA-ED [     ] Masked Transformer [     ] Yr                          Dataset LSMDC LSMDC Charades Charades YouCook-II Results BLEU  .  -   .    .   .   METEOR  .   .    .    .   .   CIDEr   .   .    .    .  - ROUGE   .    .    .  - -   FUTURE EMERGING DIRECTIONS visual reasoning dataset . Automatic video description come far since pioneer methods , especially adoption deep learning . Although performance existing methods still far humans , gap diminishing steady rate still ample room algorithmic improvements . , list several possible future emerging directions potential advance research area . Visual Reasoning : Although video VQA still nascent stage , beyond VQA visual reasoning problem . promising ﬁeld explore . model made answer particular question reason chose particular answer . example video road side parking marks shown , question “ vehicle parked ? ” , model answers correctly , “ Yes ” . next question “ ? ” model reasons parking sign road means legal park . Another example expla- nations generated self driving cars [    ] system keeps passengers conﬁdence generating natural language descriptions reasons behind decisions e.g . slow , take turn etc . example visual reasoning models MAC Network [    ] able think reason giving promising results CLEVR [    ] , Visual Dialogue : Similar audio dialogue ( e.g . Siri , Hello Google , Alexa ECHO ) , visual dialogue [    ] another promising ﬂourishing ﬁeld , especially era look forward interact robots . visual dialogue , given video , model asked series questions sequen- tially dialogue/conversation manner . model tries answer ( matter right wrong ) questions . different visual reasoning model argues reasons lead model choose particular answers . Audio Video : majority computer vision research focused video description , without help audio , audio naturally present videos . Audio help video description providing background information instance , sound train , ocean , trafﬁc visual cue presence . Audio ad- ditionally provide semantic information example , person saying side phone . also provide clues story , context sometimes explicitly mention object action complement video information . Therefore , using audio video description models certainly improve per- formance [    ] , [     ] . External Knowledge : video description , time comparing performance humans extensive domain prior knowledge . humans watch clip describe , time ’ rely solely visual ( even audio ) content . Instead , additionally employ background knowledge . Similarly , would interesting promising approach augment video description techniques prior external knowledge [     ] . approach shown signiﬁcantly better performance visual question answer- ing methods likely improve video description accuracy . Addressing Finite Model Capacity : Existing methods trying perform end-to-end training using much data possible better learning . However , approach inherently limited learning matter big training dataset becomes , never cover combinatorial complexity real world events . Therefore , learning use data rather learning data , important may help improve upcoming system performances . Video Description Subtitle Generation : conjunction machine translation , video captioning may used automatic video subtitling . Currently manual , time consuming , costly process . line research beneﬁcial entertainment , one largest industries world , potentially help improve comprehension audiovisual material visually hearing impaired , second language learners . Automatic Evaluation Measures : far video description relied automatic metrics designed machine trans- lation image captioning tasks . date automatic video description ( even captioning ) evaluation metric purpose designed . Although metrics designed image captioning relevant , limita- tions . problem going exacerbate future dense video captioning story telling tasks . need evaluation metric closer human judg- ments encapsulate diversities realizations visual content . promising research direction use machine learning learn metric rather hand engineer .   CONCLUSION presented ﬁrst comprehensive literature survey video description research , starting classical meth- ods based Subject-Verb-Object ( SVO ) tuples sophisticated statistical deep learning based methods . reviewed popular benchmark datasets commonly used training testing models discussed international competitions/challenges regularly held promote video description research . discussed , detail , available automatic evaluation metrics video description , highlighting attributes limitations . presented comprehensive summary results obtained recent methods benchmark datasets using metrics . results show relative performance existing methods also highlight varying difﬁculty levels datasets robust- ness trustworthiness evaluation metrics . Finally , put forward recommendations future research    directions likely push boundaries research area . algorithm design perspective , although LSTMs shown competitive caption generation performance , interpretablity intelligibility underlying mod- els low . Speciﬁcally , hard differentiate much visual features contributed generation spe- ciﬁc word compared bias comes naturally language model adopted . problem exacerbated aim diagnose generation erroneous captions . example , see caption “ red ﬁre hydrant ” generated video description model frame containing “ white ﬁre hydrant ” , difﬁcult ascertain whether color feature incorrectly encoded visual feature extractor due bias used language model towards “ red ﬁre hydrants ” . Future research must focus improving diagnostic mechanisms pin point problematic part architectures improved replaced . survey shows major bottleneck hindering progress along line research lack effec- tive purposely designed video description evaluation metrics . Current metrics adopted either machine translation image captioning fall short measuring quality machine generated video captions agreement human judgments . One way improve metrics increase number reference sentences . believe purpose built metrics learned data key advancing video description research . challenges come diverse nature videos . instance , multiple activities video , captions represent activities , could lead low video description performance model . Similarly , longer duration videos pose challenges since action features encode short term actions trajectory features C D features [     ] dependent video segment lengths . feature extractors suitable static smoothly changing images hence struggle handle abrupt scene changes . Current methods rather simplify visual encoding part representing holistic videos frames . Attention models may need explored focus spatially temporally signiﬁcant parts video . Similarly , temporal modeling visual features quite rudimentary existing methods . methods either use mean pooling completely discards temporal information use C D model model    frames . Future re- search focus designing better temporal modeling architectures preferably learn end-to-end fashion rather disentangling visual description temporal model temporal modeling language description . ACKNOWLEDGEMENTS authors acknowledge Marcus Rohrbach ( Facebook AI Research ) valuable input . research supported ARC Discovery Grant DP          DP          . REFERENCES [   ] TREC Video Retrieval Evaluation ( TRECVID ) Challenge . https : // trecvid.nist.gov/ . [   ] Casting words transcription service ,     . http : //castingwords . com/ . [   ] Describing Understanding Video Large Scale Movie Description Challenge ( LSMDC ) ,     . https : //sites.google.com/ site/describingmovies/ . [   ] Microsoft Research - Video Text ( MSR-VTT ) Challenge ,      . http : //ms-multimedia-challenge.com/    /challenge . [   ] Activity Net Captions Challenge , Evaluations ,     . https : // github.com/ranjaykrishna/densevid eval . [   ] Activity Net Captions Challenge , Results ,     . http : //activity-net . org/challenges/    /evaluation.html [   ] Activity Net Captions Challenge , Task  : Dense-Captioning Events Videos ,     . http : //activity-net.org/challenges/    /index . html . [   ] Activity Net Challenge ,     . http : //activity-net.org/challenges/     /index.html . [   ] Language Vision ,     . https : //www.sciencedirect.com/ journal/computer-vision-and-image-understanding/vol/    . [    ] Large Scale Movie Description Challenge ( LSMDC ) https : //competitions.codalab.org/ Online Evaluations , competitions/     # learn details-evaluation .      . [    ] Large Scale Movie Description Challenge ( LSMDC ) Online     . https : //competitions.codalab.org/competitions/ Results ,      # results . [    ] N. Aafaq , N. Akhtar , W. Liu , S. Z. Gilani A. Mian .      . Spatio-Temporal Dynamics Semantic Attribute Enriched Visual Encoding Video Captioning . IEEE CVPR . [    ] J. Alayrac , P. Bojanowski , N. Agrawal , J. Sivic , I. Laptev , . Lacoste-Julien .      . Unsupervised learning narrated instruc- tion videos . IEEE CVPR . [    ] P. Anderson , B. Fernando , M. Johnson , S. Gould .      . Spice : Semantic propositional image caption evaluation . IEEE ECCV [    ] B. Andrei , E. Georgios , H. Daniel , M. Krystian , N. Siddharth , X . Caiming , Z. Yibiao .      . Workshop Language Vision CVPR      . [    ] B. Andrei , M. Tao , N. Siddharth , Z. Quanshi , S. Nishant , L. Jiebo , S. Rahul .      . Workshop Language Vision CVPR     . http : //languageandvision.com/ . [    ] R. Anna , T. Atousa , R. Marcus , P. Christopher , L. Hugo , C. Aaron , S. Bernt .      . Joint Video Language Understanding Workshop ICCV      . [    ] S. Antol , A. Agrawal , J. Lu , M. Mitchell , D. Batra , C. L. Zitnick , D. Parikh .      . VQA : Visual question answering . IEEE ICCV . [    ] G. Awad , J. Fiscus , M. Michel , D. Joy , W. Kraaij , A. F. Smeaton , G . Qunot , M. Eskevich , R. Aly , G. J. F. Jones , et al .      . Evaluating Video Search , Video Event Detection , Localization Hyperlink- ing , TRECVID      . [    ] D. Bahdanau , K. Cho , Y. Bengio .      . Neural machine translation jointly learning align translate . arXiv preprint arXiv:    .     , (      ) . [    ] N. Ballas , L. Yao , C. Pal , A. Courville .      . Delving deeper convolutional networks learning video representations . arXiv preprint arXiv:    .      , (      ) . [    ] S. Banerjee A. Lavie .      . METEOR : automatic metric MT evaluation improved correlation human judgments . ACL workshop intrinsic extrinsic evaluation measures MT and/or summarization .   -   . [    ] L. Baraldi , C. Grana , R. Cucchiara .      . Hierarchical Boundary-Aware Neural Encoder Video Captioning . IEEE CVPR [    ] A. Barbu , A. Bridge , Z. Burchill , D. Coroian , S. Dickinson , S. Fidler , A. Michaux , S. Mussman , S. Narayanaswamy , D. Salvi , et al .      . Video sentences . arXiv preprint arXiv:    .     , (      ) . [    ] K. Barnard , P. Duygulu , D. Forsyth , N. D. Freitas , D. M. Blei , M. I. Jordan .      . Matching words pictures . Journal Machine Learning Research   , Feb (      ) ,     -     . [    ] T. Berg , A. Berg , J. Edwards , M. Maire , R. White , Y. Teh , E . Learned-Miller , D. A. Forsyth .      . Names faces news . IEEE CVPR . [    ] A. F. Bobick A. D. Wilson .      . state-based approach representation recognition gesture . IEEE TPAMI    ,    (      ) ,     -     .    [    ] O. Bojar , R. Chatterjee , C. Federmann , Y. Graham , B. Haddow , . Huang , M. Huck , P. Koehn , Q. Liu , V. Logacheva , et al .      .  nd Conference Machine Translation .    -    . [    ] G. Burghouts , H. Bouma , R. D. Hollander , V. D. Broek , K . Schutte .      . Recognition    human behaviors video . Int . Symp . Optronics Defense Security , OPTRO . [    ] F. C. Heilbron , V. Escorcia , B. Ghanem , J. Carlos Niebles ,      . Activitynet : large-scale video benchmark human activity understanding . IEEE CVPR . [    ] M. Brand .      . ” Inverse hollywood problem ” : video scripts storyboards via causal analysis . AAAI/IAAI . Citeseer ,    -    . [    ] R. Chaudhry , A. Ravichandran , G. Hager , R. Vidal .      . His- tograms oriented optical ﬂow Binet-Cauchy kernels nonlinear dynamical systems recognition human actions , CVPR      . [    ] D. Chen W. Dolan .      . Collecting highly parallel data paraphrase evaluation . ACL : Human Language Technologies- Volume   . ACL ,    -    . [    ] D. Chen , W. Dolan , S. Raghavan , T. Huynh , R. Mooney .      . Collecting highly parallel data paraphrase evaluation . JAIR : - Volume    . ACL ,    -    . [    ] X. Chen , H. Fang , T. Lin , R. Vedantam , S. Gupta , P. Dollr , C. Zitnick .      . Microsoft COCO captions : Data collection evaluation server . arXiv preprint arXiv:    .      , (      ) . [    ] Y. Chen , S. Wang , W. Zhang , Q. Huang .      . Less : Picking Informative Frames Video Captioning . arXiv preprint arXiv:    .      , (      ) . [    ] K. Cho , B. V. Merrinboer , D. Bahdanau , Y. Bengio .      . properties neural machine translation : Encoder-decoder approaches . arXiv preprint arXiv:    .     , (      ) . [    ] J. Corso .      . GBS : Guidance Semantics-Using High-Level Vi- sual Inference Improve Vision-Based Mobile Robot Localization . Technical Report . State Univ New York Buffalo Amherst . [    ] N. Dalal B. Triggs .      . Histograms oriented gradients human detection . IEEE Computer Society Conference CVPR . [    ] N. Dalal , B. Triggs , C. Schmid .      . Human detection using oriented histograms ﬂow appearance . IEEE ECCV . [    ] P. Das , C. Xu , R. F. Doell , J. J. Corso .      . thousand frames words : Lingual description videos latent topics sparse object stitching . IEEE CVPR . [    ] A. Das , S. Kottur , K. Gupta , A. Singh , D. Yadav , J. M. F. Moura , . Parikh , D. Batra .      . Visual Dialog . IEEE CVPR [    ] J. Deng , K. Li , M. , H. Su , L. Fei-Fei .      . Construction analysis large scale image ontology . Vision Sciences Society     ,   (      ) . [    ] D. Ding , F. Metze , S. Rawat , P. F. Schulam , S. Burger , E. Younessian , L. Bao , M. G. Christel , A. Hauptmann .      . Beyond audio video retrieval : towards multimedia summarization .  nd ACM International Conference Multimedia Retrieval ( ICMR ) . [    ] J. Donahue , L. A. Hendricks , S. Guadarrama , M. Rohrbach , . Venugopalan , K. Saenko , T. Darrell .      . Long-term RCNN visual recognition description . IEEE CVPR . [    ] J. Dong , X. Li , W. Lan , Y. Huo , C. G. M. Snoek .      . Early embedding late reranking video captioning . Proceedings      ACM Multimedia Conference . ACM ,     -     . [    ] D. Elliott F. Keller .      . Comparing automatic evaluation measures image description . Proceedings   nd Annual Meeting Association Computational Linguistics : Short Papers , Vol .     .     . [    ] M. Everingham , L. V. Gool , C. K. I. Williams , J. Winn , . Zisserman .      . pascal visual object classes ( voc ) challenge . IJCV    ,   (      ) ,    -    . [    ] H. Fang , S. Gupta , F. Iandola , R. K. Srivastava , L. Deng , P. Dollr , J. Gao , X . , M. Mitchell , J. C. Platt , et al .      . captions visual concepts back . IEEE CVPR . [    ] A. Farhadi , M. Hejrati , M. A. Sadeghi , P. Young , C. Rashtchian , J. Hockenmaier , D. Forsyth .      . Every picture tells story : Generating sentences images . IEEE ECCV . [    ] C. Fellbaum .      . WordNet . Wiley Online Library [    ] P. Felzenszwalb , D. McAllester , D. Ramanan .      . discrimi- natively trained , multiscale , deformable part model . IEEE CVPR . [    ] P. F. Felzenszwalb , R. B. Girshick , D. McAllester .      . Cascade object detection deformable part models . IEEE CVPR . [    ] P. F. Felzenszwalb , R. B. Girshick , D. McAllester , D. Ramanan .      . Object detection discriminatively trained part-based models . IEEE TPAMI    ,   (      ) ,     -     .    [    ] Z. Gan , C. Gan , X . , Y. Pu , K. Tran , J. Gao , L. Carin , L. Deng .      . Semantic Compositional Networks visual captioning . IEEE CVPR . [    ] M. Kilickaya , A. Erdem , N. Ikizler-Cinbis , E. Erdem .      . Re- evaluating automatic metrics image captioning . arXiv preprint arXiv:    .      , (      ) . [    ] A. George , B. Asad , F. Jonathan , J. David , D. Andrew , M. Willie , M. Martial , S. Alan , G. Yvette , K. Wessel .      . TRECVID      : Evaluating Ad-hoc Instance Video Search , Events Detection , Video Captioning , Hyperlinking . Proceedings TRECVID      . [    ] B. Ghanem , J. Niebles , C. Snoek , F. Heilbron , H. Alwassel , R . Khrisna , V. Escorcia , K. Hata , S. Buch .      . ActivityNet Chal- lenge      Summary . arXiv preprint arXiv:    .      , (      ) . [    ] S. Gella , M. Lewis , M. Rohrbach .      . Dataset Telling Stories Social Media Videos . Proc      Conference Empirical Methods Natural Language Processing .        . [    ] S. Gong T. Xiang .      . Recognition group activities using dynamic probabilistic networks . IEEE ICCV . [    ] Y. Graham , G. Awad , A. Smeaton .      . Evaluation Auto- matic Video Captioning Using Direct Assessment . arXiv preprint arXiv:    .      , (      ) . [    ] Y. Graham , T. Baldwin , A. Moffat , J. Zobel .      . machine translation systems evaluated crowd alone . Natural Language Engineering    ,   (      ) ,  -   . [    ] A. Graves N. Jaitly .      . Towards end-to-end speech recog- nition recurrent neural networks . Proceedings   st International Conference Machine Learning ( ICML-   ) .     -      . [    ] A. Graves , A. Mohamed , G. Hinton .      . Speech recognition deep recurrent neural networks . IEEE Int Conference Acoustics , Speech Signal Processing ( ICASSP ) .     -     . [    ] S. Guadarrama , N. Krishnamoorthy , G. Malkarnenkar , S. Venu- gopalan , R. Mooney , T. Darrell , K. Saenko .      . Recognizing describing activities using semantic hierarchies zero-shot recognition . IEEE ICCV . [    ] S. Guadarrama , L. Riano , D. Golland , D. Go , Y. Jia , D. Klein , P. Abbeel , T. Darrell , et al .      . Grounding spatial relations human-robot interaction . Intelligent Robots Systems ( IROS ) .     -     . [    ] A. Hakeem , Y. Sheikh , M. Shah .      . CASEE : Hierarchical Event Representation Analysis Videos . AAAI .    -    . [    ] L. Han , A. L. Kashyap , T. Finin , J. Mayﬁeld , J. Weese .      . UMBC-EBIQUITY-CORE : semantic textual similarity systems . Second Joint Conference Lexical Computational Semantics ( * SEM ) , Volume   : Proceedings Main Conference Shared Task : Semantic Textual Similarity , Vol .   .   -   . [    ] P. Hanckmann , K. Schutte , G. J. Burghouts .      . Automated textual descriptions wide range video events    human actions . IEEE ECCV . [    ] D. Harwath , A. Recasens , D. Suris , G. Chuang , A. Torralba , J . Glass .      . Jointly Discovering Visual Objects Spoken Words Raw Sensory Input . IEEE ECCV . [    ] K. , X. Zhang , S. Ren , J . Sun .      . Deep residual learning image recognition . IEEE CVPR . [    ] S. Hochreiter J. Schmidhuber .      . Long Short-Term Memory . Neural Computation   ,   (      ) ,     -     . [    ] S. Hongeng , F. Brmond , R. Nevatia .      . Bayesian framework video surveillance application . Pattern Recognition ,      . Proceedings .   th International Conference , Vol .   . IEEE ,    -     . [    ] Drew A. Hudson , Christopher D. Manning .      . Compositional Attention Networks Machine Reasoning . ICLR . [    ] Y. Jia , E. Shelhamer , J. Donahue , S. Karayev , J . Long , R. Girshick , . Guadarrama , T. Darrell .      . Caffe : Convolutional architecture fast feature embedding . Proceedings   nd ACM International Conference Multimedia . ACM ,    -    . [    ] Q. Jin , J. Chen , S. Chen , Y. Xiong , A. Hauptmann .      . Describing videos using multi-modal fusion . Proceedings      ACM Multimedia Conference . ACM ,     -     . [    ] J. Johnson , B. Hariharan , L. V. D. Maaten , L. Fei-Fei , C. L. Zitnick , R. Girshick .      . CLEVR : Diagnostic Dataset Compositional Language Elementary Visual Reasoning . IEEE CVPR . [    ] M. U. G. Khan Y. Gotoh .      . Describing video contents natural language . Workshop Innovative Hybrid Approaches Processing Textual Data . ACL ,   -   . [    ] J. Kim , A. Rohrbach , T. Darrell , J . Canny , Z. Akata .      . Textual Explanations Self-Driving Vehicles , ECCV      . [    ] W. Kim , J . Park , C. Kim .      . novel method efﬁcient indooroutdoor image classiﬁcation . Journal Signal Processing Systems    ,   (      ) ,    -    . [    ] R. Kiros , R. Salakhutdinov , R. S. Zemel .      . Unifying visual- semantic embeddings multimodal neural language models . arXiv preprint arXiv:    .     , (      ) . [    ] P. Koehn , H. Hoang , A. Birch , C. Callison-Burch , M. Federico , N. Bertoldi , B. Cowan , WadeShen , C. Moran , R. Zens , et al .      . Moses : Open source toolkit statistical machine translation . Proceedings   th annual meeting ACL interactive poster demonstration sessions.ACL ,    -    . [    ] A. Kojima , T. Tamura , K. Fukunaga .      . Natural language description human activities video images based concept hierarchy actions . IJCV    ,   (      ) ,    -    . [    ] D. Koller , N. Heinze , H. Nagel .      . Algorithmic charac- terization vehicle trajectories image sequences motion verbs . IEEE Computer Society Conference CVPR .   -   . [    ] R. Krishna , K. Hata , F. Ren , L. Fei-Fei , J. C. Niebles .      . Dense-Captioning Events Videos . arXiv:    .      , (      ) . [    ] N. Krishnamoorthy , G. Malkarnenkar , R. J. Mooney , K. Saenko , S. Guadarrama .      . Generating Natural-Language Video Descriptions Using Text-Mined Knowledge . AAAI , Vol .   .   . [    ] A. Krizhevsky , I. Sutskever , G. E. Hinton .      . ImageNet clas- siﬁcation deep convolutional neural networks . Advances Neural Information Processing Systems .     -     . [    ] P. Kuchi , P. Gabbur , P. S. Bhat , S. S. David .      . Human face detection tracking using skin color modeling connected component operators . IETE Journal Research    ,  -  (      ) ,        . [    ] G. Kulkarni , V. Premraj , S. Dhar , S. Li , Y. Choi , A. C. Berg , T. L. Berg .      . Baby talk : Understanding generating image descriptions . IEEE CVPR . [    ] T. D. Kulkarni , K. Narasimhan , A. Saeedi , J. Tenenbaum .      . Hierarchical deep reinforcement learning : Integrating tem- poral abstraction intrinsic motivation . Advances Neural Information Processing Systems .     -     . [    ] M. Kusner , . Sun , N. Kolkin , K. Weinberger .      . word embeddings document distances . International Conference Machine Learning ( ICML ) . [    ] I. Langkilde-Geary K. Knight . Halogen input representation . [    ] M. W. Lee , A. Hakeem , N. Haering , S. Zhu .      . Save : framework semantic annotation visual events . IEEE Computer Society Conference CVPR Workshops .  -  . [    ] L. Li B. Gong .      . End-to-End Video Captioning Mul- titask Reinforcement Learning . arXiv preprint arXiv:    .      , (      ) . [    ] S. Li , G. Kulkarni , T. L. Berg , A. C. Berg , Y. Choi .      . Composing simple image descriptions using web-scale n-grams . Conference Computational Natural Language Learning ( CNLL ) . [    ] Y. Li , T. Yao , Y. Pan , H. Chao , T. Mei .      . Jointly Localizing Describing Events Dense Video Captioning . IEEE CVPR . [    ] C. Lin .      . Rouge : package automatic evaluation sum- maries . : Text Summarization Branches . [    ] T. Lin , M. Maire , S. Belongie , J. Hays , P. Perona , D. Ramanan , P . Dollr , C. L. Zitnick .      . Microsoft coco : Common Objects Context . IEEE ECCV . [     ] Y. Liu Z. Shi .      . Boosting video description generation explicitly translating frame-level captions . Proceedings      ACM Multimedia Conference . ACM ,    -    . [     ] D. G. Lowe .      . Object recognition local scale-invariant features . IEEE ICCV . [     ] I. Maglogiannis , D. Vouyioukas , C. Aggelopoulos .      . Face detection recognition natural human emotion using Markov random ﬁelds . Personal Ubiquitous Computing , Vol .    ,   ,   -     . [     ] M. Malinowski M. Fritz .      . multi-world approach question answering real-world scenes based uncertain input . Advances Neural Information Processing Systems .     -     . [    ] M. U. G. Khan , L. Zhang , Y. Gotoh .      . Human focused video description . IEEE International Conference Computer Vision Workshops ( ICCV Workshops ) . [     ] J. Mao , X. Wei , Y. Yang , J. Wang , Z. Huang , A. L. Yuille .      . Learning like child : Fast novel visual concept learning sentence descriptions images . IEEE ICCV . [     ] M. Margaret , M. Ishan , H. Ting-Hao , F. Frank .      . Story Telling Workshop Visual Story Telling Challenge NAACL      . [     ] C. Matuszek , D. Fox , K. Koscher .      . Following directions using statistical machine translation .  th ACM/IEEE Interna- tional Conference Human-Robot Interaction ( HRI ) . [     ] T. Mikolov , I. Sutskever , K. Chen , G. S. Corrado , J . Dean .      . Distributed representations words phrases compositionality . Advances Neural Information Processing Systems .     -     . [     ] V. Mnih , K. Kavukcuoglu , D. Silver , A. Graves , I. Antonoglou , D. Wierstra , M. Riedmiller .      . Playing atari deep reinforcement learning . arXiv preprint arXiv:    .     , (      ) . [     ] V. Mnih , K. Kavukcuoglu , D. Silver , . A. Rusu , J. Veness , M. G. Bellemare , A. Graves , M. Riedmiller , A. K. Fidjeland , G . Ostrovski , S. Petersen , C. Beattie , A. Sadik , I. Antonoglou , H. King , D. Kumaran , D. Wierstra , S. Legg , D. Hassbis .      . Human- level control deep reinforcement learning . Nature , Vol .     ,      ,     . [     ] D. Moore I. Essa .      . Recognizing multitasked activities video using stochastic context-free grammar . AAAI/IAAI .    -    . [     ] R. Nevatia , J. Hobbs , B. Bolles .      . ontology video event representation . CVPR Workshop .    -    . [     ] F. Nishida S. Takamatsu .      . Japanese-English translation internal expressions . Proceedings  th conference Computational linguistics-Volume   . Academia Praha ,    -    . [     ] F. Nishida , S. Takamatsu , T. Tani , T. Doi .      . Feedback correcting information post editing machine translation system . Proceedings   th conference Computational linguistics-Volume   . ACL ,    -    . [     ] A. Owens , . A. Efros .      . Audio-Visual Scene Analysis Self-Supervised Multisensory Features . IEEE ECCV . [     ] P. Pan , Z. Xu , Y. Yang , F. Wu , Y. Zhuang .      . Hierarchical recurrent neural encoder video representation application captioning . IEEE CVPR . [     ] Y. Pan , T. Mei , T. Yao , H. Li , Y. Rui .      . Jointly modeling embedding translation bridge video language . IEEE CVPR . [     ] Y. Pan , T. Yao , H. Li , T. Mei .      . Video Captioning Transferred Semantic Attributes . IEEE CVPR . [     ] K. Papineni , S. Roukos , T. Ward , W. Zhu .      . BLEU : method automatic evaluation machine translation . Pro- ceedings   th annual meeting ACL .    -    . [     ] R. Pasunuru M. Bansal .      . Reinforced video captioning entailment rewards . arXiv preprint arXiv:    .      , (      ) . [     ] S. Phan , G. E. Henter , Y. Miyao , S. Satoh .      . Consensus- based Sequence Training Video Captioning . arXiv preprint arXiv:    .      , (      ) . [     ] C. S. Pinhanez A. F. Bobick .      . Human action detection using pnf propagation temporal constraints . IEEE Computer Society Conference CVPR . [     ] C. Pollard . . Sag .      . Head-driven phrase structure grammar . University Chicago Press . [     ] J. Qin , C. Shizhe , C. Jia , Chen , H. Alexander .      . RUC- CMU : System Descriptions Dense Video Captioning Task . arXiv preprint arXiv:    .      , (      ) . [     ] V. Ramanishka , A. Das , D. H. Park , S. Venugopalan , L. . Hendricks , M. Rohrbach , K. Saenko .      . Multimodal video description . Proceedings ACM Multimedia Conference . ACM ,     -     . [     ] M. Regneri , M. Rohrbach , D. Wetzel , S. Thater , B. Schiele , . Pinkal .      . Grounding action descriptions videos . Transactions Association Computational Linguistics , Vol .   ,   -   . [     ] E. Reiter R. Dale .      . Building natural language generation systems . Cambridge university press . [     ] M. Ren , R. Kiros , R. Zemel .      . Exploring models data image question answering . Advances Neural Information Processing Systems .     -     . [     ] Z. Ren , X. Wang , N. Zhang , X. Lv , L. Li .      . Deep reinforce- ment learning-based image captioning embedding reward . arXiv preprint arXiv:    .      , (      ) . [     ] S. Robertson .      . Understanding inverse document frequency : theoretical arguments IDF . Journal documentation , Vol .    ,   ,        . [     ] A. Rohrbach , M. Rohrbach , W. Qiu , A. Friedrich , M. Pinkal , B. Schiele .      . Coherent multi-sentence video description    variable level detail . German Conference Pattern Recognition . [     ] A. Rohrbach , M. Rohrbach , B. Schiele .      . long-short story movie description . German Conference Pattern Recognition .    -    . [     ] A. Rohrbach , M. Rohrbach , N. Tandon , B. Schiele .      . dataset movie description . IEEE CVPR . [     ] A. Rohrbach , A. Torabi , M. Rohrbach , N. Tandon , C. Pal , H . Larochelle , A. Courville , B. Schiele .      . Movie description . IJCV , Vol .     ,   ,   -    . [     ] M. Rohrbach , S. Amin , M. Andriluka , B. Schiele .      . database ﬁne grained activity detection cooking activities . IEEE CVPR . [     ] M. Rohrbach , W. Qiu , I. Titov , S. Thater , M. Pinkal , B. Schiele .      . Translating video content natural language descriptions . IEEE ICCV . [     ] M. Rohrbach , M. Regneri , M. Andriluka , S. Amin , M. Pinkal , B. Schiele .      . Script data attribute-based recognition composite activities . IEEE ECCV . [     ] D. Roy .      . Semiotic schemas : framework grounding language action perception . Artiﬁcial Intelligence , Vol .     (  -   ) ,    -    . [     ] D. Roy E. Reiter .      . Connecting Language World . Artiﬁcial Intelligence , Vol .     (  -  ) ,  -   . [     ] Y. Rubner , C. Tomasi , L. J. Guibas .      . earth mover ’ distance metric image retrieval . IJCV , Vol .    ,   ,   -    . [     ] O. Russakovsky , J. Deng , H. Su , J. Krause , S. Satheesh , S. , Z. Huang , A. Karpathy , A. Khosla , M. Bernstein , A. C. Berg , L. Fei-Fei .      . ImageNet large scale visual recognition challenge . IJCV , Vol .     ,   ,    -    . [     ] M. Schuster K. K. Paliwal .      . Bidirectional Recurrent Neural Networks . IEEE Transactions Signal Processing , Vol .    ,    ,     -     . [     ] Z. Shen , J. Li , Z. Su , M. Li , Y. Chen , Y. Jiang , X. Xue .      . Weakly Supervised Dense Video Captioning . IEEE CVPR . [     ] R. Shetty J. Laaksonen .      . Frame-and segment-level fea- tures candidate pool evaluation video caption generation . Proceedings      ACM Multimedia Conference . ACM ,     -     . [     ] J. Shi C. Tomasi .      . Good features track . IEEE CVPR . [     ] A. Shin , K. Ohnishi , T. Harada .      . Beyond caption narrative : Video captioning multiple sentences . IEEE In- ternational Conference Image Processing ( ICIP ) . [     ] G. A. Sigurdsson , G. Varol , X. Wang , A. Farhadi , I. Laptev , . Gupta .      . Hollywood homes : Crowdsourcing data collection activity understanding . IEEE ECCV . [     ] K. Simonyan A. Zisserman .      . deep convolu- tional networks large-scale image recognition . arXiv preprint arXiv:    .     , (      ) . [     ] N. Srivastava , E. Mansimov , R. Salakhudinov .      . Unsu- pervised learning video representations using LSTMs . Inter- national Conference Machine Learning ( ICML ) .    -    . [     ] C. Sun R. Nevatia .      . Semantic aware video transcription using random forest classiﬁers . IEEE ECCV . [     ] I. Sutskever , O. Vinyals , Q. V. Le .      . Sequence sequence learning neural networks . Advances Neural Information Processing Systems .     -     . [     ] C. Szegedy , W. Liu , Y. Jia , P. Sermanet , S. Reed , D. Anguelov , . Erhan , V. Vanhoucke , A. Rabinovich .      . Going deeper convolutions . IEEE CVPR . [     ] S. Tellex , T. Kollar , S. Dickerson , M. R. Walter , Ashis Gopal Banerjee , Seth J Teller , Nicholas Roy .      . Understanding Natural Language Commands Robotic Navigation Mobile Manipulation . AAAI . [     ] J. Thomason , S. Venugopalan , S. Guadarrama , K. Saenko , R. J. Mooney .      . Integrating Language Vision Generate Natural Language Descriptions Videos Wild . Coling , Vol .   ,   ,   . [     ] C. Tomasi T. Kanade .      . Detection tracking point features . [     ] A. Torabi , C. Pal , H. Larochelle , A. Courville .      . Using descriptive video services create large data source video annotation research . arXiv preprint arXiv:    .      , (      ) . [     ] A. Torralba , K. P. Murphy , W. T. Freeman , M. A. Rubin .      . Context-based vision system place object recognition . IEEE ICCV . [     ] D. Tran , L. D. Bourdev , R. Fergus , L. Torresani , . Paluri .      . C D : Generic Features Video Analysis . CoRR abs/    .     , (      ) . [     ] R. Vedantam , C. L. Zitnick , D. Parikh .      . Cider : Consensus-based image description evaluation . IEEE CVPR . [     ] S. Venugopalan , L. A. Hendricks , R. Mooney , K. Saenko .      . Improving LSTM-based video description linguistic knowledge mined text . arXiv preprint arXiv:    .      , (      ) . [     ] S. Venugopalan , M. Rohrbach , J. Donahue , R. Mooney , T. Darrell , K. Saenko .      . Sequence sequence-video text . IEEE ICCV . [     ] S. Venugopalan , H. Xu , J. Donahue , M. Rohrbach , R. Mooney , K. Saenko .      . Translating videos natural language using deep recurrent neural networks . arXiv preprint arXiv:    .     , (      ) . [     ] A. S. Vezhnevets , S. Osindero , T. Schaul , N. Heess , M. Jaderberg , D. Silver , K. Kavukcuoglu .      . Feudal networks hierarchi- cal reinforcement learning . arXiv preprint arXiv:    .      , (      ) . [     ] O. Vinyals , A. Toshev , S. Bengio , D. Erhan .      . Show tell : neural image caption generator . IEEE CVPR . [     ] P. Viola M. Jones .      . Rapid object detection using boosted cascade simple features . IEEE CVPR . [     ] Harm de Vries , Kurt Shuster , Dhruv Batra , Devi Parikh , Jason We- ston , Douwe Kiela .      . Talk Walk : Navigating New York City Grounded Dialogue . CoRRabs/    .      (      ) . [     ] B. Wang , L. , W. Zhang , W. Liu .      . Reconstruction Network Video Captioning . IEEE CVPR . [     ] H. Wang , M. M. Ullah , A. Klaser , I. Laptev , C. Schmid .      . Evaluation local spatio-temporal features action recognition . BMVC     -British Machine Vision Conference . BMVA Press ,    -  . [     ] J. Wang , W. Jiang , L. , W. Liu , Y. Xu .      . Bidirectional Attentive Fusion Context Gating Dense Video Captioning . IEEE CVPR . [     ] J. Wang , W. Wang , Y. Huang , L. Wang , T. Tan .      . M  : Multi- modal Memory Modelling Video Captioning . CVPR . [     ] J. K. Wang R. Gaizauskas .      . Cross-validating Image Description Datasets Evaluation Metrics . Proceedings   th Language Resources Evaluation Conference . European Language Resources Association ,     -     . [     ] X. Wang , W. Chen , J. Wu , Y. Wang , W. Y. Wang .      . Video Captioning via Hierarchical Reinforcement Learning . arXiv preprint arXiv:    .      , (      ) . [     ] X. Wu , G. Li , Q. Cao , Q. Ji , L. Lin .      . Interpretable Video Captioning via Trajectory Structured Localization . IEEE CVPR . [     ] Q. Wu , P. Wang , C. Shen , A. Dick , A. Hengel .      . Ask Any- thing : Free-form Visual Question Answering Based Knowledge External Sources . IEEE CVPR . [     ] H. Xu , B. Li , V. Ramanishka , L. Sigal , K. Saenko .      . Joint Event Detection Description Continuous Video Streams . arXiv preprint arXiv:    .      , (      ) . [     ] H. Xu , S. Venugopalan , V. Ramanishka , M. Rohrbach , K . Saenko .      . multi-scale multiple instance video description network . arXiv preprint arXiv:    .      , (      ) . [     ] J. Xu , T. Mei , T. Yao , Y. Rui .      . MSR-VTT : large video description dataset bridging video language . IEEE CVPR . [     ] R. Xu , C. Xiong , W. Chen , J. J. Corso .      . Jointly Modeling Deep Video Compositional Text Bridge Vision Language Uniﬁed Framework . AAAI , Vol .   ,   . [     ] L. Yao , A. Torabi , K. Cho , N. Ballas , C. Pal , H. Larochelle , . Courville .      . Describing videos exploiting temporal structure . IEEE ICCV . [     ] T. Yao , Y. Li , Z. Qiu , F. Long , Y. Pan , D. Li , T. Mei .      . MSR Asia MSM ActivityNet Challenge      : Trimmed Action Recognition , Temporal Action Proposals Dense-Captioning Events Videos . [     ] P. Young , A. Lai , M. Hodosh , J. Hockenmaier .      . image descriptions visual denotations : New similarity metrics semantic inference event descriptions . ACL , Vol .   ,   -   . [     ] H. Yu J. M. Siskind .      . Grounded Language Learning Video Sentences . ACL (   ) .   -   . [     ] H. Yu J. M. Siskind .      . Learning Describe Video Weak Supervision Exploiting Negative Sentential Information . AAAI .     -     . [     ] H. Yu , J. Wang , Z. Huang , Y. Yang , W. Xu .      . Video paragraph captioning using hierarchical recurrent neural networks . IEEE CVPR .    [     ] L. Yu , E. Park , A. C. Berg , T. L. Berg .      . Visual madlibs : Fill blank description generation question answering . IEEE ICCV . [     ] Y. Yu , J. Choi , Y. Kim , K. Yoo , S. Lee , G. Kim .      . Super- vising Neural Attention Models Video Captioning Human Gaze Data . IEEE CVPR . [     ] Y. Yu , H. Ko , J. Choi , G. Kim .      . End-to-end concept word detection video captioning , retrieval , question answering . arXiv preprint arXiv:    .      , (      ) . [     ] K. Zeng , T. Chen , J. C. Niebles , M. Sun .      . Title Generation User Generated Videos . IEEE ECCV . [     ] X. Zhang , K. Gao , Y. Zhang , D. Zhang , J. Li , Q. Tian .      . Task-Driven Dynamic Fusion : Reducing Ambiguity Video Description . IEEE CVPR . [     ] L. Zhou , Y. Kalantidis , X. Chen , J. Corso , . Rohrbach .      . Grounded Video Description.arXiv preprint arXiv:    .      (      ) . J . [     ] L. Zhou , C. Xu , J. J. Corso .      . Towards automatic learning procedures web instructional videos . Thirty-Second AAAI Conference Artiﬁcial Intelligence . [     ] L. Zhou , Y. Zhou , J. J. Corso , R. Socher , C. Xiong .      . End- to-End Dense Video Captioning Masked Transformer . IEEE CVPR . [     ] S. Zhu D. Mumford .      . stochastic grammar images . Foundations Trends Computer Graphics Vision , Vol .   ,   ,    -    . Nayyer Aafaq received degree distinc- tion Avionics College Aeronau- tical Engineering ( CAE ) , National University Sciences Technology ( NUST ) , Pakistan ,      MS degree high distinction Sys- tems Engineering Queensland University Technology ( QUT ) , Australia ,      . currently working towards Ph.D. degree University Western Australia ( UWA ) . recipient SIRF scholarship UWA . served Research Assistant STG Research Institute , Pakistan ,           lecturer College Aeronautical Engineering ( CAE ) , NUST , Pakistan      till      . current research interests includes Deep Learning , Video Analysis intersection Natural Language Processing ( NLP ) , Computer Vision ( CV ) Machine Learning . Ajmal Mian Professor Computer Science University Western Australia . com- pleted PhD institution      distinction received Australasian Distinguished Doctoral Dissertation Award Computing Research Education Associa- tion Australasia . received prestigious Australian Postdoctoral Australian Research Fellowships           respectively . received UWA Outstanding Young Investiga- tor Award      , West Australian Early Ca- reer Scientist Year      award , Vice-Chancellors Mid-Career Research Award      , Aspire Professional Development Award      Excellence Research Supervision Award      . published     scientiﬁc papers reputable journals confer- ences . secured eight Australian Research Council grants , Na- tional Health Medical Research Council grant DAAD German Australian research cooperation grant . served guest editor Pattern Recognition , Computer Vision Image Understanding Image Vision Computing journals . research interests include computer vision , machine learning ,  D shape analysis , hyperspectral image analysis , pattern recognition , multimodal biometrics .    Wei Liu received PhD University Newcastle , Australia      . working Department Computer Science Software Engineering University Western Australia , co-lead faculty ’ Big Data research group . research impact ﬁeld knowledge discovery natural lan- guage text data evident series highly cited papers , reputable top data mining knowledge management journals con- ferences published . include example , ACM Computer Surveys , Journal Data Mining Knowledge Discovery , Knowledge Information Systems , Inter- national Conference Data Engineering ( ICDE ) , ACM International Conference Information Knowledge Management ( CIKM ) . three Australian Research Council Grants several industry grants . current research focuses deep learning methods knowledge graph construction natural language text , sequential data mining text mining . Syed Zulqarnain Gilani received PhD University Western Australia working Research Fellow . MS EE National University Sci- ences Technology ( NUST ) , Pakistan      secured Presidents Gold Medal . research interests include  D facial morphomet- rics applications syndrome delineation machine learning . Mubarak Shah , Trustee chair professor computer science , founding director Center Research Computer Vision University Central Florida ( UCF ) . editor international book series video computing , editor-in-chief Machine Vision Applications journal , associate editor ACM Computing Surveys journal . program cochair CVPR      , associate editor IEEE T-PAMI , guest editor special issue International Journal Computer Vision Video Computing . research interests include video surveillance , visual tracking , human activity recognition , visual analysis crowded scenes , video registration , UAV video analysis , . ACM distinguished speaker . IEEE distinguished visitor speaker     -     received IEEE Outstanding Engineering Educator Award      .      , awarded Pegasus Professor Award , highest award UCF . received Harris Corporations Engineering Achievement Award      , TOKTEN awards UNDP      ,      ,      , Teaching Incentive Program Award           , Research Incentive Award           , Millionaires Club Awards           , University Distinguished Researcher Award      , Honorable mention ICCV      ? Challenge Problem , nominated Best Paper Award ACM Multimedia Conference      . fellow IEEE , AAAS , IAPR , SPIE . 