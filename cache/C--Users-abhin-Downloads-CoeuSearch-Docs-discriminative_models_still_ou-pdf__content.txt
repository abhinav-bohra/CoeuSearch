Discriminative Models Still Outperform Generative Models Aspect Based Sentiment Analysis Cross-Domain Cross-Lingual Settings Anonymous ACL submission                                                                                                                                                                     Abstract Aspect-based Sentiment Analysis ( ABSA ) helps explain customers ’ opinions towards products services . past , ABSA mod- els discriminative , recently gen- erative models used generate as- pects polarities directly text . con- trast , discriminative models first select aspects text , classify aspect ’ polarity . Previous results showed gener- ative models outperform discriminative mod- els several English ABSA datasets . , rigorously contrast discriminative gen- erative models several settings . com- pare model types cross-lingual , cross- domain , cross- lingual domain , un- derstand generalizability settings mono-lingual English in-domain . thorough evaluation shows , contrary previous studies , discriminative models still clearly outperform generative models almost settings .   Introduction Online reviews make easy customers share feelings products services quick efficient way . , business owner , mean deluge comments variety concerns . Companies millions customers receive massive amount online reviews analyzed manually , thus , automation needed . natural languages receive research effort compared languages ( e.g . English vs . Swahili ) . Although community remarkably accelerated improvement English NLP tech- niques , techniques languages lag behind . Working lower resource language challeng- ing task , datasets , lexicons , models exist . Thus , utilizing cross-lingual approaches important migrate knowledge across languages . ABSA involves predicting aspect terms associated sentiment polarities . example ,   `` service good restaurant , food '' two aspect terms ( “ restaurant ” “ food ” ) , associated sentiments `` posi- tive '' `` negative '' , respectively . work , conduct comparative study two different ABSA model types ( discriminative generative ) . Discriminative models commonly use sequence labeling techniques detect aspects given review ( extraction ) , use an- step classify aspects ( classification ) . hand , generative models use encoder- decoder language models generate aspects sentiment polarities together without separate steps extraction classification . worth mentioning discriminative models extraction classification steps ( Li et al. ,      ,     a ; Hu et al. ,      ) . However , results showed tasks together always lead better performance . results previous works ( Zhang et al. ,      ; Yan et al. ,      ) showed generative mod- els achieve better performance discriminative models trained evaluated English in-domain setting . recent studies compared generative discriminative models English in- domain setting , none explored efficiency cross-lingual cross-domain settings . aim study evaluate performance two model types cross-lingual cross-domain settings . Additionally , propose challeng- ing setting : cross-lingual cross-domain . results demonstrated generative models perform worse discriminative models proposed scenarios .   Methodology Experimental Setup  .  Datasets experiments , consider several languages domains valid evaluation . languages use SemEval datasets - Restaurant                                                                                                                                                             Datasets Rest  en Rest  es Rest  ru Lap   MAMSEn Data Split Train Val Test Train Val Test Train Val Test Train Val Test Train Val Test # Pos                                                             # Neg                                                         # Neu                                                 Table   : Datasets ’ statistics - Count aspects sen- timent polarities sampled cleaned datasets . Multiple aspects exist single record ( Rest   ) ( Pontiki et al. ,      ) English , Span- ish , Russian . domains use Rest   Laptop ( Lap   ) SemEval ( Pontiki et al. ,      ) widely used literature evaluation purposes ( Li et al. ,     b ; Tian et al. ,      ; Liang et al. ,      ) . addition pre- vious domains , use MAMS dataset ABSA ( Jiang et al. ,      ) . MAMS dataset ( Jiang et al. ,      ) recently developed challenge dataset sentence contains least two aspects different polarities , making dataset challenging SemEval datasets . remove sentences opinions aspect terms multiple sentiments datasets , seen previously studies ( Tian et al. ,      ; Tang et al. ,      ) . SemEval datasets , since validation sets given , sam- ple    % training dataset use valida- tion . datasets considered vary terms type content training set size . fair comparison , reduce larger train- ing datasets equal number records . purpose , sample     records training datasets , minimum number training instances across datasets ( cleaned Rest  es training dataset     records ) . Table   presents datasets ’ statistics cleaning sampling .  .  Models Baselines generative model , use approach pro- posed ( Zhang et al. ,      ) , encoder- decoder T -based model . model takes re- view input generates aspects polarities . aspect-polarity terms fol- lowing format : `` waiter positive < sep > food nega- tive '' , indicating presence two aspect terms ( `` waiter '' `` food '' ) , associated polari- ties ( `` positive '' `` negative '' ) . Since multiple aspect-polarity pairs single review , add separator token `` < sep > '' demarcate separation multiple aspect-polarity pairs . mono-lingual setting , model trained English generates English aspect-polarity pairs . move cross-lingual setting , ask multilingual model generate aspect- polarity pairs language used training process . Thus , use approach augments training data version translated test language ( Riabi et al. ,      ) . require additional annotated data solve issue . Appendix A.  , give details regarding approach taken . discriminative model , consider SPAN-BERT model ( Hu et al. ,      ) one state-of-the-art models uses BERT trans- former . good performance mono-lingual datasets , used baseline generative model released Zhang et al . (      ) . SPAN-BERT model extracts spans ( continuous span text ) multiple target aspect terms using decoder heuristic classifies polarities using contextualised span representations . discriminative generative models ref- erenced use transformers trained solely English , need modify train- ing languages . make experiments consistent , use multilingual versions base transformers . generative model , use multilingual T  ( mT -base ) model ( hugging- face implementation mT -base  ) . SPAN- BERT model , use multilingual BERT model Google . order understand performance models , set two baselines : mono-lingual in- domain , random selection baseline . mono-lingual in-domain , train model dataset define theoretical performance ceiling . random baseline allow us see cross-lingual domain results better chance . random baseline , model pick aspect words text ( excluding stop words ) , polarities random . details refer Appendix A.  .  https : //huggingface.co/transformers/ model_doc/mt .html                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            .  Preprocessing Evaluation find generative model sometimes gen- erates different variant term , e.g . plural singular . Prior evaluating model outputs , perform normalisation process . normalising , remove characters `` , '' , `` . `` , `` ” '' sentences , lower-case lemmatise words , remove common stop words . idea nor- malising generated output similar Zhang et al . (      ) , Levenshtein distance used align generated aspect words closest words existing original sentence . Compared , normalisation process followed exact matching stricter . Levenshtein distance may align model ’ predictions unrelated words original sentence . example , generated word - `` salmon '' , least distance word `` '' words original sentence , `` salmon '' get aligned `` '' , mentioned Zhang et al . (      ) , loose matching . model outputs gold data nor- malised , exact matching used com- pare predicted aspect-polarity terms cor- responding aspect-polarity terms gold data . consider hit aspect term polarity term match . use standard evalu- ation metrics calculating ABSA scores , Micro- Precision , Recall F  . use evaluation code released Li et al . (     a )   .   Results Discussion  .  Monolingual In-Domain First , evaluate models train test data dataset type language , get results random selection base- line . Table   presents results model . detailed results refer Appendix A.  . mono-lingual perspective , see discriminative model performs better gen- erative almost datasets except Rest  en . experiments , evaluated models using mono-lingual version transform- ers models , noticed similar scenario ; generative approach performed better discriminative one Rest  en Lap  en datasets . Thus , seems generative ap- proach works best English datasets . random baseline results datasets  http : //github.com/lixin ever/E E-TBSA DomainLang Discriminative Generative Rest  En  .    .   Rest  Es  .   Rest  Ru  .   Lap  En  .   MAMSEn  .    .    .    .    .   Table   : Mono-lingual in-domain F  scores . Bolded results best among models . Train → Test Discriminative Generative Es → En  .   ( -   % )  .   ( -  % ) Ru → En  .   ( -   % )  .   ( -  % ) En → Ru  .   ( -  % )  .   ( -   % ) Es → Ru  .   ( -   % )  .   ( -  % ) En → Es  .   ( -  % )  .   ( -   % ) Ru → Es  .   ( -   % )  .   ( -   % ) Table   : Cross-lingual F  scores using Rest   several languages . Bolded results best per model test language . Bracketed % values show performance decrease compared mono-lingual , in-domain re- sult   . around   % F  ( individual results seen   )  .  Cross-Lingual Table   presents cross-lingual results . de- tailed results refer Appendix A.  . cross- lingual perspective , clearly see mod- els , perform random . discrimina- tive model , notice train En- glish , obtain highest F  results . largest decrease performance happens train Russian test Spanish . Interestingly , train Russian test languages , obtain highest results generative model . Overall , performance drop generative cross-lingual results compared monolingual ones high , considering dis- criminative model ’ results . conclude discriminative model generalizes better generative one cross-lingual setting .  .  Cross-Domain Table   presents cross-domain results . de- tails found Appendix A.  . Generally , con- sidering models ’ results , training Rest  En MamsEn datasets produced highest results . Like Rest   dataset , Mams dataset contains reviews related restaurants . Thus sur- prising training one two datasets testing gives higher results com- pared training Lap   . However , see gap larger experiment generative model . observation demonstrates                                                                                                                           Train → Test Rest  En → Lap  En MAMSEn → Lap  En Lap  En → Rest  En MAMSEn → Rest  En Rest  En → MAMSEn Lap  En → MAMSEn Discriminative Generative  .   ( -   % )  .   ( -   % )  .   ( -   % )  .   ( -   % )  .  ( -   % )  .   ( -   % )  .   ( -   % )  .   ( -   % )  .   ( -   % )  .   ( -  % )  .   ( -   % )  .   ( -   % ) Table   : Cross-domain F  scores . Bolded results best per model test language . Bracketed % val- ues show performance decrease compared mono- lingual , in-domain result   . Train → Test Rest  Es → Lap  En Rest  Ru → Lap  En Lap  En → Rest  Es Lap  En → Rest  Ru Discriminative Generative  .   ( -   % )  .   ( -   % )  .   ( -   % )  .   ( -   % )  .  ( -   % )  .   ( -   % )  .   ( -  % )  .   ( -   % ) Table   : Cross-domain cross-lingual F  scores . Bolded results best per model test language ,   train language compare . Bracketed % values show performance decrease compared mono-lingual , in-domain result   . generative model domain sensitive .  .  Cross-Lingual Cross-Domain experiment , evaluate models extreme setting , combines previous cross-lingual cross-domain . Table   shows evaluation results . details found Appendix A.  . see larger drop com- pared results cross-lingual experiment ( see Table   ) , except test Rest  es us- ing discriminative model ; training Rest  en Lap  en gives F  result . Similar previous results , generative model achieves lower results compared discriminative one .   Discussions Conclusion work , compared two types ABSA models terms performance differences . compare models across languages do- mains . Previous studies showed generative models achieve higher results discrimina- tive ones across almost available English ABSA datasets . However , results study demonstrated generative models perform lower discriminative ones proposed sce- narios , namely , cross-lingual , cross-domain , cross- lingual domain . experimented datasets three lan- guages , three different domains . Briefly , results showed generative model language domain sensitive . Generative models sample words entire data distribu- tion might sensitive training data size compared discriminative models classify words original sentence . Given around     instances train- ing , generative model generalize well discriminative domains lan- guages . generative model outperformed discrim- inative model English mono-lingual experiment , perhaps due favourable bias mT  model towards English language . Recent studies showed Multilingual encoder-decoder transformers perform well languages English ( Tang et al. ,      ; Fan et al. ,      ) . Another explanation variation results could model uses different encoder . discriminative model uses BERT encoder whereas generative one uses mT  encoder . Additionally , also possible evaluation process strict hurts gener- ative model . Nevertheless , results useful comparison state art models model type . future , plan investigate effect using common encoder models . Considering random selection baseline experiments , conclude generative mod- els capable generating correct aspects polarities . results showed generative model , worst case ( training Lap  En testing MAMSEn ) , performs better ran- dom baseline   % F  . hand , discriminative model worst case ( training Rest  Ru testing Lap  En ) , performed better random baseline    % F  . results suggest using generative models cross- lingual domain settings ; dis- criminative models accurate reliable . future work , plan study generative models task . also plan study types models scenarios like conflicting polarities ( aspects positive negative polarities ) . References Angela Fan , Shruti Bhosale , Holger Schwenk , Zhiyi , Ahmed El-Kishky , Siddharth Goyal , Mandeep Baines , Onur Celebi , Guillaume Wenzek , Vishrav Chaudhary , et al .      . Beyond english-centric mul-                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       tilingual machine translation . Journal Machine Learning Research ,   : –   . Minghao Hu , Yuxing Peng , Zhen Huang , Dongsheng Li , Yiwei Lv .      . Open-domain targeted senti- ment analysis via span-based extraction classifi- cation . Proceedings   th Annual Meeting Association Computational Linguistics , pages    –    , Florence , Italy . Association Computa- tional Linguistics . Qingnan Jiang , Lei Chen , Ruifeng Xu , Xiang Ao , Min Yang .      . challenge dataset effec- tive models aspect-based sentiment analysis . Proceedings      Conference Empirical Methods Natural Language Processing  th International Joint Conference Natural Lan- guage Processing ( EMNLP-IJCNLP ) , pages     –      , Hong Kong , China . Association Computa- tional Linguistics . Jiawen Li , Yudianto Sujana , Hung-Yu Kao .      . Exploiting microblog conversation structures de- tect rumors . Proceedings   th International Conference Computational Linguistics , pages     –     , Barcelona , Spain ( Online ) . International Committee Computational Linguistics . Xin Li , Lidong Bing , Piji Li , Wai Lam .     a . unified model opinion target extraction target sentiment prediction . Proceedings AAAI Conference Artificial Intelligence , pages     –      . Xin Li , Lidong Bing , Wenxuan Zhang , Wai Lam .     b . Exploiting BERT end-to-end aspect-based sentiment analysis . Proceedings  th Work- shop Noisy User-generated Text ( W-NUT      ) , pages   –   , Hong Kong , China . Association Computational Linguistics . Yunlong Liang , Fandong Meng , Jinchao Zhang , Yufeng Chen , Jinan Xu , Jie Zhou .      . dependency syntactic knowledge augmented interactive architec- ture end-to-end aspect-based sentiment analysis . Neurocomputing ,    :   –    . Maria Pontiki , Dimitris Galanis , Haris Papageorgiou , Ion Androutsopoulos , Suresh Manandhar , Moham- mad AL-Smadi , Mahmoud Al-Ayyoub , Yanyan Zhao , Bing Qin , Orphée De Clercq , Véronique Hoste , Marianna Apidianaki , Xavier Tannier , Na- talia Loukachevitch , Evgeniy Kotelnikov , Nuria Bel , Salud María Jiménez-Zafra , Gül¸sen Eryi˘git .      . SemEval-     task   : Aspect based sentiment analysis . Proceedings   th International Workshop Semantic Evaluation ( SemEval-     ) , pages   –   , San Diego , California . Association Computational Linguistics . Maria Pontiki , Dimitris Galanis , John Pavlopoulos , Har- ris Papageorgiou , Ion Androutsopoulos , Suresh Manandhar .      . SemEval-     task   : Aspect based sentiment analysis . Proceedings  th International Workshop Semantic Evaluation ( Se- mEval      ) , pages   –   , Dublin , Ireland . Associa- tion Computational Linguistics . Arij Riabi , Thomas Scialom , Rachel Keraron , Benoît Sagot , Djamé Seddah , Jacopo Staiano .      . Synthetic data augmentation zero-shot cross- lingual question answering . Proceedings      Conference Empirical Methods Natural Lan- guage Processing ( EMNLP ) . Duyu Tang , Bing Qin , Ting Liu .      . Aspect level sentiment classification deep memory network . Proceedings      Conference Empirical Methods Natural Language Processing , pages    –     , Austin , Texas . Association Computational Linguistics . Yuqing Tang , Chau Tran , Xian Li , Peng-Jen Chen , Na- man Goyal , Vishrav Chaudhary , Jiatao Gu , An- gela Fan .      . Multilingual translation exten- sible multilingual pretraining finetuning . arXiv preprint arXiv:    .      . Yuanhe Tian , Guimin Chen , Yan Song .      . Aspect-based sentiment analysis type-aware graph convolutional networks layer ensemble . Proceedings      Conference North American Chapter Association Computa- tional Linguistics : Human Language Technologies , pages     –     , Online . Association Computa- tional Linguistics . Hang Yan , Junqi Dai , Tuo Ji , Xipeng Qiu , Zheng Zhang .      . unified generative framework Proceedings aspect-based sentiment analysis .   th Annual Meeting Association Computational Linguistics   th International Joint Conference Natural Language Processing ( Volume   : Long Papers ) , pages     –     , Online . Association Computational Linguistics . Wenxuan Zhang , Xin Li , Yang Deng , Lidong Bing , Wai Lam .      . Towards generative aspect-based sentiment analysis . Proceedings   th An- nual Meeting Association Computational Linguistics   th International Joint Confer- ence Natural Language Processing ( Volume   : Short Papers ) , pages    –    , Online . Association Computational Linguistics . Appendix A.  Generative models Cross-lingual Setting section , provide details regarding proposed approach ( Riabi et al. ,      ) solve issue controlling generated lan- guage . idea method , instance , train English generate Spanish , translate English training data Spanish ( using Google Translator ) include                                                                                                                                                                                                                                                                                                                                                           training part original English language . Additionally , control target language , use specific prompt ( token ) per language ( < LANG > ) , corresponds desired target language ( e.g . Spanish : Spanish_review ) . trans- late language another , discard instances translated aspect terms exist translated review . important SPAN- BERT models terms indices needed . Also , sample equal number translated training instances languages (     instances per language ) , prepared monolingual training data . consistency , train SPAN-BERT model data . A.  Random Baseline consider randomised model baselining performance considered models . How- ever , instead randomly assigning positive , negative , neutral none labels words sen- tence , give randomised model biased edge knowledge test dataset . considered test datasets , see gold predic- tions see distribution different polarities . e.g . positive polarity assigned   % words dataset . consider distribution polarities assigning randomly . Moreover , prevent randomised model assigning polarities stop words . A.  Detailed Results detailed results experi- ments conducted . precision , recall F  values found .   DomainLang Rest  En Rest  Es Rest  Ru Lap  En MAMSEn Discriminative F   .    .    .    .    .   R  .    .    .    .    .   P  .    .    .    .    .   Generative P  .    .    .    .   .   R  .    .    .    .    .  F   .    .    .    .    .   Random Selection P  .    .    .    .    .   R  .    .    .    .    .   F   .    .    .    .    .   Table   : Mono-lingual in-domain results . Bolded results best among models . Train → Test Rest  Es → Rest  En Rest  Ru → Rest  En Rest  En → Rest  Ru Rest  Es → Rest  Ru Rest  En → Rest  Es Rest  Ru → Rest  Es Discriminative P  .    .    .    .    .    .   R  .    .    .    .    .    .   F   .   ( -  % )  .   ( -  % )  .   ( -  % )  .   ( -  % )  .   ( -  % )  .   ( -   % ) P  .    .   .    .    .    .   Generative R  .    .    .    .    .   .   F   .   ( -   % )  .   ( -   % )  .   ( -   % )  .   ( -   % )  .   ( -   % )  .   ( -   % ) Table   : Cross-lingual results . Bolded results best per model test language . percentage values brackets represent amount drop compared mono-lingual in-domain result . Train → Test Rest  En → Lap  En MAMSEn → Lap  En Lap  En → Rest  En MAMSEn → Rest  En Rest  En → MAMSEn Lap  En → MAMSEn Discriminative P  .    .    .    .    .    .   R  .   .    .    .    .    .   F   .   ( -   % )  .   ( -   % )  .   ( -   % )  .   ( -  % )  .   ( -   % )  .   ( -   % ) P  .    .    .    .    .    .   Generative R  .    .    .    .    .    .   F   .   ( -   % )  .   ( -   % )  .   ( -   % )  .   ( -   % )  .  ( -   % )  .   ( -   % ) Table   : Cross-domain results . Bolded results best per model test language . percentage values brackets represent amount drop compared mono-lingual in-domain result . Train → Test Rest  Es → Lap  En Rest  Ru → Lap  En Lap  En → Rest  Es Lap  En → Rest  Ru Discriminative P  .    .   .    .   R  .    .    .    .   F   .  ( -   % )  .   ( -   % )  .   ( -  % )  .   ( -   % ) P  .    .    .    .   Generative R  .    .    .    .   F   .   ( -   % )  .   ( -   % )  .   ( -   % )  .   ( -   % ) Table   : Cross-domain cross-lingual results . Bolded results best per model test language . percentage values brackets represent amount drop compared mono-lingual in-domain result .   