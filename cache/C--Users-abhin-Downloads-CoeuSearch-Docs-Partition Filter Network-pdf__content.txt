Partition Filter Network Joint Entity Relation Extraction Zhiheng Yan  , Chong Zhang  , Jinlan Fu ,  , Qi Zhang ∗ Zhongyu Wei   School Computer Science , Shanghai Key Laboratory Intelligent Information Processing , Fudan University , Shanghai , China  National University Singapore , Singapore  School Data Science , Fudan University , Shanghai , China { zhyan   , chongzhang   , qz , zywei } @ fudan.edu.cn jinlanjonna @ gmail.com Abstract answering ( Diefenbach et al. ,      ) text summarization ( Gupta Lehal ,      ) .         p e      ] L C .  c [   v           .         : v  X r  joint entity relation extraction , existing work either sequentially encode task-speciﬁc features , leading imbalance inter-task feature interaction features extracted later direct contact come ﬁrst . encode entity features relation features parallel manner , meaning feature representation learning task largely independent except input sharing . propose partition ﬁlter network model two-way interaction tasks properly , fea- ture encoding decomposed two steps : partition ﬁlter . encoder , leverage two gates : entity relation gate , segment neurons two task partitions one shared partition . shared partition represents inter-task information valuable tasks evenly shared across two tasks ensure proper two-way interaction . task partitions represent intra-task information formed concerted efforts gates , making sure encoding task- speciﬁc features dependent upon . Experiment results six public datasets show model performs signiﬁcantly better previous approaches . addition , con- trary previous work claimed , relation auxiliary experiments suggest prediction contributory named entity prediction non-negligible way . source code found https : //github.com/ Coopercoppers/PFN .   Introduction Joint entity relation extraction intend simultaneously extract entity relation facts given text form relational triples ( , r , ) . extracted information provides supplement many studies , knowledge graph construction ( Riedel et al. ,      ) , question ∗∗ Corresponding author . Figure   : Partition process cell neurons . Entity relation gate used divide neurons task- related task-unrelated ones . Neurons relating tasks form shared partition rest form two task partitions . Conventionally , Named Entity Recognition ( NER ) Relation Extraction ( ) performed pipelined manner ( Zelenko et al. ,      ; Chan Roth ,      ) . approaches ﬂawed consider intimate connection NER . Also , error propagation another drawback pipeline methods .  order conquer issues , joint extracting entity relation proposed demonstrates stronger performance tasks . early work , joint methods mainly rely elaborate feature engineering establish interaction NER ( Yu Lam ,      ; Li Ji ,      ; Miwa Sasaki ,      ) . Recently , end-to-end neural network shown successful extracting relational triples ( Zeng et al. ,      ; Gupta et al. ,      ; Katiyar Cardie ,      ; Shen et al. ,      ) since become mainstream joint entity relation extraction . According differences encoding task- speciﬁc features , existing methods divided two categories : sequential encoding parallel encoding . sequential task-speciﬁc features generated encoding , sequentially , means features extracted ﬁrst affected extracted later . Zeng et al . (      ) Wei et al . (      ) Cell StateNERREPartition✂✂RE-SpecificSharedNER-Specific      typical examples category . methods extract features different tasks predeﬁned order . parallel encoding , task-speciﬁc features generated independently using shared input . Compared sequential encoding , models build scheme need worry implication encoding order . example , Fu et al . (      ) encodes entity relation information separately using common features derived GCN encoder . Since task- speciﬁc features extracted isolated sub- modules , approach falls category parallel encoding . However , encoding designs fail model two-way interaction NER tasks properly . sequential encoding , interaction unidirectional speciﬁed order , result- ing different amount information exposed NER task . parallel encoding , although encoding order longer concern , interaction present input sharing . Considering adding two-way interaction feature encoding , adopt joint encoding . alternative encoding design : design encodes task-speciﬁc features jointly single encoder exist mutual section inter-task communication . work , instantiate joint encoding partition ﬁlter encoder . encoder ﬁrst sorts partitions neuron according contribution individual tasks entity relation gates . process , two task partitions one shared partition formed ( see ﬁgure   ) . individual task partitions shared partition combined generate task-speciﬁc features , ﬁltering irrelevant information stored opposite task partition . Task interaction encoder achieved two ways : First , partitions , especially task- speciﬁc ones , formed concerted efforts entity relation gates , allowing interaction formation entity relation features determined partitions . Second , shared partition , represents information useful task , equally accessible formation task-speciﬁc features , ensuring balanced two-way interaction . contributions work summarized :   . propose partition ﬁlter network , frame- work designed speciﬁcally joint encoding . method capable encoding task- speciﬁc features guarantees proper two- way interaction NER .   . conduct extensive experiments six datasets . main results show method superior baseline ap- proaches , ablation study provides in- sight works best framework .   . Contrary previous work claimed , auxiliary experiments suggest relation prediction contributory named entity prediction non-negligible way .   Related Work recent years , joint entity relation extraction approaches focusing tackling triple overlapping problem modelling task interac- tion . Solutions issues explored recent works ( Zheng et al. ,      ; Zeng et al. ,      ,      ; Fu et al. ,      ; Wei et al. ,      ) . triple overlapping problem refers triples sharing entity ( SEO , i.e . SingleEntityOverlap ) entities ( EPO , i.e . EntityPairOverlap ) . example , `` Adam Joe born USA '' , since triples ( Adam , birthplace , USA ) ( Joe , birthplace , USA ) share one entity `` USA '' , categorized SEO triples ; `` Adam born USA lived ever since '' , triples ( Adam , birthplace , USA ) ( Adam , residence , USA ) share entities time , thus categorized EPO triples . Generally , two ways tackling problem . One generative methods like seq seq ( Zeng et al. ,      ,      ) entity relation mentions decoded multiple times output sequence , another modeling relation separately sequences ( Wei et al. ,      ) , graphs ( Fu et al. ,      ) tables ( Wang Lu ,      ) . method uses relation-speciﬁc tables ( Miwa Sasaki ,      ) handle relation separately . Task interaction modeling , however , well handled previous work . previous approaches , Task interaction achieved entity relation prediction sharing features ( Tran Kavuluru ,      ; Wang et al. ,     b ) . could problematic information entity relation could sometimes contradictory . Also , models use sequential encoding ( Bekoulis et al. ,     b ; Eberts Ulges ,      ; Wei et al. ,      ) parallel encoding ( Fu et al. ,      ) lack proper two-way interaction feature extraction , predictions made features suffer problem improper interaction . work , partition ﬁlter encoder built joint encoding capable handling communication inter-task information appropriately avoid problem sequential parallel encoding ( exposure bias insufﬁcient interaction ) , keeping intra-task information away opposite task mitigate problem negative transfer tasks .   Problem Formulation framework split joint entity relation ex- traction two sub-tasks : NER . Formally , Given input sequence = { w  , . . . , wL } L tokens , wi denotes i-th token sequence s. NER , aim extract typed entities whose set denoted , ⟨wi , e , wj⟩ ∈ signiﬁes token wi wj start end token entity typed e ∈ E. E represents set entity types . Concerning , goal identify head-only triples whose set denoted , triple ⟨wi , r , wj⟩ ∈ indicates tokens wi wj corresponding start token subject object entity relation r ∈ R. R represents set relation types . Combining results NER , able extract relational triples complete entity spans .   Model describe model design section . model consists partition ﬁlter encoder two task units , namely NER unit unit . partition ﬁlter encoder used generate task- speciﬁc features , sent task units input entity relation prediction . discuss component detail following three sub-sections .  .  Partition Filter Encoder Similar LSTM , partition ﬁlter encoder recurrent feature encoder information stored intermediate memories . time step , encoder ﬁrst divides neurons three partitions : entity partition , relation partition shared par- tition . generates task-speciﬁc features selecting combining partitions , ﬁltering information irrelevant task . shown ﬁgure   , module designed speciﬁcally jointly extract task-speciﬁc features , strictly follows two steps : partition ﬁlter . Partition step performs neuron partition divide cell neurons three partitions : Two task partitions storing intra-task information , namely entity partition relation partition , well one shared partition storing inter-task information . neuron divided candidate cell ˜ct representing current information previous cell ct−  representing history information . ct−  direct input last time step ˜ct calculated manner LSTM : ˜ct = tanh ( Linear ( [ xt ; ht−  ] ) ) (   ) Linear stands operation linear transformation . leverage entity gate ˜e relation gate ˜r , referred master gates ( Shen et al. ,      ) , neuron partition . illustrated ﬁgure   , gate , represents one speciﬁc task , divide neurons two segments according usefulness designated task . example , entity gate ˜e separate neurons two partitions : NER-related NER-unrelated . shared partition formed combining partition results gates . Neurons shared partition regarded information valuable tasks . order model two- way interaction properly , inter-task information shared partition evenly accessible tasks ( discussed ﬁlter subsection ) . addition , information valuable one task invisible opposing task stored individual task partitions . gates calculated using cummax activation   , function cummax ( ⋅ ) = cumsum ( sof tmax ( ⋅ ) ) whose output seen approximation binary gate form (   , . . . ,   ,   , . . . ,   ) : ˜e = cummax ( Linear ( [ xt ; ht−  ] ) ) ˜r =   − cummax ( Linear ( [ xt ; ht−  ] ) ) (   ) intuition behind equation (   ) identify two cut-off points , displayed scissors ﬁgure   , naturally divide set neurons three segments . result , gates divide neurons three partitions , entity partition ρe , relation partition ρr shared partition ρs . Partitions  cumsum ( x  , x  , . . . , xn−  , xn ) = ( x  , x  + x  , . . . , x  + x  + ⋅ ⋅ ⋅ + xn−  , x  + x  + ⋅ ⋅ ⋅ + xn−  + xn ) . ( ) Framework Partition Filter Network ( b ) Inner Mechanism Partition Filter Figure   : ( ) Overview PFN . framework consists three components : partition ﬁlter encoder , NER unit unit . task units , use table-ﬁlling word pair prediction . Orange , yellow green represents NER-related , shared RE-related component features . ( b ) Detailed depiction partition ﬁlter encoder one single time step . decompose feature encoding two steps : partition ﬁlter ( shown gray area ) . partition , ﬁrst segment neurons two task partitions one shared partition . ﬁlter , partitions selected combined form task-speciﬁc features shared features , ﬁltering information irrelevant task . previous cell ct−  formulated :   ρs , ct−  = ˜ect−  ○ ˜rct−  ρe , ct−  = ˜ect−  − ρs , ct−  ρr , ct−  = ˜rct−  − ρs , ct−  (   ) Note add three partitions , result equal one . guarantees forward message passing , information discarded ensure message overloaded , similar forgetting mechanism LSTM . , aggregate partition information target cells , three partitions formed result . three partitions , add related information cells : ρe = ρe , ct−  ○ ct−  + ρe , ˜ct ○ ˜ct ρr = ρr , ct−  ○ ct−  + ρr , ˜ct ○ ˜ct ρs = ρs , ct−  ○ ct−  + ρs , ˜ct ○ ˜ct (   ) Filter propose three types memory block : entity memory , relation memory shared mem- ory . denote µe entity memory , µr  The calculation candidate cell ˜ct identical equation (   ) therefore shown . practically relation memory µs shared memory . µe , information entity partition shared partition selected . contrast , information relation partition , assume irrelevant even harmful named entity recognition task , ﬁltered . logic applies µr well , information entity partition ﬁltered rest kept . addition , information shared partition stored µs : µe = ρe + ρs ; µr = ρr + ρs ; µs = ρs (   ) Note inter-task information shared partition accessible entity memory relation memory , allowing balanced interaction NER . Whereas sequential parallel encoding , relation features direct impact formation entity features . updating information memory , entity features , relation features hr shared features hs generated corresponding memories : = tanh ( µe ) hr = tanh ( µr ) hs = tanh ( µs ) Following partition ﬁlter steps , information (   ) ……PartitionFilterPartitionFilterPartitionFilterPartitionFilterPartitionFilter𝑥 $ 𝑥 % 𝑥 & 𝑥 ’ 𝑥 ( GlobalFeatureNER-specificFeatureNERUnitsREUnits……SharedFeatureRE-specificFeaturẽ𝑟 ’ % & ’ ̃𝑒 ’ % & ’ ✂✂𝜌 $ 𝑐 ! '' # 𝜌 % 𝜌 & 𝜇 $ 𝜇 % 𝜇 & ℎ $ ℎ % ℎ & 𝑐 ! ℎ ! '' # ̃𝑐 ! 𝑥 ! PartitionFilter✂✂̃𝑟̃ ’ % ̃𝑒̃ ’ % ℎ ! three memories used form cell state ct , used generate hidden state ht ( hidden cell state time step input next time step ) : ct = Linear ( [ µe , ; µr , ; µs , ] ) ht = tanh ( ct ) (   )  .  Global Representation model , employ unidirectional encoder feature encoding . backward encoder bidirectional setting replaced task-speciﬁc global representation capture semantics future context . Empirically shows effective . task , global representation combination task-speciﬁc features shared features computed : already covered NER unit . Similar NER , consider relation extraction relation-speciﬁc table ﬁlling problem . Given relation label set R , relation l ∈ R , ﬁll table whose element rl ij represents probability word wi word wj starting word subject object entity . way , extract triples revolving around relation l one relation table . triple ( wi , l , wj ) , similar NER unit , triple representation hr ij calculated follows : ij relation score rl ; hr ij = ELU ( Linear ( [ hr hr rl ij = p ( r = ⟨wi , l , wj⟩ ∣r ∈ ) ij ) ) , ∀l ∈ R = σ ( Linear ( hr j ; hgr ] ) ) (    )  .  Training Inference given training dataset , loss function L guides model training consists two parts : Lner NER unit Lre unit : (   ) hge , = tanh ( Linear [ , ; hs , ] ) hgr , = tanh ( Linear [ hr , ; hs , ] ) hge = maxpool ( hge,  , . . . , hge , L ) hgr = maxpool ( hgr,  , . . . , hgr , L )  .  Task Units model consists two task units : NER unit unit . NER unit , objective identify categorize entity spans given sentence . speciﬁcally , task treated type-speciﬁc table ﬁlling problem . Given entity type set E , type k , ﬁll table whose element ek ij represents probability word wi word wj start end position entity type k. word pair ( wi , wj ) , concatenate word-level entity features  j , well sentence-level global features hge feeding fully-connected layer ELU activation get entity span representation ij : ij = ELU ( Linear ( [  ; j ; hge ] ) ) (   ) span representation , predict whether span entity type k feeding feed forward neural layer : ek ij = p ( e = ⟨wi , k , wj⟩ ∣e ∈ ) ij ) ) , ∀k ∈ E = σ ( Linear ( (    ) Lner = ∑ ˆek ij ∈S Lre = ∑ ˆrl ij ∈T BCELoss ( ek ij , ˆek ij ) BCELoss ( rl ij , ˆrl ij ) (    ) ˆek ij ˆrl ij respectively ground truth label entity table relation table . ek ij predicted ones . adopt BCELoss task  . training objective minimize loss function L , computed Lner + Lre . ij rl inference , extract relational triples combining results NER unit . legitimate triple prediction ( sk , n ) l relation label , k k′ entity type labels , indexes , j , n respectively starting ending index subject entity object entity , following conditions satisﬁed : , j , l , ok′ ij ≥ λe ; ek′ ek mn ≥ λe ; rl im ≥ λr (    ) λe λr threshold hyper-parameters entity relation prediction , set  .  without ﬁne-tuning .   Experiment σ represents sigmoid activation function . Computation unit mostly symmetrical NER unit . Given set gold relation triples denoted , unit aims identify triples sentence . predict starting word entity unit entity span prediction  .  Dataset , Evaluation Implementation Details evaluate model six datasets . NYT ( Riedel et al. ,      ) , WebNLG ( Zeng et al. ,      ) ,  BCELoss ( x , ) = − ( ylogx + (   − ) log (   − x ) ) . ADE ( Gurulingappa et al. ,      ) , SciERC ( Luan et al. ,      ) , ACE   ACE   ( Walker et al. ,      ) . Descriptions datasets found Appendix . Following previous work , assess model NYT/WebNLG partial match , tail entity annotated . Besides , entity type information annotated datasets , set type entities single label `` NONE '' , entity type would predicted model . ACE   , ACE   , ADE SciERC , assess model exact match head tail entity annotated . ADE ACE   ,   -fold  - fold cross validation used evaluate model respectively ,    % training set used construct development set . evaluation metrics , report F  scores NER . NER , entity seen correct type boundary correct . , triple correct types , boundaries entities relation type correct . addition , report Macro-F  score ADE Micro-F  score datasets . choose model parameters based performance development set ( best average F  score NER ) report results test set . details hyper- parameters found Appendix B  .  Main Result Table   shows comparison model existing approaches . partially annotated datasets WebNLG NYT , setting BERT . , model achieves  .  % improvement WebNLG performance NYT slightly better previous SOTA TpLinker ( Wang et al. ,     b )  .  % margin . argue NYT generated distant supervision , annotation entity relation often incomplete wrong . Compared TpLinker , strength method reinforce two-way interaction entity relation . However , dealing noisy data , strength might counter-productive error propagation tasks ampliﬁed well . NER , method shows distinct advantage baselines report ﬁgures . Compared Casrel ( Wei et al. ,      ) , competitive method , F  scores  .  % / .  % higher NYT/WebNLG . proves exposing relation information Method NER NYT △ CopyRE ( Zeng et al. ,      ) GraphRel ( Fu et al. ,      ) CopyRL ( Zeng et al. ,      ) Casrel ( Wei et al. ,      ) † TpLinker ( Wang et al. ,     b ) † PFN† WebNLG △ CopyRE ( Zeng et al. ,      ) GraphRel ( Fu et al. ,      ) CopyRL ( Zeng et al. ,      ) Casrel ( Wei et al. ,      ) † TpLinker ( Wang et al. ,     b ) † PFN† -   .    .    .    .    .  (   .  )   .    .    .    .  - -   .    .    .    .    .  (   .  )   .    .    .    .  - ADE ▲ Multi-head ( Bekoulis et al. ,     b )   .    .  Multi-head + ( Bekoulis et al. ,     a )   .    .    .    .  Rel-Metric ( Tran Kavuluru ,      ) SpERT ( Eberts Ulges ,      ) †   .    .  Table-Sequence ( Wang Lu ,      ) ‡   .    .  PFN†   .    .  PFN‡   .    .  ACE   △ Structured Perceptron ( Li Ji ,      ) SPTree ( Miwa Bansal ,      ) Multi-turn QA ( Li et al. ,      ) † Table-Sequence ( Wang Lu ,      ) ‡ PURE ( Zhong Chen ,      ) ‡ PFN‡ ACE   △ Structured Perceptron ( Li Ji ,      ) SPTree ( Miwa Bansal ,      ) Multi-turn QA ( Li et al. ,      ) † Table-Sequence ( Wang Lu ,      ) ‡ PURE ( Zhong Chen ,      ) ‡ PFN‡ SciERC △ SPE ( Wang et al. ,     a ) § PURE ( Zhong Chen ,      ) § PFN§   .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .  † , Table   : Experiment results six datasets . ‡ § denotes use BERT , ALBERT SCIBERT ( Devlin et al. ,      ; Lan et al. ,      ; Beltagy et al. ,      ) pre-trained embedding . △ ▲ denotes use micro-F  macro-F  score . NER results Casrel reported average score head tail entity . Results PURE reported single-sentence setting fair comparison . NER , present Casrel , leads better performance entity recognition . Furthermore , model demonstrates strong performance fully annotated datasets ADE , ACE   , ACE   SciERC . ADE , model surpasses table-sequence ( Wang Lu ,      )  .  % / .  % NER/RE . ACE   , model surpasses PURE ( Zhong Chen ,      )  .  % results weaker performance NER  .  % . argue could attributed fact , unlike former three datasets , ACE   contains many entities belong triple . Thus utilizing relation information entity prediction might fruitful datasets ( PURE pipeline approach relation information unseen entity prediction ) . ACE   , model surpasses PURE  .  % / .  % NER/RE . SciERC , model surpasses PURE  .  % / .  % NER/RE . Overall , performance model shows remarkable improvement previous baselines .  .  Ablation Study section , take closer look check effectiveness framework relation extrac- tion concerning ﬁve different aspects : number encoder layer , bidirectional versus unidirectional , encoding scheme , partition granularity decod- ing strategy . Number Encoder Layers Similar recur- rent neural network , stack partition ﬁlter encoder arbitrary number layers . examine frameworks three layers . shown table   , adding layers partition ﬁlter encoder leads improvement F -score . shows one layer good enough encoding task-speciﬁc features . Bidirection Vs Unidirection Normally need two partition ﬁlter encoders ( one reverse or- der ) model interaction forward backward context . However , discussed section  .  , model replaces backward encoder global representation let future context visible word , achieving similar effect bidirectional settings . order ﬁnd works best , compare two methods ablation study . table   , ﬁnd unidirectional encoder global representation outperforms bidirectional encoder without global representation , showing global representation suitable providing future context word backward encoder . addition , global representation involved , Ablation Layers Bidirection Vs Unidirection Encoding Scheme Partition Granularity Decoding Strategy Settings N=  N=  N=  Unidirection ( w/o gl . ) Bidirection ( w/o gl . ) Joint Sequential Parallel Fine-grained Coarse Universal Selective P   .    .    .    .    .    .    .    .    .    .    .    .    .    .  R   .    .    .    .    .    .    .    .    .    .    .    .    .    .  F   .    .    .    .    .    .    .    .    .    .    .    .    .    .  Table   : Ablation study SciERC . P , R F represent precision , recall F  relation scores . best results marked bold . gl . second experiment short global representation . unidirectional encoder achieves similar result F  score compared bidirectional encoder , indicating global representation alone enough capturing semantics future context . Encoding Scheme replace partition ﬁlter encoder two LSTM variants examine effectiveness encoder . parallel setting , use two LSTM encoders learn task-speciﬁc features separately , interaction allowed except sharing input . sequential setting one-way interaction allowed , entity features generated ﬁrst LSTM encoder fed second one produce relation features . table   , observe partition ﬁlter outperforms LSTM variants large margin , proving effectiveness encoder modelling two-way interaction two encoding schemes . Partition Granularity Similar ( Shen et al. ,      ) , split neurons several chunks perform partition within chunk . chunk shares entity gate relation gate . Thus partition results chunks remain . example ,    -dimension neuron set , split    chunks ,    neurons , two   -dimension gates needed neuron partition . refer operation coarse contrast , ﬁne-grained partition partition . seen special case neurons split one chunk . compare ﬁne-grained partition ( chunk size =     ) coarse partition Dataset ACE   ACE   SciERC Entity Type Total In-triple Out-of-triple Diff Total In-triple Out-of-triple Diff Total In-triple Out-of-triple Diff P   .    .    .    .    .    .    .   .    .    .    .    .  R   .    .    .   .    .    .    .   .    .    .    .   .  F   .    .    .   .    .    .    .   .    .    .    .    .  Ratio  .    .    .   -  .    .    .   -  .    .    .   - Table   : NER Results different entity types . Entities split two groups : In-triple Out-of-triple based whether appear relational triples . Diff performance difference In- triple Out-of-triple . Ratio number entities given type divided number total entities test set ( train , dev test set combined ACE   ) . Results ACE   averaged  -folds ( chunk size =    ) . Table   shows ﬁne-grained partition performs better coarse partition . surprising coarse partition , assumption performing neuron partition chunk might strong encoder separate information task properly . Decoding Strategy pipeline-like methods , relation prediction performed entities system considers valid entity prediction . argue better way relation prediction take account invalid word pairs . refer former strategy selective decoding latter one universal decoding . selective decoding , predict relation scores entities deemed valid entity scores calculated NER unit . Table   shows universal decoding , negative instances included , better selective decoding . Apart mitigating error propagation , argue universal decoding similar contrastive learning negative instances helps better identify positive instances implicit comparison .   Effects Relation Signal Entity Recognition widely accepted fact entity recognition helps predicting relations , effect rela- tion signals entity prediction remains divergent among researchers . two auxiliary experiments , ﬁnd absence relation signals considerable bearing entity recognition .  .  Analysis Entity Prediction Different Types Table   , NER performance model consistently better baselines except ACE   performance falls short non-negligible margin . argued could attributed fact ACE   contains many entities belong triples . corroborate claim , section try quantify performance gap entity prediction entities belong certain triples relation entities . former ones referred In-triple entities latter Out-of-triple entities . split entities two groups test NER performance group ACE  /ACE  /SciERC . NYT/WebNLG/ADE , since Out-of-triple entity non-existent , evaluation performed datasets . shown table   , huge gap In-triple entity prediction Out-of- triple entity prediction , especially SciERC diff score reaches   .  % . argue might attributed fact entity prediction SciERC generally harder given involves identiﬁcation scientiﬁc terms also average length entities SciERC longer . Another observation diff score largely attributed difference precision , means without guidance relational signal , model tends over-optimistic entity prediction . addition , compared PURE ( Zhong Chen ,      ) ﬁnd overall performance NER negatively correlated percentage out-of-triple entities dataset . especially ACE   , performance model relatively weak ,    % entities Out-of-triple . phenomenon manifest weakness joint model : Joint modeling NER might somewhat harmful entity prediction inference patterns In-triple Out-of-triple entity different , considering dynamic relation information entity prediction different In-triple Out- of-triple entity . Model ConcatSent CrossCategory Average Ori → Aug Decline Ori → Aug Decline Ori → Aug Decline Ori → Aug Decline Ori → Aug Decline Decline   . →  .  BiLSTM-CRF   . →  .  BERT-base ( cased ) BERT-base ( uncased )   . →  .    . →  .  TENER   . →  .  Flair   . →  .  PFN   . →  .    . →  .    . →  .    . →  .    . →  .    . →  .    . →  .    . →  .    . →  .    . →  .    . →  .    . →  .    . →  .    . →  .    . →  .    . →  .    . →  .    . →  .    . →  .    . →  .    . →  .    . →  .    . →  .    . →  .    .   .    .    .   .   .    .   .    .    .    .   .    .    .    .    .    .   .    .    .    .    .    .   .   .   .   .   .   .   .   .   .   .   .   .   .  SwapLonger EntTypos OOV Table   : Robustness test NER input perturbation ACE   , baseline results test ﬁles copied https : //www.textflint.io/  .  Robustness Test Named Entity Recognition use robustness test evaluate model adverse circumstances . case , use domain transformation methods NER ( Wang et al. ,      ) . compared baselines relation-free models , including BiLSTM- CRF ( Huang et al. ,      ) , BERT ( Devlin et al. ,      ) , TENER ( Yan et al. ,      ) Flair- Embeddings ( Akbik et al. ,      ) . Descriptions transformation methods found Appendix table   , observe model mostly resilient input perturbations compared baselines , especially category CrossCategory , probably attributed fact relation signals used training impose type constraints entities , thus inference entity types less affected semantic meaning target entity , rather ( relational ) context surrounding entity .  .  Relation Signal Helps Predicting Entities Contrary ( Zhong Chen ,      ) claimed ( relation signal minimal effects entity prediction ) , ﬁnd several clues suggest otherwise . First , section  .  , observe In-triple entities much easier predict Out-of-triple entities , suggests relation signals useful entity prediction . Second , section  .  , perform robustness test NER evaluate model ’ capability input perturbation . robustness test compare method - joint model relation-free baselines . result suggests method much resilient adverse circumstances , could ( least partially ) explained introduction relation signals . sum , ﬁnd relation signals non-negligible effect entity prediction . reason ( Zhong Chen ,      ) conclude relation information minimal inﬂuence entity prediction probably due selective bias , meaning evaluated dataset ACE   contains large proportion Out-of-triple entities (    % ) , essence require relation signal .   Conclusion paper , encode task-speciﬁc features newly proposed model : Partition Filter Net- work joint entity relation extraction . Instead extracting task-speciﬁc features sequential parallel manner , employ partition ﬁlter encoder generate task-speciﬁc features jointly order model two-way inter-task interaction properly . conduct extensive experiments six datasets verify effectiveness model . Overall experiment results demonstrate model superior previous baselines entity relation prediction . Furthermore , dissection several aspects model ablation study sheds light works best framework . Lastly , contrary previous work claimed , auxiliary experiments suggest relation prediction contributory named entity prediction non-negligible way .   Acknowledgements authors wish thank anonymous reviewers helpful comments . work partially funded China National Key R & Program ( No.    YFB        ) , National Natural Science Foundation China ( No.         ,          ) , Shanghai Municipal Science Technology Major Project ( No.    SHZDZX     ) . References Alan Akbik , Tanja Bergmann , Duncan Blythe , Kashif Rasul , Stefan Schweter , Roland Vollgraf .      . FLAIR : easy-to-use framework state-of-the- art NLP . Proceedings      Conference North American Chapter Association Computational Linguistics ( Demonstrations ) , pages   –   , Minneapolis , Minnesota . Association Computational Linguistics . Giannis Bekoulis , Johannes Deleu , Thomas Demeester , Chris Develder .     a . Adversarial training multi-context joint entity relation extraction . Proceedings      Conference Empirical Methods Natural Language Processing , pages     –     , Brussels , Belgium . Association Computational Linguistics . Giannis Bekoulis , Johannes Deleu , Thomas Demeester , Chris Develder .     b . Joint entity recognition relation extraction multi-head selection problem . Expert Systems Applications ,    :  –    . Iz Beltagy , Kyle Lo , Arman Cohan .      . SciBERT : pretrained language model scientiﬁc Proceedings      Conference text . Empirical Methods Natural Language Processing  th International Joint Conference Natural Language Processing ( EMNLP-IJCNLP ) , pages     –     , Hong Kong , China . Association Computational Linguistics . Yee Seng Chan Dan Roth .      . Exploiting syntactico-semantic structures relation extrac- Proceedings   th Annual Meeting tion . Association Computational Linguistics : Human Language Technologies , pages    –    , Portland , Oregon , USA . Association Computa- tional Linguistics . Jacob Devlin , Ming-Wei Chang , Kenton Lee , BERT : Pre-training Kristina Toutanova .      . transformers language deep bidirectional      understanding . Conference North American Chapter Association Computational Linguistics : Human Language Technologies , Volume   ( Long Short Papers ) , pages     –     , Minneapolis , Minnesota . Association Computational Linguistics . Proceedings Dennis Diefenbach , Vanessa Lopez , Kamal Singh , Pierre Maret .      . Core techniques question answering systems knowledge bases : survey . Knowledge Information systems ,    (   ) :   –     . Markus Eberts Adrian Ulges .      . Span-based joint entity relation extraction transformer pre-training . arXiv preprint arXiv:    .      . Tsu-Jui Fu , Peng-Hsuan Li , Wei-Yun .      . GraphRel : Modeling text relational graphs joint entity relation extraction . Proceedings Association  Computational Linguistics , pages     –     , Florence , Italy . Association Computational Linguistics .   th Annual Meeting Pankaj Gupta , Hinrich Schütze , Bernt Andrassy .      . Table ﬁlling multi-task recurrent neural network joint entity relation extraction . Proceedings COLING      ,   th Interna- tional Conference Computational Linguistics : Technical Papers , pages     –     , Osaka , Japan . COLING      Organizing Committee . Vishal Gupta Gurpreet Singh Lehal .      . survey text summarization extractive techniques . Journal Emerging Technologies Web Intelli- gence ,  :   –    . Harsha Gurulingappa , Abdul Mateen Rajput , Angus Roberts , Juliane Fluck , Martin Hofmann-Apitius , Luca Toldo .      . Development benchmark corpus support automatic extraction drug- related adverse effects medical case reports . Journal biomedical informatics ,    (   ) :   –    . Zhiheng Huang , Wei Xu , Kai Yu .      . Bidirec- tional lstm-crf models sequence tagging . arXiv preprint arXiv:    .      . Arzoo Katiyar Claire Cardie .      . Going limb : Joint extraction entity mentions relations without dependency trees . Proceedings   th Annual Meeting Association Computational Linguistics ( Volume   : Long Papers ) , pages    –    , Vancouver , Canada . Association Computational Linguistics . Diederik P. Kingma Jimmy Ba .      . Adam :  rd Inter- method stochastic optimization . national Conference Learning Representations , ICLR      , San Diego , CA , USA , May  -  ,      , Conference Track Proceedings . Zhenzhong Lan , Mingda Chen , Sebastian Goodman , Kevin Gimpel , Piyush Sharma , Radu Soricut .      . ALBERT : lite BERT self-supervised  th Inter- learning language representations . national Conference Learning Representations , ICLR      , Addis Ababa , Ethiopia , April   -   ,      . OpenReview.net . Qi Li Heng Ji .      . Incremental joint extraction Proceedings entity mentions relations .   nd Annual Meeting Association Computational Linguistics ( Volume   : Long Papers ) , pages    –    , Baltimore , Maryland . Association Computational Linguistics . Xiaoya Li , Fan Yin , Zijun Sun , Xiayu Li , Arianna Yuan , Duo Chai , Mingxin Zhou , Jiwei Li .      . Entity-relation extraction multi-turn question an- swering . Proceedings   th Annual Meeting Association Computational Linguistics , pages     –     , Florence , Italy . Association Computational Linguistics . Yi Luan , Luheng , Mari Ostendorf , Hannaneh Multi-task identiﬁcation relations , coreference scientiﬁc Proceedings Hajishirzi .      . entities , knowledge graph construction .      Conference Empirical Methods Natural Language Processing , pages     –     , Brussels , Belgium . Association Computational Linguistics . Makoto Miwa Mohit Bansal .      . End-to-end relation extraction using LSTMs sequences tree structures . Proceedings   th Annual Meeting Association Computational Lin- guistics ( Volume   : Long Papers ) , pages     –     , Berlin , Germany . Association Computational Linguistics . Makoto Miwa Yutaka Sasaki .      . Modeling joint entity relation extraction table representation . Proceedings      Confer- ence Empirical Methods Natural Language Processing ( EMNLP ) , pages     –     , Doha , Qatar . Association Computational Linguistics . Sebastian Riedel , Limin Yao , Andrew McCallum .      . Modeling relations mentions without labeled text . Joint European Conference Machine Learning Knowledge Discovery Databases , pages    –    . Springer . Sebastian Riedel , Limin Yao , Andrew McCallum , Benjamin M. Marlin .      . Relation extraction matrix factorization universal schemas . Proceedings      Conference North American Chapter Association Computa- tional Linguistics : Human Language Technologies , pages   –   , Atlanta , Georgia . Association Computational Linguistics . Yikang Shen , Shawn Tan , Alessandro Sordoni , Aaron C. Courville .      . Ordered neurons : Integrating tree structures recurrent neural  th International Conference networks . Learning Representations , ICLR      , New Orleans , LA , USA , May  -  ,      . OpenReview.net . Yongliang Shen , Xinyin , Yechun Tang , Weiming Lu .      . trigger-sense memory ﬂow framework joint entity relation extraction . Proceedings Web Conference      , pages     –     . Nitish Srivastava , Geoffrey Hinton , Alex Krizhevsky , Ilya Sutskever , Ruslan Salakhutdinov .      . Dropout : simple way prevent neural networks overﬁtting . journal machine learning research ,    (   ) :    –     . Tung Tran Ramakanth Kavuluru .      . Neural metric learning fast end-to-end relation extrac- tion . arXiv preprint arXiv:    .      . Christopher Walker , Stephanie Strassel , Julie Medero , Kazuaki Maeda .      . Ace      multilingual Linguistic Data Consortium , training corpus . Philadelphia ,   :   .      Conference Empirical Methods Natural Language Processing ( EMNLP ) , pages     –     , Online . Association Computational Linguistics . Xiao Wang , Qin Liu , Tao Gui , Qi Zhang , Yicheng Zou , Xin Zhou , Jiacheng Ye , Yongxin Zhang , Rui Zheng , Zexiong Pang , et al .      . Textﬂint : Uniﬁed multilingual robustness evaluation toolkit natural language processing . Yijun Wang , Changzhi Sun , Yuanbin Wu , Junchi Yan , Peng Gao , Guotong Xie .     a . Pre- training entity relation encoder intra-span  inter-span information .      Conference Empirical Methods Natural Language Processing ( EMNLP ) , pages     –     , Online . Association Computational Linguistics . Proceedings Yucheng Wang , Bowen Yu , Yueyang Zhang , Tingwen Limin Sun .     b . Liu , Hongsong Zhu , TPLinker : Single-stage joint extraction entities relations token pair  Proceedings   th International Conference Computational Linguistics , pages     –     , Barcelona , Spain ( Online ) . International Committee Computational Linguistics . linking . Zhepei Wei , Jianlin Su , Yue Wang , Yuan Tian , Yi Chang .      . novel cascade binary tagging  framework relational Proceedings  Association Computational Linguistics , pages     –     , Online . Association Computational Linguistics .   th Annual Meeting triple extraction . Hang Yan , Bocao Deng , Xiaonan Li , Xipeng Qiu .      . Tener : adapting transformer encoder arXiv preprint named entity recognition . arXiv:    .      . Xiaofeng Yu Wai Lam .      . Jointly identifying entities extracting relations encyclopedia text Coling      : via graphical model approach . Posters , pages     –     , Beijing , China . Coling      Organizing Committee . Dmitry Zelenko , Chinatsu Aone , Anthony Richardella .      . Kernel methods relation extraction . Proceedings      Conference Empirical Methods Natural Language Process- ing ( EMNLP      ) , pages   –   . Association Computational Linguistics . Daojian Zeng , Kang Liu , Siwei Lai , Guangyou Zhou , Jun Zhao .      . Relation classiﬁcation via convolutional deep neural network . Proceedings COLING      ,   th International Conference Computational Linguistics : Technical Papers , pages     –     , Dublin , Ireland . Dublin City University Association Computational Lin- guistics . Jue Wang Wei Lu .      . Two better one : Joint entity relation extraction Proceedings table-sequence encoders . Xiangrong Zeng , Shizhu , Daojian Zeng , Kang Liu , Shengping Liu , Jun Zhao .      . Learning extraction order multiple relational facts Dataset NYT WebNLG ADE ACE   ACE   SciERC Train   ,     ,      ,    # Sentences Dev  ,         ,    (   -fold )  ,     ,    (  -fold )     Test  ,         ,         ,    ∣E∣ ∣R∣ - -                        Table   : Statistics datasets . ∣E∣ ∣R∣ numbers entity relation types . NYT WebNLG , entity type information annotated . learning .  sentence reinforcement Proceedings      Conference Empir- ical Methods Natural Language Processing  th International Joint Conference Natural Language Processing ( EMNLP-IJCNLP ) , pages    –    , Hong Kong , China . Association Computational Linguistics . Xiangrong Zeng , Daojian Zeng , Shizhu , Kang Liu , Jun Zhao .      . Extracting relational facts end-to-end neural model copy mechanism . Proceedings   th Annual Meeting Association Computational Linguistics ( Volume   : Long Papers ) , pages    –    , Melbourne , Aus- tralia . Association Computational Linguistics . Suncong Zheng , Feng Wang , Hongyun Bao , Yuexing Hao , Peng Zhou , Bo Xu .      . Joint extraction entities relations based novel tagging scheme . Proceedings   th Annual Meeting Association Computational Linguistics ( Volume   : Long Papers ) , pages     –     , Vancouver , Canada . Association Computational Linguistics . Zexuan Zhong Danqi Chen .      . frustratingly easy approach entity relation extraction . North American Association Computational Linguistics ( NAACL ) . Dataset evaluate model six datasets . NYT ( Riedel et al. ,      ) sampled New York Times news articles annotated distant supervision . WebNLG originally created Natural Language Generation task applied ( Zeng et al. ,      ) relation extraction dataset . ACE   ACE   ( Walker et al. ,      ) collected various sources , including news articles online forums . ADE ( Gurulingappa et al. ,      ) contains medical descriptions adverse effects drug use . SciERC ( Luan et al. ,      ) collected     AI paper abstracts originally used scientiﬁc knowledge graph construction . Following previous work , ﬁlter samples containing overlapping entities ADE , makes  .  % whole dataset . Statistics datasets found table   B Implementation Details leverage pre-trained language models embedding layer . Following previous work , versions use bert-base-cased , albert- xxlarge-v  scibert-scivocab-uncased . Batch size learning rate set  /    e-  / e-  SciERC/Others respectively . order prevent overﬁtting , dropout ( Srivastava et al. ,      ) used word embedding , entity span triple representation task units ( set  .  ) . use Adam ( Kingma Ba ,      ) optimize model parameters train model     epochs . Also , prevent gradient explosion , gradient clipping applied training . C Analysis Overlapping Pattern Triple Number comprehensive evaluation , assess model NYT/WebNLG datasets dif- ferent triple overlapping patterns ( see section   detailed description patterns ) sentences containing different number triples . Since previous work com- pare triple overlapping pattern triple number ADE/ACE  /ACE  /ScIERC given EPO triples non-existent datasets , compari- son result included datasets . shown ﬁgure   , model mostly superior two baselines three categories . Interestingly normal class , model performs signiﬁcantly better WebNLG , score NYT basically par TpLinker . argue could probably caused fact NYT , generated distant supervision , much noisier WebNLG . Besides , sentences normal triples likely much noisier sentences EPO SEO triples since higher chance incomplete annotation . Thus unsurprising signiﬁcant improvement achieved predicting normal triples NYT . Besides , ﬁgure   observe model performs better sentences ﬁve triples datasets , interaction entity relation becomes complex . strong performance sentences conﬁrms ( ) NYT ( b ) WebNLG Figure   : F -score relation triple extraction sentences three different overlapping patterns . ( ) NYT ( b ) WebNLG Figure   : F -score relational triple extraction sentences containing N triples , N ranges   ≥  .   . methods include inserting descriptions entities , unfair might introduce new entity relation . superiority model baselines . Details Robustness Test Descriptions transformation methods used Table   listed follows :   . ConcatSent - Concatenate sentences longer one .   . CrossCategory - Entity Swap swaping entities ones labeled different types .   . EntTypos - Swap/delete/add random character entities .   . OOV - Entity Swap out-of-vocabulary entities .   . SwapLonger - Substitute short entities longer ones . Transformations viable following reasons :   . input sentence . restricted one triple per   . methods include entity swap , already covered NER .   . methods include relation-speciﬁc transfor- mations ( Age , Employee , Birth ) ACE   type relations . CasRelTPLinkerPFN              .   .   .   .   .   .   .   .   . NormalEPOSEOCasRelTPLinkerPFN              .   .   .   .   .   .   .   .   . NormalEPOSEOCasRelTPLinkerPFN                    .   .   .   .   .   .   .   .   .   .   .   .   .   .   . N= N= N= N= N≥ CasRelTPLinkerPFN                    .   .   .   .   .   .   .   .   .   .   .   .   .   .   . N= N= N= N= N≥ 