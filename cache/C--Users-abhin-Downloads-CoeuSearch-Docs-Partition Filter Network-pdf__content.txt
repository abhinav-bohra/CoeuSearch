A Partition Filter Network for Joint Entity and Relation Extraction Zhiheng Yan1, Chong Zhang1, Jinlan Fu1,2, Qi Zhang1∗ and Zhongyu Wei3 1School of Computer Science, Shanghai Key Laboratory of Intelligent Information Processing, Fudan University, Shanghai, China 2National University of Singapore, Singapore 3School of Data Science, Fudan University, Shanghai, China {zhyan20, chongzhang20, qz, zywei}@fudan.edu.cn jinlanjonna@gmail.com Abstract answering (Diefenbach et al., 2018) and text summarization (Gupta and Lehal, 2010). 1 2 0 2 p e S 1 1 ] L C . s c [ 8 v 2 0 2 2 1 . 8 0 1 2 : v i X r a In joint entity and relation extraction, existing work either sequentially encode task-speciﬁc features, leading to an imbalance in inter-task feature interaction where features extracted later have no direct contact with those that come ﬁrst. Or they encode entity features and relation features in a parallel manner, meaning that feature representation learning for each task is largely independent of each other except for input sharing. We propose a partition ﬁlter network to model two-way interaction between tasks properly, where fea- ture encoding is decomposed into two steps: partition and ﬁlter. In our encoder, we leverage two gates: entity and relation gate, to segment neurons into two task partitions and one shared partition. The shared partition represents inter-task information valuable to both tasks and is evenly shared across two tasks to ensure proper two-way interaction. The task partitions represent intra-task information and are formed through concerted efforts of both gates, making sure that encoding of task- speciﬁc features is dependent upon each other. Experiment results on six public datasets show that our model performs signiﬁcantly better than previous approaches. In addition, con- trary to what previous work has claimed, our that relation auxiliary experiments suggest prediction is contributory to named entity prediction in a non-negligible way. The source code can be found at https://github.com/ Coopercoppers/PFN. 1 Introduction Joint entity and relation extraction intend to simultaneously extract entity and relation facts in the given text to form relational triples as (s, r, o). The extracted information provides a supplement to many studies, such as knowledge graph construction (Riedel et al., 2013), question ∗∗ Corresponding author. Figure 1: Partition process of cell neurons. Entity and relation gate are used to divide neurons into task- related and task-unrelated ones. Neurons relating to both tasks form the shared partition while the rest form two task partitions. Conventionally, Named Entity Recognition (NER) and Relation Extraction (RE) are performed in a pipelined manner (Zelenko et al., 2002; Chan and Roth, 2011). These approaches are ﬂawed in that they do not consider the intimate connection between NER and RE. Also, error propagation is another drawback of pipeline methods. In order to conquer these issues, joint extracting entity and relation is proposed and demonstrates stronger performance on both tasks. In early work, joint methods mainly rely on elaborate feature engineering to establish interaction between NER and RE (Yu and Lam, 2010; Li and Ji, 2014; Miwa and Sasaki, 2014). Recently, end-to-end neural network has shown to be successful in extracting relational triples (Zeng et al., 2014; Gupta et al., 2016; Katiyar and Cardie, 2017; Shen et al., 2021) and has since become the mainstream of joint entity and relation extraction. According to their differences in encoding task- speciﬁc features, most of the existing methods can be divided into two categories: sequential encoding and parallel encoding. In sequential task-speciﬁc features are generated encoding, sequentially, which means features extracted ﬁrst are not affected by those that are extracted later. Zeng et al. (2018) and Wei et al. (2020) are Cell StateNERREPartition✂✂RE-SpecificSharedNER-Specific            typical examples of this category. Their methods extract features for different tasks in a predeﬁned order. In parallel encoding, task-speciﬁc features are generated independently using shared input. Compared with sequential encoding, models build on this scheme do not need to worry about the implication of encoding order. For example, Fu et al. (2019) encodes entity and relation information separately using common features derived from their GCN encoder. Since both task- speciﬁc features are extracted through isolated sub- modules, this approach falls into the category of parallel encoding. However, both encoding designs above fail to model two-way interaction between NER and RE tasks properly. In sequential encoding, interaction is only unidirectional with a speciﬁed order, result- ing in different amount of information exposed to NER and RE task. In parallel encoding, although encoding order is no longer a concern, interaction is only present in input sharing. Considering adding two-way interaction in feature encoding, we adopt joint encoding. an alternative encoding design: This design encodes task-speciﬁc features jointly with a single encoder where there should exist some mutual section for inter-task communication. In this work, we instantiate joint encoding with a partition ﬁlter encoder. Our encoder ﬁrst sorts and partitions each neuron according to its contribution to individual tasks with entity and relation gates. During this process, two task partitions and one shared partition are formed (see ﬁgure 1). Then individual task partitions and shared partition are combined to generate task-speciﬁc features, ﬁltering out irrelevant information stored in the opposite task partition. Task interaction in our encoder is achieved in two ways: First, the partitions, especially the task- speciﬁc ones, are formed through concerted efforts of entity and relation gates, allowing for interaction between the formation of entity and relation features determined by these partitions. Second, the shared partition, which represents information useful to both task, is equally accessible to the formation of both task-speciﬁc features, ensuring balanced two-way interaction. The contributions of our work are summarized below: 1. We propose partition ﬁlter network, a frame- work designed speciﬁcally for joint encoding. This method is capable of encoding task- speciﬁc features and guarantees proper two- way interaction between NER and RE. 2. We conduct extensive experiments on six datasets. The main results show that our method is superior to other baseline ap- proaches, and the ablation study provides in- sight into what works best for our framework. 3. Contrary to what previous work has claimed, our auxiliary experiments suggest that relation prediction is contributory to named entity prediction in a non-negligible way. 2 Related Work In recent years, joint entity and relation extraction approaches have been focusing on tackling triple overlapping problem and modelling task interac- tion. Solutions to these issues have been explored in recent works (Zheng et al., 2017; Zeng et al., 2018, 2019; Fu et al., 2019; Wei et al., 2020). The triple overlapping problem refers to triples sharing the same entity (SEO, i.e. SingleEntityOverlap) or entities (EPO, i.e. EntityPairOverlap). For example, In "Adam and Joe were born in the USA", since triples (Adam, birthplace, USA) and (Joe, birthplace, USA) share only one entity "USA", they should be categorized as SEO triples; or in "Adam was born in the USA and lived there ever since", triples (Adam, birthplace, USA) and (Adam, residence, USA) share both entities at the same time, thus should be categorized as EPO triples. Generally, there are two ways in tackling the problem. One is through generative methods like seq2seq (Zeng et al., 2018, 2019) where entity and relation mentions can be decoded multiple times in output sequence, another is by modeling each relation separately with sequences (Wei et al., 2020), graphs (Fu et al., 2019) or tables (Wang and Lu, 2020). Our method uses relation-speciﬁc tables (Miwa and Sasaki, 2014) to handle each relation separately. Task interaction modeling, however, has not been well handled by most of the previous work. In some of the previous approaches, Task interaction is achieved with entity and relation prediction sharing the same features (Tran and Kavuluru, 2019; Wang et al., 2020b). This could be problematic as information about entity and relation could sometimes be contradictory. Also, as models that use sequential encoding (Bekoulis et al., 2018b; Eberts and Ulges, 2019; Wei et al., 2020) or parallel encoding (Fu et al., 2019) lack proper two-way interaction in feature extraction, predictions made on these features suffer the problem of improper interaction. In our work, the partition ﬁlter encoder is built on joint encoding and is capable of handling communication of inter-task information more appropriately to avoid the problem of sequential and parallel encoding (exposure bias and insufﬁcient interaction), while keeping intra-task information away from the opposite task to mitigate the problem of negative transfer between the tasks. 3 Problem Formulation Our framework split up joint entity and relation ex- traction into two sub-tasks: NER and RE. Formally, Given an input sequence s = {w1, . . . , wL} with L tokens, wi denotes the i-th token in sequence s. For NER, we aim to extract all typed entities whose set is denoted as S, where ⟨wi, e, wj⟩ ∈ S signiﬁes that token wi and wj are the start and end token of an entity typed e ∈ E. E represents the set of entity types. Concerning RE, the goal is to identify all head-only triples whose set is denoted as T , each triple ⟨wi, r, wj⟩ ∈ T indicates that tokens wi and wj are the corresponding start token of subject and object entity with relation r ∈ R. R represents the set of relation types. Combining the results from both NER and RE, we should be able to extract relational triples with complete entity spans. 4 Model We describe our model design in this section. Our model consists of a partition ﬁlter encoder and two task units, namely NER unit and RE unit. The partition ﬁlter encoder is used to generate task- speciﬁc features, which will be sent to task units as input for entity and relation prediction. We will discuss each component in detail in the following three sub-sections. 4.1 Partition Filter Encoder Similar to LSTM, the partition ﬁlter encoder is a recurrent feature encoder with information stored in intermediate memories. In each time step, the encoder ﬁrst divides neurons into three partitions: entity partition, relation partition and shared par- tition. Then it generates task-speciﬁc features by selecting and combining these partitions, ﬁltering out information irrelevant to each task. As shown in ﬁgure 2, this module is designed speciﬁcally to jointly extract task-speciﬁc features, which strictly follows two steps: partition and ﬁlter. Partition This step performs neuron partition to divide cell neurons into three partitions: Two task partitions storing intra-task information, namely entity partition and relation partition, as well as one shared partition storing inter-task information. The neuron to be divided are candidate cell ˜ct representing current information and previous cell ct−1 representing history information. ct−1 is the direct input from the last time step and ˜ct is calculated in the same manner as LSTM: ˜ct = tanh(Linear([xt; ht−1])) (1) where Linear stands for the operation of linear transformation. We leverage entity gate ˜e and relation gate ˜r, which are referred to as master gates in (Shen et al., 2019), for neuron partition. As illustrated in ﬁgure 1, each gate, which represents one speciﬁc task, will divide neurons into two segments according to their usefulness to the designated task. For example, entity gate ˜e will separate neurons into two partitions: NER-related and NER-unrelated. The shared partition is formed by combining partition results from both gates. Neurons in the shared partition can be regarded as information valuable to both tasks. In order to model two- way interaction properly, inter-task information in the shared partition is evenly accessible to both tasks (which will be discussed in the ﬁlter subsection). In addition, information valuable to only one task is invisible to the opposing task and will be stored in individual task partitions. The gates are calculated using cummax activation 1, function cummax (⋅) = cumsum(sof tmax(⋅)) whose output can be seen as approximation of a binary gate with the form of (0, . . . , 0, 1, . . . , 1): ˜e = cummax(Linear([xt; ht−1])) ˜r = 1 − cummax(Linear([xt; ht−1])) (2) The intuition behind equation (2) is to identify two cut-off points, displayed as scissors in ﬁgure 2, which naturally divide a set of neurons into three segments. As a result, the gates will divide neurons into three partitions, entity partition ρe, relation partition ρr and shared partition ρs. Partitions for 1cumsum(x1, x2, . . . , xn−1, xn) = (x1, x1 + x2, . . . , x1 + x2 + ⋅ ⋅ ⋅ + xn−1, x1 + x2 + ⋅ ⋅ ⋅ + xn−1 + xn). (a) Framework of Partition Filter Network (b) Inner Mechanism of Partition Filter Figure 2: (a) Overview of PFN. The framework consists of three components: partition ﬁlter encoder, NER unit and RE unit. In task units, we use table-ﬁlling for word pair prediction. Orange, yellow and green represents NER-related, shared and RE-related component or features. (b) Detailed depiction of partition ﬁlter encoder in one single time step. We decompose feature encoding into two steps: partition and ﬁlter (shown in the gray area). In partition, we ﬁrst segment neurons into two task partitions and one shared partition. Then in ﬁlter, partitions are selected and combined to form task-speciﬁc features and shared features, ﬁltering out information irrelevant to each task. previous cell ct−1 are formulated as below: 2 ρs,ct−1 = ˜ect−1 ○ ˜rct−1 ρe,ct−1 = ˜ect−1 − ρs,ct−1 ρr,ct−1 = ˜rct−1 − ρs,ct−1 (3) Note that if you add up all three partitions, the result is not equal to one. This guarantees that in forward message passing, some information is discarded to ensure that message is not overloaded, which is similar to the forgetting mechanism in LSTM. Then, we aggregate partition information from both target cells, and three partitions are formed as a result. For all three partitions, we add up all related information from both cells: ρe = ρe,ct−1 ○ ct−1 + ρe,˜ct ○ ˜ct ρr = ρr,ct−1 ○ ct−1 + ρr,˜ct ○ ˜ct ρs = ρs,ct−1 ○ ct−1 + ρs,˜ct ○ ˜ct (4) Filter We propose three types of memory block: entity memory, relation memory and shared mem- ory. Here we denote µe as entity memory, µr as 2The calculation for candidate cell ˜ct identical to equation (3) and therefore not shown. is practically relation memory and µs as shared memory. In µe, information in entity partition and shared partition are selected. In contrast, information in relation partition, which we assume is irrelevant or even harmful to named entity recognition task, is ﬁltered out. The same logic applies to µr as well, where information in entity partition is ﬁltered out and the rest is kept. In addition, information in shared partition will be stored in µs: µe = ρe + ρs; µr = ρr + ρs; µs = ρs (5) Note that inter-task information in the shared partition is accessible to both entity memory and relation memory, allowing balanced interaction between NER and RE. Whereas in sequential and parallel encoding, relation features have no direct impact on the formation of entity features. After updating information in each memory, entity features he, relation features hr and shared features hs are generated with corresponding memories: he = tanh(µe) hr = tanh(µr) hs = tanh(µs) Following the partition and ﬁlter steps, information (6) ……PartitionFilterPartitionFilterPartitionFilterPartitionFilterPartitionFilter𝑥$𝑥%𝑥&𝑥’𝑥(GlobalFeatureNER-specificFeatureNERUnitsREUnits……SharedFeatureRE-specificFeaturẽ𝑟’%&’̃𝑒’%&’✂✂𝜌$𝑐!"#𝜌%𝜌&𝜇$𝜇%𝜇&ℎ$ℎ%ℎ&𝑐!ℎ!"#̃𝑐!𝑥!PartitionFilter✂✂̃𝑟̃’%̃𝑒̃’%ℎ!in all three memories is used to form cell state ct, which will then be used to generate hidden state ht (The hidden and cell state at time step t are input to the next time step): ct = Linear([µe,t; µr,t; µs,t]) ht = tanh(ct) (7) 4.2 Global Representation In our model, we employ a unidirectional encoder for feature encoding. The backward encoder in the bidirectional setting is replaced with task-speciﬁc global representation to capture the semantics of future context. Empirically this shows to be more effective. For each task, global representation is the combination of task-speciﬁc features and shared features computed by: already covered in NER unit. Similar to NER, we consider relation extraction as a relation-speciﬁc table ﬁlling problem. Given a relation label set R, for each relation l ∈ R, we ﬁll out a table whose element rl ij represents the probability of word wi and word wj being starting word of subject and object entity. In this way, we can extract all triples revolving around relation l with one relation table. For each triple (wi, l, wj), similar to NER unit, triple representation hr ij are calculated as follows: ij and relation score rl i ; hr ij = ELU(Linear([hr hr rl ij = p(r = ⟨wi, l, wj⟩ ∣r ∈ T ) ij)), ∀l ∈ R = σ(Linear(hr j ; hgr ])) (11) 4.4 Training and Inference For a given training dataset, the loss function L that guides the model during training consists of two parts: Lner for NER unit and Lre for RE unit: (8) hge,t = tanh(Linear[he,t; hs,t]) hgr,t = tanh(Linear[hr,t; hs,t]) hge = maxpool(hge,1, . . . , hge,L) hgr = maxpool(hgr,1, . . . , hgr,L) 4.3 Task Units Our model consists of two task units: NER unit and RE unit. In NER unit, the objective is to identify and categorize all entity spans in a given sentence. More speciﬁcally, the task is treated as a type-speciﬁc table ﬁlling problem. Given a entity type set E, for each type k, we ﬁll out a table whose element ek ij represents probability of word wi and word wj being start and end position of an entity with type k. For each word pair (wi, wj), we concatenate word-level entity features he i and he j, as well as sentence-level global features hge before feeding it into a fully-connected layer with ELU activation to get entity span representation he ij: ij = ELU(Linear([he he i ; he j; hge])) (9) With the span representation, we can predict whether the span is an entity with type k by feeding it into a feed forward neural layer: ek ij = p(e = ⟨wi, k, wj⟩ ∣e ∈ S) ij)), ∀k ∈ E = σ(Linear(he (10) Lner = ∑ ˆek ij ∈S Lre = ∑ ˆrl ij ∈T BCELoss(ek ij, ˆek ij) BCELoss(rl ij, ˆrl ij) (12) ˆek ij and ˆrl ij are respectively ground truth label of entity table and relation table. ek ij are the predicted ones. We adopt BCELoss for each task3. The training objective is to minimize the loss function L, which is computed as Lner + Lre. ij and rl During inference, we extract relational triples by combining results from both NER and RE unit. For each legitimate triple prediction (sk m,n) where l is the relation label, k and k′ are the entity type labels, and the indexes i, j and m, n are respectively starting and ending index of subject entity s and object entity o, the following conditions should be satisﬁed: i,j, l, ok′ ij ≥ λe; ek′ ek mn ≥ λe; rl im ≥ λr (13) λe and λr are threshold hyper-parameters for entity and relation prediction, both set to be 0.5 without further ﬁne-tuning. 5 Experiment where σ represents sigmoid activation function. Computation in RE unit is mostly symmetrical to NER unit. Given a set of gold relation triples denoted as T , this unit aims to identify all triples in the sentence. We only predict starting word of each entity in this unit as entity span prediction is 5.1 Dataset, Evaluation and Implementation Details We evaluate our model on six datasets. NYT (Riedel et al., 2010), WebNLG (Zeng et al., 2018), 3BCELoss(x, y) = −(ylogx + (1 − y)log(1 − x)). ADE (Gurulingappa et al., 2012), SciERC (Luan et al., 2018), ACE04 and ACE05 (Walker et al., 2006). Descriptions of the datasets can be found in Appendix A. Following previous work, we assess our model on NYT/WebNLG under partial match, where only the tail of an entity is annotated. Besides, as entity type information is not annotated in these datasets, we set the type of all entities to a single label "NONE", so entity type would not be predicted in our model. On ACE05, ACE04, ADE and SciERC, we assess our model under exact match where both head and tail of an entity are annotated. For ADE and ACE04, 10-fold and 5- fold cross validation are used to evaluate the model respectively, and 15% of the training set is used to construct the development set. For evaluation metrics, we report F1 scores in both NER and RE. In NER, an entity is seen as correct only if its type and boundary are correct. In RE, A triple is correct only if the types, boundaries of both entities and their relation type are correct. In addition, we report Macro-F1 score in ADE and Micro-F1 score in other datasets. We choose our model parameters based on the performance in the development set (the best average F1 score of NER and RE) and report the results on the test set. More details of hyper- parameters can be found in Appendix B 5.2 Main Result Table 1 shows the comparison of our model with existing approaches. In partially annotated datasets WebNLG and NYT, under the setting of BERT. For RE, our model achieves 1.7% improvement in WebNLG but performance in NYT is only slightly better than previous SOTA TpLinker (Wang et al., 2020b) by 0.5% margin. We argue that this is because NYT is generated with distant supervision, and annotation for entity and relation are often incomplete and wrong. Compared to TpLinker, the strength of our method is to reinforce two-way interaction between entity and relation. However, when dealing with noisy data, the strength might be counter-productive as error propagation between both tasks is ampliﬁed as well. For NER, our method shows a distinct advantage over baselines that report the ﬁgures. Compared to Casrel (Wei et al., 2020), a competitive method, our F1 scores are 2.3%/2.5% higher in NYT/WebNLG. This proves that exposing relation information to Method NER RE NYT △ CopyRE (Zeng et al., 2018) GraphRel (Fu et al., 2019) CopyRL (Zeng et al., 2019) Casrel (Wei et al., 2020) † TpLinker (Wang et al., 2020b) † PFN† WebNLG △ CopyRE (Zeng et al., 2018) GraphRel (Fu et al., 2019) CopyRL (Zeng et al., 2019) Casrel (Wei et al., 2020) † TpLinker (Wang et al., 2020b) † PFN† - 86.2 58.7 89.2 61.9 72.1 (93.5) 89.6 91.9 95.8 92.4 - - 82.1 37.1 91.9 42.9 61.6 (95.5) 91.8 91.9 98.0 93.6 - ADE ▲ Multi-head (Bekoulis et al., 2018b) 86.4 74.6 Multi-head + AT (Bekoulis et al., 2018a) 86.7 75.5 87.1 77.3 Rel-Metric (Tran and Kavuluru, 2019) SpERT (Eberts and Ulges, 2019) † 89.3 79.2 Table-Sequence (Wang and Lu, 2020) ‡ 89.7 80.1 PFN† 89.6 80.0 PFN‡ 91.3 83.2 ACE05 △ Structured Perceptron (Li and Ji, 2014) SPTree (Miwa and Bansal, 2016) Multi-turn QA (Li et al., 2019) † Table-Sequence (Wang and Lu, 2020) ‡ PURE (Zhong and Chen, 2021) ‡ PFN‡ ACE04 △ Structured Perceptron (Li and Ji, 2014) SPTree (Miwa and Bansal, 2016) Multi-turn QA (Li et al., 2019) † Table-Sequence (Wang and Lu, 2020) ‡ PURE (Zhong and Chen, 2021) ‡ PFN‡ SciERC △ SPE (Wang et al., 2020a) § PURE (Zhong and Chen, 2021) § PFN§ 80.8 49.5 83.4 55.6 84.8 60.2 89.5 64.3 89.7 65.6 89.0 66.8 79.7 45.3 81.8 48.4 83.6 49.4 88.6 59.6 88.8 60.2 89.3 62.5 68.0 34.6 66.6 35.6 66.8 38.4 †, Table 1: Experiment results on six datasets. ‡ and § denotes the use of BERT, ALBERT and SCIBERT(Devlin et al., 2019; Lan et al., 2020; Beltagy et al., 2019) pre-trained embedding. △ and ▲ denotes the use of micro-F1 and macro-F1 score. NER results of Casrel are its reported average score of head and tail entity. Results of PURE are reported in single-sentence setting for fair comparison. NER, which is not present in Casrel, leads to better performance in entity recognition. Furthermore, our model demonstrates strong performance in fully annotated datasets ADE, ACE05, ACE04 and SciERC. For ADE, our model surpasses table-sequence (Wang and Lu, 2020) by 1.6%/3.1% in NER/RE. For ACE05, our model surpasses PURE (Zhong and Chen, 2021) by 1.2% in RE but results in weaker performance in NER by 0.7%. We argue that it could be attributed to the fact that, unlike the former three datasets, ACE05 contains many entities that do not belong to any triple. Thus utilizing relation information for entity prediction might not be as fruitful as that in other datasets (PURE is a pipeline approach where relation information is unseen to entity prediction). In ACE04, our model surpasses PURE by 0.5%/2.3% in NER/RE. In SciERC, our model surpasses PURE by 0.2%/2.8% in NER/RE. Overall, the performance of our model shows remarkable improvement against previous baselines. 5.3 Ablation Study In this section, we take a closer look and check the effectiveness of our framework in relation extrac- tion concerning ﬁve different aspects: number of encoder layer, bidirectional versus unidirectional, encoding scheme, partition granularity and decod- ing strategy. Number of Encoder Layers Similar to recur- rent neural network, we stack our partition ﬁlter encoder with an arbitrary number of layers. Here we only examine frameworks with no more than three layers. As shown in table 2, adding layers to our partition ﬁlter encoder leads to no improvement in F1-score. This shows that one layer is good enough for encoding task-speciﬁc features. Bidirection Vs Unidirection Normally we need two partition ﬁlter encoders (one in reverse or- der) to model interaction between forward and backward context. However, as discussed in section 4.2, our model replaces the backward encoder with a global representation to let future context be visible to each word, achieving a similar effect with bidirectional settings. In order to ﬁnd out which works best, we compare these two methods in our ablation study. From table 2, we ﬁnd that unidirectional encoder with global representation outperforms bidirectional encoder without global representation, showing that global representation is more suitable in providing future context for each word than backward encoder. In addition, when global representation is involved, Ablation Layers Bidirection Vs Unidirection Encoding Scheme Partition Granularity Decoding Strategy Settings N=1 N=2 N=3 Unidirection (w/o gl.) Bidirection (w/o gl.) Joint Sequential Parallel Fine-grained Coarse Universal Selective P 40.6 39.9 40.0 40.6 40.5 40.4 39.9 40.6 40.0 36.0 40.6 39.3 40.6 38.5 R 36.5 35.7 36.2 36.5 34.6 36.2 35.3 36.5 34.2 34.4 36.5 35.5 36.5 36.3 F 38.4 37.7 38.0 38.4 37.3 38.2 37.5 38.4 36.9 35.1 38.4 37.3 38.4 37.4 Table 2: Ablation study on SciERC. P, R and F represent precision, recall and F1 relation scores. The best results are marked in bold. gl. in the second experiment is short for global representation. unidirectional encoder achieves similar result in F1 score compared to bidirectional encoder, indicating that global representation alone is enough in capturing semantics of future context. Encoding Scheme We replace our partition ﬁlter encoder with two LSTM variants to examine the effectiveness of our encoder. In the parallel setting, we use two LSTM encoders to learn task-speciﬁc features separately, and no interaction is allowed except for sharing the same input. In the sequential setting where only one-way interaction is allowed, entity features generated from the ﬁrst LSTM encoder is fed into the second one to produce relation features. From table 2, we observe that our partition ﬁlter outperforms LSTM variants by a large margin, proving the effectiveness of our encoder in modelling two-way interaction over the other two encoding schemes. Partition Granularity Similar to (Shen et al., 2019), we split neurons into several chunks and perform partition within each chunk. Each chunk shares the same entity gate and relation gate. Thus partition results for all chunks remain the same. For example, with a 300-dimension neuron set, if we split it into 10 chunks, each with 30 neurons, only two 30-dimension gates are needed for neuron partition. We refer to the above operation as coarse In contrast, our ﬁne-grained partition partition. can be seen as a special case as neurons are split into only one chunk. We compare our ﬁne-grained partition (chunk size = 300) with coarse partition Dataset ACE05 ACE04 SciERC Entity Type Total In-triple Out-of-triple Diff Total In-triple Out-of-triple Diff Total In-triple Out-of-triple Diff P 89.3 95.9 85.8 10.1 89.1 94.3 87.1 7.2 64.8 78.0 38.9 39.1 R 88.8 92.1 86.9 5.2 89.6 91.2 89.2 3.0 69.0 71.1 61.7 9.4 F 89.0 94.0 86.3 7.7 89.3 92.7 88.1 4.6 66.8 74.4 47.8 26.6 Ratio 1.00 0.36 0.64 - 1.00 0.71 0.29 - 1.00 0.78 0.22 - Table 3: NER Results on different entity types. Entities are split into two groups: In-triple and Out-of-triple based on whether they appear in relational triples or not. Diff is the performance difference between In- triple and Out-of-triple. Ratio is number of entities of given type divided by number of total entities in the test set (train, dev and test set combined in ACE04). Results of ACE04 are averaged over 5-folds (chunk size = 10). Table 2 shows that ﬁne-grained partition performs better than coarse partition. It is not surprising as in coarse partition, the assumption of performing the same neuron partition for each chunk might be too strong for the encoder to separate information for each task properly. Decoding Strategy In pipeline-like methods, relation prediction is performed on entities that the system considers as valid in their entity prediction. We argue that a better way for relation prediction is to take into account all the invalid word pairs. We refer to the former strategy as selective decoding and the latter one as universal decoding. For selective decoding, we only predict the relation scores for entities deemed as valid by their entity scores calculated in the NER unit. Table 2 shows that universal decoding, where all the negative instances are included, is better than selective decoding. Apart from mitigating error propagation, we argue that universal decoding is similar to contrastive learning as negative instances helps to better identify the positive instances through implicit comparison. 6 Effects of Relation Signal on Entity Recognition It is a widely accepted fact that entity recognition helps in predicting relations, but the effect of rela- tion signals on entity prediction remains divergent among researchers. Through two auxiliary experiments, we ﬁnd that the absence of relation signals has a considerable bearing on entity recognition. 6.1 Analysis on Entity Prediction of Different Types In Table 1, NER performance of our model is consistently better than other baselines except for ACE05 where the performance falls short with a non-negligible margin. We argued that it could be attributed to the fact that ACE05 contains many entities that do not belong to any triples. To corroborate our claim, in this section we try to quantify the performance gap of entity prediction between entities that belong to certain triples and those that have no relation with other entities. The former ones are referred to as In-triple entities and the latter as Out-of-triple entities. We split the entities into two groups and test the NER performance of each group in ACE05/ACE04/SciERC. In NYT/WebNLG/ADE, since Out-of-triple entity is non-existent, evaluation is not performed on these datasets. As is shown in table 3, there is a huge gap between In-triple entity prediction and Out-of- triple entity prediction, especially in SciERC where the diff score reaches 26.6%. We argue that it might be attributed to the fact that entity prediction in SciERC is generally harder given that it involves identiﬁcation of scientiﬁc terms and also the average length of entities in SciERC are longer. Another observation is that the diff score is largely attributed to the difference of precision, which means that without guidance from relational signal, our model tends to be over-optimistic about entity prediction. In addition, compared to PURE (Zhong and Chen, 2021) we ﬁnd that the overall performance of NER is negatively correlated with the percentage of out-of-triple entities in the dataset. especially in ACE05, where the performance of our model is relatively weak, over 64% of the entities are Out-of-triple. This phenomenon is a manifest of the weakness in joint model: Joint modeling of NER and RE might be somewhat harmful to entity prediction as the inference patterns of In-triple and Out-of-triple entity are different, considering that the dynamic between relation information and entity prediction is different for In-triple and Out- of-triple entity. Model ConcatSent CrossCategory Average Ori → Aug Decline Ori → Aug Decline Ori → Aug Decline Ori → Aug Decline Ori → Aug Decline Decline 83.0→82.2 BiLSTM-CRF 87.3→86.2 BERT-base(cased) BERT-base(uncased) 88.8→88.7 84.2→83.4 TENER 85.5→85.2 Flair 89.1→87.9 PFN 82.9→43.5 87.4→48.1 88.7→46.0 84.7→39.6 84.6→44.9 89.0→80.5 82.9→64.2 87.4→79.0 88.7→74.6 84.7→51.5 84.6→81.3 89.0→80.4 82.5→73.5 87.5→83.1 89.1→83.0 84.5→76.6 86.1→81.5 89.6→86.9 82.9→67.7 87.4→82.1 88.7→78.5 84.7→31.1 84.6→73.1 89.0→84.3 18.7 8.4 14.1 33.2 3.3 8.6 15.2 5.3 10.2 53.6 11.5 4.7 39.4 39.3 42.7 45.1 39.7 8.5 16.6 11.6 14.6 28.1 11.9 5.1 0.8 1.1 0.1 0.8 0.3 1.2 9.0 4.1 6.1 7.9 4.6 2.7 SwapLonger EntTypos OOV Table 4: Robustness test of NER against input perturbation in ACE05, baseline results and test ﬁles are copied from https://www.textflint.io/ 6.2 Robustness Test on Named Entity Recognition We use robustness test to evaluate our model under adverse circumstances. In this case, we use the domain transformation methods of NER from (Wang et al., 2021). The compared baselines are all relation-free models, including BiLSTM- CRF (Huang et al., 2015), BERT (Devlin et al., 2019), TENER (Yan et al., 2019) and Flair- Embeddings (Akbik et al., 2019). Descriptions of the transformation methods can be found in Appendix D From table 4, we observe that our model is mostly more resilient against input perturbations compared to other baselines, especially in the category of CrossCategory, which is probably attributed to the fact that relation signals used in our training impose type constraints on entities, thus inference of entity types is less affected by the semantic meaning of target entity itself, but rather the (relational) context surrounding the entity. 6.3 Does Relation Signal Helps in Predicting Entities Contrary to what (Zhong and Chen, 2021) has claimed (that relation signal has minimal effects on entity prediction), we ﬁnd several clues that suggest otherwise. First, in section 6.1, we observe that In-triple entities are much more easier to predict than Out-of-triple entities, which suggests that relation signals are useful to entity prediction. Second, in section 6.2, we perform robustness test in NER to evaluate our model’s capability against input perturbation. In the robustness test we compare our method - the only joint model to other relation-free baselines. The result suggests that our method is much more resilient against adverse circumstances, which could be (at least partially) explained by the introduction of relation signals. To sum up, we ﬁnd that relation signals do have non-negligible effect on entity prediction. The reason for (Zhong and Chen, 2021) to conclude that relation information has minimal inﬂuence on entity prediction is most probably due to selective bias, meaning that the evaluated dataset ACE05 contains a large proportion of Out-of-triple entities (64%), which in essence does not require any relation signal themselves. 7 Conclusion In this paper, we encode task-speciﬁc features with our newly proposed model: Partition Filter Net- work in joint entity and relation extraction. Instead of extracting task-speciﬁc features in a sequential or parallel manner, we employ a partition ﬁlter encoder to generate task-speciﬁc features jointly in order to model two-way inter-task interaction properly. We conduct extensive experiments on six datasets to verify the effectiveness of our model. Overall experiment results demonstrate that our model is superior to previous baselines in entity and relation prediction. Furthermore, dissection on several aspects of our model in ablation study sheds some light on what works best in our framework. Lastly, contrary to what previous work has claimed, our auxiliary experiments suggest that relation prediction is contributory to named entity prediction in a non-negligible way. 8 Acknowledgements The authors wish to thank the anonymous reviewers for their helpful comments. This work was partially funded by China National Key R&D Program (No.2018YFB1005104), National Natural Science Foundation of China (No.62076069, 61976056), Shanghai Municipal Science and Technology Major Project (No.2021SHZDZX0103). References Alan Akbik, Tanja Bergmann, Duncan Blythe, Kashif Rasul, Stefan Schweter, and Roland Vollgraf. 2019. FLAIR: An easy-to-use framework for state-of-the- art NLP. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), pages 54–59, Minneapolis, Minnesota. Association for Computational Linguistics. Giannis Bekoulis, Johannes Deleu, Thomas Demeester, and Chris Develder. 2018a. Adversarial training for multi-context joint entity and relation extraction. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2830–2836, Brussels, Belgium. Association for Computational Linguistics. Giannis Bekoulis, Johannes Deleu, Thomas Demeester, and Chris Develder. 2018b. Joint entity recognition and relation extraction as a multi-head selection problem. Expert Systems with Applications, 114:34– 45. Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciBERT: A pretrained language model for scientiﬁc In Proceedings of the 2019 Conference on text. Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3615–3620, Hong Kong, China. Association for Computational Linguistics. Yee Seng Chan and Dan Roth. 2011. Exploiting syntactico-semantic structures for relation extrac- In Proceedings of the 49th Annual Meeting tion. of the Association for Computational Linguistics: Human Language Technologies, pages 551–560, Portland, Oregon, USA. Association for Computa- tional Linguistics. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and BERT: Pre-training Kristina Toutanova. 2019. transformers for language of deep bidirectional the 2019 understanding. Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics. In Proceedings of Dennis Diefenbach, Vanessa Lopez, Kamal Singh, and Pierre Maret. 2018. Core techniques of question answering systems over knowledge bases: a survey. Knowledge and Information systems, 55(3):529– 569. Markus Eberts and Adrian Ulges. 2019. Span-based joint entity and relation extraction with transformer pre-training. arXiv preprint arXiv:1909.07755. Tsu-Jui Fu, Peng-Hsuan Li, and Wei-Yun Ma. 2019. GraphRel: Modeling text as relational graphs for joint entity and relation extraction. In Proceedings the Association of for Computational Linguistics, pages 1409–1418, Florence, Italy. Association for Computational Linguistics. the 57th Annual Meeting of Pankaj Gupta, Hinrich Schütze, and Bernt Andrassy. 2016. Table ﬁlling multi-task recurrent neural network for joint entity and relation extraction. In Proceedings of COLING 2016, the 26th Interna- tional Conference on Computational Linguistics: Technical Papers, pages 2537–2547, Osaka, Japan. The COLING 2016 Organizing Committee. Vishal Gupta and Gurpreet Singh Lehal. 2010. A survey of text summarization extractive techniques. Journal of Emerging Technologies in Web Intelli- gence, 2:258–268. Harsha Gurulingappa, Abdul Mateen Rajput, Angus Roberts, Juliane Fluck, Martin Hofmann-Apitius, and Luca Toldo. 2012. Development of a benchmark corpus to support the automatic extraction of drug- related adverse effects from medical case reports. Journal of biomedical informatics, 45(5):885–892. Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirec- tional lstm-crf models for sequence tagging. arXiv preprint arXiv:1508.01991. Arzoo Katiyar and Claire Cardie. 2017. Going out on a limb: Joint extraction of entity mentions and relations without dependency trees. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 917–928, Vancouver, Canada. Association for Computational Linguistics. Diederik P. Kingma and Jimmy Ba. 2015. Adam: A In 3rd Inter- method for stochastic optimization. national Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings. Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2020. ALBERT: A lite BERT for self-supervised In 8th Inter- learning of language representations. national Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net. Qi Li and Heng Ji. 2014. Incremental joint extraction In Proceedings of entity mentions and relations. of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 402–412, Baltimore, Maryland. Association for Computational Linguistics. Xiaoya Li, Fan Yin, Zijun Sun, Xiayu Li, Arianna Yuan, Duo Chai, Mingxin Zhou, and Jiwei Li. 2019. Entity-relation extraction as multi-turn question an- swering. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1340–1350, Florence, Italy. Association for Computational Linguistics. Yi Luan, Luheng He, Mari Ostendorf, and Hannaneh Multi-task identiﬁcation of relations, and coreference for scientiﬁc In Proceedings Hajishirzi. 2018. entities, knowledge graph construction. of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3219–3232, Brussels, Belgium. Association for Computational Linguistics. Makoto Miwa and Mohit Bansal. 2016. End-to-end relation extraction using LSTMs on sequences and tree structures. In Proceedings of the 54th Annual Meeting of the Association for Computational Lin- guistics (Volume 1: Long Papers), pages 1105–1116, Berlin, Germany. Association for Computational Linguistics. Makoto Miwa and Yutaka Sasaki. 2014. Modeling joint entity and relation extraction with table representation. In Proceedings of the 2014 Confer- ence on Empirical Methods in Natural Language Processing (EMNLP), pages 1858–1869, Doha, Qatar. Association for Computational Linguistics. Sebastian Riedel, Limin Yao, and Andrew McCallum. 2010. Modeling relations and their mentions without labeled text. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 148–163. Springer. Sebastian Riedel, Limin Yao, Andrew McCallum, and Benjamin M. Marlin. 2013. Relation extraction with matrix factorization and universal schemas. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies, pages 74–84, Atlanta, Georgia. Association for Computational Linguistics. Yikang Shen, Shawn Tan, Alessandro Sordoni, and Aaron C. Courville. 2019. Ordered neurons: Integrating tree structures into recurrent neural In 7th International Conference on networks. Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net. Yongliang Shen, Xinyin Ma, Yechun Tang, and Weiming Lu. 2021. A trigger-sense memory ﬂow framework for joint entity and relation extraction. In Proceedings of the Web Conference 2021, pages 1704–1715. Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: a simple way to prevent neural networks from overﬁtting. The journal of machine learning research, 15(1):1929–1958. Tung Tran and Ramakanth Kavuluru. 2019. Neural metric learning for fast end-to-end relation extrac- tion. arXiv preprint arXiv:1905.07458. Christopher Walker, Stephanie Strassel, Julie Medero, and Kazuaki Maeda. 2006. Ace 2005 multilingual Linguistic Data Consortium, training corpus. Philadelphia, 57:45. 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1706–1721, Online. Association for Computational Linguistics. Xiao Wang, Qin Liu, Tao Gui, Qi Zhang, Yicheng Zou, Xin Zhou, Jiacheng Ye, Yongxin Zhang, Rui Zheng, Zexiong Pang, et al. 2021. Textﬂint: Uniﬁed multilingual robustness evaluation toolkit for natural language processing. Yijun Wang, Changzhi Sun, Yuanbin Wu, Junchi Yan, Peng Gao, and Guotong Xie. 2020a. Pre- training entity relation encoder with intra-span and the inter-span information. 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1692–1705, Online. Association for Computational Linguistics. In Proceedings of Yucheng Wang, Bowen Yu, Yueyang Zhang, Tingwen and Limin Sun. 2020b. Liu, Hongsong Zhu, TPLinker: Single-stage joint extraction of entities and relations through token pair In Proceedings of the 28th International Conference on Computational Linguistics, pages 1572–1582, Barcelona, Spain (Online). International Committee on Computational Linguistics. linking. Zhepei Wei, Jianlin Su, Yue Wang, Yuan Tian, and Yi Chang. 2020. A novel cascade binary tagging In framework for relational Proceedings of the Association for Computational Linguistics, pages 1476–1488, Online. Association for Computational Linguistics. the 58th Annual Meeting of triple extraction. Hang Yan, Bocao Deng, Xiaonan Li, and Xipeng Qiu. 2019. Tener: adapting transformer encoder arXiv preprint for named entity recognition. arXiv:1911.04474. Xiaofeng Yu and Wai Lam. 2010. Jointly identifying entities and extracting relations in encyclopedia text In Coling 2010: via a graphical model approach. Posters, pages 1399–1407, Beijing, China. Coling 2010 Organizing Committee. Dmitry Zelenko, Chinatsu Aone, and Anthony Richardella. 2002. Kernel methods for relation extraction. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Process- ing (EMNLP 2002), pages 71–78. Association for Computational Linguistics. Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou, and Jun Zhao. 2014. Relation classiﬁcation via convolutional deep neural network. In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 2335–2344, Dublin, Ireland. Dublin City University and Association for Computational Lin- guistics. Jue Wang and Wei Lu. 2020. Two are better than one: Joint entity and relation extraction with In Proceedings of the table-sequence encoders. Xiangrong Zeng, Shizhu He, Daojian Zeng, Kang Liu, Shengping Liu, and Jun Zhao. 2019. Learning the extraction order of multiple relational facts Dataset NYT WebNLG ADE ACE05 ACE04 SciERC Train 56,195 5,019 10,051 #Sentences Dev 5,000 500 4,272 (10-fold) 2,424 8,683 (5-fold) 275 Test 5,000 703 2,050 551 1,861 ∣E∣ ∣R∣ - - 2 7 7 6 24 170 1 6 6 7 Table 5: Statistics of datasets. ∣E∣ and ∣R∣ are numbers of entity and relation types. In NYT and WebNLG, entity type information is not annotated. learning. In in a sentence with reinforcement Proceedings of the 2019 Conference on Empir- ical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 367–377, Hong Kong, China. Association for Computational Linguistics. Xiangrong Zeng, Daojian Zeng, Shizhu He, Kang Liu, and Jun Zhao. 2018. Extracting relational facts by an end-to-end neural model with copy mechanism. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 506–514, Melbourne, Aus- tralia. Association for Computational Linguistics. Suncong Zheng, Feng Wang, Hongyun Bao, Yuexing Hao, Peng Zhou, and Bo Xu. 2017. Joint extraction of entities and relations based on a novel tagging scheme. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1227–1236, Vancouver, Canada. Association for Computational Linguistics. Zexuan Zhong and Danqi Chen. 2021. A frustratingly easy approach for entity and relation extraction. In North American Association for Computational Linguistics (NAACL). A Dataset We evaluate our model on six datasets. NYT (Riedel et al., 2010) is sampled from New York Times news articles and annotated by distant supervision. WebNLG is originally created for Natural Language Generation task and is applied by (Zeng et al., 2018) as a relation extraction dataset. ACE05 and ACE04 (Walker et al., 2006) are collected from various sources, including news articles and online forums. ADE (Gurulingappa et al., 2012) contains medical descriptions of adverse effects of drug use. SciERC (Luan et al., 2018) is collected from 500 AI paper abstracts originally used for scientiﬁc knowledge graph construction. Following previous work, we ﬁlter out samples containing overlapping entities in ADE, which makes up only 2.8% of the whole dataset. Statistics of the datasets can be found in table 5 B Implementation Details We leverage pre-trained language models as our embedding layer. Following previous work, the versions we use are bert-base-cased, albert- xxlarge-v1 and scibert-scivocab-uncased. Batch size and learning rate are set to be 4/20 and 1e- 5/2e-5 for SciERC/Others respectively. In order to prevent overﬁtting, dropout (Srivastava et al., 2014) is used in our word embedding, entity span and triple representation of task units (set to be 0.1). We use Adam (Kingma and Ba, 2015) to optimize our model parameters and train our model for 100 epochs. Also, to prevent gradient explosion, gradient clipping is applied during training. C Analysis on Overlapping Pattern and Triple Number For more comprehensive evaluation, we assess our model on NYT/WebNLG datasets on dif- ferent triple overlapping patterns (see section 2 for the detailed description of these patterns) and sentences containing a different number of triples. Since previous work does not com- pare triple overlapping pattern and triple number in ADE/ACE05/ACE04/ScIERC given that EPO triples are non-existent in these datasets, compari- son result is not included for these datasets. As is shown in ﬁgure 3, Our model is mostly superior to the other two baselines in all three categories. Interestingly in normal class, our model performs signiﬁcantly better in WebNLG, but the score in NYT is basically on par with TpLinker. We argue that this could probably be caused by the fact that NYT, generated by distant supervision, is much more noisier than WebNLG. Besides, sentences of normal triples are likely to be much noisier than sentences of EPO and SEO triples since there is a higher chance for incomplete annotation. Thus it is unsurprising that no signiﬁcant improvement is achieved in predicting normal triples of NYT. Besides, from ﬁgure 4 we observe that our model performs better in sentences with more than ﬁve triples on both datasets, where interaction between entity and relation becomes very complex. The strong performance in those sentences conﬁrms the (a) NYT (b) WebNLG Figure 3: F1-score of relation triple extraction on sentences with three different overlapping patterns. (a) NYT (b) WebNLG Figure 4: F1-score of relational triple extraction on sentences containing N triples, with N ranges from 1 to ≥5. 4. The methods include inserting descriptions of entities, which is unfair because it might introduce new entity and relation. superiority of our model against other baselines. D Details of Robustness Test Descriptions of the transformation methods used in Table 4 are listed as follows: 1. ConcatSent - Concatenate sentences to a longer one. 2. CrossCategory - Entity Swap by swaping entities with ones that can be labeled by different types. 3. EntTypos - Swap/delete/add random character for entities. 4. OOV - Entity Swap by out-of-vocabulary entities. 5. SwapLonger - Substitute short entities for longer ones. Transformations of RE are not viable for the following reasons: 1. The input sentence. is restricted to one triple per 2. The methods include entity swap, which is already covered in NER. 3. The methods include relation-speciﬁc transfor- mations (Age, Employee, Birth) and ACE05 does not have these type of relations. CasRelTPLinkerPFN86889092949687.390.190.292.094.095.391.493.494.1NormalEPOSEOCasRelTPLinkerPFN86889092949689.487.991.694.795.394.792.292.594.0NormalEPOSEOCasRelTPLinkerPFN82848688909294969888.290.090.590.392.892.991.993.193.794.296.196.383.790.092.6N=1N=2N=3N=4N≥5CasRelTPLinkerPFN82848688909294969889.388.091.390.890.192.494.294.695.692.493.394.790.991.693.3N=1N=2N=3N=4N≥5