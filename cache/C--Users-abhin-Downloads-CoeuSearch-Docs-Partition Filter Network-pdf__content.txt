Partition Filter Network Joint Entity Relation Extraction Zhiheng Yan  , Chong Zhang  , Jinlan Fu ,  , Qi Zhang ‚àó Zhongyu Wei   School Computer Science , Shanghai Key Laboratory Intelligent Information Processing , Fudan University , Shanghai , China  National University Singapore , Singapore  School Data Science , Fudan University , Shanghai , China { zhyan   , chongzhang   , qz , zywei } @ fudan.edu.cn jinlanjonna @ gmail.com Abstract answering ( Diefenbach et al. ,      ) text summarization ( Gupta Lehal ,      ) .         p e      ] L C .  c [   v           .         : v  X r  joint entity relation extraction , existing work either sequentially encode task-speciÔ¨Åc features , leading imbalance inter-task feature interaction features extracted later direct contact come Ô¨Årst . encode entity features relation features parallel manner , meaning feature representation learning task largely independent except input sharing . propose partition Ô¨Ålter network model two-way interaction tasks properly , fea- ture encoding decomposed two steps : partition Ô¨Ålter . encoder , leverage two gates : entity relation gate , segment neurons two task partitions one shared partition . shared partition represents inter-task information valuable tasks evenly shared across two tasks ensure proper two-way interaction . task partitions represent intra-task information formed concerted efforts gates , making sure encoding task- speciÔ¨Åc features dependent upon . Experiment results six public datasets show model performs signiÔ¨Åcantly better previous approaches . addition , con- trary previous work claimed , relation auxiliary experiments suggest prediction contributory named entity prediction non-negligible way . source code found https : //github.com/ Coopercoppers/PFN .   Introduction Joint entity relation extraction intend simultaneously extract entity relation facts given text form relational triples ( , r , ) . extracted information provides supplement many studies , knowledge graph construction ( Riedel et al. ,      ) , question ‚àó‚àó Corresponding author . Figure   : Partition process cell neurons . Entity relation gate used divide neurons task- related task-unrelated ones . Neurons relating tasks form shared partition rest form two task partitions . Conventionally , Named Entity Recognition ( NER ) Relation Extraction ( ) performed pipelined manner ( Zelenko et al. ,      ; Chan Roth ,      ) . approaches Ô¨Çawed consider intimate connection NER . Also , error propagation another drawback pipeline methods .  order conquer issues , joint extracting entity relation proposed demonstrates stronger performance tasks . early work , joint methods mainly rely elaborate feature engineering establish interaction NER ( Yu Lam ,      ; Li Ji ,      ; Miwa Sasaki ,      ) . Recently , end-to-end neural network shown successful extracting relational triples ( Zeng et al. ,      ; Gupta et al. ,      ; Katiyar Cardie ,      ; Shen et al. ,      ) since become mainstream joint entity relation extraction . According differences encoding task- speciÔ¨Åc features , existing methods divided two categories : sequential encoding parallel encoding . sequential task-speciÔ¨Åc features generated encoding , sequentially , means features extracted Ô¨Årst affected extracted later . Zeng et al . (      ) Wei et al . (      ) Cell StateNERREPartition‚úÇ‚úÇRE-SpecificSharedNER-Specific      typical examples category . methods extract features different tasks predeÔ¨Åned order . parallel encoding , task-speciÔ¨Åc features generated independently using shared input . Compared sequential encoding , models build scheme need worry implication encoding order . example , Fu et al . (      ) encodes entity relation information separately using common features derived GCN encoder . Since task- speciÔ¨Åc features extracted isolated sub- modules , approach falls category parallel encoding . However , encoding designs fail model two-way interaction NER tasks properly . sequential encoding , interaction unidirectional speciÔ¨Åed order , result- ing different amount information exposed NER task . parallel encoding , although encoding order longer concern , interaction present input sharing . Considering adding two-way interaction feature encoding , adopt joint encoding . alternative encoding design : design encodes task-speciÔ¨Åc features jointly single encoder exist mutual section inter-task communication . work , instantiate joint encoding partition Ô¨Ålter encoder . encoder Ô¨Årst sorts partitions neuron according contribution individual tasks entity relation gates . process , two task partitions one shared partition formed ( see Ô¨Ågure   ) . individual task partitions shared partition combined generate task-speciÔ¨Åc features , Ô¨Åltering irrelevant information stored opposite task partition . Task interaction encoder achieved two ways : First , partitions , especially task- speciÔ¨Åc ones , formed concerted efforts entity relation gates , allowing interaction formation entity relation features determined partitions . Second , shared partition , represents information useful task , equally accessible formation task-speciÔ¨Åc features , ensuring balanced two-way interaction . contributions work summarized :   . propose partition Ô¨Ålter network , frame- work designed speciÔ¨Åcally joint encoding . method capable encoding task- speciÔ¨Åc features guarantees proper two- way interaction NER .   . conduct extensive experiments six datasets . main results show method superior baseline ap- proaches , ablation study provides in- sight works best framework .   . Contrary previous work claimed , auxiliary experiments suggest relation prediction contributory named entity prediction non-negligible way .   Related Work recent years , joint entity relation extraction approaches focusing tackling triple overlapping problem modelling task interac- tion . Solutions issues explored recent works ( Zheng et al. ,      ; Zeng et al. ,      ,      ; Fu et al. ,      ; Wei et al. ,      ) . triple overlapping problem refers triples sharing entity ( SEO , i.e . SingleEntityOverlap ) entities ( EPO , i.e . EntityPairOverlap ) . example , `` Adam Joe born USA '' , since triples ( Adam , birthplace , USA ) ( Joe , birthplace , USA ) share one entity `` USA '' , categorized SEO triples ; `` Adam born USA lived ever since '' , triples ( Adam , birthplace , USA ) ( Adam , residence , USA ) share entities time , thus categorized EPO triples . Generally , two ways tackling problem . One generative methods like seq seq ( Zeng et al. ,      ,      ) entity relation mentions decoded multiple times output sequence , another modeling relation separately sequences ( Wei et al. ,      ) , graphs ( Fu et al. ,      ) tables ( Wang Lu ,      ) . method uses relation-speciÔ¨Åc tables ( Miwa Sasaki ,      ) handle relation separately . Task interaction modeling , however , well handled previous work . previous approaches , Task interaction achieved entity relation prediction sharing features ( Tran Kavuluru ,      ; Wang et al. ,     b ) . could problematic information entity relation could sometimes contradictory . Also , models use sequential encoding ( Bekoulis et al. ,     b ; Eberts Ulges ,      ; Wei et al. ,      ) parallel encoding ( Fu et al. ,      ) lack proper two-way interaction feature extraction , predictions made features suffer problem improper interaction . work , partition Ô¨Ålter encoder built joint encoding capable handling communication inter-task information appropriately avoid problem sequential parallel encoding ( exposure bias insufÔ¨Åcient interaction ) , keeping intra-task information away opposite task mitigate problem negative transfer tasks .   Problem Formulation framework split joint entity relation ex- traction two sub-tasks : NER . Formally , Given input sequence = { w  , . . . , wL } L tokens , wi denotes i-th token sequence s. NER , aim extract typed entities whose set denoted , ‚ü®wi , e , wj‚ü© ‚àà signiÔ¨Åes token wi wj start end token entity typed e ‚àà E. E represents set entity types . Concerning , goal identify head-only triples whose set denoted , triple ‚ü®wi , r , wj‚ü© ‚àà indicates tokens wi wj corresponding start token subject object entity relation r ‚àà R. R represents set relation types . Combining results NER , able extract relational triples complete entity spans .   Model describe model design section . model consists partition Ô¨Ålter encoder two task units , namely NER unit unit . partition Ô¨Ålter encoder used generate task- speciÔ¨Åc features , sent task units input entity relation prediction . discuss component detail following three sub-sections .  .  Partition Filter Encoder Similar LSTM , partition Ô¨Ålter encoder recurrent feature encoder information stored intermediate memories . time step , encoder Ô¨Årst divides neurons three partitions : entity partition , relation partition shared par- tition . generates task-speciÔ¨Åc features selecting combining partitions , Ô¨Åltering information irrelevant task . shown Ô¨Ågure   , module designed speciÔ¨Åcally jointly extract task-speciÔ¨Åc features , strictly follows two steps : partition Ô¨Ålter . Partition step performs neuron partition divide cell neurons three partitions : Two task partitions storing intra-task information , namely entity partition relation partition , well one shared partition storing inter-task information . neuron divided candidate cell Àúct representing current information previous cell ct‚àí  representing history information . ct‚àí  direct input last time step Àúct calculated manner LSTM : Àúct = tanh ( Linear ( [ xt ; ht‚àí  ] ) ) (   ) Linear stands operation linear transformation . leverage entity gate Àúe relation gate Àúr , referred master gates ( Shen et al. ,      ) , neuron partition . illustrated Ô¨Ågure   , gate , represents one speciÔ¨Åc task , divide neurons two segments according usefulness designated task . example , entity gate Àúe separate neurons two partitions : NER-related NER-unrelated . shared partition formed combining partition results gates . Neurons shared partition regarded information valuable tasks . order model two- way interaction properly , inter-task information shared partition evenly accessible tasks ( discussed Ô¨Ålter subsection ) . addition , information valuable one task invisible opposing task stored individual task partitions . gates calculated using cummax activation   , function cummax ( ‚ãÖ ) = cumsum ( sof tmax ( ‚ãÖ ) ) whose output seen approximation binary gate form (   , . . . ,   ,   , . . . ,   ) : Àúe = cummax ( Linear ( [ xt ; ht‚àí  ] ) ) Àúr =   ‚àí cummax ( Linear ( [ xt ; ht‚àí  ] ) ) (   ) intuition behind equation (   ) identify two cut-off points , displayed scissors Ô¨Ågure   , naturally divide set neurons three segments . result , gates divide neurons three partitions , entity partition œÅe , relation partition œÅr shared partition œÅs . Partitions  cumsum ( x  , x  , . . . , xn‚àí  , xn ) = ( x  , x  + x  , . . . , x  + x  + ‚ãÖ ‚ãÖ ‚ãÖ + xn‚àí  , x  + x  + ‚ãÖ ‚ãÖ ‚ãÖ + xn‚àí  + xn ) . ( ) Framework Partition Filter Network ( b ) Inner Mechanism Partition Filter Figure   : ( ) Overview PFN . framework consists three components : partition Ô¨Ålter encoder , NER unit unit . task units , use table-Ô¨Ålling word pair prediction . Orange , yellow green represents NER-related , shared RE-related component features . ( b ) Detailed depiction partition Ô¨Ålter encoder one single time step . decompose feature encoding two steps : partition Ô¨Ålter ( shown gray area ) . partition , Ô¨Årst segment neurons two task partitions one shared partition . Ô¨Ålter , partitions selected combined form task-speciÔ¨Åc features shared features , Ô¨Åltering information irrelevant task . previous cell ct‚àí  formulated :   œÅs , ct‚àí  = Àúect‚àí  ‚óã Àúrct‚àí  œÅe , ct‚àí  = Àúect‚àí  ‚àí œÅs , ct‚àí  œÅr , ct‚àí  = Àúrct‚àí  ‚àí œÅs , ct‚àí  (   ) Note add three partitions , result equal one . guarantees forward message passing , information discarded ensure message overloaded , similar forgetting mechanism LSTM . , aggregate partition information target cells , three partitions formed result . three partitions , add related information cells : œÅe = œÅe , ct‚àí  ‚óã ct‚àí  + œÅe , Àúct ‚óã Àúct œÅr = œÅr , ct‚àí  ‚óã ct‚àí  + œÅr , Àúct ‚óã Àúct œÅs = œÅs , ct‚àí  ‚óã ct‚àí  + œÅs , Àúct ‚óã Àúct (   ) Filter propose three types memory block : entity memory , relation memory shared mem- ory . denote ¬µe entity memory , ¬µr  The calculation candidate cell Àúct identical equation (   ) therefore shown . practically relation memory ¬µs shared memory . ¬µe , information entity partition shared partition selected . contrast , information relation partition , assume irrelevant even harmful named entity recognition task , Ô¨Åltered . logic applies ¬µr well , information entity partition Ô¨Åltered rest kept . addition , information shared partition stored ¬µs : ¬µe = œÅe + œÅs ; ¬µr = œÅr + œÅs ; ¬µs = œÅs (   ) Note inter-task information shared partition accessible entity memory relation memory , allowing balanced interaction NER . Whereas sequential parallel encoding , relation features direct impact formation entity features . updating information memory , entity features , relation features hr shared features hs generated corresponding memories : = tanh ( ¬µe ) hr = tanh ( ¬µr ) hs = tanh ( ¬µs ) Following partition Ô¨Ålter steps , information (   ) ‚Ä¶‚Ä¶PartitionFilterPartitionFilterPartitionFilterPartitionFilterPartitionFilterùë• $ ùë• % ùë• & ùë• ‚Äô ùë• ( GlobalFeatureNER-specificFeatureNERUnitsREUnits‚Ä¶‚Ä¶SharedFeatureRE-specificFeatureÃÉùëü ‚Äô % & ‚Äô ÃÉùëí ‚Äô % & ‚Äô ‚úÇ‚úÇùúå $ ùëê ! '' # ùúå % ùúå & ùúá $ ùúá % ùúá & ‚Ñé $ ‚Ñé % ‚Ñé & ùëê ! ‚Ñé ! '' # ÃÉùëê ! ùë• ! PartitionFilter‚úÇ‚úÇÃÉùëüÃÉ ‚Äô % ÃÉùëíÃÉ ‚Äô % ‚Ñé ! three memories used form cell state ct , used generate hidden state ht ( hidden cell state time step input next time step ) : ct = Linear ( [ ¬µe , ; ¬µr , ; ¬µs , ] ) ht = tanh ( ct ) (   )  .  Global Representation model , employ unidirectional encoder feature encoding . backward encoder bidirectional setting replaced task-speciÔ¨Åc global representation capture semantics future context . Empirically shows effective . task , global representation combination task-speciÔ¨Åc features shared features computed : already covered NER unit . Similar NER , consider relation extraction relation-speciÔ¨Åc table Ô¨Ålling problem . Given relation label set R , relation l ‚àà R , Ô¨Åll table whose element rl ij represents probability word wi word wj starting word subject object entity . way , extract triples revolving around relation l one relation table . triple ( wi , l , wj ) , similar NER unit , triple representation hr ij calculated follows : ij relation score rl ; hr ij = ELU ( Linear ( [ hr hr rl ij = p ( r = ‚ü®wi , l , wj‚ü© ‚à£r ‚àà ) ij ) ) , ‚àÄl ‚àà R = œÉ ( Linear ( hr j ; hgr ] ) ) (    )  .  Training Inference given training dataset , loss function L guides model training consists two parts : Lner NER unit Lre unit : (   ) hge , = tanh ( Linear [ , ; hs , ] ) hgr , = tanh ( Linear [ hr , ; hs , ] ) hge = maxpool ( hge,  , . . . , hge , L ) hgr = maxpool ( hgr,  , . . . , hgr , L )  .  Task Units model consists two task units : NER unit unit . NER unit , objective identify categorize entity spans given sentence . speciÔ¨Åcally , task treated type-speciÔ¨Åc table Ô¨Ålling problem . Given entity type set E , type k , Ô¨Åll table whose element ek ij represents probability word wi word wj start end position entity type k. word pair ( wi , wj ) , concatenate word-level entity features  j , well sentence-level global features hge feeding fully-connected layer ELU activation get entity span representation ij : ij = ELU ( Linear ( [  ; j ; hge ] ) ) (   ) span representation , predict whether span entity type k feeding feed forward neural layer : ek ij = p ( e = ‚ü®wi , k , wj‚ü© ‚à£e ‚àà ) ij ) ) , ‚àÄk ‚àà E = œÉ ( Linear ( (    ) Lner = ‚àë ÀÜek ij ‚ààS Lre = ‚àë ÀÜrl ij ‚ààT BCELoss ( ek ij , ÀÜek ij ) BCELoss ( rl ij , ÀÜrl ij ) (    ) ÀÜek ij ÀÜrl ij respectively ground truth label entity table relation table . ek ij predicted ones . adopt BCELoss task  . training objective minimize loss function L , computed Lner + Lre . ij rl inference , extract relational triples combining results NER unit . legitimate triple prediction ( sk , n ) l relation label , k k‚Ä≤ entity type labels , indexes , j , n respectively starting ending index subject entity object entity , following conditions satisÔ¨Åed : , j , l , ok‚Ä≤ ij ‚â• Œªe ; ek‚Ä≤ ek mn ‚â• Œªe ; rl im ‚â• Œªr (    ) Œªe Œªr threshold hyper-parameters entity relation prediction , set  .  without Ô¨Åne-tuning .   Experiment œÉ represents sigmoid activation function . Computation unit mostly symmetrical NER unit . Given set gold relation triples denoted , unit aims identify triples sentence . predict starting word entity unit entity span prediction  .  Dataset , Evaluation Implementation Details evaluate model six datasets . NYT ( Riedel et al. ,      ) , WebNLG ( Zeng et al. ,      ) ,  BCELoss ( x , ) = ‚àí ( ylogx + (   ‚àí ) log (   ‚àí x ) ) . ADE ( Gurulingappa et al. ,      ) , SciERC ( Luan et al. ,      ) , ACE   ACE   ( Walker et al. ,      ) . Descriptions datasets found Appendix . Following previous work , assess model NYT/WebNLG partial match , tail entity annotated . Besides , entity type information annotated datasets , set type entities single label `` NONE '' , entity type would predicted model . ACE   , ACE   , ADE SciERC , assess model exact match head tail entity annotated . ADE ACE   ,   -fold  - fold cross validation used evaluate model respectively ,    % training set used construct development set . evaluation metrics , report F  scores NER . NER , entity seen correct type boundary correct . , triple correct types , boundaries entities relation type correct . addition , report Macro-F  score ADE Micro-F  score datasets . choose model parameters based performance development set ( best average F  score NER ) report results test set . details hyper- parameters found Appendix B  .  Main Result Table   shows comparison model existing approaches . partially annotated datasets WebNLG NYT , setting BERT . , model achieves  .  % improvement WebNLG performance NYT slightly better previous SOTA TpLinker ( Wang et al. ,     b )  .  % margin . argue NYT generated distant supervision , annotation entity relation often incomplete wrong . Compared TpLinker , strength method reinforce two-way interaction entity relation . However , dealing noisy data , strength might counter-productive error propagation tasks ampliÔ¨Åed well . NER , method shows distinct advantage baselines report Ô¨Ågures . Compared Casrel ( Wei et al. ,      ) , competitive method , F  scores  .  % / .  % higher NYT/WebNLG . proves exposing relation information Method NER NYT ‚ñ≥ CopyRE ( Zeng et al. ,      ) GraphRel ( Fu et al. ,      ) CopyRL ( Zeng et al. ,      ) Casrel ( Wei et al. ,      ) ‚Ä† TpLinker ( Wang et al. ,     b ) ‚Ä† PFN‚Ä† WebNLG ‚ñ≥ CopyRE ( Zeng et al. ,      ) GraphRel ( Fu et al. ,      ) CopyRL ( Zeng et al. ,      ) Casrel ( Wei et al. ,      ) ‚Ä† TpLinker ( Wang et al. ,     b ) ‚Ä† PFN‚Ä† -   .    .    .    .    .  (   .  )   .    .    .    .  - -   .    .    .    .    .  (   .  )   .    .    .    .  - ADE ‚ñ≤ Multi-head ( Bekoulis et al. ,     b )   .    .  Multi-head + ( Bekoulis et al. ,     a )   .    .    .    .  Rel-Metric ( Tran Kavuluru ,      ) SpERT ( Eberts Ulges ,      ) ‚Ä†   .    .  Table-Sequence ( Wang Lu ,      ) ‚Ä°   .    .  PFN‚Ä†   .    .  PFN‚Ä°   .    .  ACE   ‚ñ≥ Structured Perceptron ( Li Ji ,      ) SPTree ( Miwa Bansal ,      ) Multi-turn QA ( Li et al. ,      ) ‚Ä† Table-Sequence ( Wang Lu ,      ) ‚Ä° PURE ( Zhong Chen ,      ) ‚Ä° PFN‚Ä° ACE   ‚ñ≥ Structured Perceptron ( Li Ji ,      ) SPTree ( Miwa Bansal ,      ) Multi-turn QA ( Li et al. ,      ) ‚Ä† Table-Sequence ( Wang Lu ,      ) ‚Ä° PURE ( Zhong Chen ,      ) ‚Ä° PFN‚Ä° SciERC ‚ñ≥ SPE ( Wang et al. ,     a ) ¬ß PURE ( Zhong Chen ,      ) ¬ß PFN¬ß   .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .  ‚Ä† , Table   : Experiment results six datasets . ‚Ä° ¬ß denotes use BERT , ALBERT SCIBERT ( Devlin et al. ,      ; Lan et al. ,      ; Beltagy et al. ,      ) pre-trained embedding . ‚ñ≥ ‚ñ≤ denotes use micro-F  macro-F  score . NER results Casrel reported average score head tail entity . Results PURE reported single-sentence setting fair comparison . NER , present Casrel , leads better performance entity recognition . Furthermore , model demonstrates strong performance fully annotated datasets ADE , ACE   , ACE   SciERC . ADE , model surpasses table-sequence ( Wang Lu ,      )  .  % / .  % NER/RE . ACE   , model surpasses PURE ( Zhong Chen ,      )  .  % results weaker performance NER  .  % . argue could attributed fact , unlike former three datasets , ACE   contains many entities belong triple . Thus utilizing relation information entity prediction might fruitful datasets ( PURE pipeline approach relation information unseen entity prediction ) . ACE   , model surpasses PURE  .  % / .  % NER/RE . SciERC , model surpasses PURE  .  % / .  % NER/RE . Overall , performance model shows remarkable improvement previous baselines .  .  Ablation Study section , take closer look check effectiveness framework relation extrac- tion concerning Ô¨Åve different aspects : number encoder layer , bidirectional versus unidirectional , encoding scheme , partition granularity decod- ing strategy . Number Encoder Layers Similar recur- rent neural network , stack partition Ô¨Ålter encoder arbitrary number layers . examine frameworks three layers . shown table   , adding layers partition Ô¨Ålter encoder leads improvement F -score . shows one layer good enough encoding task-speciÔ¨Åc features . Bidirection Vs Unidirection Normally need two partition Ô¨Ålter encoders ( one reverse or- der ) model interaction forward backward context . However , discussed section  .  , model replaces backward encoder global representation let future context visible word , achieving similar effect bidirectional settings . order Ô¨Ånd works best , compare two methods ablation study . table   , Ô¨Ånd unidirectional encoder global representation outperforms bidirectional encoder without global representation , showing global representation suitable providing future context word backward encoder . addition , global representation involved , Ablation Layers Bidirection Vs Unidirection Encoding Scheme Partition Granularity Decoding Strategy Settings N=  N=  N=  Unidirection ( w/o gl . ) Bidirection ( w/o gl . ) Joint Sequential Parallel Fine-grained Coarse Universal Selective P   .    .    .    .    .    .    .    .    .    .    .    .    .    .  R   .    .    .    .    .    .    .    .    .    .    .    .    .    .  F   .    .    .    .    .    .    .    .    .    .    .    .    .    .  Table   : Ablation study SciERC . P , R F represent precision , recall F  relation scores . best results marked bold . gl . second experiment short global representation . unidirectional encoder achieves similar result F  score compared bidirectional encoder , indicating global representation alone enough capturing semantics future context . Encoding Scheme replace partition Ô¨Ålter encoder two LSTM variants examine effectiveness encoder . parallel setting , use two LSTM encoders learn task-speciÔ¨Åc features separately , interaction allowed except sharing input . sequential setting one-way interaction allowed , entity features generated Ô¨Årst LSTM encoder fed second one produce relation features . table   , observe partition Ô¨Ålter outperforms LSTM variants large margin , proving effectiveness encoder modelling two-way interaction two encoding schemes . Partition Granularity Similar ( Shen et al. ,      ) , split neurons several chunks perform partition within chunk . chunk shares entity gate relation gate . Thus partition results chunks remain . example ,    -dimension neuron set , split    chunks ,    neurons , two   -dimension gates needed neuron partition . refer operation coarse contrast , Ô¨Åne-grained partition partition . seen special case neurons split one chunk . compare Ô¨Åne-grained partition ( chunk size =     ) coarse partition Dataset ACE   ACE   SciERC Entity Type Total In-triple Out-of-triple Diff Total In-triple Out-of-triple Diff Total In-triple Out-of-triple Diff P   .    .    .    .    .    .    .   .    .    .    .    .  R   .    .    .   .    .    .    .   .    .    .    .   .  F   .    .    .   .    .    .    .   .    .    .    .    .  Ratio  .    .    .   -  .    .    .   -  .    .    .   - Table   : NER Results different entity types . Entities split two groups : In-triple Out-of-triple based whether appear relational triples . Diff performance difference In- triple Out-of-triple . Ratio number entities given type divided number total entities test set ( train , dev test set combined ACE   ) . Results ACE   averaged  -folds ( chunk size =    ) . Table   shows Ô¨Åne-grained partition performs better coarse partition . surprising coarse partition , assumption performing neuron partition chunk might strong encoder separate information task properly . Decoding Strategy pipeline-like methods , relation prediction performed entities system considers valid entity prediction . argue better way relation prediction take account invalid word pairs . refer former strategy selective decoding latter one universal decoding . selective decoding , predict relation scores entities deemed valid entity scores calculated NER unit . Table   shows universal decoding , negative instances included , better selective decoding . Apart mitigating error propagation , argue universal decoding similar contrastive learning negative instances helps better identify positive instances implicit comparison .   Effects Relation Signal Entity Recognition widely accepted fact entity recognition helps predicting relations , effect rela- tion signals entity prediction remains divergent among researchers . two auxiliary experiments , Ô¨Ånd absence relation signals considerable bearing entity recognition .  .  Analysis Entity Prediction Different Types Table   , NER performance model consistently better baselines except ACE   performance falls short non-negligible margin . argued could attributed fact ACE   contains many entities belong triples . corroborate claim , section try quantify performance gap entity prediction entities belong certain triples relation entities . former ones referred In-triple entities latter Out-of-triple entities . split entities two groups test NER performance group ACE  /ACE  /SciERC . NYT/WebNLG/ADE , since Out-of-triple entity non-existent , evaluation performed datasets . shown table   , huge gap In-triple entity prediction Out-of- triple entity prediction , especially SciERC diff score reaches   .  % . argue might attributed fact entity prediction SciERC generally harder given involves identiÔ¨Åcation scientiÔ¨Åc terms also average length entities SciERC longer . Another observation diff score largely attributed difference precision , means without guidance relational signal , model tends over-optimistic entity prediction . addition , compared PURE ( Zhong Chen ,      ) Ô¨Ånd overall performance NER negatively correlated percentage out-of-triple entities dataset . especially ACE   , performance model relatively weak ,    % entities Out-of-triple . phenomenon manifest weakness joint model : Joint modeling NER might somewhat harmful entity prediction inference patterns In-triple Out-of-triple entity different , considering dynamic relation information entity prediction different In-triple Out- of-triple entity . Model ConcatSent CrossCategory Average Ori ‚Üí Aug Decline Ori ‚Üí Aug Decline Ori ‚Üí Aug Decline Ori ‚Üí Aug Decline Ori ‚Üí Aug Decline Decline   . ‚Üí  .  BiLSTM-CRF   . ‚Üí  .  BERT-base ( cased ) BERT-base ( uncased )   . ‚Üí  .    . ‚Üí  .  TENER   . ‚Üí  .  Flair   . ‚Üí  .  PFN   . ‚Üí  .    . ‚Üí  .    . ‚Üí  .    . ‚Üí  .    . ‚Üí  .    . ‚Üí  .    . ‚Üí  .    . ‚Üí  .    . ‚Üí  .    . ‚Üí  .    . ‚Üí  .    . ‚Üí  .    . ‚Üí  .    . ‚Üí  .    . ‚Üí  .    . ‚Üí  .    . ‚Üí  .    . ‚Üí  .    . ‚Üí  .    . ‚Üí  .    . ‚Üí  .    . ‚Üí  .    . ‚Üí  .    . ‚Üí  .    .   .    .    .   .   .    .   .    .    .    .   .    .    .    .    .    .   .    .    .    .    .    .   .   .   .   .   .   .   .   .   .   .   .   .   .  SwapLonger EntTypos OOV Table   : Robustness test NER input perturbation ACE   , baseline results test Ô¨Åles copied https : //www.textflint.io/  .  Robustness Test Named Entity Recognition use robustness test evaluate model adverse circumstances . case , use domain transformation methods NER ( Wang et al. ,      ) . compared baselines relation-free models , including BiLSTM- CRF ( Huang et al. ,      ) , BERT ( Devlin et al. ,      ) , TENER ( Yan et al. ,      ) Flair- Embeddings ( Akbik et al. ,      ) . Descriptions transformation methods found Appendix table   , observe model mostly resilient input perturbations compared baselines , especially category CrossCategory , probably attributed fact relation signals used training impose type constraints entities , thus inference entity types less affected semantic meaning target entity , rather ( relational ) context surrounding entity .  .  Relation Signal Helps Predicting Entities Contrary ( Zhong Chen ,      ) claimed ( relation signal minimal effects entity prediction ) , Ô¨Ånd several clues suggest otherwise . First , section  .  , observe In-triple entities much easier predict Out-of-triple entities , suggests relation signals useful entity prediction . Second , section  .  , perform robustness test NER evaluate model ‚Äô capability input perturbation . robustness test compare method - joint model relation-free baselines . result suggests method much resilient adverse circumstances , could ( least partially ) explained introduction relation signals . sum , Ô¨Ånd relation signals non-negligible effect entity prediction . reason ( Zhong Chen ,      ) conclude relation information minimal inÔ¨Çuence entity prediction probably due selective bias , meaning evaluated dataset ACE   contains large proportion Out-of-triple entities (    % ) , essence require relation signal .   Conclusion paper , encode task-speciÔ¨Åc features newly proposed model : Partition Filter Net- work joint entity relation extraction . Instead extracting task-speciÔ¨Åc features sequential parallel manner , employ partition Ô¨Ålter encoder generate task-speciÔ¨Åc features jointly order model two-way inter-task interaction properly . conduct extensive experiments six datasets verify effectiveness model . Overall experiment results demonstrate model superior previous baselines entity relation prediction . Furthermore , dissection several aspects model ablation study sheds light works best framework . Lastly , contrary previous work claimed , auxiliary experiments suggest relation prediction contributory named entity prediction non-negligible way .   Acknowledgements authors wish thank anonymous reviewers helpful comments . work partially funded China National Key R & Program ( No.    YFB        ) , National Natural Science Foundation China ( No.         ,          ) , Shanghai Municipal Science Technology Major Project ( No.    SHZDZX     ) . References Alan Akbik , Tanja Bergmann , Duncan Blythe , Kashif Rasul , Stefan Schweter , Roland Vollgraf .      . FLAIR : easy-to-use framework state-of-the- art NLP . Proceedings      Conference North American Chapter Association Computational Linguistics ( Demonstrations ) , pages   ‚Äì   , Minneapolis , Minnesota . Association Computational Linguistics . Giannis Bekoulis , Johannes Deleu , Thomas Demeester , Chris Develder .     a . Adversarial training multi-context joint entity relation extraction . Proceedings      Conference Empirical Methods Natural Language Processing , pages     ‚Äì     , Brussels , Belgium . Association Computational Linguistics . Giannis Bekoulis , Johannes Deleu , Thomas Demeester , Chris Develder .     b . Joint entity recognition relation extraction multi-head selection problem . Expert Systems Applications ,    :  ‚Äì    . Iz Beltagy , Kyle Lo , Arman Cohan .      . SciBERT : pretrained language model scientiÔ¨Åc Proceedings      Conference text . Empirical Methods Natural Language Processing  th International Joint Conference Natural Language Processing ( EMNLP-IJCNLP ) , pages     ‚Äì     , Hong Kong , China . Association Computational Linguistics . Yee Seng Chan Dan Roth .      . Exploiting syntactico-semantic structures relation extrac- Proceedings   th Annual Meeting tion . Association Computational Linguistics : Human Language Technologies , pages    ‚Äì    , Portland , Oregon , USA . Association Computa- tional Linguistics . Jacob Devlin , Ming-Wei Chang , Kenton Lee , BERT : Pre-training Kristina Toutanova .      . transformers language deep bidirectional      understanding . Conference North American Chapter Association Computational Linguistics : Human Language Technologies , Volume   ( Long Short Papers ) , pages     ‚Äì     , Minneapolis , Minnesota . Association Computational Linguistics . Proceedings Dennis Diefenbach , Vanessa Lopez , Kamal Singh , Pierre Maret .      . Core techniques question answering systems knowledge bases : survey . Knowledge Information systems ,    (   ) :   ‚Äì     . Markus Eberts Adrian Ulges .      . Span-based joint entity relation extraction transformer pre-training . arXiv preprint arXiv:    .      . Tsu-Jui Fu , Peng-Hsuan Li , Wei-Yun .      . GraphRel : Modeling text relational graphs joint entity relation extraction . Proceedings Association  Computational Linguistics , pages     ‚Äì     , Florence , Italy . Association Computational Linguistics .   th Annual Meeting Pankaj Gupta , Hinrich Sch√ºtze , Bernt Andrassy .      . Table Ô¨Ålling multi-task recurrent neural network joint entity relation extraction . Proceedings COLING      ,   th Interna- tional Conference Computational Linguistics : Technical Papers , pages     ‚Äì     , Osaka , Japan . COLING      Organizing Committee . Vishal Gupta Gurpreet Singh Lehal .      . survey text summarization extractive techniques . Journal Emerging Technologies Web Intelli- gence ,  :   ‚Äì    . Harsha Gurulingappa , Abdul Mateen Rajput , Angus Roberts , Juliane Fluck , Martin Hofmann-Apitius , Luca Toldo .      . Development benchmark corpus support automatic extraction drug- related adverse effects medical case reports . Journal biomedical informatics ,    (   ) :   ‚Äì    . Zhiheng Huang , Wei Xu , Kai Yu .      . Bidirec- tional lstm-crf models sequence tagging . arXiv preprint arXiv:    .      . Arzoo Katiyar Claire Cardie .      . Going limb : Joint extraction entity mentions relations without dependency trees . Proceedings   th Annual Meeting Association Computational Linguistics ( Volume   : Long Papers ) , pages    ‚Äì    , Vancouver , Canada . Association Computational Linguistics . Diederik P. Kingma Jimmy Ba .      . Adam :  rd Inter- method stochastic optimization . national Conference Learning Representations , ICLR      , San Diego , CA , USA , May  -  ,      , Conference Track Proceedings . Zhenzhong Lan , Mingda Chen , Sebastian Goodman , Kevin Gimpel , Piyush Sharma , Radu Soricut .      . ALBERT : lite BERT self-supervised  th Inter- learning language representations . national Conference Learning Representations , ICLR      , Addis Ababa , Ethiopia , April   -   ,      . OpenReview.net . Qi Li Heng Ji .      . Incremental joint extraction Proceedings entity mentions relations .   nd Annual Meeting Association Computational Linguistics ( Volume   : Long Papers ) , pages    ‚Äì    , Baltimore , Maryland . Association Computational Linguistics . Xiaoya Li , Fan Yin , Zijun Sun , Xiayu Li , Arianna Yuan , Duo Chai , Mingxin Zhou , Jiwei Li .      . Entity-relation extraction multi-turn question an- swering . Proceedings   th Annual Meeting Association Computational Linguistics , pages     ‚Äì     , Florence , Italy . Association Computational Linguistics . Yi Luan , Luheng , Mari Ostendorf , Hannaneh Multi-task identiÔ¨Åcation relations , coreference scientiÔ¨Åc Proceedings Hajishirzi .      . entities , knowledge graph construction .      Conference Empirical Methods Natural Language Processing , pages     ‚Äì     , Brussels , Belgium . Association Computational Linguistics . Makoto Miwa Mohit Bansal .      . End-to-end relation extraction using LSTMs sequences tree structures . Proceedings   th Annual Meeting Association Computational Lin- guistics ( Volume   : Long Papers ) , pages     ‚Äì     , Berlin , Germany . Association Computational Linguistics . Makoto Miwa Yutaka Sasaki .      . Modeling joint entity relation extraction table representation . Proceedings      Confer- ence Empirical Methods Natural Language Processing ( EMNLP ) , pages     ‚Äì     , Doha , Qatar . Association Computational Linguistics . Sebastian Riedel , Limin Yao , Andrew McCallum .      . Modeling relations mentions without labeled text . Joint European Conference Machine Learning Knowledge Discovery Databases , pages    ‚Äì    . Springer . Sebastian Riedel , Limin Yao , Andrew McCallum , Benjamin M. Marlin .      . Relation extraction matrix factorization universal schemas . Proceedings      Conference North American Chapter Association Computa- tional Linguistics : Human Language Technologies , pages   ‚Äì   , Atlanta , Georgia . Association Computational Linguistics . Yikang Shen , Shawn Tan , Alessandro Sordoni , Aaron C. Courville .      . Ordered neurons : Integrating tree structures recurrent neural  th International Conference networks . Learning Representations , ICLR      , New Orleans , LA , USA , May  -  ,      . OpenReview.net . Yongliang Shen , Xinyin , Yechun Tang , Weiming Lu .      . trigger-sense memory Ô¨Çow framework joint entity relation extraction . Proceedings Web Conference      , pages     ‚Äì     . Nitish Srivastava , Geoffrey Hinton , Alex Krizhevsky , Ilya Sutskever , Ruslan Salakhutdinov .      . Dropout : simple way prevent neural networks overÔ¨Åtting . journal machine learning research ,    (   ) :    ‚Äì     . Tung Tran Ramakanth Kavuluru .      . Neural metric learning fast end-to-end relation extrac- tion . arXiv preprint arXiv:    .      . Christopher Walker , Stephanie Strassel , Julie Medero , Kazuaki Maeda .      . Ace      multilingual Linguistic Data Consortium , training corpus . Philadelphia ,   :   .      Conference Empirical Methods Natural Language Processing ( EMNLP ) , pages     ‚Äì     , Online . Association Computational Linguistics . Xiao Wang , Qin Liu , Tao Gui , Qi Zhang , Yicheng Zou , Xin Zhou , Jiacheng Ye , Yongxin Zhang , Rui Zheng , Zexiong Pang , et al .      . TextÔ¨Çint : UniÔ¨Åed multilingual robustness evaluation toolkit natural language processing . Yijun Wang , Changzhi Sun , Yuanbin Wu , Junchi Yan , Peng Gao , Guotong Xie .     a . Pre- training entity relation encoder intra-span  inter-span information .      Conference Empirical Methods Natural Language Processing ( EMNLP ) , pages     ‚Äì     , Online . Association Computational Linguistics . Proceedings Yucheng Wang , Bowen Yu , Yueyang Zhang , Tingwen Limin Sun .     b . Liu , Hongsong Zhu , TPLinker : Single-stage joint extraction entities relations token pair  Proceedings   th International Conference Computational Linguistics , pages     ‚Äì     , Barcelona , Spain ( Online ) . International Committee Computational Linguistics . linking . Zhepei Wei , Jianlin Su , Yue Wang , Yuan Tian , Yi Chang .      . novel cascade binary tagging  framework relational Proceedings  Association Computational Linguistics , pages     ‚Äì     , Online . Association Computational Linguistics .   th Annual Meeting triple extraction . Hang Yan , Bocao Deng , Xiaonan Li , Xipeng Qiu .      . Tener : adapting transformer encoder arXiv preprint named entity recognition . arXiv:    .      . Xiaofeng Yu Wai Lam .      . Jointly identifying entities extracting relations encyclopedia text Coling      : via graphical model approach . Posters , pages     ‚Äì     , Beijing , China . Coling      Organizing Committee . Dmitry Zelenko , Chinatsu Aone , Anthony Richardella .      . Kernel methods relation extraction . Proceedings      Conference Empirical Methods Natural Language Process- ing ( EMNLP      ) , pages   ‚Äì   . Association Computational Linguistics . Daojian Zeng , Kang Liu , Siwei Lai , Guangyou Zhou , Jun Zhao .      . Relation classiÔ¨Åcation via convolutional deep neural network . Proceedings COLING      ,   th International Conference Computational Linguistics : Technical Papers , pages     ‚Äì     , Dublin , Ireland . Dublin City University Association Computational Lin- guistics . Jue Wang Wei Lu .      . Two better one : Joint entity relation extraction Proceedings table-sequence encoders . Xiangrong Zeng , Shizhu , Daojian Zeng , Kang Liu , Shengping Liu , Jun Zhao .      . Learning extraction order multiple relational facts Dataset NYT WebNLG ADE ACE   ACE   SciERC Train   ,     ,      ,    # Sentences Dev  ,         ,    (   -fold )  ,     ,    (  -fold )     Test  ,         ,         ,    ‚à£E‚à£ ‚à£R‚à£ - -                        Table   : Statistics datasets . ‚à£E‚à£ ‚à£R‚à£ numbers entity relation types . NYT WebNLG , entity type information annotated . learning .  sentence reinforcement Proceedings      Conference Empir- ical Methods Natural Language Processing  th International Joint Conference Natural Language Processing ( EMNLP-IJCNLP ) , pages    ‚Äì    , Hong Kong , China . Association Computational Linguistics . Xiangrong Zeng , Daojian Zeng , Shizhu , Kang Liu , Jun Zhao .      . Extracting relational facts end-to-end neural model copy mechanism . Proceedings   th Annual Meeting Association Computational Linguistics ( Volume   : Long Papers ) , pages    ‚Äì    , Melbourne , Aus- tralia . Association Computational Linguistics . Suncong Zheng , Feng Wang , Hongyun Bao , Yuexing Hao , Peng Zhou , Bo Xu .      . Joint extraction entities relations based novel tagging scheme . Proceedings   th Annual Meeting Association Computational Linguistics ( Volume   : Long Papers ) , pages     ‚Äì     , Vancouver , Canada . Association Computational Linguistics . Zexuan Zhong Danqi Chen .      . frustratingly easy approach entity relation extraction . North American Association Computational Linguistics ( NAACL ) . Dataset evaluate model six datasets . NYT ( Riedel et al. ,      ) sampled New York Times news articles annotated distant supervision . WebNLG originally created Natural Language Generation task applied ( Zeng et al. ,      ) relation extraction dataset . ACE   ACE   ( Walker et al. ,      ) collected various sources , including news articles online forums . ADE ( Gurulingappa et al. ,      ) contains medical descriptions adverse effects drug use . SciERC ( Luan et al. ,      ) collected     AI paper abstracts originally used scientiÔ¨Åc knowledge graph construction . Following previous work , Ô¨Ålter samples containing overlapping entities ADE , makes  .  % whole dataset . Statistics datasets found table   B Implementation Details leverage pre-trained language models embedding layer . Following previous work , versions use bert-base-cased , albert- xxlarge-v  scibert-scivocab-uncased . Batch size learning rate set  /    e-  / e-  SciERC/Others respectively . order prevent overÔ¨Åtting , dropout ( Srivastava et al. ,      ) used word embedding , entity span triple representation task units ( set  .  ) . use Adam ( Kingma Ba ,      ) optimize model parameters train model     epochs . Also , prevent gradient explosion , gradient clipping applied training . C Analysis Overlapping Pattern Triple Number comprehensive evaluation , assess model NYT/WebNLG datasets dif- ferent triple overlapping patterns ( see section   detailed description patterns ) sentences containing different number triples . Since previous work com- pare triple overlapping pattern triple number ADE/ACE  /ACE  /ScIERC given EPO triples non-existent datasets , compari- son result included datasets . shown Ô¨Ågure   , model mostly superior two baselines three categories . Interestingly normal class , model performs signiÔ¨Åcantly better WebNLG , score NYT basically par TpLinker . argue could probably caused fact NYT , generated distant supervision , much noisier WebNLG . Besides , sentences normal triples likely much noisier sentences EPO SEO triples since higher chance incomplete annotation . Thus unsurprising signiÔ¨Åcant improvement achieved predicting normal triples NYT . Besides , Ô¨Ågure   observe model performs better sentences Ô¨Åve triples datasets , interaction entity relation becomes complex . strong performance sentences conÔ¨Årms ( ) NYT ( b ) WebNLG Figure   : F -score relation triple extraction sentences three different overlapping patterns . ( ) NYT ( b ) WebNLG Figure   : F -score relational triple extraction sentences containing N triples , N ranges   ‚â•  .   . methods include inserting descriptions entities , unfair might introduce new entity relation . superiority model baselines . Details Robustness Test Descriptions transformation methods used Table   listed follows :   . ConcatSent - Concatenate sentences longer one .   . CrossCategory - Entity Swap swaping entities ones labeled different types .   . EntTypos - Swap/delete/add random character entities .   . OOV - Entity Swap out-of-vocabulary entities .   . SwapLonger - Substitute short entities longer ones . Transformations viable following reasons :   . input sentence . restricted one triple per   . methods include entity swap , already covered NER .   . methods include relation-speciÔ¨Åc transfor- mations ( Age , Employee , Birth ) ACE   type relations . CasRelTPLinkerPFN              .   .   .   .   .   .   .   .   . NormalEPOSEOCasRelTPLinkerPFN              .   .   .   .   .   .   .   .   . NormalEPOSEOCasRelTPLinkerPFN                    .   .   .   .   .   .   .   .   .   .   .   .   .   .   . N= N= N= N= N‚â• CasRelTPLinkerPFN                    .   .   .   .   .   .   .   .   .   .   .   .   .   .   . N= N= N= N= N‚â• 