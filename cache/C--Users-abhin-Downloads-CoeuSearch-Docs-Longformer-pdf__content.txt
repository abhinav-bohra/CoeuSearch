Longformer : Long-Document Transformer Arman Cohan∗ Iz Beltagy∗ Allen Institute Artiﬁcial Intelligence , Seattle , WA , USA { beltagy , matthewp , armanc } @ allenai.org Matthew E. Peters∗         c e    ] L C .  c [   v           .         : v  X r  Abstract Transformer-based models unable pro- cess long sequences due self-attention operation , scales quadratically sequence length . address limitation , introduce Longformer attention mechanism scales linearly sequence length , making easy process documents thousands tokens longer . Longformer ’ attention mechanism drop-in replacement standard self-attention combines local windowed attention task moti- vated global attention . Following prior work long-sequence transformers , evaluate Longformer character-level language mod- eling achieve state-of-the-art results text  enwik  . contrast prior work , also pretrain Longformer ﬁnetune variety downstream tasks . pretrained Longformer consistently out- performs RoBERTa long document tasks sets new state-of-the-art results Wiki- Hop TriviaQA . ﬁnally introduce Longformer-Encoder-Decoder ( LED ) , Long- former variant supporting long document generative sequence-to-sequence tasks , demonstrate effectiveness arXiv sum- marization dataset.    Introduction Transformers ( Vaswani et al. ,      ) achieved state-of-the-art results wide range natu- ral language tasks including generative language modeling ( Dai et al. ,      ; Radford et al. ,      ) discriminative language understanding ( De- vlin et al. ,      ) . success partly due self-attention component enables net- work capture contextual information entire sequence . powerful , memory computational requirements self-attention grow ∗ Equal contribution .  https : //github.com/allenai/longformer full Runtime memory self- Figure   : attention different implementations Long- former ’ self-attention ; Longformer-loop non- vectorized , Longformer-chunk vectorized , Longformer-cuda custom cuda kernel im- plementations . Longformer ’ memory usage scales linearly sequence length , unlike full self-attention mechanism runs memory Different long sequences current GPUs . implementations vary speed , vectorized Longformer-chunk fastest . details section  .  . quadratically sequence length , making infea- sible ( expensive ) process long sequences . address limitation , present Long- former , modiﬁed Transformer architecture self-attention operation scales linearly sequence length , making versatile pro- cessing long documents ( Fig   ) . advan- tage natural language tasks long docu- ment classiﬁcation , question answering ( QA ) , coreference resolution , existing approaches partition shorten long context smaller sequences fall within typical     token limit BERT-style pretrained models . parti- tioning could potentially result loss important cross-partition information , mitigate problem , existing methods often rely complex architectures address interactions . hand , proposed Longformer able build contextual representations entire con- text using multiple layers attention , reducing       need task-speciﬁc architectures . Model Recent work addressed computational in- efﬁciency Transformers long sequences ( see Tab .   ) . However , primarily focus autore- gressive language modeling ( LM ) , appli- cation long document transformers document- level NLP tasks transfer learning setting ( Dai Le ,      ; Peters et al. ,      ; Howard Ruder ,      ; Devlin et al. ,      ) remained largely unexplored . address gap show Longformer ’ attention mechanism act drop-in replacement self-attention mecha- nism pretrained Transformers , leads gains across suite document NLP tasks . Longformer ’ attention mechanism combina- tion windowed local-context self-attention end task motivated global attention encodes inductive bias task . ablations controlled trials show attention types essential – local attention primarily used build contextual representations , global attention allows Longformer build full sequence representations prediction . ﬁrst evaluate Longformer autoregressive character-level language modeling using com- bination windowed new dilated attention pattern , allowing model process sequences   K characters modern GPUs . achieve state-of-the-art results text  enwik  benchmark datasets , demonstrating effective- ness Longformer long document modeling . , evaluate Longformer ’ ability re- place full self-attention operation existing pretrained models , pretrain masked language modeling ( MLM ) objective , continuing RoBERTa ( Liu et al. ,      ) released checkpoint . pretraining , apply downstream language tasks ﬁnetuning demonstrate Longformer consistently outper- forms RoBERTa wide range document-level natural language tasks including text classiﬁcation , QA , coreference resolution , achieving state-of- the-art results two datasets . ﬁnally introduce variant Longformer instead encoder-only Transformer architecture , follows encoder-decoder ar- chitecture similar original Transformer model ( Vaswani et al. ,      ) , in- tended sequence-to-sequence ( seq seq ) learn- ing ( Sutskever et al. ,      ) . call model Longformer-Encoder-Decoder ( LED ) uses   attention char-LM pretrain matrix tasks Transformer-XL (      ) Adaptive Span (      ) Compressive (      ) Reformer (      ) Sparse (      ) Routing (      ) BP-Transformer (      ) Blockwise (      ) Longformer ltr ltr ltr sparse sparse sparse sparse sparse sparse yes yes yes yes yes yes yes  yes multiple       MT QA        yes yes Table   : Summary prior work adapting Trans- formers long documents . ltr : left-to-right . Longformer ’ efﬁcient attention pattern en- coder network , allowing address long docu- ment seq seq tasks summarization . demonstrate effectiveness LED arXiv summarization dataset ( Cohan et al. ,      ) .   Related Work Long-Document Transformers Tab .   summa- rizes recent prior work long documents . Two types self-attention approaches ex- plored . ﬁrst left-to-right ( ltr ) approach processes document chunks moving left-to-right . models success- ful autoregressive language modeling , unsuitable transfer learning approaches tasks beneﬁt bidirectional context . work falls within general approach deﬁnes form sparse attention pattern avoids computing full quadratic attention matrix multiplication . model similar attention pattern Sparse Trans- former ( Child et al. ,      ) , uses form dilated sliding window blocks size  x  pro- vided BlockSparse ( Gray et al. ,      ) . implementation ( §  ) also includes custom CUDA kernel , ﬂexible maintainable BlockSparse implemented C++ , designed speciﬁc version TensorFlow . also introduce additional task motivated global at- tention patterns suitable common NLP tasks ( §  ) show essential good perfor- mance transfer learning setting . models tried tasks autoregres- sive language modeling , step forward arguably focusing language modeling primary evaluation led develop- ment models limited applicability . BP- Transformer ( Ye et al. ,      ) evaluated machine ( ) Full n  attention ( b ) Sliding window attention ( c ) Dilated sliding window ( ) Global+sliding window Figure   : Comparing full self-attention pattern conﬁguration attention patterns Longformer . translation ( MT ) , ’ explore pretrain- ﬁnetune setting . Blockwise attention ( Qiu et al. ,      ) pretrained models evaluated question answering ( QA ) . However , evaluation limited ’ include language modeling , QA datasets relatively short docu- ments,  therefore effectiveness model long document tasks remains unexplored . Task-speciﬁc Models Long Documents Many task-speciﬁc approaches devel- oped workaround     limit pretrained transformer models like BERT . simplest ap- proach truncates document , commonly used classiﬁcation ( Xie et al. ,      ) . An- approach chunks document chunks length     ( could overlapping ) , processes chunk separately , combines activa- tions task speciﬁc model ( Joshi et al. ,      ) . third approach popular multihop open domain QA tasks uses two-stage model ﬁrst stage retrieves relevant documents passed onto second stage answer extrac- tion ( Clark Gardner ,      ; Chen et al. ,      ) . approaches suffer information loss due truncation cascading errors two stage approach . contrast , Longformer process long sequences without truncating chunking , allowing us adopt much simpler ap- proach concatenates available context processes single pass . contemporaneous works  explored similar ideas Longformer using local + global attention Transformers , pre-training long document natural language tasks . particu- lar , ETC ( Ainslie et al. ,      ) uses similar local + global attention instead full self-attention scale Transformers long documents . Different Longformer , ETC uses relative position em-  SQuAD contexts typically ﬁt within     limit , MRQA constructed dropping long-document examples .  All published arXiv Longformer . beddings ( used Autoregres- sive LM setting ) , introduces additional training objective ( CPC loss ) pre-training , conﬁg- ures global attention slightly different way . shows strong results several tasks including reading comprehension classiﬁcation . GMAT ( Gupta Berant ,      ) uses similar idea global locations input serving global memory . BigBird ( Zaheer et al. ,      ) exten- sion ETC evaluation additional tasks , including summarization . Importantly , the- oretical analysis , BigBird shows sparse Trans- formers universal approximators sequence functions preserve properties full self-attention .   Longformer original Transformer model self-attention component ( n  ) time memory complex- ity n input sequence length . address challenge , sparsify full self-attention matrix according “ attention pattern ” specify- ing pairs input locations attending one another . Unlike full self-attention , proposed atten- tion pattern scales linearly input sequence , making efﬁcient longer sequences . sec- tion discusses design implementation attention pattern .  .  Attention Pattern Sliding Window Given importance local context ( Kovaleva et al. ,      ) , attention pat- tern employs ﬁxed-size window attention sur- rounding token . Using multiple stacked lay- ers windowed attention results large receptive ﬁeld , top layers access input locations capacity build repre- sentations incorporate information across entire input , similar CNNs ( Wu et al. ,      ) . Given ﬁxed window size w , token attends     w tokens side ( Fig .  b ) . com- putation complexity pattern ( n × w ) ,   scales linearly input sequence length n . transformer ( cid:   ) layers , receptive ﬁeld size top layer ( cid:   ) × w ( assuming w ﬁxed layers ) . Depending application , might helpful use different values w layer balance efﬁciency model representation capacity ( § .  ) . Dilated Sliding Window increase receptive ﬁeld without increasing computation , sliding window “ dilated ” . analogous dilated CNNs ( van den Oord et al. ,      ) window gaps size dilation ( Fig .  c ) . Assuming ﬁxed w layers , recep- tive ﬁeld ( cid:   ) × × w , reach tens thousands tokens even small values . multi-headed attention , attention head computes different attention score . found set- tings different dilation conﬁgurations per head improves performance allowing heads without dilation focus local context , others dilation focus longer context . Global Attention state-of-the-art BERT-style models natural language tasks , optimal in- put representation differs language modeling varies task . masked language modeling ( MLM ) , model uses local context predict masked word , classiﬁcation , model ag- gregates representation whole sequence special token ( [ CLS ] case BERT ) . QA , question document concatenated , allowing model compare question document self-attention . case , windowed dilated attention ﬂexible enough learn task-speciﬁc repre- sentations . Accordingly , add “ global attention ” pre-selected input locations . Importantly , make attention operation symmetric : , token global attention attends tokens across sequence , tokens sequence attend . Fig .  d shows example sliding window attention global attention tokens custom locations . example classiﬁcation , global attention used [ CLS ] token QA global attention pro- vided question tokens . Since number tokens small relative independent n complexity combined local global attention still ( n ) . specifying global attention task speciﬁc , easy way add in- ductive bias model ’ attention , much simpler existing task speciﬁc approaches use complex architecture combine information across smaller input chunks . Linear Projections Global Attention Re- call given linear projections Q , K , V , Transformer model ( Vaswani et al. ,      ) computes attention scores follows : Attention ( Q , K , V ) = softmax ( cid:   ) ( cid:   ) QKT √ dk V (   ) use two sets projections , Qs , Ks , Vs com- pute attention scores sliding window attention , Qg , Kg , Vg compute attention scores global attention . additional projections provide ﬂexibility model different types attention , show critical best performance downstream tasks . Qg , Kg , Vg initialized values match Qs , Ks , Vs .  .  Implementation regular transformers , attention scores com- puted Eqn .   . expensive operation matrix multiplication QKT Q K n ( sequence length ) projections . Longformer , dilated sliding window attention computes ﬁxed number diagonals QKT . shown Fig .   , results linear increase memory usage compared quadratic increase full self-attention . However , imple- menting requires form banded matrix mul- tiplication supported existing deep learning libraries like PyTorch/Tensorﬂow . Fig .   compares performance three different ways implementing : loop memory efﬁcient Py- Torch implementation supports dilation unusably slow used testing ; chunks supports non-dilated case used pretraining/ﬁnetuning setting ; cuda fully functioning highly optimized custom CUDA kernel implemented using TVM ( Chen et al. ,      ) used language modeling experiments ( see Appendix details ) .   Autoregressive Language Modeling Autoregressive left-to-right language modeling loosely deﬁned estimating probability dis- tribution existing token/character given previous tokens/characters input sequence . task considered one fundamental tasks natural language recent prior work mod- eling long sequences using transformers relied   task primary evaluation ( Dai et al. ,      ; Rae et al. ,      ; Sukhbaatar et al. ,      ) . Similarly , develop evaluate model autoregressive language modeling .  .  Attention Pattern autoregressive language modeling use dilated sliding window attention . Follow- ing Sukhbaatar et al . (      ) use differing win- dow sizes across layers . particular , use small window sizes lower layers in- crease window sizes move higher layers . allows top layers learn higher-level rep- resentation entire sequence lower layers capture local information . addition , provides balance efﬁciency ( smaller win- dow sizes less computationally expensive due fewer nonzero values ) performance ( larger window sizes richer representation power often result performance improvements ) . use dilated sliding windows lower layers maximize capacity learn uti- lize immediate local context . higher layers , use small amount increasing dila- tion   heads . gives model ability directly attend distant tokens without sacriﬁcing local context .  .  Experiment Setup compare prior work focus character- level LM ( text  enwik  ; Mahoney ,      ) . Training Ideally , would like train model largest window size sequence length ﬁt modern GPU memory . How- ever , found model needs large number gradient updates learn local context ﬁrst , learning utilize longer context . accom- modate , adopt staged training procedure increase attention window size sequence length across multiple training phases . particular , ﬁrst phase start short sequence length window size , sub- sequent phase , double window size sequence length , halve learning rate . makes training fast , keeping slow part ( longest sequences window sizes ) end . train model   total phases start- ing sequence length  ,    ending sequence length   ,    last phase ( see Appendix B detailed conﬁgurations phase , hyperparameters ) . Model # Param Dev Test Dataset text  T   ( Al-Rfou et al. ,      ) Adaptive ( Sukhbaatar et al. ,      ) BP-Transformer ( Ye et al. ,      ) Longformer Dataset enwik  T   ( Al-Rfou et al. ,      ) Transformer-XL ( Dai et al. ,      ) Reformer ( Kitaev et al. ,      ) Adaptive ( Sukhbaatar et al. ,      ) BP-Transformer ( Ye et al. ,      ) Longformer   M -   M  .     M -   M  .     M   M - - - -   M  .     M -   M  .    .    .    .    .    .    .    .    .    .    .   Table   : Small model BPC text  & enwik  Model # Param Test BPC Transformer-XL (    layers ) Sparse ( Child et al. ,      ) Transformer-XL (    layers ) Adaptive ( Sukhbaatar et al. ,      ) Compressive ( Rae et al. ,      ) Routing ( Roy et al. ,      ) Longformer   M ≈   M    M    M    M ≈   M    M  .    .    .    .    .    .    .   Table   : Performance large models enwik  Evaluation evaluate sequences length   ,    . Following Dai et al . (      ) , split dataset overlapping sequences size   ,    step size     , report per- formance last     tokens sequence .  . .  Results Tab .     summarize evaluation results text  enwik  datasets . achieve new state-of-the-art text  enwik  using small models BPC  .    .   text  enwik  respectively , demonstrating effectiveness model . large models , given expensive experiments , following recent work ( Ki- taev et al. ,      ; Rae et al. ,      ) , evaluating enwik  . Tab .   shows Long- former outperforms comparable Transformer- XL model , matches performance compa- rable Sparse Transformer ( Child et al. ,      ) , matches slightly underperforms recent models twice number parameters . worth noting Adaptive Span ( Sukhbaatar et al. ,      ) Compressive Transformer ( Rae et al. ,      ) good ﬁt pretraining- ﬁnetuning paradigm discussed §  .   Model Dev BPC Model Decreasing w (        ) Fixed w ( =     ) Increasing w (        ) Dilation Dilation   heads  .    .    .    .    .   RoBERTa ( seqlen :     ) Longformer ( seqlen :  ,    ) + copy position embeddings +  K gradient updates +   K gradient updates Longformer ( train extra pos . embed . ) base large  .      .     .     .     .     .     .     .     .     .     .     .    Table   : Top : changing window size across layers . Bot- tom : with/without dilation ( @    K steps phase  ) Table   : MLM BPC RoBERTa various pre- trained Longformer conﬁgurations .  . .  Ablation Study show importance design choices attention patterns , tried different variants report controlled experiment results . make ablation study manageable , train conﬁguration    K steps  phase   conﬁguration small model text  , report BPC performance dev set . top Tab .   demonstrates impact different ways conﬁguring window sizes per layer . observe increasing window size bottom top layer leads best performance , arranging reverse way leads worse performance , using ﬁxed window size ( average window sizes conﬁguration ) leads performance . bottom Tab .   shows impact adding dilation . Adding dilation two heads leads improvement compared dilation .   Pretraining Finetuning Current state-of-the-art systems many NLP tasks ﬁnetune pretrained model task super- vision ( e.g . BERT ) . One main motivations develop model suitable long docu- ment tasks . , pretrained Longformer document corpus ﬁnetune six tasks , including classiﬁcation , QA coreference resolu- tion . resulting model process sequences  ,    tokens long (   times longer BERT )   . pretrain Longformer masked language modeling ( MLM ) , goal recover randomly masked tokens sequence . Since MLM pretraining expensive , continue pre- training RoBERTa ( Liu et al. ,      ) re- leased checkpoint , making minimal changes necessary support Longformer ’ atten- tion mechanism . Note attention pattern plugged pretrained transformer model without need change model architecture . Attention Pattern use sliding window atten- tion window size     , therefore using amount computation RoBERTa.  Position Embeddings RoBERTa uses learned absolute position embeddings maximum position     . support longer documents , add extra position embeddings support position  ,    . leverage RoBERTa ’ pretrained weights , instead randomly initializing new position embeddings , initialize copying     position embeddings RoBERTa mul- tiple times analysis BERT ’ attention heads shows strong learned bias attending local context , including previous next token ( Clark et al. ,      ) . Using copy initialization preserves local structure everywhere except parti- tion boundaries . Despite simplicity , found effective ( see Tab .   ) , allowing Longformer pretraining rapidly converge small number gradient updates . Continued MLM Pretraining pretrain Longformer using fairseq ( Ott et al. ,      ) corpus long documents compiled ( see Appendix C corpus details ) . train two model sizes , base model large model . models trained   K gradient updates sequences length  ,    , batch size    (     tokens ) , maximum learning rate  e-  , linear warmup     steps , followed power   polynomial decay . rest hyperparameters RoBERTa . Tab .   shows BPC development set training corpus . ﬁrst row shows  .     One caveat ordering end performance agree step    K . However , approximation saves huge cost running every experiment completion .  Sequences   K possible current GPUs .  Adding dilation heads § .  hurt perfor- mance , likely compatible pretrained RoBERTa weights . Retraining model scratch might needed improve performance .   Wordpieces WH TQA HQA IMDB HY nate available context single sequence . avg .   th pctl .      ,     ,     ,     ,      ,     ,     ,                 ,    Table   : Average   th percentile context length datasets wordpieces . WH : WikiHop , TQA : Triv- iaQA , HQA : HotpotQA , : OntoNotes , HY : Hyper- partisan news BPC using RoBERTa-base , comparable  .    BPC reported RoBERTa paper corpus . indicates training corpus distribution close used train RoBERTa . following two rows show per- formance Longformer pretraining randomly initialized position embeddings copied position embeddings . signiﬁcant differ- ence indicates importance copy initial- ization , relative small difference RoBERTa BPC initialized BPC indi- cates sliding window attention working well RoBERTa weights . following two rows show impact continuing pretrain- ing . Traininig  K steps improves BPC  .     .    , decreases  .    af- ter   K steps , demonstrating model learning better utilize sliding window attention longer context . Similar patterns observed RoBERTa-large Longformer-large . Frozen RoBERTa Weights also pretrained Longformer freezing RoBERTa weights , training new position embeddings . motivation conﬁguration perfectly preserve RoBERTa performance short doc- uments . conﬁguration BPC  .    (  .    initialization ) , higher  .    weights trainable .   Tasks apply Longformer multiple long document tasks , including QA , coreference resolution classiﬁcation . Tab .   shows evaluation datasets contexts signiﬁcantly longer     word- pieces . primary goal evaluate whether attention mechanism act replace- ment standard self-attention mechanism BERT style models , perform controlled tri- als strong baseline . also interested evaluating whether replace complicated task speciﬁc models necessitated BERT ’ lim- ited context simpler models concate- baseline RoBERTa based model breaks context longest possible seg- ment , passes individually RoBERTa , concatenates activations process- ing . QA tasks , also concatenate question segment RoBERTa condition ’ contextual representations context question . Longformer variant replaces RoBERTa self-attention mechanism win- dowed attention used pretraining , plus task motivated global attention . global attention uses additional linear projections ( § .  ) .  .  Question answering used three datasets : WikiHop ( Welbl et al. ,      ) , TriviaQA ( Joshi et al. ,      , Wikipedia set- ting ) , HotpotQA , ( Yang et al. ,      , distractor setting ) .  WikiHop TriviaQA follow sim- ple QA model BERT ( Devlin et al. ,      ) , concatenate question documents one long sequence , run Longformer , dataset-speciﬁc prediction layer . WikiHop uses classiﬁcation layer candidate Trivi- aQA uses loss function Clark Gardner (      ) predict answer span . include global attention question tokens answer candidates WikiHop question tokens TriviaQA . HotpotQA multihop QA dataset involves extracting answer spans evidence sentences    Wikipedia paragraphs ,   rele- vant rest distractors . use two-stage model ﬁrst selects relevant paragraphs passes second stage answer ex- traction . stages concatenate question con- text one sequence , run Longformer , use task-speciﬁc prediction layers . train models multi-task way predict relevant paragraphs , evidence sentences , answer spans question types ( yes/no/span ) jointly . Note model simpler recent SOTA models in- clude complex task-speciﬁc architectures ( e.g. , ( Tu et al. ,      ; Chen et al. ,      ; Tu et al. ,      ; Groeneveld et al. ,      ) ) . See Appendix fur- ther details models hyperparameters .  .  Coreference Resolution use OntoNotes ( Pradhan et al. ,      ) , model Joshi et al . (      ) , modiﬁcation  We use full version TriviaQA HotpotQA , simpliﬁed versions MRQA ( Fisch et al. ,      ) .   QA Coref . Classiﬁcation Model WikiHop TriviaQA HotpotQA OntoNotes IMDB Hyperpartisan RoBERTa-base Longformer-base   .    .    .    .    .    .    .    .    .    .    .    .  Table   : Summary ﬁnetuning results QA , coreference resolution , document classiﬁcation . Results development sets comparing Longformer-base RoBERTa-base . TriviaQA , Hyperpartisan metrics F  , WikiHop IMDB use accuracy , HotpotQA joint F  , OntoNotes average F  . system Lee et al . (      ) replace ELMo BERT . Longformer system straightfor- ward adaption baseline model replacing RoBERTa Longformer extending se- quence length . ’ use global attention task .  .  Document Classiﬁcation evaluate IMDB ( Maas et al. ,      ) Hy- perpartisan news detection ( Kiesel et al. ,      ) datasets.  IMDB standard sentiment classiﬁca- tion datasets consisting movie reviews . documents dataset short ,   .  % larger     wordpieces ( Tab .   ) . Documents Hyperpartisan relatively long , small     documents mak- ing good test Longformer ’ ability adapt limited data . use global attention [ CLS ] token .  .  Results Main Result Tab .   summarizes results ﬁnetuning experiments . observe Long- former consistently outperforms RoBERTa baseline . performance gain especially ob- vious tasks require long context WikiHop Hyperpartisan . TriviaQA , improvement modest local context often sufﬁcient answer question . case HotpotQA , supporting fact auxiliary supervision allows models easily ﬁnd relevant contexts focus local context , leading smaller gains . contrasted WikiHop includes distant supervision intermediate reasoning chains , approach excels reasoning entire context . IMDB OntoNotes datasets performance gains smaller . IMDB , majority dataset consists short documents thus expected see smaller improvements . OntoNotes ,  For Hyperpartisan split training data   /  /   train/dev/test sets , report mean F  across ﬁve seeds . Model Current∗ SOTA Longformer-large WikiHop TriviaQA HotpotQA   .    .    .    .    .    .  Table   : Leaderboard results Longformer-large time submission ( May      ) . numbers F  scores . found distance two mentions typically quite small baseline pro- cesses smaller chunks separately able stitch together mentions coreference chains without considering cross chunk interactions . Longformer-large QA also evaluate performance Longformer-large long context QA tasks . Tab .   shows Longformer-large achieves new state-of-the-art results  WikiHop TriviaQA large margins (  .    points respectively ) , HotpotQA , underperforms current state-of-the-art ( Fang et al. ,      ) point . Tab .   shows detailed results Hot- potQA compared published unpublished concurrent models . Longformer places second published leaderboard , outperforming published results except HGN ( Fang et al. ,      ) . published top performing models task ( Tu et al. ,      ; Fang et al. ,      ; Shao et al. ,      ) use GNNs ( Kipf Welling ,      ) graph network entities , seem encode important inductive bias task po- tentially improve results . Nevertheless , Longformer performs strongly outperforming methods including recent non-GNN meth- ods ( Glaß et al. ,      ; Shao et al. ,      ; Groen- eveld et al. ,      ) .   Model ans . supp . joint   Longformer-Encoder-Decoder ( LED )   .  TAP   ( ensemble ) ( Glaß et al. ,      ) SAE ( Tu et al. ,      )   .  Quark ( dev ) ( Groeneveld et al. ,      )   .    .  C F Reader ( Shao et al. ,      ) Longformer-large ETC-large† ( Ainslie et al. ,      ) GSAN-large† HGN-large ( Fang et al. ,      )   .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .  Table   : HotpotQA results distractor setting test set . Quark ’ test results available . numbers F  scores . † shows contemporaneous leaderboard sub- missions . Model Longformer ( seqlen :  ,    ) Accuracy / ∆   .    .  / - .  RoBERTa-base ( seqlen :     )   .  / + .  Longformer ( seqlen :  ,    ,    epochs ) Longformer ( seqlen :     , attention : n  )   .  / - .    .  / - .  Longformer ( seqlen :  ,    )   .  / - .  Longformer ( MLM pretraining )   .  / - .  Longformer ( linear proj . ) Longformer ( linear proj . global atten . )   .  / - .  Longformer ( pretrain extra position embed . )   .  / - .  Table    : WikiHop development set ablations  .  Ablations WikiHop Tab .    presents ablation study WikiHop development set . results use Longformer- base , ﬁne-tuned ﬁve epochs identical hy- perparameters except noted . Longformer beneﬁts longer sequences , global attention , separate projection matrices global attention , MLM pretraining , longer training . addition , conﬁgured RoBERTa-base ( seqlen :     , n  attention ) Longformer performs slightly worse RoBERTa-base , conﬁrming per- formance gains due additional pretrain- ing . Performance drops slightly using RoBERTa model pretrained unfreezing additional position embeddings , showing Longformer learn use long range context task speciﬁc ﬁne-tuning large training datasets WikiHop .  At submission time , May      . Later , BigBird ( Zaheer et al. ,      ) improved leaderboard results datasets . confounding factors using   X com- pute BigBird ’ pretraining compared Longformer , po- tentially affecting performance . original Transformer ( Vaswani et al. ,      ) consisted encoder-decoder architecture , in- tended sequence-to-sequence tasks ( Sutskever et al. ,      ) , summarization transla- tion . encoder-only Transformers effec- tive variety NLP tasks , pre-trained encoder- decoder Transformer models ( e.g . BART ( Lewis et al. ,      ) T  ( Raffel et al. ,      ) ) achieved strong results tasks like summariza- tion . Yet , models ’ efﬁciently scale seq seq tasks longer inputs . facilitate modeling long sequences seq seq learning , propose Longformer variant encoder decoder Transformer stacks instead full self-attention encoder , uses efﬁcient local+global attention pattern Longformer . decoder uses full self-attention entire encoded tokens previously decoded locations . call model Longformer-Encoder-Decoder ( LED ) scales linearly input . Since pre-training LED expensive , initialize LED parameters BART , follow BART ’ exact architecture terms number layers hidden sizes . difference process longer inputs , extend position embedding   K tokens ( BART ’  K tokens ) initialize new position embedding matrix repeatedly copying BART ’  K position embeddings    times Section   RoBERTa . Following BART , re- lease two model sizes , LED-base LED-large , respectively      layers encoder decoder stacks . evaluate LED summarization task us- ing arXiv summarization dataset ( Cohan et al. ,      ) focuses long document summariza- tion scientiﬁc domain .   th percentile document lengths   . K tokens , making appropriate testbed evaluating LED . LED ’ encoder reads document decoder gener- ates output summary . encoder uses local attention window size  ,    tokens global attention ﬁrst < > token . decoder uses full attention entire encoder previously decoded locations . standard seq seq models , LED trained using teacher forcing gold train- ing summaries uses beam search inference . Tab .    demonstrates results LED-large   K arXiv summarization task . model merely initialized BART , additional   Discourse-aware (      ) Extr-Abst-TLM (      ) Dancer (      ) Pegasus (      ) LED-large ( seqlen :  ,    ) ( ) BigBird ( seqlen :  ,    ) (      ) LED-large ( seqlen :   ,    ) ( ) R-  R-  R-L   .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .   Table    : Summarization results Longformer- Encoder-Decoder ( LED ) arXiv dataset . Met- rics left right ROUGE-  , ROUGE-  ROUGE-L . enwik  . pretrained , Longformer con- sistently outperforms RoBERTa long document tasks sets new state-of-the-art results Wik- iHop TriviaQA . present LED , encoder-decoder variant Longformer model- ing sequence-to-sequence tasks , achieve state- of-the-art results arXiv long document sum- marization task . future work , would like study pretraining objectives , especially LED , increase sequence length , explore tasks might beneﬁt model . Acknowledgment would like thank Noah Smith , Dan Weld , Dirk Groeneveld , Kyle Lo , Daniel King Doug Downey helpful discussions feedback , AI  infrastructure team technical support . References Joshua Ainslie , Santiago Ontanon , Chris Alberti , Va- clav Cvicek , Zachary Fisher , Philip Pham , Anirudh Ravula , Sumit Sanghai , Qifan Wang , Li Yang .      . ETC : Encoding long structured inputs Proceedings      Con- transformers . ference Empirical Methods Natural Language Processing ( EMNLP ) , pages    –    , Online . Asso- ciation Computational Linguistics . Rami Al-Rfou , Dokook Choe , Noah Constant , Mandy Guo , Llion Jones .      . Character-level lan- guage modeling deeper self-attention . AAAI . Danqi Chen , Adam Fisch , Jason Weston , Antoine Bordes .      . Reading wikipedia answer open- domain questions . ACL . Jifan Chen , Shih-Ting Lin , Greg Durrett .      . Multi-hop question answering via reasoning chains . arXiv preprint , abs/    .      . Tianqi Chen , Thierry Moreau , Ziheng Jiang , Lianmin Zheng , Eddie Yan , Haichen Shen , Meghan Cowan , Leyuan Wang , Yuwei Hu , Luis Ceze , et al .      . TVM : automated end-to-end optimizing com- piler deep learning . OSDI . Tianqi Chen , Bing Xu , Chiyuan Zhang , Carlos Guestrin .      . Training deep nets sublinear memory cost . arXiv preprint , abs/    .      . Rewon Child , Scott Gray , Alec Radford ,  Ilya Sutskever .      . Generating long se- quences sparse transformers . arXiv preprint , abs/    .      . Christopher Clark Matt Gardner .      . Simple effective multi-paragraph reading comprehen- sion . ACL . Figure   : ROUGE-  ROUGE-  LED vary- ing input size ( arXiv validation set ) . pre-training . observe LED achieves state- of-the-art results arXiv , slightly outperform- ing BigBird ( Zaheer et al. ,      ) . Note BigBird summarization model supports sequence length  K tokens starts continues pre-training Pegasus ( Zhang et al. ,      ) , model speciﬁcally designed pre-trained summa- rization . pre-training task-speciﬁc ini- tialization , ability process longer inputs , LED slightly outperform BigBird . im- provements possible pre-training LED . Fig .   illustrates importance sequence length showing ablility process longer input signiﬁcantly improves results .   Conclusion Future Work present Longformer , transformer-based model scalable processing long documents makes easy perform wide range document-level NLP tasks without chunk- ing/shortening long input without com- plex architecture combine information across chunks . Longformer employs attention pattern combines local global information also scaling linearly sequence length . Longformer achieves state-of-the-art results character-level language modeling tasks text      K k  k                  .    .    .    .    .    .  R R  Kevin Clark , Urvashi Khandelwal , Omer Levy , Christopher D. Manning .      . bert look ? analysis bert ’ attention . arXiv preprint , abs/    .      . Arman Cohan , Franck Dernoncourt , Doo Soon Kim , Trung Bui , Seokhwan Kim , Walter Chang , Nazli Goharian .      . discourse-aware attention model abstractive summarization long documents . NAACL-HLT      . Andrew Dai Quoc V Le .      . Semi-supervised sequence learning . NeurIPS . Zihang Dai , Zhilin Yang , Yiming Yang , Jaime G. Car- bonell , Quoc V. Le , Ruslan Salakhutdinov .      . Transformer-XL : Attentive language models beyond ﬁxed-length context . ACL . Jacob Devlin , Ming-Wei Chang , Kenton Lee , Kristina Toutanova .      . BERT : Pre-training deep bidirectional transformers language under- standing . NAACL-HLT . Yuwei Fang , Siqi Sun , Zhe Gan , Rohit Pillai , Shuo- hang Wang , Jingjing Liu .      . Hierarchical graph network multi-hop question answering . Proceedings      Conference Empirical Methods Natural Language Processing ( EMNLP ) , pages     –     , Online . Association Computa- tional Linguistics . Adam Fisch , Alon Talmor , Robin Jia , Minjoon Seo , Eu- nsol Choi , Danqi Chen .      . MRQA      shared task : Evaluating generalization reading comprehension . MRQA workshop EMNLP . Alexios Gidiotis Grigorios Tsoumakas .      . divide-and-conquer approach summarization academic articles . ArXiv , abs/    .      . Michael Glaß , Alﬁo Massimiliano Gliozzo , Rishav Chakravarti , Anthony Ferritto , Lin Pan , Gaudani Bhargav , Dinesh Garg , Avirup Sil .      . Span selection pre-training question answering . arXiv preprint , abs/    .      . Scott Gray , Alec Radford , Diederik P. Kingma .      . Gpu kernels block-sparse weights . Dirk Groeneveld , Tushar Khot , Mausam , Ashish Sabhwaral .      . simple yet strong pipeline HotpotQA . arXiv preprint , abs/    .      . Ankit Gupta Jonathan Berant .      . Gmat : Global ArXiv , memory augmentation transformers . abs/    .      . Jeremy Howard Sebastian Ruder .      . Universal language model ﬁne-tuning text classiﬁcation . ACL . Mandar Joshi , Eunsol Choi , Daniel S. Weld , Luke Zettlemoyer .      . TriviaQA : large scale dis- tantly supervised challenge dataset reading com- prehension . ACL . Mandar Joshi , Omer Levy , Luke Zettlemoyer , Daniel Weld .      . BERT coreference resolu- tion : Baselines analysis . EMNLP-IJCNLP . Johannes Kiesel , Maria Mestre , Rishabh Shukla , Em- manuel Vincent , Payam Adineh , David Corney , Benno Stein , Martin Potthast .      . SemEval-      task   : Hyperpartisan news detection .  Proceedings   th International Workshop Semantic Evaluation , pages    –    , Minneapo- lis , Minnesota , USA . Association Computational Linguistics . Thomas N Kipf Max Welling .      . Semi- supervised classiﬁcation graph convolutional networks . ICLR . Nikita Kitaev , Lukasz Kaiser , Anselm Levskaya .       . Reformer : efﬁcient transformer . ICLR . Olga V. Kovaleva , Alexey Romanov , Anna Rogers , Anna Rumshisky .      . Revealing dark secrets bert . EMNLP/IJCNLP . Kenton Lee , Luheng , Luke Zettlemoyer .      . Higher-order coreference resolution coarse-to- ﬁne inference . NAACL . Mike Lewis , Yinhan Liu , Naman Goyal , Mar- jan Ghazvininejad , Abdelrahman Mohamed , Omer Levy , Veselin Stoyanov , Luke Zettlemoyer .      . BART : Denoising sequence-to-sequence pre- training natural language generation , translation , comprehension . Proceedings   th An- nual Meeting Association Computational Linguistics , pages     –     , Online . Association Computational Linguistics . Yinhan Liu , Myle Ott , Naman Goyal , Jingfei Du , Man- dar Joshi , Danqi Chen , Omer Levy , Mike Lewis , Luke Zettlemoyer , Veselin Stoyanov .      . RoBERTa : robustly optimized bert pretraining ap- proach . arXiv preprint , abs/    .      . Andrew L. Maas , Raymond E. Daly , Peter T. Pham , Dan Huang , Andrew Y. Ng , Christopher Potts .      . Learning word vectors sentiment analy- sis . Proceedings   th Annual Meeting Association Computational Linguistics : Hu- man Language Technologies , pages    –    , Port- land , Oregon , USA . Association Computational Linguistics . Matt Mahoney .      . Large text compression bench- mark . A¨aron van den Oord , Sander Dieleman , Heiga Zen , Karen Simonyan , Oriol Vinyals , Alex Graves , Nal Kalchbrenner , Andrew W. Senior , Koray Kavukcuoglu .      . Wavenet : generative model raw audio . SSW . Myle Ott , Sergey Edunov , Alexei Baevski , Angela Fan , Sam Gross , Nathan Ng , David Grangier , fairseq : fast , extensible Michael Auli .      .    toolkit sequence modeling . NAACL-HLT      : Demonstrations . Proceedings Matthew E. Peters , Mark Neumann , Mohit Iyyer , Matt Gardner , Christopher Clark , Kenton Lee , Luke Zettlemoyer .      . Deep contextualized word repre- sentations . NAACL . Sameer Pradhan , Alessandro Moschitti , Nianwen Xue , Olga Uryupina , Yuchen Zhang .      . CoNLL-      shared task : Modeling multilingual unre- stricted coreference OntoNotes . Joint Confer- ence EMNLP CoNLL - Shared Task , pages  –   , Jeju Island , Korea . Association Computa- tional Linguistics . Ming Tu , Kevin Huang , Guangtao Wang , Jing Huang , Xiaodong , Bufang Zhou .      . Select , an- swer explain : Interpretable multi-hop reading arXiv comprehension multiple documents . preprint , abs/    .      . Ashish Vaswani , Noam Shazeer , Niki Parmar , Jakob Uszkoreit , Llion Jones , Aidan N. Gomez , Lukasz Kaiser , Illia Polosukhin .      . Attention need . NIPS . Johannes Welbl , Pontus Stenetorp , Sebastian Riedel .      . Constructing datasets multi-hop reading comprehension across documents . TACL ,  :   –    . Jiezhong Qiu , Hao , Omer Levy , Scott Yih , Sinong Wang , Jie Tang .      . Blockwise self-attention long document understanding . arXiv preprint , abs/    .      . Felix Wu , Angela Fan , Alexei Baevski , Yann Dauphin , Michael Auli .      . Pay less attention arXiv lightweight dynamic convolutions . preprint , abs/    .      . Alec Radford , Jeffrey Wu , Rewon Child , David Luan , Dario Amodei , Ilya Sutskever .      . Language models unsupervised multitask learners . Jack W. Rae , Anna Potapenko , Siddhant M. Jayaku- mar , Timothy P. Lillicrap .      . Compressive transformers long-range sequence modelling . ICLR . Colin Raffel , Noam Shazeer , Adam Roberts , Katherine Lee , Sharan Narang , Michael Matena , Yanqi Zhou , W. Li , Peter J. Liu .      . Exploring limits transfer learning uniﬁed text-to-text trans- former . J. Mach . Learn . Res. ,   :   : –   :   . Aurko Roy , Mohammad Saffar , Ashish Vaswani , David Grangier .      . Efﬁcient content-based sparse attention routing transformers . arXiv preprint , abs/    .      . Nan Shao , Yiming Cui , Ting Liu , Shijin Wang , graph structure neces- arXiv preprint , Guoping Hu .      . sary multi-hop reasoning ? abs/    .      . Sandeep Subramanian , Raymond Li , Jonathan Pilault , C. Pal .      . extractive abstractive neu- ral document summarization transformer lan- guage models . EMNLP . Sainbayar Sukhbaatar , Edouard Grave , Piotr Bo- janowski , Armand Joulin .      . Adaptive at- tention span transformers . ACL . Ilya Sutskever , Oriol Vinyals , Quoc V. Le .      . Sequence sequence learning neural networks . NIPS . Trieu H. Trinh Quoc V. Le .      . simple method commonsense reasoning . arXiv preprint , abs/    .      . Ming Tu , Jinke Huang , Xiaodong , Bowen Zhou .      . Graph sequential network reasoning sequences . NeurIPS Graph Representation Learning workshop . Qizhe Xie , Zihang Dai , Eduard H. Hovy , Minh-Thang Luong , Quoc V. Le .      . Unsupervised data augmentation consistency training . arXiv preprint , abs/    .      . Ruibin Xiong , Yunchang Yang , Di , Kai Zheng , Shu xin Zheng , Chen Xing , Huishuai Zhang , Yanyan Lan , Li-Wei Wang , Tie-Yan Liu .      . layer normalization transformer architecture . arXiv preprint , abs/    .      . Zhilin Yang , Peng Qi , Saizheng Zhang , Yoshua Ben- gio , William W. Cohen , Ruslan Salakhutdinov , HotpotQA : Christopher D. Manning .      . dataset diverse , explainable multi-hop question answering . EMNLP . Zihao Ye , Qipeng Guo , Quan Gan , Xipeng Qiu , Zheng Zhang .      . BP-Transformer : Modelling long-range context via binary partitioning . arXiv preprint , abs/    .      . Manzil Zaheer , Guru Guruganesh , Kumar Avinava Dubey , Joshua Ainslie , C. Alberti , S. Onta˜n´on , Philip Pham , Anirudh Ravula , Qifan Wang , L. Yang , A. Ahmed .      . Big bird : Transformers longer sequences . ArXiv , abs/    .      . Rowan Zellers , Ari Holtzman , Hannah Rashkin , Yonatan Bisk , Ali Farhadi , Franziska Roesner , Yejin Choi .      . Defending neural fake news . NeurIPS . Jingqing Zhang , Yao Zhao , Mohammad Saleh , Peter J Liu .     . Pegasus : Pre-training ex- tracted gap-sentences abstractive summarization . ICML . Yukun Zhu , Ryan Kiros , Richard S. Zemel , Ruslan Salakhutdinov , Raquel Urtasun , Antonio Torralba , Sanja Fidler .      . Aligning books movies : Towards story-like visual explanations watching movies reading books . ICCV , pages   –   .    Implementation Details Implementing Longformer ’ dilated sliding win- dow attention requires form banded matrix multiplication ( matrix multiplication out- put zero except certain diagonals ) directly supported existing deep learning libraries like PyTorch/Tensorﬂow . Fig .   compares runtime memory three different ways implementing . Longformer-loop naive implementation computes diagonal separately loop . memory efﬁcient computes non-zero values , unusably slow . use testing easy implement ’ use run experiments . Longformer-chunks supports non- dilated case . chunks Q K overlapping blocks size w overlap size     w , multiplies blocks , mask diagonals . compute efﬁcient uses single ma- trix multiplication operation PyTorch , consumes  x amount memory perfectly op- timized implementation consume computes zero values . compute efﬁciency , implementation suitable pretrain/ﬁnetune case . ’ ﬁnd increase memory problem setting . Longformer-cuda custom CUDA kernel implement using TVM ( Chen et al. ,      ) . fully functioning implementation at- tention ( limited Longformer-chunks ) , memory efﬁcient , fast highly optimized full self-attention.   mainly use implementation autoregres- sive language modeling experiments memory efﬁciency ( allows longest sequences ) support dilation ( needed character- LM experiments ) . Tensor Virtual Machine ( TVM ) build custom CUDA kernel using TVM ( Chen et al. ,      ) , deep learning compiler stack compiles high level description function optimized device-speciﬁc code . Using TVM , describe banded matrix multiplication high-level python   It worth noting theoretically , perfectly optimized Longformer-cuda faster n  computa- tion . However , achieving level performance requires special knowledge low-level GPU programming , similar implementing highly optimized matrix multiplication . current implementation sufﬁciently fast practical use . constructs , TVM generates corresponding CUDA code compiles GPUs . B Character LM Hyperparameters evaluate text  enwik  , contain    M characters Wikipedia split   M ,  M ,  M train , dev , test . model speci- ﬁes self-attention component works , agnostic design choices trans- former model . implementation based Transformer-XL ( Dai et al. ,      ) code   memory mechanism disabled . use relative posi- tion embeddings sinusoidal weights Dai et al . (      ) . use two different model sizes , small (    layers ,     hidden size ) model Dai et al . (      ) , large (    layers ,     hidden size ) model Child et al . (      ) . employed mixed precision training ( ﬂoating points       ) using apex   reduce memory consumption speed-up training . However , kept attention computation fp   avoid numerical instability issues.   used gradient checkpointing ( Chen et al. ,      ) reduce memory usage , ran experiments   GB RTX     GPUs . hyper- parameters stage conﬁgurations listed Tab .    . CUDA kernel supports autoregres- sive mode token attends window previous tokens . implementation also in- cludes version relative position embedding compatible dilated sliding window attention . ran small model experiments   RTX     GPUs    days . large model , ran experiments   RTX     GPUs    days . hyperparameter search similar ablation Tab .   run conﬁgu- ration    K steps text  . experimented absolute position embeddings learned po- sition embeddings , dropout values [  .  ,  .  ] ( small model ) [  .  ,  .  ] ( large model ) , pre- layernorm post-layernorm ( Xiong et al. ,      ) , learning rate ( LR ) phase  values [  . e-  ,  e-   ,  e-  ] constant cosine LR schedules , different conﬁgurations dilation ( heads ,   heads , dilation ) . Number gradient up- dates/phase reported Tab .    determined running phase validation BPC stops   https : //github.com/kimiyoung/ transformer-xl   https : //github.com/NVIDIA/apex   We found using fp   attention operation results ﬂoating point overﬂow NaNs later stages training .    getting better . C Pretraining Data order allow model learn long depen- dencies pretraining , compiled corpus long documents . data sources also included original RoBERTa pretraining including Books corpus ( Zhu et al. ,      ) plus English Wikipedia . additionally included one third subset Realnews dataset ( Zellers et al. ,      ) documents longer  ,    to- kens well one third Stories ( Trinh Le ,      ) corpus . goal include mix long short documents allow model learn longer dependencies forget in- formation original RoBERTa pretraining . statistics pretraining data shown Tab .    . Task speciﬁc model details QA classiﬁcation models imple- mented using PyTorch-Lightning   . use ofﬁcial train/dev/test splits datasets except Hyperpartisan news randomely split   /  /   train/dev/test . WikiHop Instances WikiHop consist : question , answer candidates ( ranging two candidates    candidates ) , supporting contexts ( ranging three paragraphs    paragraphs ) , correct answer . dataset pro- vide intermediate annotation multihop reasoning chains , requiring models instead infer indirect answer supervision . prepare data input Longformer RoBERTa , ﬁrst tokenize question , answer candidates , support contexts using  RoBERTa ’ wordpiece tokenizer . concatenate question answer candi- dates special tokens [ q ] question [ /q ] [ ent ] candidate  [ /ent ] ... [ ent ] candidateN [ /ent ] . contexts also concatenated using RoBERTa ’ doc- ument delimiter tokens separators : < /s > < /s > contextM context  < /s > ... < /s > . tokens [ q ] , [ /q ] , special [ ent ] , [ /ent ] added RoBERTa vocabulary randomly initialized task ﬁnetuning .   https : //github.com/PyTorchLightning/ pytorch-lightning preparing input data , compute acti- vations top layer model follows . take question answer candidates concatenate much context possible model sequence length (     RoBERTa ,  ,    Longformer ) , run sequence model , collect output activations , repeat context exhausted ( models except Longformer-large , include ﬁrst  ,    length sequence due memory re- quirements ) . activations chunks concatenated one long sequence . case Longformer , use global attention entire question answer candidate sequence . prediction , attach linear layer [ ent ] outputs single logit , average logits candidate across chunks , apply softmax use cross entropy loss correct answer candidate . Training used Adam optimizer linear warmup     gradient updates maximum LR , linear decay remainder training . used gradient accumulation effective batch size    instances , checking development ac- curacy every     gradient updates reported maximum development accuracy . hyperpa- rameters ( dropout , weight decay ) identical RoBERTa pretraining . general , ran minimal hyperparameter trials , fair comparison Longformer RoBERTa ran identical hyperparameter search Longformer-base RoBERTa-base . consisted grid search LR [  e-  ,  e-  ,  e-  ] number epochs [   ,    ,    ] . best Longformer-base conﬁguration used lr= e-  ,    epochs . ran two hyperparameter trials Longformer-large , lr= e-  number epochs [   ,    ] (   epoch model higher dev accuracy   .  , single model submitted public leaderboard test set evaluation ) . mod- els trained single RTX     GPU , Longformer-base taking day   epochs . TriviaQA TriviaQA    K ques- tion , answer , document triplets training . Doc- uments Wikipedia articles , answers named entities mentioned article . span answers question annotated , found using simple text matching . Similar WikiHop , tokenize question document using RoBERTa ’ tokenizer , form input [ ] question [ /s ]    Param Value Position Embeddings Small model conﬁg Large model conﬁg Optimizer Dropout Gradient clipping Weight Decay Layernorm Location Activation Number phases Phase   window sizes Phase   window sizes Phase   sequence length Phase   sequence length Phase   LR Phase   LR Batch size per phase # Steps per phase ( small ) # Steps per phase ( large ) Warmup LR scheduler Dilation ( small model ) Dilation ( large model ) Dilation heads Relative Sinusoidal Dai et al . (      )    layers ,   heads ,     hidden size Dai et al . (      )    layers ,   heads ,     hidden size Child et al . (      ) AdamW  .  ( small model ) ,  .  ( large model )  .    .   pre-layernorm ( Xiong et al. ,      ) GeLU      ( bottom layer ) -  ,    ( top layer )     ( bottom layer ) - ( top layer )  ,      ,    ( gpu memory limit )  .                   ,    ,    ,    ,       K ,   k ,   k ,   k ,  k    K ,   k ,   k ,  k ,  k    % phase steps maximum   K steps constant throughout phase   ( layers  -  ) ,   ( layers  -  ) ,   ( layers  -  ) ,   ( layers   -   )   ( layers  -   ) ,   ( layers   -   ) ,   ( layers   -   ) ,   ( layers   -   )   heads Table    : Hyperparameters best performing model character-level language modeling Source Tokens Avg doc len Books ( Zhu et al. ,      ) English Wikipedia Realnews ( Zellers et al. ,      ) Stories ( Trinh Le ,      )  . B  . B  . B  . B   . K      . K  . K Table    : Pretraining data document [ /s ] . truncate document  ,    wordpiece avoid slow . After- wards , get activations RoBERTa Longformer similar WikiHop ( discussed ) . use global attention question tokens . prediction , add one layer predicts beginning end answer span . distant supervision nature training data ( gold answer spans ) , use loss function Clark Gardner (      ) works like model needs get one answer span right , . Hyperparameters best conﬁguration listed Tab .    . hyperparameters similar RoBERTa ’ . hyperparameter search , tuned LR RoBERTa baseline tried rates [  e-  ,  e-  ,  e-  ] , used best ,  e-  , subsequent experiments tuning . trained Longformer-large best conﬁguration submitted output leaderboard . ran experiments   GB V    GPUs . Small model takes   day train   GPUs , large model takes   day   GPUs . HotpotQA HotpotQA dataset involves answer- ing questions set    paragraphs    different Wikipedia articles   paragraphs relevant question rest dis- includes   tasks answer span ex- tractors . traction evidence sentence identiﬁcation . model HotpotQA combines answer span extraction evidence extraction one joint model . found higher performance using two-stage Longformer model similar setup ﬁrst identiﬁes relevant paragraphs ﬁnd ﬁnal answer span evidence.   largely removing distracting paragraphs ﬁrst reduces noise ﬁnal ev- idence span detection also found im- portant recent state-of-the-art methods dataset ( Fang et al. ,      ) . Similar Wikihop TriviaQA , prepare data input Long- former , concatenate question    paragraphs one long context . particu- larly use following input format special tokens : “ [ CLS ] [ q ] question [ /q ] ( cid:    ) ( cid:    ) title  ( cid:    ) /t ( cid:    ) sent ,  [ ] sent ,  [ ] ...   The ﬁnal dev performance two stage model im- proves single stage model  .  points joint- F  metric    ( cid:    ) ( cid:    ) title  ( cid:    ) /t ( cid:    ) sent ,  [ ] sent ,  [ ] ... ” [ q ] , [ /q ] , ( cid:    ) ( cid:    ) , ( cid:    ) /t ( cid:    ) , [ ] , [ p ] special tokens representing , question start end , paragraph title start end , sentence , respectively . special tokens added Longformer vocabulary randomly initialized task ﬁnetuning . Longformer , use global attention question tokens , paragraph ti- tle start tokens well sentence tokens . model includes additional feedforward layers top paragraph title start tokens prediction relevant paragraphs , well sentence tokens predicting evidence sentences . training ﬁrst stage model , predict relevant paragraph scores training development set . keep   paragraphs whose raw score higher pre-speciﬁed threshold ( - .  ) , remove paragraphs context . train second stage model resulting shortened context . answer span extraction use BERT ’ QA model ( Devlin et al. ,      ) addition question type ( yes/no/span ) classiﬁ- cation head ﬁrst special token ( [ CLS ] ) . evidence extraction apply   layer feedfor- ward networks top representations corre- sponding sentence paragraph tokens get corresponding evidence prediction scores use binary cross entropy loss train model . inference time evidence extraction , use constrained decoding strategy similar Groen- eveld et al . (      ) ensures evidence sentences come exactly two paragraphs setup dataset . combine span , ques- tion classiﬁcation , sentence , paragraphs losses train model multitask way using lin- ear combination losses . experiments done RTX     GPUs training epoch takes approximately half day   GPUs . trained model using Adam optimizer lin- ear warmup (      steps ) linear decay . used minimal hyperparameter tuning using LRs  e-   e-  epochs     found model LR  e-    epochs work best . conduct hyperparameter search RoBERTa baseline well . rest hyperpa- rameters reported Tab    . Coreference model details coreference model straightforward adaptation coarse- to-ﬁne BERT based model Joshi et al . (      ) . preprocessing document RoBERTa wordpiece tokenizer , splits Param WikiHop TriviaQA HotpotQA Epochs LR Warmup steps Batch size Optimizer     e-         Adam    e-          Adam    e-          Adam Table    : Hyperparameters QA models . mod- els use similar scheduler linear warmup de- cay . document non-overlapping segments maximum sequence length , concatenates activations coarse-to-ﬁne clustering stage forms coreference clusters . maximum se- quence length     RoBERTa-base , chosen three trials [     ,     ,     ] using default hyperparameters original implemen- tation.   Longformer-base sequence length  ,    . Similar original implementation , different learning rates used pretrained RoBERTa parameters randomly initialized task parameters . Using larger learning rate task parameters allows optimizer adjust farther randomly initialized values with- destroying information pretrained RoBERTa parameters . Hyperparameter searches minimal con- sisted grid searches RoBERTa LR [  e-  ,  e-  ,  e-  ] task LR [  e-  ,  e-  ,  e-  ] RoBERTa Longformer fair compari- son . best conﬁguration Longformer-base RoBERTa lr= e-  , task lr= e-  . hy- perparameters original im- plementation . Training takes    hours single GPU . implementation superhack involves PyTorch Tensorﬂow sharing single process GPU . avoid re-implementing com- plicated coarse-to-ﬁne logic Tensorﬂow PyTorch ( involves highly optimized cus- tom GPU kernel originally released Lee et al . (      ) ) , devised system lower trans- former portion model passes activations gradients back forth PyTorch Ten- sorﬂow . input tensors ﬁrst run transformer PyTorch , activations col- lected top layer , transferred GPU CPU CPU Tensorﬂow back GPU run coarse-to-ﬁne clustering com- pute loss . gradients back propogated   https : //github.com/mandarjoshi  /coref    Tensorﬂow top transformer process reversed transfer PyTorch back propogation remainder model . Separate optimizers maintained identical LR schedules parameter updates . overhead approach minimal compared overall cost running model . Text classiﬁcation classiﬁcation , following BERT , used simple binary cross entropy loss top ﬁrst [ CLS ] token addition global attention [ CLS ] . used Adam opti- mizer batch sizes    linear warmup decay warmup steps equal  .  total training steps . IMDB Hyperpar- tisan news grid search LRs [  e-  ,  e-  ] epochs [    ,    ,    ] found model [  e-  ] epochs    work best . Experiments done single RTX     GPU .    