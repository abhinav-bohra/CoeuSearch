Recursive Neural Structural Correspondence Network for Cross-domain Aspect and Opinion Co-Extraction Wenya Wang†‡ and Sinno Jialin Pan† †Nanyang Technological University, Singapore ‡SAP Innovation Center Singapore {wa0001ya, sinnopan}@ntu.edu.sg Abstract Fine-grained opinion analysis aims to ex- tract aspect and opinion terms from each sentence for opinion summarization. Su- pervised learning methods have proven to be effective for this task. However, in many domains, the lack of labeled data hinders the learning of a precise extraction model. In this case, unsupervised domain adapta- tion methods are desired to transfer knowl- edge from the source domain to any un- labeled target domain. In this paper, we develop a novel recursive neural network that could reduce domain shift effectively in word level through syntactic relations. We treat these relations as invariant “pivot information” across domains to build struc- tural correspondences and generate an aux- iliary task to predict the relation between any two adjacent words in the dependency tree. In the end, we demonstrate state-of- the-art results on three benchmark datasets. 1 Introduction The problem of ﬁne-grained opinion analysis in- volves extraction of opinion targets (or aspect terms) and opinion expressions (or opinion terms) from each review sentence. For example, in the sen- tence: “They offer good appetizers”, the aspect and opinion terms are appetizers and good correspond- ingly. Many supervised deep models have been pro- posed for this problem (Liu et al., 2015; Yin et al., 2016; Wang et al., 2017), and obtained promising results. However, these methods fail to adapt well across domains, because the aspect terms from two different domains are usually disjoint, e.g., lap- top v.s. restaurant, leading to large domain shift in the feature vector space. Though unsupervised methods (Hu and Liu, 2004; Qiu et al., 2011) can deal with data with few labels, their performance is unsatisfactory compared with supervised ones. There have been a number of domain adaptation methods for coarse-grained sentiment classiﬁcation problems across domains, where an overall senti- ment polarity of a sentence or document is being predicted. Nevertheless, very few approaches exist for cross-domain ﬁne-grained opinion analysis due to the difﬁculties in ﬁne-grained adaptation, which is more challenging than coarse-grained problems. Li et al. (2012) proposed a bootstrap method based on the TrAdaBoost algorithm (Dai et al., 2007) to iteratively expand opinion and aspect lexicons in the target domain by exploiting source-domain labeled data and cross-domain common relations between aspect terms and opinion terms. However, their model requires a seed opinion lexicon in the target domain and pre-mined syntactic patterns as a bridge. Ding et al. (2017) proposed to use rules to generate auxiliary supervision on top of a recurrent neural network to learn domain-invariant hidden representation for each word. The performance highly depends on the quality of the manually de- ﬁned rules and the prior knowledge of a sentiment lexicon. In addition, the recurrent structure fails to capture the syntactic interactions among words in- trinsically for opinion extraction. The requirement for rules makes the above methods non-ﬂexible. In this paper, we propose a novel cross-domain Recursive Neural Network (RNN)1 for aspect and opinion terms co-extraction across domains. Our motivations are twofold: 1) The dependency re- lations capture the interactions among different words. These relations are especially important for identifying aspect terms and opinion terms (Qiu et al., 2011; Wang et al., 2016), which are also domain-invariant within the same language. There- fore, they can be used as “pivot” information to 1Here, we use RNN to denote recursive neural networks, rather than recurrent neural networks. bridge the gap between different domains. 2) In- spired by the idea of structural learning (Ando and Zhang, 2005), the success of target task depends on the ability of ﬁnding good predictive structures learned from other related tasks, e.g., structural cor- respondence learning (SCL) (Blitzer et al., 2006) for coarse-grained cross-domain sentiment classiﬁ- cation. Here, we aim to generate an auxiliary task on dependency relation classiﬁcation. Different from previous approaches, our auxiliary task and the target extraction task are of heterogeneous label spaces. We aim to integrate this auxiliary task with distributed relation representation learning into a recursive neural network. Speciﬁcally, we generate a dependency tree for each sentence from the dependency parser and con- struct a uniﬁed RNN that integrates an auxiliary task into the computation of each node. The aux- iliary task is to classify the dependency relation for each direct edge in the dependency tree by learning a relation feature vector. To reduce la- bel noise brought by inaccurate parsing trees, we further propose to incorporate an autoencoder into the auxiliary task to group the relations into dif- ferent clusters. Finally, to model the sequential context interaction, we develop a joint architec- ture that combines RNN with a sequential labeling model for aspect and opinion terms extraction. Ex- tensive experiments are conducted to demonstrate the advantage of our proposed model. 2 Related Work Existing works for single-domain aspect/opinion terms extraction include unsupervised methods based on association rule mining (Hu and Liu, 2004), syntactic rule propagation (Qiu et al., 2011) or topic modeling (Titov and McDonald, 2008; Lu et al., 2009; Zhang et al., 2010), as well as su- pervised methods based on extensive feature engi- neering with graphical models (Jin and Ho, 2009; Li et al., 2010) or deep learning (Liu et al., 2015; Zhang et al., 2015; Wang et al., 2017; Yin et al., 2016). Among exiting deep models, improved re- sults are obtained using dependency relations (Yin et al., 2016; Wang et al., 2016), which indicates the signiﬁcance of syntactic word interactions for tar- get term extraction. In cross-domain setting, there are very few works for aspect/opinion terms extrac- tion including a pipelined approach (Li et al., 2012) and a recurrent neural network (Ding et al., 2017). Both of the methods require manual construction of common and pivot syntactic patterns or rules, which are indicative of aspect or opinion words. There have been a number of domain adaptation approaches proposed for coarse-grained sentiment classiﬁcation. Among existing methods, one active line focuses on projecting original feature spaces of two domains into the same low-dimensional space to reduce domain shift using pivot features as a bridge (Blitzer et al., 2007; Pan et al., 2010; Bollegala et al., 2015; Yu and Jiang, 2016). An- other line learns domain-invariant features via auto- encoders (Glorot et al., 2011; Chen et al., 2012; Zhou et al., 2016). Our work is more related to the ﬁrst line by utilizing pivot information to transfer knowledge across domains, but we integrate the idea into a uniﬁed deep structure that can fully uti- lize syntactic structure for domain adaptation in ﬁne-grained sentiment analysis. 3 Problem Deﬁnition & Motivation Our task is to extract opinion and aspect terms within each review sentence. We denote a sen- tence by a sequence of tokens x = (w1, w2, ..., wn). The output is a sequence of token-level labels y = (y1, y2, ..., yn), with yi ∈ {BA, IA, BO, IO, N} that represents beginning of an aspect (BA), inside of an aspect (IA), beginning of an opinion (BO), inside of an opinion (IO) or none of the above (N). A subsequence of labels started with “BA” and fol- lowed by “IA” indicates a multi-word aspect term. In unsupervised domain adaptation, we are given a set of labeled review sentences from a source do- main DS = {(xSi, ySi)}nS i=1, and a set of unlabeled sentences from a target domain DT = {xTj }nT j=1. Our goal is to predict token-level labels on DT . Existing works for cross-domain aspect and/or opinion terms extraction require hand-coded rules and a sentiment lexicon in order to transfer knowl- edge across domains. For example in Figure 1, given a review sentence “They offer good appe- tizers” in the source domain and “The laptop has a nice screen” in the target domain. If nice has been extracted as a common sentiment word, and “OPINION-amod-ASPECT” has been identiﬁed as a common syntactic pattern from the source do- main, screen could be deduced as an aspect term us- ing the identiﬁed syntactic pattern (Li et al., 2012). Similarly, Ding et al. (2017) used a set of pre- deﬁned rules based on syntactic relations and a sentiment lexicon to generate auxiliary labels to learn high-level feature representations through a Figure 1: An example of two reviews with similar syntactic patterns. recurrent neural network. On one hand, these previous attempts have ver- iﬁed that syntactic information between words, which can be used as a bridge between domains, is crucial for domain adaptation. On the other hand, dependency-tree-based RNN (Socher et al., 2010) has proven to be effective to learn high-level fea- ture representation of each word by encoding syn- tactic relations between aspect terms and opinion terms (Wang et al., 2016). With the above ﬁndings, we propose a novel RNN named Recursive Neural Structural Correspondence Network (RNSCN) to learn high-level representation for each word across different domains. Our model is built upon depen- dency trees generated from a dependency parser. Different from previous approaches, we do not re- quire any hand-coded rules or pre-selected pivot features to construct correspondences, but rather focus on the automatically generated dependency relations as the pivots. The model associates each direct edge in the tree with a relation feature vector, which is used to predict the corresponding depen- dency relation as an auxiliary task. Note that the relation vector is the key in the model: it associates with the two interacting words and is used to construct structural correspondences between two different domains. Hence, the aux- iliary task guides the learning of relation vectors, which in turn affects their correspondingly interac- tive words. Speciﬁcally in Figure 1, the relation vector for “amod” is computed from the features of its child and parent words, and also used to pro- duce the hidden representation of its parent. For this relation path in both sentences, the auxiliary task enforces close proximity for these two relation vectors. This pushes the hidden representations for their parent nodes appetizers and screen closer to each other, provided that good and nice have sim- ilar representations. In a word, the auxiliary task bridges the gap between two different domains by drawing the words with similar syntactic properties closer to each other. However, the relation vectors may be sensitive to the accuracy of the dependency parser. It might harm the learning process when some noise ex- ists for certain relations, especially for informal texts. This problem of noisy labels has been ad- dressed using perceptual consistency (Reed et al., 2015). Inspired by the taxonomy of dependency re- lations (de Marneffe and Manning, 2008), relations with similar functionalities could be grouped to- gether, e.g., dobj, iobj and pobj all indicate objects. We propose to use an auto-encoder to automatically group these relations in an unsupervised manner. The reconstruction loss serves as the consistency objective that reduces label noise by aligning rela- tion features with their intrinsic relation group. 4 Proposed Methodology Our model consists of two components. The ﬁrst component is a Recursive Neural Structural Cor- respondence Network (RNSCN), and the second component is a sequence labeling classiﬁer. In this paper, we focus on Gated Recurrent Unit (GRU) as an implementation for the sequence labeling classi- ﬁer. We choose GRU because it is able to deal with long-term dependencies compared to a simple Re- current neural network and requires less parameters making it easier to train than LSTM. The resultant deep learning model is denoted by RNSCN-GRU. We also implement Conditional Random Field as the sequence labeling classiﬁer, and denote the model by RNSCN-CRF accordingly. The overall architecture of RNSCN-GRU with- out auto-encoder on relation denoising is shown in Figure 2. The left and right are two example sentences from the source and the target domain, respectively. In the ﬁrst component, RNSCN, an auxiliary task to predict the dependency relation for each direct edge is integrated into a dependency- tree-based RNN. We generate a relation vector for each direct edge from its child node to parent node, and use it to predict the relation and produce the hidden representation for the parent node in the de- pendency tree. To address the issues of noisy rela- tion labels, we further incorporate an auto-encoder into RNSCN, as will be shown in Figure 3. While RNSCN mainly focuses on syntactic in- teractions among the words, the second component, GRU, aims to compute linear-context interactions. GRU takes the hidden representation of each word computed from RNSCN as inputs and further pro- duces ﬁnal representation of each word by taking linear contexts into consideration. We describe each component in detail in the following sections. Theyo(cid:6)ergoodappetizersnsubjdobjamodThelaptophasascreennicedetdetnsubjdobjamodRESTAURANTLAPTOPFigure 2: The architecture of RNSCN-GRU. 4.1 Recursive Neural Structural Correspondence Network Moreover, the relation vector r43 is used for the auxiliary task on relation prediction: RNSCN is built on the dependency tree of each sen- tence, which is pre-generated from a dependency parser. Speciﬁcally, each node in the tree is asso- ciated with a word wn, an input word embedding xn ∈ Rd and a transformed hidden representation hn ∈ Rd. Each direct edge in the dependency tree associates with a relation feature vector rnm ∈ Rd nm ∈ RK, where and a true relation label vector yR K is the total number of dependency relations, n and m denote the indices of the parent and child word of the dependency edge, respectively. Based on the dependency tree, the hidden representations are generated in a recursive manner from leaf nodes until reaching the root node. Consider the source- domain sentence shown in Figure 2 as an illustra- tive example, we ﬁrst compute hidden representa- tions for leaf nodes they and good: h1=tanh(Wxx1 + b), h3=tanh(Wxx3 + b), where Wx ∈ Rd×d transforms word embeddings to hidden space. For non-leaf node appetizer, we ﬁrst generate the relation vector r43 for the depen- dency edge x4 (appetizers) amod −−−−→ x3 (good) by r43 = tanh(Whh3 + Wxx4), where Wh ∈ Rd×d transforms the hidden repre- sentation to the relation vector space. We then compute the hidden representation for appetizer: h4 = tanh(Wamodr43 + Wxx4 + b). ˆyR 43 = softmax(WRr43 + bR), where WR ∈ RK×d is the relation classiﬁca- tion matrix. The supervised relation classiﬁer en- forces close proximity of similar {rnm}’s in the dis- tributed relation vector space. The relation features bridge the gap of word representations in different domains by incorporating them into the forward computations. In general, the hidden representation hn for a non-leaf node is produced through hn = tanh( (cid:88) WRnmrnm + Wxxn + b), (1) m∈Mn where rnm = tanh(Wh ·hm +Wx ·xn), Mn is the set of child nodes of wn, and WRnm is the relation transformation matrix tied with each relation Rnm. The predicted label vector ˆyR nm for rnm is ˆyR nm = softmax(WR · rnm + bR). (2) Here we adopt the the cross-entropy loss for re- lation classiﬁcation between the predicted label vector ˆyR nm and the ground-truth yR nm to encode relation side information into feature learning: (cid:96)R = K (cid:88) k=1 −yR nm[k] log ˆyR nm[k]. (3) Through the auxiliary task, similar relations en- force participating words close to each other so appetizersgoodo(cid:12)ertheyrootdobjamodnsubjh4h3h(cid:14)h2r2(cid:14)r43r24x(cid:14)x2x3x4y(cid:5)43y(cid:5)2(cid:14)y(cid:5)24niceahaslaptopdetamodnsubjh6h5h(cid:14)h3r2(cid:14)r65r36x2x3x4x5y(cid:5)2(cid:14)y(cid:5)65y(cid:5)36x6x(cid:14)Thescreendobjh2r32y(cid:5)32deth4r64y(cid:5)64h(cid:1)(cid:14)h(cid:1)3h(cid:1)4h(cid:1)(cid:14)h(cid:1)2h(cid:1)3h(cid:1)4h(cid:1)5h(cid:1)6h(cid:1)2RNSCNGRUSourceTargetthat words with similar syntactic functionalities are clustered across domains. On the other hand, the pre-trained word embeddings group semantically- similar words. By taking them as input to RNN, together with the auxiliary task, our model encodes both semantic and syntactic information. 4.2 Reduce Label Noise with Auto-encoders As discussed in Section 3, it might be hard to learn an accurate relation classiﬁer when each class is a unique relation, because the dependency parser may generate incorrect relations as noisy labels. To address it, we propose to integrate an autoencoder into RNSCN. Suppose there is a set of latent groups of relations: G = {1, 2, ..., |G|}, where each rela- tion belongs to only one group. For each relation vector, rnm, an autoencoder is performed before feeding it into the auxiliary classiﬁer (2). The goal is to encode the relation vector to a probability dis- tribution of assigning this relation to any group. As can be seen Figure 3, each relation vector rnm is ﬁrst passed through the autoencoder as follows, p(Gnm = i|rnm) = exp(r(cid:62) nmWencgi) exp(r(cid:62) nmWencgj) (cid:80) j∈G , (4) where Gnm denotes the inherent relation group for rnm, gi ∈ Rd represents the feature embedding for group i, and Wenc∈Rd×d is the encoding matrix that computes bilinear interactions between relation vector rnm and relation group embedding gi. Thus, p(Gnm = i|rnm) represents the probability of rnm being mapped to group i. An accumulated relation group embedding is computed as: gnm = |G| (cid:88) i=1 p(Gnm = i|rnm)gi. (5) For decoding, the decoder takes gnm as input and tries to reconstruct the relation feature input rnm. Moreover, gnm is also used as the higher-level fea- ture vector for rnm for predicting the relation label. Therefore, the objective for the auxiliary task in (3) becomes: (cid:96)R = (cid:96)R1 + α(cid:96)R2 + β(cid:96)R3, where (cid:96)R1 = (cid:107)rnm − Wdecgnm(cid:107)2 2 , (cid:96)R2 = (cid:96)R3 = K (cid:88) −yR nm[k] log ˆyR nm[k], k=1 (cid:13) (cid:13) 2 (cid:13)I − ¯G(cid:62) ¯G (cid:13) (cid:13) (cid:13) F . (6) (7) (8) (9) Figure 3: An autoencoder for relation grouping. Here (cid:96)R1 is the reconstruction loss with Wdec being the decoding matrix, (cid:96)R2 follows (3) with ˆyR nm = softmax(WRgnm + bR) and (cid:96)R3 is the regularization term on the correlations among la- tent groups with I being the identity matrix and ¯G being a normalized group embedding matrix that consists of normalized gi’s as column vectors. This regularization term enforces orthogonality between gi and gj for i (cid:54)= j. α and β are used to con- trol the trade-off among different losses. With the auto-encoder, the auxiliary task of relation classi- ﬁcation is conditioned on group assignment. The reconstruction loss further ensures the consistency between relation features and groupings, which is supposed to dominate classiﬁcation loss when the observed labels are inaccurate. We denote RNSCN with auto-encoder by RNSCN+. 4.3 Joint Models for Sequence Labeling RNSCN or RNSCN+ focuses on capturing and rep- resenting syntactic relations to build a bridge be- tween domains and learn more powerful represen- tations for tokens. However, it ignores the linear- chain correlations among tokens within a sentence, which is important for aspect and opinion terms ex- traction. Therefore, we propose a joint model, de- noted by RNSCN-GRU (RNSCN+-GRU), which integrates a GRU-based recurrent neural network on top of RNSCN (RNSCN+), i.e., the input for GRU is the hidden representations hn learned by RNSCN or RNSCN+ for the n-th token in the sen- tence. For simplicity in presentation, we denote the computation of GRU by using the notation fGRU . To be speciﬁc, by taking hn as input, the ﬁnal fea- ture representation h(cid:48) n for each word is obtained through n = fGRU (h(cid:48) h(cid:48) n−1, hn; Θ), (10) where Θ is the collection of the GRU parameters. The ﬁnal token-level prediction is made through ˆyn = softmax(Wl · h(cid:48) n + bl), where Wl ∈ R5×d(cid:48) transforms a d(cid:48)-dimensional feature vector to class probabilities (note that we (11) y(cid:2)nmauto(cid:5)encoderrnmg(cid:1)g2gj(cid:1)jWencgnmencoder(cid:2)nmdecodeWdecauto(cid:5)encodergroupem(cid:8)eddingrnmhmxnhny(cid:2)nmhave 5 different classes as deﬁned in Section 3). The second joint model, namely RNSCN-CRF, combines a linear-chain CRF with RNSCN to learn the discriminative mapping from high-level fea- tures to labels. The advantage of CRF is to learn sequential interactions between each pair of adja- cent words as well as labels and provide structural outputs. Formally, the joint model aims to output a sequence of labels with maximum conditional probability given its input. Denote by y a sequence of labels for a sentence and by H the embedding matrix for each sentence (each column denotes a hidden feature vector of a word in the sentence learned by RNSCN), the inference is computed as: ˆy= arg max y p(y|H) = arg max y 1 Z(H) c∈C (cid:89) exp(cid:104)Wc, g(H, yc)(cid:105)(12) where C indicates the set of different cliques (unary and pairwise cliques in the context of linear-chain). Wc is tied for each different yc, which indicates the labels for clique c. The operator (cid:104)·, ·(cid:105) is the element-wise multiplication, and g(·) produces the concatenation of {hn}’s in a context window of each word. The above two models both consider the sequential interaction of the words within each sentence, but the formalization and training are totally different. We will report the results for both joint models in the experiment section. 4.4 Training Recall that in our cross-domain setting, the labels for terms extraction are only available in the source domain, but the auxiliary relation labels can be automatically produced for both domains via the dependency parser. Besides the source domain la- beled data DS = {(xSi, ySi)}nS i=1, we denote by j )}nR DR = {(rj, yR j=1 the combined source and tar- get domain data with auxiliary relation labels. For training, the total loss consists of token-prediction loss (cid:96)S and relation-prediction loss (cid:96)R: L = (cid:88) DS (cid:96)S(ySi, ˆySi) + γ (cid:96)R(rj, yR j ), (13) (cid:88) DR where γ is the trade-off parameter, (cid:96)S is the cross- entropy loss between the predicted extraction label in (11) and the ground-truth, and (cid:96)R is deﬁned in (6) for RNSCN+ or (3) for RNSCN. For RNSCN- CRF, the loss becomes the negative log probability of the true label given the corresponding input: (cid:96)S(ySi, ˆySi) = − log(ySi|hSi). (14) Dataset Description Restaurant Laptop Device R L D # Sentences Training Testing 1,460 961 959 4,381 2,884 2,877 5,841 3,845 3,836 Table 1: Data statistics with number of sentences. The parameters for token-level predictions and relation-level predictions are updated jointly such that the information from the auxiliary task could be propagated to the target task to obtain better performance. This idea is in accordance with struc- tural learning proposed by Ando and Zhang (2005), which shows that multiple related tasks are use- ful for ﬁnding the optimal hypothesis space. In our case, the set of multiple tasks includes the tar- get terms extraction task and the auxiliary relation prediction task, which are closely related. The pa- rameters are all shared across domains. The joint model is trained using back-propagation from the top layer of GRU or CRF to RNSCN until reaching to the input word embeddings in the bottom. 5 Experiments 5.1 Data & Experimental Setup The data is taken from the benchmark customer re- views in three different domains, namely restaurant, laptop and digital devices. The restaurant domain contains a combination of restaurant reviews from SemEval 2014 task 4 subtask 1 (Pontiki et al., 2014) and SemEval 2015 task 12 subtask 1 (Pontiki et al., 2015). The laptop domain consists of laptop re- views from SemEval 2014 task 4 subtask 1. For digital device, we take reviews from (Hu and Liu, 2004) containing sentences from 5 digital devices. The statistics for each domain are shown in Table 1. In our experiments, we randomly split the data in each domain into training set and testing set with the proportion being 3:1. To obtain more rigorous result, we make three random splits for each do- main and test the learned model on each split. The number of sentences for training and testing after each split is also shown in Table 1. Each sentence is labeled with aspect terms and opinion terms. For each cross-domain task, we conduct both inductive and transductive experiments. Speciﬁ- cally, we train our model only on the training sets from both (labeled) source and (unlabeled) target domains. For testing, the inductive results are ob- tained using the test data from the target domain, and the transductive results are obtained using the (unlabeled) training data from the target domain. The evaluation metric we used is F1 score. Fol- lowing the setting from existing work, only exact match could be counted as correct. For experimental setup, we use Stanford Depen- dency Parser (Klein and Manning, 2003) to gen- erate dependency trees. There are in total 43 dif- ferent dependency relations, i.e. 43 classes for the auxiliary task. We set the number of latent rela- tion groups as 20. The input word features for RNSCN are pre-trained word embeddings using word2vec (Mikolov et al., 2013) which is trained on 3M reviews from the Yelp dataset2 and electron- ics dataset in Amazon reviews3 (McAuley et al., 2015). The dimension of word embeddings is 100. Because of the relatively small size of the training data compared with the number of parameters, we ﬁrstly pre-train RNSCN for 5 epochs with mini- batch size 30 and rmsprop initialized at 0.01. The joint model of RNSCN+-GRU is then trained with rmsprop initialized at 0.001 and mini-batch size 30. The trade-off parameter α, β and γ are set to be 1, 0.001 and 0.1, respectively. The hidden-layer dimension for GRU is 50, and the context win- dow size is 3 for input feature vectors of GRU. For the joint model of RNSCN-CRF, we implement SGD with a decaying learning rate initialized at 0.02. The context window size is also 3 in this case. Both joint models are trained for 10 epochs. 5.2 Comparison & Results We compared our proposed model with several baselines and variants of the proposed model: • RNCRF: A joint model of recursive neural network and CRF proposed by (Wang et al., 2016) for single-domain aspect and opinion terms extraction. We make all the parameters shared across domains for target prediction. • RNGRU: A joint model of RNN and GRU. The hidden layer of RNN is taken as input for GRU. We share all the parameters across domains, similar to RNCRF. • CrossCRF: A linear-chain CRF with hand- engineered features that are useful for cross- domain settings (Jakob and Gurevych, 2010), e.g., POS tags, dependency relations. • RAP: The Relational Adaptive bootstraPping method proposed by (Li et al., 2012) that uses TrAdaBoost to expand lexicons. 2http://www.yelp.com/dataset challenge 3http://jmcauley.ucsd.edu/data/amazon/links.html • Hier-Joint: A recent deep model proposed by Ding et al. (2017) that achieves state-of- the-art performance on aspect terms extraction across domains. • RNSCN-GRU: Our proposed joint model in- tegrating auxiliary relation prediction task into RNN that is further combined with GRU. • RNSCN-CRF: The second proposed model similar to RNSCN-GRU, which replace GRU with CRF. • RNSCN+-GRU: Our ﬁnal joint model with auto-encoders to reduce auxiliary label noise. Note that we do not implement other recent deep adaptation models for comparison (Chen et al., 2012; Yang and Hospedales, 2015), because Hier- Joint (Ding et al., 2017) has already demonstrated better performances than these models. The overall comparison results with the baselines are shown in Table 2 with average F1 scores and standard deviations over three random splits. Clearly, the re- sults for aspect terms (AS) transfer are much lower than opinion terms (OP) transfer, which indicate that the aspect terms are usually quite different across domains, whereas the opinion terms could be more common and similar. Hence the ability to adapt the aspect extraction from the source do- main to the target domain becomes more crucial. On this behalf, our proposed model shows clear advantage over other baselines for this more dif- ﬁcult transfer problem. Speciﬁcally, we achieve 6.77%, 5.88%, 10.55% improvement over the best- performing baselines for aspect extraction in R→L, L→D and D→L, respectively. By comparing with RNCRF and RNGRU, we show that the structural correspondence network is indeed effective when integrated into RNN. To show the effect of the integration of the au- toencoder, we conduct experiments over different variants of the proposed model in Table 3. RNSCN- GRU represents the model without autoencoder, which achieves much better F1 scores on most ex- periments compared with the baselines in Table 2. RNSCN+-GRU outperforms RNSCN-GRU in al- most all experiments. This indicates the autoen- coder automatically learns data-dependent group- ings, which is able to reduce unnecessary label noise. To further verify that the autoencoder indeed reduces label noise when the parser is inaccurate, we generate new noisy parse trees by replacing some relations within each sentence with a random Models CrossCRF RAP Hier-Joint RNCRF RNGRU RNSCN-CRF RNSCN-GRU RNSCN+-GRU R→L R→D L→R L→D D→R D→L AS 19.72 (1.82) 25.92 (2.75) 33.66 (1.47) 24.26 (3.97) 24.23 (2.41) 35.26 (1.31) 37.77 (0.45) 40.43 (0.96) OP 59.20 (1.34) 62.72 (0.49) - - 60.86 (3.35) 60.65 (1.04) 61.67 (1.35) 62.35 (1.85) 65.85 (1.50) AS 21.07 (0.44) 22.63 (0.52) 33.20 (0.52) 24.31 (2.57) 20.49 (2.68) 32.00 (1.48) 33.02 (0.58) 35.10 (0.62) OP 52.05 (1.67) 54.44 (2.20) - - 51.28 (1.78) 52.28 (2.69) 52.81 (1.29) 57.54 (1.27) 60.17 (0.75) AS 28.19 (0.58) 46.90 (1.64) 48.10 (1.45) 40.88 (2.09) 39.78 (0.61) 53.38 (1.49) 53.18 (0.75) 52.91 (1.82) OP 65.52 (0.89) 67.98 (1.05) - - 66.50 (1.48) 62.99 (0.95) 67.60 (0.99) 71.44 (0.97) 72.51 (1.03) AS 29.96 (1.69) 34.54 (0.64) 31.25 (0.49) 31.52 (1.40) 32.51 (1.12) 34.63 (1.38) 35.65 (0.77) 40.42 (0.70) OP 56.17 (1.49) 54.25 (1.65) - - 55.85 (1.09) 52.24 (2.37) 56.22 (1.10) 60.02 (0.80) 61.15 (0.60) AS 6.59 (0.49) 45.44 (1.61) 47.97 (0.46) 34.59 (1.34) 38.15 (2.82) 48.13 (0.71) 49.62 (0.34) 48.36 (1.14) OP 39.38 (3.06) 60.67 (2.15) - - 63.89 (1.59) 64.21 (1.11) 65.06 (0.66) 69.42 (2.27) 73.75 (1.76) AS 24.22 (2.54) 28.22 (2.42) 34.74 (2.27) 40.59 (0.80) 39.44 (2.79) 46.71 (1.16) 45.92 (1.14) 51.14 (1.68) OP 46.67 (2.43) 59.79 (4.18) - 60.17 (1.20) 60.85 (1.25) 61.88 (1.52) 63.85 (1.97) 71.18 (1.58) Table 2: Comparisons with different baselines. Models RNSCN-GRU RNSCN-GRU (r) RNSCN+-GRU RNSCN+-GRU (r) R→L R→D L→R L→D D→R D→L AS 37.77 32.97 40.43 39.27 OP 62.35 50.18 65.85 59.41 AS 33.02 26.21 35.10 33.42 OP 57.54 53.58 60.17 57.24 AS 53.18 35.88 52.91 45.79 OP 71.44 65.73 72.51 69.96 AS 35.65 32.87 40.42 38.21 OP 60.02 57.57 61.15 59.12 AS 49.62 40.03 48.36 45.36 OP 69.42 67.34 73.75 72.84 AS 45.92 40.06 51.14 50.45 OP 63.85 59.18 71.18 68.05 Table 3: Comparisons with different variants of the proposed model. OUT IN Hier-Joint RNSCN+-GRU* RNSCN+ RNSCN+-GRU Hier-Joint RNSCN+-GRU* RNSCN+ RNSCN+-GRU R→L R→D L→R L→D D→R D→L AS 33.66 39.06 31.60 40.43 32.41 40.34 30.76 41.27 OP - - 65.89 65.85 - - 63.65 65.44 AS 33.20 34.07 24.37 35.10 29.79 30.75 22.48 33.58 OP - - 60.01 60.17 - - 59.24 60.28 AS 48.10 47.98 39.58 52.91 47.04 48.69 39.54 52.48 OP - - 71.03 72.51 - - 70.25 72.10 AS 31.25 38.51 34.40 40.42 31.26 37.40 35.32 39.73 OP - - 60.47 61.15 - - 60.00 60.18 AS 47.97 47.49 41.02 48.36 47.41 46.49 37.75 47.10 OP - - 71.23 73.75 - - 70.64 72.19 AS 34.74 48.49 45.54 51.14 33.80 48.50 43.72 50.23 OP - - 69.00 71.18 - - 68.27 70.21 Table 4: Comparisons with different transfer setting. relation. Speciﬁcally, in each source domain, for each relation that connects to any aspect or opin- ion word, it has 0.5 probability of being replaced by any other relation. In Table 3, We denote the model with noisy relations with (r). Obviously, the performance of RNSCN-GRU without an autoen- coder signiﬁcantly deteriorates when the auxiliary labels are very noisy. On the contrary, RNSCN+- GRU (r) achieves acceptable results compared to RNSCN+-GRU. This proves that the autoencoder makes the model more robust to label noise and helps to adapt the information more accurately to the target data. Note that a large drop for L → R in aspect extraction might be caused by a large por- tion of noisy replacements for this particular data which makes it too hard to train a good classiﬁer. This may not greatly inﬂuence opinion extraction, as shown, because the two domains usually share many common opinion terms. However, the signif- icant difference in aspect terms makes the learning more dependent on common relations. The above comparisons are made using the test data from target domains which are not available during training (i.e., the inductive setting). For more complete comparison, we also conduct exper- iments in the transductive setting. We pick our best model RNSCN+-GRU, and show the effect of dif- ferent components. To do that, we ﬁrst remove the sequential structure on top, resulting in RNSCN+. Moreover, we create another variant by removing opinion term labels to show the effect of the dou- ble propogation between aspect terms and opinion terms. The resulting model is named RNSCN+- GRU*. As shown in Table 4, we denote by OUT and IN the inductive and transductive setting, re- spectively. The results shown are the average F1 scores among three splits4. In general, RNSCN+- GRU shows similar performances for both induc- tive and transductive settings. This indicates the 4We omit standard deviation here due to the limit of space. G 1 2 3 4 5 6 Word this, the, their, my, here, it, I, our, not quality, jukebox, maitre-d, sauces, portions, volume, friend, noodles, calamari in, slightly, often, overall, regularly, since, back, much, ago handy, tastier, white, salty, right, vibrant, ﬁrst, ok get, went, impressed, had, try, said, recommended, call, love is, are, feels, believes, seems, like, will, would Table 5: Case studies on word clustering robustness and the ability to learn well when test data is not presented during training. Without opin- ion labels, RNSCN+-GRU* still achieves better results than Hier-Joint most of the time. Its lower performance compared to RNSCN+-GRU also in- dicates that in the cross-domain setting, the dual information between aspects and opinions is bene- ﬁcial to ﬁnd appropriate and discriminative relation feature space. Finally, the results for RNSCN+ by removing GRU are lower than the joint model, which proves the importance of combining syntac- tic tree structure with sequential modeling. To qualitatively show the effect of the auxiliary task with auto-encoders for clustering syntactically similar words across domains, we provide some case studies on the predicted groups of some words in Table 5. Speciﬁcally, for each relation in the dependency tree, we use (4) to obtain the most probable group to assign the word in the child node. The left column shows the predicted group index with the right column showing the corresponding words. Clearly, the words in the same group have similar syntactic functionalities, whereas the word types vary across groups. In the end, we verify the robustness and capa- bility of the model by conducting sensitivity stud- ies and experiments with varying number of unla- beled target data for training, respectively. Figure 4 shows the sensitivity test for L→D, which indi- cates that changing of the trade-off parameter γ or the number of groups |G| does not affect the model’s performance greatly, i.e., less than 1% for aspect extraction and 2% for opinion extraction. This proves that our model is robust and stable against small variations. Figure 5 compares the results of RNSCN+-GRU with Hier-Joint when increasing the proportion of unlabeled target train- ing data from 0 to 1. Obviously, our model shows steady improvement with the increasing number of unlabeled target data. This pattern proves our (a) On trade-off parameter. (b) On number of groups. Figure 4: Sensitivity studies for L→D. (a) F1-aspect on R→L (b) F1-aspect on D→L Figure 5: F1 vs proportion of unlabeled target data. model’s capability of learning from target domain for adaptation. 6 Conclusion We propose a novel dependency-tree-based RNN, namely RNSCN (or RNSCN+), for domain adap- tation. The model integrates an auxiliary task into representation learning of nodes in the dependency tree. The adaptation takes place in a common re- lation feature space, which builds the structural correspondences using syntactic relations among the words in each sentence. We further develop a joint model to combine RNSCN/RNSCN+ with a sequential labeling model for terms extraction. Acknowledgements This work is supported by NTU Singapore Nanyang Assistant Professorship (NAP) grant M4081532.020, MOE AcRF Tier-1 grant 2016- T1-001-159, and Fuji Xerox Corporation through joint research on Multilingual Semantic Analysis. References Rie Kubota Ando and Tong Zhang. 2005. A framework for learning predictive structures from multiple tasks and unlabeled data. JMLR 6:1817–1853. John Blitzer, Mark Dredze, and Fernando Pereira. 2007. Biographies, bollywood, boomboxes and blenders: 0.10.20.30.40.50.60.70.80.91.00.580.590.600.610.620.630.64f1-opinion0.10.20.30.40.50.60.70.80.91.0trade-off parameter (γ)0.360.370.380.390.400.410.42f1-aspect5101520253035400.580.590.600.610.620.630.64f1-opinion510152025303540number of groups (|G|)0.360.370.380.390.400.410.42f1-aspect0/71/72/73/74/75/76/77/7proportion of unlabeled target data0.280.290.300.310.320.330.34f1 (Hier-Joint)0/71/72/73/74/75/76/77/70.350.360.370.380.390.400.41f1 (ours)0/71/72/73/74/75/76/77/7proportion of unlabeled target data0.300.310.320.330.340.350.36f1 (Hier-Joint)0/71/72/73/74/75/76/77/70.460.470.480.490.500.510.52f1 (ours)Domain adaptation for sentiment classiﬁcation. ACL. pages 187–205. In John Blitzer, Ryan McDonald, and Fernando Pereira. 2006. Domain adaptation with structural correspon- dence learning. In EMNLP. pages 120–128. Danushka Bollegala, Takanori Maehara, and Ken ichi Kawarabayashi. 2015. Unsupervised cross-domain In ACL. pages 730– word representation learning. 740. Minmin Chen, Zhixiang Xu, Kilian Q. Weinberger, and Fei Sha. 2012. Marginalized denoising autoen- coders for domain adaptation. In ICML. pages 1627– 1634. Wenyuan Dai, Qiang Yang, Gui-Rong Xue, and Yong Yu. 2007. Boosting for transfer learning. In ICML. pages 193–200. Marie C. de Marneffe and Christopher D. Manning. 2008. The stanford typed dependencies representa- tion. In CrossParser. pages 1–8. Ying Ding, Jianfei Yu, and Jing Jiang. 2017. Recur- rent neural networks with auxiliary labels for cross- In AAAI. pages domain opinion target extraction. 3436–3442. Xavier Glorot, Antoine Bordes, and Yoshua Bengio. 2011. Domain adaptation for large-scale sentiment classiﬁcation: A deep learning approach. In ICML. pages 97–110. Minqing Hu and Bing Liu. 2004. Mining and summa- rizing customer reviews. In KDD. pages 168–177. Yue Lu, ChengXiang Zhai, and Neel Sundaresan. 2009. Rated aspect summarization of short comments. In WWW. pages 131–140. Julian McAuley, Christopher Targett, Qinfeng Shi, and Anton van den Hengel. 2015. Image-based rec- ommendations on styles and substitutes. In SIGIR. pages 43–52. Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efﬁcient estimation of word represen- tations in vector space. CoRR abs/1301.3781. Sinno Jialin Pan, Xiaochuan Ni, Jian-Tao Sun, Qiang Yang, and Zheng Chen. 2010. Cross-domain senti- ment classiﬁcation via spectral feature alignment. In WWW. pages 751–760. Maria Pontiki, Dimitris Galanis, Haris Papageorgiou, Suresh Manandhar, and Ion Androutsopoulos. 2015. SemEval-2015 task 12: Aspect based sentiment analysis. In SemEval 2015. pages 486–495. Maria Pontiki, Dimitris Galanis, John Pavlopoulos, Harris Papageorgiou, Ion Androutsopoulos, and Suresh Manandhar. 2014. Semeval-2014 task 4: As- In SemEval. pages pect based sentiment analysis. 27–35. Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen. 2011. Opinion word expansion and target extrac- tion through double propagation. Comput. Linguist. 37(1):9–27. Scott E. Reed, Honglak Lee, Dragomir Anguelov, Christian Szegedy, Dumitru Erhan, and Andrew Ra- binovich. 2015. Training deep neural networks on noisy labels with bootstrapping. In ICLR 2015. Niklas Jakob and Iryna Gurevych. 2010. Extracting opinion targets in a single- and cross-domain setting In EMNLP. pages with conditional random ﬁelds. 1035–1045. Richard Socher, Christopher D. Manning, and An- drew Y. Ng. 2010. Learning Continuous Phrase Rep- resentations and Syntactic Parsing with Recursive Neural Networks. In NIPS Workshop. pages 1–9. Wei Jin and Hung Hay Ho. 2009. A novel lexical- ized hmm-based learning framework for web opin- ion mining. In ICML. pages 465–472. Dan Klein and Christopher D. Manning. 2003. Accu- rate unlexicalized parsing. In ACL. pages 423–430. Fangtao Li, Chao Han, Minlie Huang, Xiaoyan Zhu, Ying-Ju Xia, Shu Zhang, and Hao Yu. 2010. Structure-aware review mining and summarization. In COLING. pages 653–661. Fangtao Li, Sinno Jialin Pan, Ou Jin, Qiang Yang, and Xiaoyan Zhu. 2012. Cross-domain co-extraction of In ACL. pages 410– sentiment and topic lexicons. 419. Ivan Titov and Ryan McDonald. 2008. Modeling online reviews with multi-grain topic models. In WWW. pages 111–120. Wenya Wang, Sinno Jialin Pan, Daniel Dahlmeier, and Xiaokui Xiao. 2016. Recursive neural conditional random ﬁelds for aspect-based sentiment analysis. In EMNLP. pages 616–626. Wenya Wang, Sinno Jialin Pan, Daniel Dahlmeier, and Xiaokui Xiao. 2017. Coupled multi-layer tensor net- work for co-extraction of aspect and opinion terms. In AAAI. pages 3316–3322. Yongxin Yang and Timothy M. Hospedales. 2015. A uniﬁed perspective on multi-domain and multi-task learning. In ICLR. Pengfei Liu, Shaﬁq Joty, and Helen Meng. 2015. Fine- grained opinion mining with recurrent neural net- In EMNLP. pages works and word embeddings. 1433–1443. Yichun Yin, Furu Wei, Li Dong, Kaimeng Xu, Ming Zhang, and Ming Zhou. 2016. Unsupervised word and dependency path embeddings for aspect term ex- traction. In IJCAI. pages 2979–2985. Jianfei Yu and Jing Jiang. 2016. Learning sentence em- beddings with auxiliary tasks for cross-domain sen- timent classiﬁcation. In EMNLP. pages 236–246. Lei Zhang, Bing Liu, Suk Hwan Lim, and Eamonn O’Brien-Strain. 2010. Extracting and ranking prod- In COLING. uct features in opinion documents. pages 1462–1470. Meishan Zhang, Yue Zhang, and Duy Tin Vo. 2015. Neural networks for open domain targeted sentiment. In EMNLP. Guangyou Zhou, Zhiwen Xie, Jimmy Xiangji Huang, and Tingting He. 2016. Bi-transferring deep neural networks for domain adaptation. In ACL. pages 322– 332. 