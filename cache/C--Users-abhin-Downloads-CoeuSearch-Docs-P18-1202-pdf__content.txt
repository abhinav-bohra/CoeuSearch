Recursive Neural Structural Correspondence Network Cross-domain Aspect Opinion Co-Extraction Wenya Wang†‡ Sinno Jialin Pan† †Nanyang Technological University , Singapore ‡SAP Innovation Center Singapore { wa    ya , sinnopan } @ ntu.edu.sg Abstract Fine-grained opinion analysis aims ex- tract aspect opinion terms sentence opinion summarization . Su- pervised learning methods proven effective task . However , many domains , lack labeled data hinders learning precise extraction model . case , unsupervised domain adapta- tion methods desired transfer knowl- edge source domain un- labeled target domain . paper , develop novel recursive neural network could reduce domain shift effectively word level syntactic relations . treat relations invariant “ pivot information ” across domains build struc- tural correspondences generate aux- iliary task predict relation two adjacent words dependency tree . end , demonstrate state-of- the-art results three benchmark datasets .   Introduction problem ﬁne-grained opinion analysis in- volves extraction opinion targets ( aspect terms ) opinion expressions ( opinion terms ) review sentence . example , sen- tence : “ offer good appetizers ” , aspect opinion terms appetizers good correspond- ingly . Many supervised deep models pro- posed problem ( Liu et al. ,      ; Yin et al. ,      ; Wang et al. ,      ) , obtained promising results . However , methods fail adapt well across domains , aspect terms two different domains usually disjoint , e.g. , lap- top v.s . restaurant , leading large domain shift feature vector space . Though unsupervised methods ( Hu Liu ,      ; Qiu et al. ,      ) deal data labels , performance unsatisfactory compared supervised ones . number domain adaptation methods coarse-grained sentiment classiﬁcation problems across domains , overall senti- ment polarity sentence document predicted . Nevertheless , approaches exist cross-domain ﬁne-grained opinion analysis due difﬁculties ﬁne-grained adaptation , challenging coarse-grained problems . Li et al . (      ) proposed bootstrap method based TrAdaBoost algorithm ( Dai et al. ,      ) iteratively expand opinion aspect lexicons target domain exploiting source-domain labeled data cross-domain common relations aspect terms opinion terms . However , model requires seed opinion lexicon target domain pre-mined syntactic patterns bridge . Ding et al . (      ) proposed use rules generate auxiliary supervision top recurrent neural network learn domain-invariant hidden representation word . performance highly depends quality manually de- ﬁned rules prior knowledge sentiment lexicon . addition , recurrent structure fails capture syntactic interactions among words in- trinsically opinion extraction . requirement rules makes methods non-ﬂexible . paper , propose novel cross-domain Recursive Neural Network ( RNN )   aspect opinion terms co-extraction across domains . motivations twofold :   ) dependency re- lations capture interactions among different words . relations especially important identifying aspect terms opinion terms ( Qiu et al. ,      ; Wang et al. ,      ) , also domain-invariant within language . There- fore , used “ pivot ” information  Here , use RNN denote recursive neural networks , rather recurrent neural networks . bridge gap different domains .   ) In- spired idea structural learning ( Ando Zhang ,      ) , success target task depends ability ﬁnding good predictive structures learned related tasks , e.g. , structural cor- respondence learning ( SCL ) ( Blitzer et al. ,      ) coarse-grained cross-domain sentiment classiﬁ- cation . , aim generate auxiliary task dependency relation classiﬁcation . Different previous approaches , auxiliary task target extraction task heterogeneous label spaces . aim integrate auxiliary task distributed relation representation learning recursive neural network . Speciﬁcally , generate dependency tree sentence dependency parser con- struct uniﬁed RNN integrates auxiliary task computation node . aux- iliary task classify dependency relation direct edge dependency tree learning relation feature vector . reduce la- bel noise brought inaccurate parsing trees , propose incorporate autoencoder auxiliary task group relations dif- ferent clusters . Finally , model sequential context interaction , develop joint architec- ture combines RNN sequential labeling model aspect opinion terms extraction . Ex- tensive experiments conducted demonstrate advantage proposed model .   Related Work Existing works single-domain aspect/opinion terms extraction include unsupervised methods based association rule mining ( Hu Liu ,      ) , syntactic rule propagation ( Qiu et al. ,      ) topic modeling ( Titov McDonald ,      ; Lu et al. ,      ; Zhang et al. ,      ) , well su- pervised methods based extensive feature engi- neering graphical models ( Jin Ho ,      ; Li et al. ,      ) deep learning ( Liu et al. ,      ; Zhang et al. ,      ; Wang et al. ,      ; Yin et al. ,      ) . Among exiting deep models , improved re- sults obtained using dependency relations ( Yin et al. ,      ; Wang et al. ,      ) , indicates signiﬁcance syntactic word interactions tar- get term extraction . cross-domain setting , works aspect/opinion terms extrac- tion including pipelined approach ( Li et al. ,      ) recurrent neural network ( Ding et al. ,      ) . methods require manual construction common pivot syntactic patterns rules , indicative aspect opinion words . number domain adaptation approaches proposed coarse-grained sentiment classiﬁcation . Among existing methods , one active line focuses projecting original feature spaces two domains low-dimensional space reduce domain shift using pivot features bridge ( Blitzer et al. ,      ; Pan et al. ,      ; Bollegala et al. ,      ; Yu Jiang ,      ) . An- line learns domain-invariant features via auto- encoders ( Glorot et al. ,      ; Chen et al. ,      ; Zhou et al. ,      ) . work related ﬁrst line utilizing pivot information transfer knowledge across domains , integrate idea uniﬁed deep structure fully uti- lize syntactic structure domain adaptation ﬁne-grained sentiment analysis .   Problem Deﬁnition & Motivation task extract opinion aspect terms within review sentence . denote sen- tence sequence tokens x = ( w  , w  , ... , wn ) . output sequence token-level labels = ( y  , y  , ... , yn ) , yi ∈ { BA , IA , BO , IO , N } represents beginning aspect ( BA ) , inside aspect ( IA ) , beginning opinion ( BO ) , inside opinion ( IO ) none ( N ) . subsequence labels started “ BA ” fol- lowed “ IA ” indicates multi-word aspect term . unsupervised domain adaptation , given set labeled review sentences source do- main DS = { ( xSi , ySi ) } nS i=  , set unlabeled sentences target domain DT = { xTj } nT j=  . goal predict token-level labels DT . Existing works cross-domain aspect and/or opinion terms extraction require hand-coded rules sentiment lexicon order transfer knowl- edge across domains . example Figure   , given review sentence “ offer good appe- tizers ” source domain “ laptop nice screen ” target domain . nice extracted common sentiment word , “ OPINION-amod-ASPECT ” identiﬁed common syntactic pattern source do- main , screen could deduced aspect term us- ing identiﬁed syntactic pattern ( Li et al. ,      ) . Similarly , Ding et al . (      ) used set pre- deﬁned rules based syntactic relations sentiment lexicon generate auxiliary labels learn high-level feature representations Figure   : example two reviews similar syntactic patterns . recurrent neural network . one hand , previous attempts ver- iﬁed syntactic information words , used bridge domains , crucial domain adaptation . hand , dependency-tree-based RNN ( Socher et al. ,      ) proven effective learn high-level fea- ture representation word encoding syn- tactic relations aspect terms opinion terms ( Wang et al. ,      ) . ﬁndings , propose novel RNN named Recursive Neural Structural Correspondence Network ( RNSCN ) learn high-level representation word across different domains . model built upon depen- dency trees generated dependency parser . Different previous approaches , re- quire hand-coded rules pre-selected pivot features construct correspondences , rather focus automatically generated dependency relations pivots . model associates direct edge tree relation feature vector , used predict corresponding depen- dency relation auxiliary task . Note relation vector key model : associates two interacting words used construct structural correspondences two different domains . Hence , aux- iliary task guides learning relation vectors , turn affects correspondingly interac- tive words . Speciﬁcally Figure   , relation vector “ amod ” computed features child parent words , also used pro- duce hidden representation parent . relation path sentences , auxiliary task enforces close proximity two relation vectors . pushes hidden representations parent nodes appetizers screen closer , provided good nice sim- ilar representations . word , auxiliary task bridges gap two different domains drawing words similar syntactic properties closer . However , relation vectors may sensitive accuracy dependency parser . might harm learning process noise ex- ists certain relations , especially informal texts . problem noisy labels ad- dressed using perceptual consistency ( Reed et al. ,      ) . Inspired taxonomy dependency re- lations ( de Marneffe Manning ,      ) , relations similar functionalities could grouped to- gether , e.g. , dobj , iobj pobj indicate objects . propose use auto-encoder automatically group relations unsupervised manner . reconstruction loss serves consistency objective reduces label noise aligning rela- tion features intrinsic relation group .   Proposed Methodology model consists two components . ﬁrst component Recursive Neural Structural Cor- respondence Network ( RNSCN ) , second component sequence labeling classiﬁer . paper , focus Gated Recurrent Unit ( GRU ) implementation sequence labeling classi- ﬁer . choose GRU able deal long-term dependencies compared simple Re- current neural network requires less parameters making easier train LSTM . resultant deep learning model denoted RNSCN-GRU . also implement Conditional Random Field sequence labeling classiﬁer , denote model RNSCN-CRF accordingly . overall architecture RNSCN-GRU with- auto-encoder relation denoising shown Figure   . left right two example sentences source target domain , respectively . ﬁrst component , RNSCN , auxiliary task predict dependency relation direct edge integrated dependency- tree-based RNN . generate relation vector direct edge child node parent node , use predict relation produce hidden representation parent node de- pendency tree . address issues noisy rela- tion labels , incorporate auto-encoder RNSCN , shown Figure   . RNSCN mainly focuses syntactic in- teractions among words , second component , GRU , aims compute linear-context interactions . GRU takes hidden representation word computed RNSCN inputs pro- duces ﬁnal representation word taking linear contexts consideration . describe component detail following sections . Theyo ( cid:  ) ergoodappetizersnsubjdobjamodThelaptophasascreennicedetdetnsubjdobjamodRESTAURANTLAPTOP Figure   : architecture RNSCN-GRU .  .  Recursive Neural Structural Correspondence Network Moreover , relation vector r   used auxiliary task relation prediction : RNSCN built dependency tree sen- tence , pre-generated dependency parser . Speciﬁcally , node tree asso- ciated word wn , input word embedding xn ∈ Rd transformed hidden representation hn ∈ Rd . direct edge dependency tree associates relation feature vector rnm ∈ Rd nm ∈ RK , true relation label vector yR K total number dependency relations , n denote indices parent child word dependency edge , respectively . Based dependency tree , hidden representations generated recursive manner leaf nodes reaching root node . Consider source- domain sentence shown Figure   illustra- tive example , ﬁrst compute hidden representa- tions leaf nodes good : h =tanh ( Wxx  + b ) , h =tanh ( Wxx  + b ) , Wx ∈ Rd×d transforms word embeddings hidden space . non-leaf node appetizer , ﬁrst generate relation vector r   depen- dency edge x  ( appetizers ) amod −−−−→ x  ( good ) r   = tanh ( Whh  + Wxx  ) , Wh ∈ Rd×d transforms hidden repre- sentation relation vector space . compute hidden representation appetizer : h  = tanh ( Wamodr   + Wxx  + b ) . ˆyR    = softmax ( WRr   + bR ) , WR ∈ RK×d relation classiﬁca- tion matrix . supervised relation classiﬁer en- forces close proximity similar { rnm } ’ dis- tributed relation vector space . relation features bridge gap word representations different domains incorporating forward computations . general , hidden representation hn non-leaf node produced hn = tanh ( ( cid:   ) WRnmrnm + Wxxn + b ) , (   ) m∈Mn rnm = tanh ( Wh ·hm +Wx ·xn ) , Mn set child nodes wn , WRnm relation transformation matrix tied relation Rnm . predicted label vector ˆyR nm rnm ˆyR nm = softmax ( WR · rnm + bR ) . (   ) adopt cross-entropy loss re- lation classiﬁcation predicted label vector ˆyR nm ground-truth yR nm encode relation side information feature learning : ( cid:   ) R = K ( cid:   ) k=  −yR nm [ k ] log ˆyR nm [ k ] . (   ) auxiliary task , similar relations en- force participating words close appetizersgoodo ( cid:   ) ertheyrootdobjamodnsubjh h h ( cid:   ) h r  ( cid:   ) r  r  x ( cid:   ) x x x y ( cid:  )   y ( cid:  )   ( cid:   ) ( cid:  )   niceahaslaptopdetamodnsubjh h h ( cid:   ) h r  ( cid:   ) r  r  x x x x y ( cid:  )   ( cid:   ) ( cid:  )   y ( cid:  )   x x ( cid:   ) Thescreendobjh r  y ( cid:  )   deth r  y ( cid:  )   h ( cid:  ) ( cid:   ) h ( cid:  )  h ( cid:  )  h ( cid:  ) ( cid:   ) h ( cid:  )  h ( cid:  )  h ( cid:  )  h ( cid:  )  h ( cid:  )  h ( cid:  )  RNSCNGRUSourceTarget words similar syntactic functionalities clustered across domains . hand , pre-trained word embeddings group semantically- similar words . taking input RNN , together auxiliary task , model encodes semantic syntactic information .  .  Reduce Label Noise Auto-encoders discussed Section   , might hard learn accurate relation classiﬁer class unique relation , dependency parser may generate incorrect relations noisy labels . address , propose integrate autoencoder RNSCN . Suppose set latent groups relations : G = {   ,   , ... , |G| } , rela- tion belongs one group . relation vector , rnm , autoencoder performed feeding auxiliary classiﬁer (   ) . goal encode relation vector probability dis- tribution assigning relation group . seen Figure   , relation vector rnm ﬁrst passed autoencoder follows , p ( Gnm = i|rnm ) = exp ( r ( cid:   ) nmWencgi ) exp ( r ( cid:   ) nmWencgj ) ( cid:   ) j∈G , (   ) Gnm denotes inherent relation group rnm , gi ∈ Rd represents feature embedding group , Wenc∈Rd×d encoding matrix computes bilinear interactions relation vector rnm relation group embedding gi . Thus , p ( Gnm = i|rnm ) represents probability rnm mapped group . accumulated relation group embedding computed : gnm = |G| ( cid:   ) i=  p ( Gnm = i|rnm ) gi . (   ) decoding , decoder takes gnm input tries reconstruct relation feature input rnm . Moreover , gnm also used higher-level fea- ture vector rnm predicting relation label . Therefore , objective auxiliary task (   ) becomes : ( cid:   ) R = ( cid:   ) R  + α ( cid:   ) R  + β ( cid:   ) R  ,  ( cid:   ) R  = ( cid:    ) rnm − Wdecgnm ( cid:    )     , ( cid:   ) R  = ( cid:   ) R  = K ( cid:   ) −yR nm [ k ] log ˆyR nm [ k ] , k=  ( cid:   ) ( cid:   )   ( cid:   ) − ¯G ( cid:   ) ¯G ( cid:   ) ( cid:   ) ( cid:   ) F . (   ) (   ) (   ) (   ) Figure   : autoencoder relation grouping . ( cid:   ) R  reconstruction loss Wdec decoding matrix , ( cid:   ) R  follows (   ) ˆyR nm = softmax ( WRgnm + bR ) ( cid:   ) R  regularization term correlations among la- tent groups identity matrix ¯G normalized group embedding matrix consists normalized gi ’ column vectors . regularization term enforces orthogonality gi gj ( cid:   ) = j. α β used con- trol trade-off among different losses . auto-encoder , auxiliary task relation classi- ﬁcation conditioned group assignment . reconstruction loss ensures consistency relation features groupings , supposed dominate classiﬁcation loss observed labels inaccurate . denote RNSCN auto-encoder RNSCN+ .  .  Joint Models Sequence Labeling RNSCN RNSCN+ focuses capturing rep- resenting syntactic relations build bridge be- tween domains learn powerful represen- tations tokens . However , ignores linear- chain correlations among tokens within sentence , important aspect opinion terms ex- traction . Therefore , propose joint model , de- noted RNSCN-GRU ( RNSCN+-GRU ) , integrates GRU-based recurrent neural network top RNSCN ( RNSCN+ ) , i.e. , input GRU hidden representations hn learned RNSCN RNSCN+ n-th token sen- tence . simplicity presentation , denote computation GRU using notation fGRU . speciﬁc , taking hn input , ﬁnal fea- ture representation h ( cid:   ) n word obtained  n = fGRU ( h ( cid:   ) h ( cid:   ) n−  , hn ; Θ ) , (    ) Θ collection GRU parameters . ﬁnal token-level prediction made ˆyn = softmax ( Wl · h ( cid:   ) n + bl ) , Wl ∈ R ×d ( cid:   ) transforms ( cid:   ) -dimensional feature vector class probabilities ( note (    ) ( cid:  ) nmauto ( cid:  ) encoderrnmg ( cid:  ) g gj ( cid:  ) jWencgnmencoder ( cid:  ) nmdecodeWdecauto ( cid:  ) encodergroupem ( cid:  ) eddingrnmhmxnhny ( cid:  ) nm   different classes deﬁned Section   ) . second joint model , namely RNSCN-CRF , combines linear-chain CRF RNSCN learn discriminative mapping high-level fea- tures labels . advantage CRF learn sequential interactions pair adja- cent words well labels provide structural outputs . Formally , joint model aims output sequence labels maximum conditional probability given input . Denote sequence labels sentence H embedding matrix sentence ( column denotes hidden feature vector word sentence learned RNSCN ) , inference computed : ˆy= arg max  p ( y|H ) = arg max    Z ( H ) c∈C ( cid:   ) exp ( cid:    ) Wc , g ( H , yc ) ( cid:    ) (    ) C indicates set different cliques ( unary pairwise cliques context linear-chain ) . Wc tied different yc , indicates labels clique c. operator ( cid:    ) · , · ( cid:    ) element-wise multiplication , g ( · ) produces concatenation { hn } ’ context window word . two models consider sequential interaction words within sentence , formalization training totally different . report results joint models experiment section .  .  Training Recall cross-domain setting , labels terms extraction available source domain , auxiliary relation labels automatically produced domains via dependency parser . Besides source domain la- beled data DS = { ( xSi , ySi ) } nS i=  , denote j ) } nR DR = { ( rj , yR j=  combined source tar- get domain data auxiliary relation labels . training , total loss consists token-prediction loss ( cid:   ) relation-prediction loss ( cid:   ) R : L = ( cid:   ) DS ( cid:   ) ( ySi , ˆySi ) + γ ( cid:   ) R ( rj , yR j ) , (    ) ( cid:   ) DR γ trade-off parameter , ( cid:   ) cross- entropy loss predicted extraction label (    ) ground-truth , ( cid:   ) R deﬁned (   ) RNSCN+ (   ) RNSCN . RNSCN- CRF , loss becomes negative log probability true label given corresponding input : ( cid:   ) ( ySi , ˆySi ) = − log ( ySi|hSi ) . (    ) Dataset Description Restaurant Laptop Device R L  # Sentences Training Testing  ,             ,     ,     ,     ,     ,     ,    Table   : Data statistics number sentences . parameters token-level predictions relation-level predictions updated jointly information auxiliary task could propagated target task obtain better performance . idea accordance struc- tural learning proposed Ando Zhang (      ) , shows multiple related tasks use- ful ﬁnding optimal hypothesis space . case , set multiple tasks includes tar- get terms extraction task auxiliary relation prediction task , closely related . pa- rameters shared across domains . joint model trained using back-propagation top layer GRU CRF RNSCN reaching input word embeddings bottom .   Experiments  .  Data & Experimental Setup data taken benchmark customer re- views three different domains , namely restaurant , laptop digital devices . restaurant domain contains combination restaurant reviews SemEval      task   subtask   ( Pontiki et al. ,      ) SemEval      task    subtask   ( Pontiki et al. ,      ) . laptop domain consists laptop re- views SemEval      task   subtask   . digital device , take reviews ( Hu Liu ,      ) containing sentences   digital devices . statistics domain shown Table   . experiments , randomly split data domain training set testing set proportion  :  . obtain rigorous result , make three random splits do- main test learned model split . number sentences training testing split also shown Table   . sentence labeled aspect terms opinion terms . cross-domain task , conduct inductive transductive experiments . Speciﬁ- cally , train model training sets ( labeled ) source ( unlabeled ) target domains . testing , inductive results ob- tained using test data target domain , transductive results obtained using ( unlabeled ) training data target domain . evaluation metric used F  score . Fol- lowing setting existing work , exact match could counted correct . experimental setup , use Stanford Depen- dency Parser ( Klein Manning ,      ) gen- erate dependency trees . total    dif- ferent dependency relations , i.e .    classes auxiliary task . set number latent rela- tion groups    . input word features RNSCN pre-trained word embeddings using word vec ( Mikolov et al. ,      ) trained  M reviews Yelp dataset  electron- ics dataset Amazon reviews  ( McAuley et al. ,      ) . dimension word embeddings     . relatively small size training data compared number parameters , ﬁrstly pre-train RNSCN   epochs mini- batch size    rmsprop initialized  .   . joint model RNSCN+-GRU trained rmsprop initialized  .    mini-batch size    . trade-off parameter α , β γ set   ,  .     .  , respectively . hidden-layer dimension GRU    , context win- dow size   input feature vectors GRU . joint model RNSCN-CRF , implement SGD decaying learning rate initialized  .   . context window size also   case . joint models trained    epochs .  .  Comparison & Results compared proposed model several baselines variants proposed model : • RNCRF : joint model recursive neural network CRF proposed ( Wang et al. ,      ) single-domain aspect opinion terms extraction . make parameters shared across domains target prediction . • RNGRU : joint model RNN GRU . hidden layer RNN taken input GRU . share parameters across domains , similar RNCRF . • CrossCRF : linear-chain CRF hand- engineered features useful cross- domain settings ( Jakob Gurevych ,      ) , e.g. , POS tags , dependency relations . • RAP : Relational Adaptive bootstraPping method proposed ( Li et al. ,      ) uses TrAdaBoost expand lexicons .  http : //www.yelp.com/dataset challenge  http : //jmcauley.ucsd.edu/data/amazon/links.html • Hier-Joint : recent deep model proposed Ding et al . (      ) achieves state-of- the-art performance aspect terms extraction across domains . • RNSCN-GRU : proposed joint model in- tegrating auxiliary relation prediction task RNN combined GRU . • RNSCN-CRF : second proposed model similar RNSCN-GRU , replace GRU CRF . • RNSCN+-GRU : ﬁnal joint model auto-encoders reduce auxiliary label noise . Note implement recent deep adaptation models comparison ( Chen et al. ,      ; Yang Hospedales ,      ) , Hier- Joint ( Ding et al. ,      ) already demonstrated better performances models . overall comparison results baselines shown Table   average F  scores standard deviations three random splits . Clearly , re- sults aspect terms ( ) transfer much lower opinion terms ( OP ) transfer , indicate aspect terms usually quite different across domains , whereas opinion terms could common similar . Hence ability adapt aspect extraction source do- main target domain becomes crucial . behalf , proposed model shows clear advantage baselines dif- ﬁcult transfer problem . Speciﬁcally , achieve  .   % ,  .   % ,   .   % improvement best- performing baselines aspect extraction R→L , L→D D→L , respectively . comparing RNCRF RNGRU , show structural correspondence network indeed effective integrated RNN . show effect integration au- toencoder , conduct experiments different variants proposed model Table   . RNSCN- GRU represents model without autoencoder , achieves much better F  scores ex- periments compared baselines Table   . RNSCN+-GRU outperforms RNSCN-GRU al- experiments . indicates autoen- coder automatically learns data-dependent group- ings , able reduce unnecessary label noise . verify autoencoder indeed reduces label noise parser inaccurate , generate new noisy parse trees replacing relations within sentence random Models CrossCRF RAP Hier-Joint RNCRF RNGRU RNSCN-CRF RNSCN-GRU RNSCN+-GRU R→L R→D L→R L→D D→R D→L    .   (  .   )   .   (  .   )   .   (  .   )   .   (  .   )   .   (  .   )   .   (  .   )   .   (  .   )   .   (  .   ) OP   .   (  .   )   .   (  .   ) - -   .   (  .   )   .   (  .   )   .   (  .   )   .   (  .   )   .   (  .   )    .   (  .   )   .   (  .   )   .   (  .   )   .   (  .   )   .   (  .   )   .   (  .   )   .   (  .   )   .   (  .   ) OP   .   (  .   )   .   (  .   ) - -   .   (  .   )   .   (  .   )   .   (  .   )   .   (  .   )   .   (  .   )    .   (  .   )   .   (  .   )   .   (  .   )   .   (  .   )   .   (  .   )   .   (  .   )   .   (  .   )   .   (  .   ) OP   .   (  .   )   .   (  .   ) - -   .   (  .   )   .   (  .   )   .   (  .   )   .   (  .   )   .   (  .   )    .   (  .   )   .   (  .   )   .   (  .   )   .   (  .   )   .   (  .   )   .   (  .   )   .   (  .   )   .   (  .   ) OP   .   (  .   )   .   (  .   ) - -   .   (  .   )   .   (  .   )   .   (  .   )   .   (  .   )   .   (  .   )   .   (  .   )   .   (  .   )   .   (  .   )   .   (  .   )   .   (  .   )   .   (  .   )   .   (  .   )   .   (  .   ) OP   .   (  .   )   .   (  .   ) - -   .   (  .   )   .   (  .   )   .   (  .   )   .   (  .   )   .   (  .   )    .   (  .   )   .   (  .   )   .   (  .   )   .   (  .   )   .   (  .   )   .   (  .   )   .   (  .   )   .   (  .   ) OP   .   (  .   )   .   (  .   ) -   .   (  .   )   .   (  .   )   .   (  .   )   .   (  .   )   .   (  .   ) Table   : Comparisons different baselines . Models RNSCN-GRU RNSCN-GRU ( r ) RNSCN+-GRU RNSCN+-GRU ( r ) R→L R→D L→R L→D D→R D→L    .     .     .     .   OP   .     .     .     .      .     .     .     .   OP   .     .     .     .      .     .     .     .   OP   .     .     .     .      .     .     .     .   OP   .     .     .     .      .     .     .     .   OP   .     .     .     .      .     .     .     .   OP   .     .     .     .   Table   : Comparisons different variants proposed model .   Hier-Joint RNSCN+-GRU * RNSCN+ RNSCN+-GRU Hier-Joint RNSCN+-GRU * RNSCN+ RNSCN+-GRU R→L R→D L→R L→D D→R D→L    .     .     .     .     .     .     .     .   OP - -   .     .   - -   .     .      .     .     .     .     .     .     .     .   OP - -   .     .   - -   .     .      .     .     .     .     .     .     .     .   OP - -   .     .   - -   .     .      .     .     .     .     .     .     .     .   OP - -   .     .   - -   .     .      .     .     .     .     .     .     .     .   OP - -   .     .   - -   .     .      .     .     .     .     .     .     .     .   OP - -   .     .   - -   .     .   Table   : Comparisons different transfer setting . relation . Speciﬁcally , source domain , relation connects aspect opin- ion word ,  .  probability replaced relation . Table   , denote model noisy relations ( r ) . Obviously , performance RNSCN-GRU without autoen- coder signiﬁcantly deteriorates auxiliary labels noisy . contrary , RNSCN+- GRU ( r ) achieves acceptable results compared RNSCN+-GRU . proves autoencoder makes model robust label noise helps adapt information accurately target data . Note large drop L → R aspect extraction might caused large por- tion noisy replacements particular data makes hard train good classiﬁer . may greatly inﬂuence opinion extraction , shown , two domains usually share many common opinion terms . However , signif- icant difference aspect terms makes learning dependent common relations . comparisons made using test data target domains available training ( i.e. , inductive setting ) . complete comparison , also conduct exper- iments transductive setting . pick best model RNSCN+-GRU , show effect dif- ferent components . , ﬁrst remove sequential structure top , resulting RNSCN+ . Moreover , create another variant removing opinion term labels show effect dou- ble propogation aspect terms opinion terms . resulting model named RNSCN+- GRU * . shown Table   , denote inductive transductive setting , re- spectively . results shown average F  scores among three splits  . general , RNSCN+- GRU shows similar performances induc- tive transductive settings . indicates  We omit standard deviation due limit space . G             Word , , , , , , , , quality , jukebox , maitre-d , sauces , portions , volume , friend , noodles , calamari , slightly , often , overall , regularly , since , back , much , ago handy , tastier , white , salty , right , vibrant , ﬁrst , ok get , went , impressed , , try , said , recommended , call , love , , feels , believes , seems , like , , would Table   : Case studies word clustering robustness ability learn well test data presented training . Without opin- ion labels , RNSCN+-GRU * still achieves better results Hier-Joint time . lower performance compared RNSCN+-GRU also in- dicates cross-domain setting , dual information aspects opinions bene- ﬁcial ﬁnd appropriate discriminative relation feature space . Finally , results RNSCN+ removing GRU lower joint model , proves importance combining syntac- tic tree structure sequential modeling . qualitatively show effect auxiliary task auto-encoders clustering syntactically similar words across domains , provide case studies predicted groups words Table   . Speciﬁcally , relation dependency tree , use (   ) obtain probable group assign word child node . left column shows predicted group index right column showing corresponding words . Clearly , words group similar syntactic functionalities , whereas word types vary across groups . end , verify robustness capa- bility model conducting sensitivity stud- ies experiments varying number unla- beled target data training , respectively . Figure   shows sensitivity test L→D , indi- cates changing trade-off parameter γ number groups |G| affect model ’ performance greatly , i.e. , less   % aspect extraction   % opinion extraction . proves model robust stable small variations . Figure   compares results RNSCN+-GRU Hier-Joint increasing proportion unlabeled target train- ing data     . Obviously , model shows steady improvement increasing number unlabeled target data . pattern proves ( ) trade-off parameter . ( b ) number groups . Figure   : Sensitivity studies L→D . ( ) F -aspect R→L ( b ) F -aspect D→L Figure   : F  vs proportion unlabeled target data . model ’ capability learning target domain adaptation .   Conclusion propose novel dependency-tree-based RNN , namely RNSCN ( RNSCN+ ) , domain adap- tation . model integrates auxiliary task representation learning nodes dependency tree . adaptation takes place common re- lation feature space , builds structural correspondences using syntactic relations among words sentence . develop joint model combine RNSCN/RNSCN+ sequential labeling model terms extraction . Acknowledgements work supported NTU Singapore Nanyang Assistant Professorship ( NAP ) grant M       .    , MOE AcRF Tier-  grant     - T -   -    , Fuji Xerox Corporation joint research Multilingual Semantic Analysis . References Rie Kubota Ando Tong Zhang .      . framework learning predictive structures multiple tasks unlabeled data . JMLR  :    –     . John Blitzer , Mark Dredze , Fernando Pereira .      . Biographies , bollywood , boomboxes blenders :  .  .  .  .  .  .  .  .  .  .  .   .   .   .   .   .   .  f -opinion .  .  .  .  .  .  .  .  .  . trade-off parameter ( γ )  .   .   .   .   .   .   .  f -aspect                .   .   .   .   .   .   .  f -opinion               number groups ( |G| )  .   .   .   .   .   .   .  f -aspect /  /  /  /  /  /  /  / proportion unlabeled target data .   .   .   .   .   .   .  f  ( Hier-Joint )  /  /  /  /  /  /  /  /  .   .   .   .   .   .   .  f  ( )  /  /  /  /  /  /  /  / proportion unlabeled target data .   .   .   .   .   .   .  f  ( Hier-Joint )  /  /  /  /  /  /  /  /  .   .   .   .   .   .   .  f  ( ) Domain adaptation sentiment classiﬁcation . ACL . pages    –    .  John Blitzer , Ryan McDonald , Fernando Pereira .      . Domain adaptation structural correspon- dence learning . EMNLP . pages    –    . Danushka Bollegala , Takanori Maehara , Ken ichi Kawarabayashi .      . Unsupervised cross-domain ACL . pages    – word representation learning .     . Minmin Chen , Zhixiang Xu , Kilian Q. Weinberger , Fei Sha .      . Marginalized denoising autoen- coders domain adaptation . ICML . pages     –      . Wenyuan Dai , Qiang Yang , Gui-Rong Xue , Yong Yu .      . Boosting transfer learning . ICML . pages    –    . Marie C. de Marneffe Christopher D. Manning .      . stanford typed dependencies representa- tion . CrossParser . pages  –  . Ying Ding , Jianfei Yu , Jing Jiang .      . Recur- rent neural networks auxiliary labels cross- AAAI . pages domain opinion target extraction .     –     . Xavier Glorot , Antoine Bordes , Yoshua Bengio .      . Domain adaptation large-scale sentiment classiﬁcation : deep learning approach . ICML . pages   –    . Minqing Hu Bing Liu .      . Mining summa- rizing customer reviews . KDD . pages    –    . Yue Lu , ChengXiang Zhai , Neel Sundaresan .      . Rated aspect summarization short comments . WWW . pages    –    . Julian McAuley , Christopher Targett , Qinfeng Shi , Anton van den Hengel .      . Image-based rec- ommendations styles substitutes . SIGIR . pages   –   . Tomas Mikolov , Kai Chen , Greg Corrado , Jeffrey Dean .      . Efﬁcient estimation word represen- tations vector space . CoRR abs/    .     . Sinno Jialin Pan , Xiaochuan Ni , Jian-Tao Sun , Qiang Yang , Zheng Chen .      . Cross-domain senti- ment classiﬁcation via spectral feature alignment . WWW . pages    –    . Maria Pontiki , Dimitris Galanis , Haris Papageorgiou , Suresh Manandhar , Ion Androutsopoulos .      . SemEval-     task    : Aspect based sentiment analysis . SemEval     . pages    –    . Maria Pontiki , Dimitris Galanis , John Pavlopoulos , Harris Papageorgiou , Ion Androutsopoulos , Suresh Manandhar .      . Semeval-     task   : As- SemEval . pages pect based sentiment analysis .   –   . Guang Qiu , Bing Liu , Jiajun Bu , Chun Chen .      . Opinion word expansion target extrac- tion double propagation . Comput . Linguist .    (   ) : –   . Scott E. Reed , Honglak Lee , Dragomir Anguelov , Christian Szegedy , Dumitru Erhan , Andrew Ra- binovich .      . Training deep neural networks noisy labels bootstrapping . ICLR      . Niklas Jakob Iryna Gurevych .      . Extracting opinion targets single- cross-domain setting EMNLP . pages conditional random ﬁelds .     –     . Richard Socher , Christopher D. Manning , An- drew Y. Ng .      . Learning Continuous Phrase Rep- resentations Syntactic Parsing Recursive Neural Networks . NIPS Workshop . pages  –  . Wei Jin Hung Hay Ho .      . novel lexical- ized hmm-based learning framework web opin- ion mining . ICML . pages    –    . Dan Klein Christopher D. Manning .      . Accu- rate unlexicalized parsing . ACL . pages    –    . Fangtao Li , Chao Han , Minlie Huang , Xiaoyan Zhu , Ying-Ju Xia , Shu Zhang , Hao Yu .      . Structure-aware review mining summarization . COLING . pages    –    . Fangtao Li , Sinno Jialin Pan , Ou Jin , Qiang Yang , Xiaoyan Zhu .      . Cross-domain co-extraction ACL . pages    – sentiment topic lexicons .     . Ivan Titov Ryan McDonald .      . Modeling online reviews multi-grain topic models .  WWW . pages    –    . Wenya Wang , Sinno Jialin Pan , Daniel Dahlmeier , Xiaokui Xiao .      . Recursive neural conditional random ﬁelds aspect-based sentiment analysis . EMNLP . pages    –    . Wenya Wang , Sinno Jialin Pan , Daniel Dahlmeier , Xiaokui Xiao .      . Coupled multi-layer tensor net- work co-extraction aspect opinion terms . AAAI . pages     –     . Yongxin Yang Timothy M. Hospedales .      . uniﬁed perspective multi-domain multi-task learning . ICLR . Pengfei Liu , Shaﬁq Joty , Helen Meng .      . Fine- grained opinion mining recurrent neural net- EMNLP . pages works word embeddings .     –     . Yichun Yin , Furu Wei , Li Dong , Kaimeng Xu , Ming Zhang , Ming Zhou .      . Unsupervised word dependency path embeddings aspect term ex- traction . IJCAI . pages     –     . Jianfei Yu Jing Jiang .      . Learning sentence em- beddings auxiliary tasks cross-domain sen- timent classiﬁcation . EMNLP . pages    –    . Lei Zhang , Bing Liu , Suk Hwan Lim , Eamonn ’ Brien-Strain .      . Extracting ranking prod- COLING . uct features opinion documents . pages     –     . Meishan Zhang , Yue Zhang , Duy Tin Vo .      . Neural networks open domain targeted sentiment . EMNLP . Guangyou Zhou , Zhiwen Xie , Jimmy Xiangji Huang , Tingting .      . Bi-transferring deep neural networks domain adaptation . ACL . pages    –     . 