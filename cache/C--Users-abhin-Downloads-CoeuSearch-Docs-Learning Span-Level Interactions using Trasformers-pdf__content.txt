Learning Span-Level Interactions for Aspect Sentiment Triplet Extraction Lu Xu* 1, 2, Yew Ken Chia* 1, 2, Lidong Bing2 1 Singapore University of Technology and Design 2 DAMO Academy, Alibaba Group {xu lu,yewken chia}@mymail.sutd.edu.sg l.bing@alibaba-inc.com Abstract Aspect Sentiment Triplet Extraction (ASTE) is the most recent subtask of ABSA which outputs triplets of an aspect target, its asso- ciated sentiment, and the corresponding opin- ion term. Recent models perform the triplet extraction in an end-to-end manner but heav- ily rely on the interactions between each tar- get word and opinion word. Thereby, they cannot perform well on targets and opinions which contain multiple words. Our proposed span-level approach explicitly considers the interaction between the whole spans of tar- gets and opinions when predicting their sen- timent relation. Thus, it can make predic- tions with the semantics of whole spans, en- suring better sentiment consistency. To ease the high computational cost caused by span enumeration, we propose a dual-channel span pruning strategy by incorporating supervision from the Aspect Term Extraction (ATE) and Opinion Term Extraction (OTE) tasks. This strategy not only improves computational efﬁ- ciency but also distinguishes the opinion and target spans more properly. Our framework si- multaneously achieves strong performance for the ASTE as well as ATE and OTE tasks. In particular, our analysis shows that our span- level approach achieves more signiﬁcant im- provements over the baselines on triplets with multi-word targets or opinions. 1 1 Introduction Aspect-Based Sentiment Analysis (ABSA) (Liu, 2012; Pontiki et al., 2014) is an aggregation of sev- eral ﬁne-grained sentiment analysis tasks, and its various subtasks are designed with the aspect tar- get as the fundamental item. For the example in ∗ Equal contribution. Lu Xu and Yew Ken Chia are under the Joint PhD Program between Alibaba and Singapore University of Technology and Design. 1We make our code publicly available at https:// github.com/chiayewken/Span-ASTE. Figure 1: An example of ASTE. The spans highlighted in orange are target terms, and the span in blue is opin- ion term. The “-” on top of target terms indicates nega- tive sentiment. Figure 1, the aspect targets are “Windows 8” and “touchscreen functions”. Aspect Sentiment Classi- ﬁcation (ASC) (Dong et al., 2014; Zhang et al., 2016; Yang et al., 2017; Li et al., 2018a; Tang et al., 2019) is one of the most well-explored subtasks of ABSA and aims to predict the sentiment polarity of a given aspect target. However, it is not always practical to assume that the aspect target is pro- vided. Aspect Term Extraction (ATE) (Yin et al., 2016; Li et al., 2018b; Ma et al., 2019) focuses on extracting aspect targets, while Opinion Term Extraction (OTE) (Yang and Cardie, 2012; Klinger and Cimiano, 2013; Yang and Cardie, 2013) aims to extract the opinion terms which largely determine the sentiment polarity of the sentence or the cor- responding target term. Aspect Sentiment Triplet Extraction (ASTE) (Peng et al., 2019) is the most recently proposed subtask of ABSA, which forms a more complete picture of the sentiment informa- tion through the triplet of an aspect target term, the corresponding opinion term, and the expressed sen- timent. For the example in Figure 1, there are two triplets: (“Windows 8”, “not enjoy”, Negative) and (“touchscreen functions”, “not enjoy”, Negative). The initial approach to ASTE (Peng et al., 2019) was a two-stage pipeline. The ﬁrst stage extracts target terms and their sentiments via a joint labeling scheme 2, as well as the opinion terms with stan- 2For example, the joint tag “B-POS” denotes the beginning --dard BIOES 3 tags. The second stage then couples the extracted target and opinion terms to determine their paired sentiment relation. We know that in ABSA, the aspect sentiment is mostly determined by the opinion terms expressed on the aspect target (Qiu et al., 2011; Yang and Cardie, 2012). How- ever, this pipeline approach breaks the interaction within the triplet structure. Moreover, pipeline ap- proaches usually suffer from the error propagation problem. Recent end-to-end approaches (Wu et al., 2020; Xu et al., 2020b; Zhang et al., 2020) can jointly extract the target and opinion terms and classify their sentiment relation. One drawback is that they heavily rely on word-to-word interactions to pre- dict the sentiment relation for the target-opinion pair. Note that it is common for the aspect tar- gets and opinions to contain multiple words, which accounts for roughly one-third of triplets in the benchmark datasets. However, the previous meth- ods (Wu et al., 2020; Zhang et al., 2020) predict the sentiment polarity for each word-word pair inde- pendently, which cannot guarantee their sentiment consistency when forming a triplet. As a result, this prediction limitation on triplets that contain multi-word targets or opinions inevitably hurts the overall ASTE performance. For the example in Figure 1, by only considering the word-to-word in- teractions, it is easy to wrongly predict that “enjoy” expresses a positive sentiment on “Windows”. Xu et al. (2020b) proposed a position-aware tagging scheme to allow the model to couple each word in a target span with all possible opinion spans, i.e., aspect word to opinion span interactions (or vice versa, aspect span to opinion word interactions). However, it still cannot directly model the span-to- span interactions between the whole target spans and opinion spans. In this paper, we propose a span-based model for ASTE (Span-ASTE), which for the ﬁrst time di- rectly captures the span-to-span interactions when predicting the sentiment relation of an aspect tar- get and opinion pair. Of course, it can also con- sider the single-word aspects or opinions properly. Our model explicitly generates span representa- tions for all possible target and opinion spans, and their paired sentiment relation is independently predicted for all possible target and opinion pairs. Span-based methods have shown encouraging per- of a target span with positive sentiment polarity. 3A common tagging scheme for sequence labeling, denot- ing “begin, inside, outside, end and single” respectively. formance on other tasks, such as coreference res- olution (Lee et al., 2017), semantic role labeling (He et al., 2018a), and relation extraction (Luan et al., 2019; Wadden et al., 2019). However, they cannot be directly applied to the ASTE task due to different task-speciﬁc characteristics. Our contribution can be summarized as follows: • We tailor a span-level approach to explic- itly consider the span-to-span interactions for the ASTE task and conduct extensive analy- sis to demonstrate its effectiveness. Our ap- proach signiﬁcantly improves performance, especially on triplets which contain multi- word targets or opinions. • We propose a dual-channel span pruning strat- egy by incorporating explicit supervision from the ATE and OTE tasks to ease the high com- putational cost caused by span enumeration and maximize the chances of pairing valid target and opinion candidates together. • Our proposed Span-ASTE model outperforms the previous methods signiﬁcantly not only for the ASTE task, but also for the ATE and OTE tasks on four benchmark datasets with both BiLSTM and BERT encoders. 2 Span-based ASTE 2.1 Task Formulation Let X = {x1, x2, ..., xn} denote a sentence of n tokens, let S = {s1,1, s1,2, ..., si,j, ..., sn,n} be the set of all possible enumerated spans in X, with i and j indicating the start and end positions of a span in the sentence. We limit the span length as 0 ≤ j − i ≤ L. The objective of the ASTE task is to extract all possible triplets in X. Each sentiment triplet is deﬁned as (target, opinion, sentiment) where sentiment ∈ {P ositive, N egative, N eutral}. 2.2 Model Architecture As shown in Figure 2, Span-ASTE consists of three modules: sentence encoding, mention module, and triplet module. For the given example, the sentence is ﬁrst input to the sentence encoding module to obtain the token-level representation, from which we derive the span-level representation for each enumerated span, such as “did not enjoy”, “Win- dows 8”. We then adopt the ATE and OTE tasks to supervise our proposed dual-channel span prun- ing strategy which obtains the pruned target and Figure 2: Span-ASTE model structure. opinion candidates, such as “Windows 8” and “not enjoy” respectively. Finally, each target candidate and opinion candidate are coupled to determine the sentiment relation between them. 2.2.1 Sentence Encoding We explore two encoding methods to obtain the contextualized representation for each word in a sentence: BiLSTM and BERT. BiLSTM We ﬁrst obtain the word repre- sentations {e1, e2, ..., ei, ..., en} from the 300- dimension pre-trained GloVe (Pennington et al., 2014) embeddings which are then contextualized by a bidirectional LSTM (Hochreiter and Schmid- huber, 1997) layer. The ith token is represented as: hi = [ −→ hi; ←− hi] (1) −→ hi and where ward and backward LSTMs respectively. ←− hi are the hidden states of the for- BERT An alternative encoding method is to use a pre-trained language model such as BERT (De- vlin et al., 2019) to obtain the contextualized word representations x = [x1, x2, ..., xn]. For words that are tokenized as multiple word pieces, we use mean pooling to aggregate their representations . Span Representation We deﬁne each span rep- resentation si,j ∈ S as: si,j = (cid:40) [hi; hj; fwidth(i, j)] [xi; xj; fwidth(i, j)] if BiLSTM if BERT (2) where fwidth(i, j) produces a trainable feature em- bedding representing the span width (i.e., j − i + 1). Besides the concatenation of the start token, end token, and width representations, the span repre- sentation si,j can also be formed by max-pooling or mean-pooling across all token representations of the span from position i to j. The experimental results can be found in the ablation study. 2.2.2 Mention Module ATE & OTE Tasks We employ the ABSA sub- tasks of ATE and OTE to guide our dual-channel span pruning strategy through the scores of the predicted opinion and target span. Note that the target terms and opinion terms are not yet paired together at this stage. The mention mod- ule takes the representation of each enumerated span si,j as input and predicts the mention types m ∈ {T arget, Opinion, Invalid}. P (m|si,j) = softmax(FFNNm(si,j)) (3) where FFNN denotes a feed-forward neural net- work with non-linear activation. Pruned Target and Opinion For a sentence X of length n, the number of enumerated spans is O(n2), while the number of possible pairs between all opinion and target candidate spans is O(n4) at the later stage (i.e., the triplet module). As such, it is not computationally practical to consider all possible pairwise interactions when using a span- based approach. Previous works (Luan et al., 2019; Wadden et al., 2019) employ a pruning strategy to reduce the number of spans, but they only prune the spans to a single pool which is a mix of different mention types. This strategy does not fully consider PP-Span Enumerator-All consecutive subsequences <= 8 words-Span Extractor-Concat(start token (768), end token (768), width (20))-[bs, num_spans, 1556]-Mention Head-Feedforward (1556 -> 150 -> 150 -> 3) {opinion, target, null}-CrossEntropyLoss-Relation Head-Pruner-Feedforward (1556 -> 150 -> 150 -> 1)-Keep top-k span embeds-K = sequence_length * 0.5-[bs, top_k, 1556]-Pairwise embeds: -Concat(span_a (1556), span_b (1556), multiply (1556)) -[bs, top_k, top_k, 4668]-Feedforward (4668 -> 150 -> 150 -> 4) {pos, neg, neutral, null}-CrossEntropyLoss-New Relation Head-Opinion Pruner -[bs, top_k, 1556]-Mention loss-Target Pruner-[bs, top_k, 1556]-Mention loss-Pairwise embeds:-Restrict to opinion-target pairsSpan EnumeratorBERTSpan Embed ExtractorMentionHeadRelation HeadFinal LossInput Sequence[bs=1, seq_len][bs, seq_len, 768][bs, num_spans, 2][bs, num_spans, 1556]weight=1.0weight=0.2Opinion StartOpinion EndTargetStartTarget End+Pruned TargetsPruned OpinionsΦtripledfddfdLegend++++The Koreandishesaretastybutcostly.The Korean dishestastycostly.+++++Contextual Encoding (Eq. 2)Span EnumerationThe Korean dishes++are tasty but costlyThe, tastyThe, costlyKorean dishes, tastyKorean dishes, costlyTarget & Opinion Pruning(Eq. 3)<Opinion><Opinion><Null><Null>Auxiliary Span Classification (Eq. 10)<Null><Pos><Neg><Null>Pair EnumerationSentiment Triplet Classification(Eq. 6, 7, 8)<Null><Target><Null>Loss(Eq. 11)Element-wise addition of mention scoreElement-wise addition of mention scoreConcatenate OperationLegend+Distance EmbeddingFFNNFFNN<Null><Opinion><Target>The Koreandishesaretastybutcostly.The Korean dishestastycostly.The Korean dishesare tasty but costly+++++++FFNNThecostlytastyKorean dishesKorean dishes, tastyKorean dishes, costlyKorean dishes, tastyKorean dishes, costlyThe ++++The KoreandishesaretastyandcheapKorean dishestastycheap++++Contextual Encoding (Eq. 2)Span Enumerationdishes++are tasty and cheapdishes, tastydishes, cheapKorean dishes, tastyKorean dishes, cheapTwo-Way Span Pruning(Eq. 3)<Positive><Negative><Neutral>Opinion-Target Pair EnumerationSentiment Triplet Classification(Eq. 6, 7, 8)Loss(Eq. 11)Concatenate OperationLegend+Distance EmbeddingFFNNFFNNdishesKorean dishes<Invalid>tastycheapTheare tasty and cheapTop Target CandidatesTop Opinion CandidatesInvalid CandidatesSupervision by Opinion and Target Co-Extraction SubtasksSpan Boundary EmbeddingContextual EmbeddingSpan Width EmbeddingdishesKorean dishestastycheapTheare tasty and cheapTop Target CandidatesTop Opinion CandidatesInvalid Candidates++++++++KKK+LegendFFNNSpan Width EmbeddingConcatenate Operation+Start-End EmbeddingContextual EmbeddingDistance EmbeddingTop-k Select OperationFFNN+++++++++++++FFFFFFFFFFFFFP+FDataset Rest 14 Lap 14 Rest 15 Rest 16 #S, # +, # 0, # -, #SW #MW #S, # +, # 0, # -, #SW #MW #S, # +, # 0, # -, #SW #MW #S, # +, # 0, # -, #SW #MW Train 1266 1692 166 480 1586 Dev 388 404 Test 657 773 54 119 66 155 310 492 752 906 817 126 517 824 189 219 169 36 141 190 337 328 364 63 116 291 636 605 783 25 205 156 148 185 11 53 252 322 317 25 143 678 165 297 50 329 335 857 1015 84 210 252 11 29 188 326 407 918 76 216 78 344 476 123 170 Table 1: Statistics of datasets. #S denotes the number of sentences. # +, # 0, and # - denote the numbers of positive, neutral, and negative sentiment triplets respectively. #SW denotes the number of triplets where both target and opinion terms are single-word spans. #MW denotes the number of triplets where at least one of the target or opinion terms are multi-word spans. the structure of an aspect sentiment triplet as it does not recognize the fundamental difference between a target and an opinion term. Hence, we propose to use a dual-channel pruning strategy which results in two separate pruned pools of aspects and opin- ions. This minimizes computational costs while maximizing the chance of pairing valid opinion and target spans together. The opinion and target candidates are selected based on the scores of the mention types for each span based on Equation 3: Φtarget(si,j) = P (m = target|si,j) Φopinion(si,j) = P (m = opinion|si,j) (4) We use the mention scores Φtarget and Φopinion to select the top candidates from the enumer- ated spans and obtain the target candidate pool St = {..., st a,b, ...} and the opinion candidate pool So = {..., so c,d, ...} respectively. To consider a pro- portionate number of candidates for each sentence, the number of selected spans for both pruned target and opinion candidates is nz, where n is the sen- tence length and z is a threshold hyper-parameter. Note that although the pruning operation prevents the gradient ﬂow back to the FFNN in the mention module, it is already receiving supervision from the ATE and OTE tasks. Hence, our model can be trained end-to-end without any issue or instability. 2.2.3 Triplet Module Target Opinion Pair Representation We ob- tain the target-opinion pair representation by cou- a,b ∈ St pling each target candidate representation st with each opinion candidate representation so c,d ∈ So: gst a,b,so c,d = [st a,b; so c,d; fdistance(a, b, c, d)] (5) where fdistance(a, b, c, d) produces a trainable feature embedding based on the distance (i.e., min(|b − c|, |a − d|)) between the target and opin- ion spans, following (Lee et al., 2017; He et al., 2018a; Xu et al., 2020b). the span pair representation gst Sentiment Relation Classiﬁer Then, we in- to put a feed-forward neural network to determine relation r ∈ the probability of sentiment R = {P ositive, N egative, N eutral, Invalid} between the target st a,b and the opinion so a,b,so c,d: c,d P (r|st a,b, so c,d) = softmax(FFNNr(gst )) (6) Invalid here indicates that the target and opinion pair has no valid sentiment relationship. a,b,so c,d 2.3 Training The training objective is deﬁned as the sum of the negative log-likelihood from both the mention mod- ule and triplet module. L = − (cid:88) si,j ∈S log P (m∗ i,j|si,j) (cid:88) − st a,b∈St,so c,d∈So log P (r∗|st a,b, so c,d) (7) where m∗ i,j is the gold mention type of the span si,j, and r∗ is the gold sentiment relation of the target and opinion span pair (st c,d). S indicates the enumerated span pool; St and So are the pruned target and opinion span candidates. a,b, so 3 Experiment 3.1 Datasets Our proposed Span-ASTE model is evaluated on four ASTE datasets released by Xu et al. (2020b), which include three datasets in the restaurant do- main and one dataset in the laptop domain. The ﬁrst version of the ASTE datasets are released by Peng et al. (2019). However, it is found that not all triplets are explicitly annotated (Xu et al., 2020b; Wu et al., 2020). Xu et al. (2020b) reﬁned the datasets with the missing triplets and removed triplets with conﬂicting sentiments. Note that these Model M T S L B i CMLA+ (Wang et al., 2017)† RINANTE+ (Dai and Song, 2019)† Li-uniﬁed-R (Li et al., 2019)† Peng et al. (2019)† Zhang et al. (2020) ∗ GTS (Wu et al., 2020)∗ JETo M =6 (Xu et al., 2020b)† Span-ASTE (Ours) T GTS (Wu et al., 2020)∗ R E B M =6 (Xu et al., 2020b) † JETo Span-ASTE (Ours) Rest 14 Lap 14 Rest 15 Rest 16 P. R. 39.18 31.42 41.04 43.24 62.70 66.13 61.50 47.13 39.38 67.35 63.66 57.10 57.91 55.13 F1 42.79 34.95 51.00 51.46 59.71 61.73 58.14 P. R. 30.09 21.71 40.56 37.38 49.62 53.35 53.03 36.92 18.66 44.28 50.38 41.07 40.99 33.89 F1 33.16 20.07 42.34 42.87 44.78 46.31 41.35 P. R. 34.56 29.88 44.72 48.07 55.63 60.10 64.37 39.84 30.06 51.39 57.51 42.51 46.89 44.33 F1 37.01 29.97 47.82 52.32 47.94 52.66 52.50 P. R. 41.34 25.68 37.33 46.96 60.95 63.28 70.94 42.10 22.30 54.51 64.24 53.35 58.56 57.00 F1 41.72 23.87 44.31 54.21 56.82 60.79 63.21 72.52 62.43 67.08 59.85 45.67 51.80 64.29 52.12 57.56 67.25 61.75 64.37 67.76 70.56 67.29 55.94 67.50 62.40 57.82 55.39 51.32 47.33 54.36 51.04 62.59 64.45 57.94 51.96 60.15 57.53 66.08 70.42 69.91 58.37 67.93 63.83 72.89 70.89 71.85 63.44 55.84 59.38 62.18 64.45 63.27 69.45 71.17 70.26 Table 2: Results on the test set of the ASTE task. †: The results are retrieved from Xu et al. (2020b). ∗: For a fair comparison, we reproduce the results using their released implementation code and conﬁguration on the same ASTE datasets released by Xu et al. (2020b). four benchmark datasets are derived from the Se- mEval Challenge (Pontiki et al., 2014, 2015, 2016), and the opinion terms are retrieved from (Fan et al., 2019). Table 1 shows the detailed statistics. 3.2 Experiment Settings When using the BiLSTM encoder, the pre-trained GloVe word embeddings are trainable. The hid- den size of the BiLSTM encoder is 300 and the dropout rate is 0.5. In the second setting, we ﬁne- tune the pre-trained BERT (Devlin et al., 2019) to encode each sentence. Speciﬁcally, we use the un- cased version of BERTbase. The model is trained for 10 epochs with a linear warmup for 10% of the training steps followed by a linear decay of the learning rate to 0. We employ AdamW as the optimizer with the maximum learning rate of 5e-5 for transformer weights and weight decay of 1e-2. For other parameter groups, we use a learning rate of 1e-3 with no weight decay. The maximum span length L is set as 8. The span pruning threshold z is set as 0.5. We select the best model weights based on the F1 scores on the development set and the reported results are the average of 5 runs with different random seeds. 4 3.3 Baselines The baselines can be summarized as two groups: pipeline methods and end-to-end methods. opinion terms with BIOES tags at the ﬁrst stage. At the second stage, the extracted targets and opinions are then paired to determine if they can form a valid triplet. Note that these approaches employ different methods to obtain the features for the ﬁrst stage. CMLA+ (Wang et al., 2017) employs an attention mechanism to consider the interaction between as- pect terms and opinion terms. RINANTE+ (Dai and Song, 2019) adopts a BiLSTM-CRF model with mined rules to capture the dependency rela- tions. Li-uniﬁed-R (Li et al., 2019) uses a uniﬁed tagging scheme to jointly extract the aspect term and associated sentiment. Peng et al. (2019) in- cludes dependency relation information when con- sidering the interaction between the aspect and opinion terms. End-to-end The end-to-end methods aim to jointly extract full triplets in a single stage. Pre- vious work by Zhang et al. (2020) and Wu et al. (2020) independently predict the sentiment relation for all possible word-word pairs, hence they require decoding heuristics to determine the overall senti- ment polarity of a triplet. JET (Xu et al., 2020b) models the ASTE task as a structured prediction problem with a position-aware tagging scheme to capture the interaction of the three elements in a triplet. 3.4 Experiment Results Pipeline For the pipeline approaches listed be- low, they are modiﬁed by Peng et al. (2019) to extract the aspect terms together with their asso- ciated sentiments via a joint labeling scheme, and 4See Appendix for more experimental settings, and also the dev results on the four datasets. Table 2 compares Span-ASTE with previous mod- els in terms of Precision (P.), Recall (R.), and F1 scores on four datasets. Under the F1 metric, our model consistently outperforms the previous works for both BiLSTM and BERT sentence en- coders. In most cases, our model signiﬁcantly out- performs other end-to-end methods in both preci- sion and recall. We also observe that the two strong pipeline methods (Li et al., 2019; Peng et al., 2019) achieved competitive recall results, but their overall performance is much worse due to the low preci- sion. Speciﬁcally, using the BiLSTM encoder with GloVe embedding, our model outperforms the best pipeline model (Peng et al., 2019) by 15.62, 8.93, 5.24, and 10.16 F1 points on the four datasets. This result indicates that our end-to-end approach can ef- fectively encode the interaction between target and opinion spans, and also alleviates the error propa- gation. In general, the other end-to-end methods are also more competitive than the pipeline meth- ods. However, due to the limitations of relying on word-level interactions, their performances are less encouraging in a few cases, such as the results on Lap 14 and Rest 15. With the BERT encoder, all three end-to-end models achieve much stronger per- formance than their LSTM-based versions, which is consistent with previous ﬁndings (Devlin et al., 2019). Our approach outperforms the previous best results GTS (Wu et al., 2020) by 4.35, 5.02, 3.12, and 2.33 F1 points on the four datasets. 3.5 Additional Experiments As mentioned in Section 2.2.2, we employ the ABSA subtasks of ATE and OTE to guide our span pruning strategy. To examine if Span-ASTE can effectively extract target spans and opinion spans, we also evaluate our model on the ATE and OTE tasks on the four datasets. Table 3 shows the com- parisons of our approach and the previous method GTS (Wu et al., 2020). 5 Without additional retrain- ing or tuning, our model can directly address the ATE and OTE tasks, with signiﬁcant performance improvement than GTS in terms of F1 scores on both tasks. Even though GTS shows a better re- call score on the Rest 16 dataset, the low precision score results in worse F1 performance. The better overall performance indicates that our span-level method not only beneﬁts the sentiment triplet ex- traction, but also improves the extraction of target and opinion terms by considering the semantics of each whole span rather than relying on decoding heuristics of tagging-based methods. 5See Appendix for the target and opinion data statistics. Note that the JET model (Xu et al., 2020b) is not able to directly solve the ATE and OTE tasks unless the evaluation is conducted based on the triplet predictions. We include such comparisons in the Appendix. Dataset Model Rest 14 Lap 14 Rest 15 Rest 16 GTS Ours GTS Ours GTS Ours GTS Ours ATE R. 85.64 87.59 82.68 86.39 81.57 84.68 89.42 88.50 P. 78.12 83.56 76.63 81.48 75.13 78.97 75.06 79.78 OTE P. R. 81.12 82.93 76.11 83.00 74.96 77.36 78.99 82.59 88.24 89.67 78.44 82.28 82.52 84.86 88.71 90.91 F1 81.69 85.50 79.53 83.86 78.21 81.72 81.61 83.89 F1 84.53 86.16 77.25 82.63 78.49 80.93 83.57 86.54 Table 3: Test results on the ATE and OTE tasks with BERT encoder. For reference, we include the results of the RACL framework (Chen and Qian, 2020) in the Appendix. RACL is the current state-of-the-art method for both tasks. However, their framework does not con- sider the pairing relation between each target and opin- ion, therefore it is difﬁcult to have a completely fair comparison. 4 Analysis 4.1 Comparison of Single-word and Multi-word Spans We compare the performance of Span-ASTE with the previous model GTS (Wu et al., 2020) for the following two settings in Table 4: Single-Word: Both target and opinion terms in a triplet are single- word spans, Multi-Word: At least one of the target or opinion terms in a triplet is a multi-word span. For the single-word setting, our method shows con- sistent improvement in terms of both precision and recall score on the four datasets, which re- sults in the improvement of F1 score. When we compare the evaluations for multi-word triplets, our model achieves more signiﬁcant improvements for F1 scores. Compared to precision, our recall shows greater improvement over the GTS approach. GTS heavily relies on word-pair interactions to ex- tract triplets, while our methods explicitly consider the span-to-span interactions. Our span enumera- tion also naturally beneﬁts the recall of multi-word spans. For both GTS and our model, multi-word triplets pose challenges and their F1 results drop by more than 10 points, even more than 20 points for Rest 14. As shown in Table 1, comparing with the single-word triplets, multi-word triplets are com- mon and account for one-third or even half of the datasets. Therefore, a promising direction for fu- ture work is to further improve the model’s perfor- mance on such difﬁcult triplets. To identify further areas for improvement, we analyze the results for the ASTE task based on whether each sentiment triplet contains a multi- Mode Model T R E B Single-Word Multi-Word GTS Ours ∆ GTS Ours ∆ Rest 14 Lap 14 Rest 15 Rest 16 P. R. F1 P. R. F1 P. R. F1 P. R. F1 79.15 79.60 66.55 65.47 74.93 79.12 70.23 68.09 +4.19 +0.46 +2.38 +2.62 +3.44 +3.04 +3.68 63.97 67.02 62.54 65.98 76.98 79.36 65.66 70.71 +5.05 69.66 71.66 73.03 66.10 70.47 74.65 +4.37 +2.00 +1.16 +1.62 76.74 77.91 49.26 55.79 56.85 55.95 62.97 61.64 +4.79 +6.53 +5.78 +2.37 +3.17 +2.90 +0.42 +10.11 +5.10 +5.80 +8.24 +7.02 46.12 49.02 52.26 54.63 41.27 44.44 48.77 53.87 52.78 58.57 47.34 57.45 56.63 62.43 55.29 63.53 50.28 50.70 Table 4: Analysis with different evaluation modes on the ASTE task. Dataset Model Rest 14 Lap 14 Rest 15 Rest 16 GTS Ours GTS Ours GTS Ours GTS Ours Multi-word Target Multi-word Opinion P. R. 56.54 65.96 55.11 56.99 51.09 55.33 62.69 66.43 49.81 57.62 44.09 48.18 51.09 60.58 65.12 72.09 F1 52.96 61.51 48.99 52.22 51.09 57.84 63.88 69.14 P. R. 50.67 49.43 37.50 34.62 43.40 37.18 28.26 36.73 41.76 47.25 26.09 26.09 35.94 45.31 24.07 33.33 F1 45.78 48.31 30.77 29.75 39.32 40.85 26.00 34.95 Table 5: Further comparison of test results for our model and GTS based on triplets of multi-word targets and opinions for the ASTE task. word target or multi-word opinion term. From Ta- ble 5, the results show that the performance is lower when the triplet contains a multi-word opinion term. This trend can be attributed to the imbalanced data distribution of triplets which contain multi-word target or opinion terms. 4.2 Pruning Efﬁciency To demonstrate the efﬁciency of the proposed dual-channel pruning strategy, we also compare it to a simpler strategy, denoted as Single-Channel (SC) which does not distinguish between opin- ion and target candidates. Figure 3 shows the comparisons. Note the mention module under this strategy does not explicitly solve the ATE and OTE tasks as it only predicts mention label m ∈ {V alid, Invalid}, where V alid means the span is either a target or an opinion span and Invalid means the span does not belong to the two groups. Given sentence length n and pruning threshold z, the number of candidates is limited to nz, and hence the computational cost scales with the number of pairwise interactions, n2z2. The dual-channel strategy considers each target-opinion pair where the pruned target and opinion candidate pools both have nz spans. Note that it is possible for the two pools to share some candidates. In com- parison, the single-channel strategy considers each 60 ) % ( 1 F 40 20 Dual-Channel Single-Channel (SC) SC-Adjusted 0.0 6 2 5 0.1 2 5 0.2 5 0.5 0 1.0 Figure 3: Dev results with respect to pruning threshold z which intuitively refers to the number of candidate spans to keep per word in the sentence. target-opinion pair where the target and opinion candidates are drawn from the same single pool of nz spans. In order to consider at least as many target and opinion candidates as the dual-channel strategy, the single-channel strategy has to scale the threshold z by two, which leads to 4 times more pairs and computational cost. We denote this set- ting in Figure 3 as SC-Adjusted. When controlling for computational efﬁciency, there is a signiﬁcant performance difference between Dual-Channel and Single-Channel in F1 score, especially for lower values of z. Although the performance gap narrows with increasing z, it is not practical for high values. According to our experimental results, we select the dual-channel pruning strategy with z = 0.5 for the reported model. 4.3 Qualitative Analysis To illustrate the differences between the models, we present sample sentences from the ASTE test set with the gold labels as well as predictions from GTS (Wu et al., 2020) and Span-ASTE in Figure 4. For the ﬁrst example, GTS correctly extracts the tar- get term “Windows 8” paired with the opinion term “not enjoy”, but the sentiment is incorrectly pre- dicted as positive. When forming the triplet, their decoding heuristic considers the sentiment inde- Figure 4: Qualitative analysis. The target and opinion terms are highlighted in orange and blue respectively. Each arc indicates the pairing relation between target and opinion terms. The sentiment polarity of each triplet is indicated above the target terms. pendently for each word-word pair: {(“Windows”, “not”, Neutral), (“8”, “not”, Neutral), (“Windows”, “enjoy”, Positive), (“8”, “enjoy”, Positive)}. Their heuristic votes the overall sentiment polarity as the most frequent label among the pairs. In the case of a tie (2 neutral and 2 positive), the heuristic has a predeﬁned bias to assign the sentiment polar- ity to positive. Similarly, the word-level method fails to capture the negative sentiment expressed by “not enjoy” on the other target term “touchscreen functions”. In the second example, it incompletely extracts the target term “Korean dishes”, resulting in the wrong triplet. For both examples, our method is able to accurately extract the target-opinion pairs and determine the overall sentiment even when each term has multiple words. 4.4 Ablation Study We conduct an ablation study to examine the per- formance of different modules and span representa- tion methods, and the results are shown in Table 6. The average F1 denotes the average dev results of Span-ASTE on the four benchmark datasets over 5 runs. Similar to the observation for coreference resolution (Lee et al., 2017), we ﬁnd that the ASTE performance is reduced when removing the span width and distance embedding. This indicates that the positional information is still useful for the ASTE task as targets and opinions which are far apart or too long are less likely to form a valid span pair. As mentioned in Section 2.2.1, we explore two other methods (i.e., max pooling and mean pooling) to form span representations instead of concatenating the span boundary token represen- tations. The negative results suggest that using pooling to aggregate the span representation is dis- advantageous due to the loss of information that is useful for distinguishing valid and invalid spans. Model Full model W/O width & distance embedding max pooling mean pooling Average F1 ∆F1 67.69 66.45 66.09 66.19 -1.24 -1.60 -1.53 Table 6: Ablation study on the development sets. 5 Related Work Sentiment Analysis is a major Natural Language Understanding (NLU) task (Wang et al., 2019) and has been extensively studied as a classiﬁcation problem at the sentence level (Raffel et al., 2020; Lan et al., 2020; Yang et al., 2020). Aspect-Based Sentiment Analysis (ABSA) (Pontiki et al., 2014) addresses various sentiment analysis tasks at a ﬁne- grained level. As mentioned in the Section 1, the subtasks mainly include ASC (Dong et al., 2014; Zhang et al., 2016; Chen et al., 2017; He et al., 2018b; Li et al., 2018a; Peng et al., 2018; Wang and Lu, 2018; He et al., 2019; Li and Lu, 2019; Xu et al., 2020a), ATE (Qiu et al., 2011; Yin et al., 2016; Li et al., 2018b; Ma et al., 2019), OTE (Hu and Liu, 2004; Yang and Cardie, 2012; Klinger and Cimiano, 2013; Yang and Cardie, 2013). There is also another subtask named Target-oriented Opin- ion Words Extraction (TOWE) (Fan et al., 2019), which aim to extract the corresponding opinion words for a given target term. Another line of research focuses on addressing different subtasks together. Aspect and Opinion Term Co-Extraction (AOTE) aiming to extract the aspect and opinion terms together (Wang et al., 2017; Ma et al., 2019; Dai and Song, 2019) and is often treated as a se- quence labeling problem. Note that AOTE does not consider the paired sentiment relationship be- tween each target and opinion term. End-to-End ABSA (Li and Lu, 2017; Ma et al., 2018; Li et al., 2019; He et al., 2019) jointly extracts each aspect +++----+term and its associated sentiment in an end-to-end manner. A few other methods are recently pro- posed to jointly solve three or more subtasks of ABSA. Chen and Qian (2020) proposed a relation aware collaborative learning framework to unify the three fundamental subtasks and achieved strong performance on each subtask and combined task. While Wan et al. (2020) focused more on aspect category related subtasks, such as Aspect Category Extraction and Aspect Category and Target Joint Extraction. ASTE (Peng et al., 2019; Wu et al., 2020; Xu et al., 2020b; Zhang et al., 2020) is the most recent development of ABSA and its aim is to extract and form the aspect term, its associated sentiment, and the corresponding opinion term into a triplet. 6 Conclusions In this work, we propose a span-level approach - Span-ASTE to learn the interactions between target spans and opinion spans for the ASTE task. It can address the limitation of the existing approaches that only consider word-to-word interactions. We also propose to include the ATE and OTE tasks as supervision for our dual-channel pruning strat- egy to reduce the number of enumerated target and opinion candidates to increase the computational ef- ﬁciency and maximize the chances of pairing valid target and opinion candidates together. Our method signiﬁcantly outperforms the previous methods for ASTE as well as ATE and OTE tasks and our analy- sis demonstrates the effectiveness of our approach. While we achieve strong performance on the ASTE task, the performance can be mostly attributed to the improvement on the multi-word triplets. As discussed in Section 4.1, there is still a signiﬁcant performance gap between single-word and multi- word triplets, and this can be a potential area for future work. References Peng Chen, Zhongqian Sun, Lidong Bing, and Wei Yang. 2017. Recurrent attention network on mem- In Proc. of ory for aspect sentiment analysis. EMNLP. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing. In Proc. of NAACL. Li Dong, Furu Wei, Chuanqi Tan, Duyu Tang, Ming Zhou, and Ke Xu. 2014. Adaptive recursive neural network for target-dependent twitter sentiment clas- siﬁcation. In Proc. of ACL. Zhifang Fan, Zhen Wu, Xinyu Dai, Shujian Huang, and Jiajun Chen. 2019. Target-oriented opinion words extraction with target-fused neural sequence label- ing. In Porc. of NAACL. Matt Gardner, Joel Grus, Mark Neumann, Oyvind Tafjord, Pradeep Dasigi, Nelson F. Liu, Matthew Peters, Michael Schmitz, and Luke S. Zettlemoyer. 2017. Allennlp: A deep semantic natural language processing platform. Luheng He, Kenton Lee, Omer Levy, and Luke Zettle- moyer. 2018a. Jointly predicting predicates and ar- In Proc. guments in neural semantic role labeling. of ACL. Ruidan He, Wee Sun Lee, Hwee Tou Ng, and Daniel Dahlmeier. 2018b. Effective attention modeling for In Proc. of aspect-level sentiment classiﬁcation. COLING. Ruidan He, Wee Sun Lee, Hwee Tou Ng, and Daniel Dahlmeier. 2019. An interactive multi-task learning network for end-to-end aspect-based sentiment anal- ysis. In Proc. of ACL. Sepp Hochreiter and J¨urgen Schmidhuber. 1997. Long short-term memory. Neural computation, 9:1735– 80. Minqing Hu and Bing Liu. 2004. Mining and summa- rizing customer reviews. In Proc. of ACM SIGKDD. R. Klinger and P. Cimiano. 2013. Joint and pipeline probabilistic models for ﬁne-grained sentiment anal- ysis: Extracting aspects, subjective phrases and their relations. In 2013 IEEE 13th International Confer- ence on Data Mining Workshops. Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2020. Albert: A lite bert for self-supervised learn- ing of language representations. In Proc. of ICLR. Kenton Lee, Luheng He, Mike Lewis, and Luke Zettle- moyer. 2017. End-to-end neural coreference resolu- tion. In Proc. of EMNLP. Zhuang Chen and Tieyun Qian. 2020. Relation-aware collaborative learning for uniﬁed aspect-based senti- ment analysis. In Proc. of ACL. Hao Li and Wei Lu. 2017. Learning latent sentiment scopes for entity-level sentiment analysis. In Proc. of AAAI. Hongliang Dai and Yangqiu Song. 2019. Neural as- pect and opinion term extraction with mined rules as weak supervision. In Proc. of ACL. Hao Li and Wei Lu. 2019. Learning explicit and im- plicit structures for targeted sentiment analysis. In Proc. of EMNLP. Xin Li, Lidong Bing, Wai Lam, and Bei Shi. 2018a. Transformation networks for target-oriented senti- ment classiﬁcation. In Proc. of ACL. Xin Li, Lidong Bing, Piji Li, and Wai Lam. 2019. A uniﬁed model for opinion target extraction and target sentiment prediction. In Proc. of AAAI. Xin Li, Lidong Bing, Piji Li, Wai Lam, and Zhimou Yang. 2018b. Aspect term extraction with history In Proc. of attention and selective transformation. IJCAI. Bing Liu. 2012. Sentiment analysis and opinion min- ing. Synthesis lectures on human language technolo- gies, 5(1):1–167. Colin Raffel, Noam Shazeer, Adam Roberts, Kather- ine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a uniﬁed text-to- text transformer. Journal of Machine Learning Re- search, 21(140):1–67. Jialong Tang, Ziyao Lu, Jinsong Su, Yubin Ge, Linfeng Song, Le Sun, and Jiebo Luo. 2019. Progressive self- supervised attention learning for aspect-level senti- ment analysis. In Proc. of ACL. David Wadden, Ulme Wennberg, Yi Luan, and Han- naneh Hajishirzi. 2019. Entity, relation, and event extraction with contextualized span representations. In Proc. of EMNLP. Yi Luan, Dave Wadden, Luheng He, Amy Shah, Mari Ostendorf, and Hannaneh Hajishirzi. 2019. A gen- eral framework for information extraction using dy- namic span graphs. In Proc. of NAACL. Hai Wan, Yufei Yang, Jianfeng Du, Yanan Liu, Kunxun Qi, and Jeff Z. Pan. 2020. Target-aspect-sentiment joint detection for aspect-based sentiment analysis. In Proc. of AAAI. Dehong Ma, Sujian Li, and Houfeng Wang. 2018. Joint learning for targeted sentiment analysis. In Proc. of EMNLP. Dehong Ma, Sujian Li, Fangzhao Wu, Xing Xie, and Houfeng Wang. 2019. Exploring sequence-to- sequence learning in aspect term extraction. In Proc. of ACL. Haiyun Peng, Yukun Ma, Yang Li, and Erik Cam- bria. 2018. Learning multi-grained aspect target se- quence for chinese sentiment analysis. Knowledge- Based Systems, 148:167–176. Haiyun Peng, Lu Xu, Lidong Bing, Fei Huang, Wei Lu, and Luo Si. 2019. Knowing what, how and why: A near complete solution for aspect-based sentiment analysis. In Proc. of AAAI. Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. Glove: Global vectors for word rep- resentation. In Proc. of EMNLP. Maria Pontiki, Dimitris Galanis, Haris Papageor- giou, Ion Androutsopoulos, Suresh Manandhar, Mo- hammed AL-Smadi, Mahmoud Al-Ayyoub, Yanyan Zhao, Bing Qin, Orph´ee De Clercq, et al. 2016. Semeval-2016 task 5: Aspect based sentiment anal- ysis. In Proc. of SemEval. Maria Pontiki, Dimitris Galanis, Haris Papageorgiou, Suresh Manandhar, and Ion Androutsopoulos. 2015. SemEval-2015 task 12: Aspect based sentiment analysis. In Proc. of SemEval. Alex Wang, Amanpreet Singh, Julian Michael, Fe- lix Hill, Omer Levy, and Samuel Bowman. 2019. GLUE: A multi-task benchmark and analysis plat- In Proc. form for natural language understanding. of ICLR. Bailin Wang and Wei Lu. 2018. Learning latent opin- In ions for aspect-level sentiment classiﬁcation. Proc. of AAAI. Wenya Wang, Sinno Jialin Pan, Daniel Dahlmeier, and Xiaokui Xiao. 2017. Coupled multi-layer attentions In for co-extraction of aspect and opinion terms. Proc. of AAAI. Zhen Wu, Chengcan Ying, Fei Zhao, Zhifang Fan, Xinyu Dai, and Rui Xia. 2020. Grid tagging scheme for aspect-oriented ﬁne-grained opinion extraction. In Findings of EMNLP. Lu Xu, Lidong Bing, Wei Lu, and Fei Huang. 2020a. Aspect sentiment classiﬁcation with aspect-speciﬁc opinion spans. In Proc. of EMNLP. Lu Xu, Hao Li, W. Lu, and Lidong Bing. 2020b. Position-aware tagging for aspect sentiment triplet extraction. In Proc. of EMNLP. Bishan Yang and Claire Cardie. 2012. Extracting opin- ion expressions with semi-Markov conditional ran- dom ﬁelds. In Proc. of EMNLP. Bishan Yang and Claire Cardie. 2013. Joint inference for ﬁne-grained opinion extraction. In Proc. of ACL. Maria Pontiki, Dimitris Galanis, John Pavlopoulos, Ion Androutsopoulos, and Harris Papageorgiou, Suresh Manandhar. 2014. Semeval-2014 task 4: As- pect based sentiment analysis. In Proc. of SemEval. Min Yang, Wenting Tu, Jingxuan Wang, Fei Xu, and Xiaojun Chen. 2017. Attention based lstm for tar- get dependent sentiment classiﬁcation. In Proc. of AAAI. Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen. 2011. Opinion word expansion and target extrac- tion through double propagation. Computational Linguistics, 37(1):9–27. Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Car- bonell, Ruslan Salakhutdinov, and Quoc V. Le. 2020. Xlnet: Generalized autoregressive pretraining for language understanding. In Proc. of NeurIPS. Yichun Yin, Furu Wei, Li Dong, Kaimeng Xu, Ming Zhang, and Ming Zhou. 2016. Unsupervised word and dependency path embeddings for aspect term ex- traction. In Proc. of IJCAI. Chen Zhang, Qiuchi Li, Dawei Song, and Benyou Wang. 2020. A multi-task learning framework for opinion triplet extraction. In Findings of EMNLP. Meishan Zhang, Yue Zhang, and Duy-Tin Vo. 2016. Gated neural networks for targeted sentiment anal- ysis. In Proc. of AAAI. A Additional Experimental Settings We run our model experiments on a Nvidia Tesla V100 GPU, with CUDA version 10.2 and PyTorch version 1.6.0. The average run time for BERT- based model is 157 sec/epoch, 115 sec/epoch, 87 sec/epoch, and 111 sec/epoch for Rest 14, Lap 14, Rest 15, and Rest 16 respectively. The total number of parameters is 2.24M when GloVe is used, and is 110M when BERT base is used. The feed-forward neural networks in the mention module and triplet module have 2 hidden layers and hidden size of 150. We use ReLU activation and dropout of 0.4 after each hidden layer. We use Xavier Normal weight initialization for the feed-forward parameters. The span width and distance embeddings have 20 and 128 dimensions respectively. Their input values are bucketed (Gardner et al., 2017) before being fed to an embedding matrix lookup: [0, 1, 2, 3, 4, 5-7, 8-15, 16-31, 32-63, 64+]. During training, the model parameters are updated after each sentence which results in a batch size of 1. For each input text sequence, we restrict it to a maximum of 512 tokens. B Additional Data Statistics Table 9 shows the number of target terms and opin- ion terms on the four datasets. C Dev Results Table 10 shows the results of our model on the development datasets. D Additional Comparisons As mentioned by footnote 5 in Section 3.5, we can- not make a direct comparison with the JET model (Xu et al., 2020b), as it is not able to directly solve the ATE and OTE tasks unless the evaluation is conducted based on the triplet results. Table 7 shows such comparisons. Our proposed method Dataset Model Rest 14 Lap 14 Rest 15 Rest 16 M =6 M =6 M =6 M =6 JETo GTS Ours JETo GTS Ours JETo GTS Ours JETo GTS Ours ATE R. 66.04 81.49 80.31 68.03 73.65 75.38 65.74 74.77 78.01 68.58 85.62 86.06 P. 83.21 83.25 86.20 83.33 82.17 87.69 83.04 80.95 81.60 83.33 82.69 84.20 OTE P. R. 83.76 86.55 87.20 77.16 81.63 85.61 81.33 80.96 80.09 89.44 83.37 84.62 77.28 86.65 84.54 75.53 74.05 76.58 68.98 76.57 81.13 80.21 86.53 88.00 F1 73.64 82.36 83.15 74.91 77.68 81.07 73.38 77.74 79.76 75.24 84.13 85.12 F1 80.39 86.60 85.85 76.34 77.66 80.84 74.65 78.70 80.61 84.57 84.92 86.28 Table 7: Test results on the ATE and OTE tasks with sub-optimal evaluation method. Our method and GTS (Wu et al., 2020) allow for ATE and OTE tasks to be predicted independently from the ASTE task. However, JETo M =6 (Xu et al., 2020b) does not. Hence, we make another comparison here by extracting all opinion and target spans from the ASTE predictions. Dataset Model P. GTS 78.50 RACL 79.90 83.56 Ours GTS 78.63 RACL 78.11 81.48 Ours GTS 74.95 RACL 75.22 78.97 Ours GTS 75.05 RACL 74.12 79.78 Ours Rest 14 Lap 14 Rest 15 Rest 16 ATE R. 87.38 87.74 87.59 81.86 81.99 86.39 82.41 81.94 84.68 89.16 89.20 88.50 OTE P. R. 82.07 80.26 82.93 76.27 75.12 83.00 74.75 76.41 77.36 78.36 79.25 82.59 88.99 87.99 89.67 79.32 79.92 82.28 81.56 82.56 84.86 88.42 89.77 90.91 F1 82.70 83.63 85.50 80.21 79.99 83.86 78.50 78.43 81.72 81.50 80.95 83.89 F1 85.39 83.94 86.16 77.77 77.43 82.63 78.01 79.35 80.93 83.09 84.17 86.54 Table 8: Additional comparison of test results on the ATE and OTE tasks. Note that RACL (Chen and Qian, 2020) does not consider supervision from target- opinion pairs, but it includes the sentiment polarities on the target terms. generally outperforms the previous two end-to-end approaches on the four datasets. As mentioned in Table 3, it is challenging to make a fair comparison between the previous ABSA framework RACL (Chen and Qian, 2020), which also address the ATE and OTE tasks while solving other ABSA subtasks, and our approach as well as the GTS (Wu et al., 2020). This is because the mentioned approaches have different task set- tings. The RACL considers the sentiment polarity on the target terms when solving the ATE and OTE tasks, but GTS and our method both consider the pairing relation between target and opinion terms. However, for reference, Table 8 shows the compar- Dataset Train Dev Test Rest 14 Lap 14 Rest 15 Rest 16 # Target # Opinion # Target # Opinion # Target # Opinion # Target # Opinion 2051 1500 1848 2086 1503 1854 1281 1296 1463 1268 1304 1474 1862 1213 1432 1941 1236 1461 1198 1296 1452 1307 1319 1475 Table 9: Additional statistics. # Target denotes the number of target terms. # Opinion denotes the numbers of opinion terms. Model Rest 14 Lap 14 Rest 15 Rest 16 P. R. F1 P. R. F1 P. R. F1 P. R. F1 Ours (BiLSTM) Ours (BERT) 66.76 68.05 53.90 65.65 59.61 66.80 60.78 63.35 49.37 58.90 54.45 61.02 69.13 70.16 60.08 71.41 64.26 70.75 71.59 72.52 61.95 71.92 66.41 72.19 Table 10: Results on the development datasets. isons of the three methods on the ATE and OTE tasks on the datasets released by Xu et al. (2020b). 