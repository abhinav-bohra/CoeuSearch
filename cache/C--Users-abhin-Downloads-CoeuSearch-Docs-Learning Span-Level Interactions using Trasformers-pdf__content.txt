Learning Span-Level Interactions Aspect Sentiment Triplet Extraction Lu Xu *   ,   , Yew Ken Chia *   ,   , Lidong Bing    Singapore University Technology Design   DAMO Academy , Alibaba Group { xu lu , yewken chia } @ mymail.sutd.edu.sg l.bing @ alibaba-inc.com Abstract Aspect Sentiment Triplet Extraction ( ASTE ) recent subtask ABSA outputs triplets aspect target , asso- ciated sentiment , corresponding opin- ion term . Recent models perform triplet extraction end-to-end manner heav- ily rely interactions tar- get word opinion word . Thereby , perform well targets opinions contain multiple words . proposed span-level approach explicitly considers interaction whole spans tar- gets opinions predicting sen- timent relation . Thus , make predic- tions semantics whole spans , en- suring better sentiment consistency . ease high computational cost caused span enumeration , propose dual-channel span pruning strategy incorporating supervision Aspect Term Extraction ( ATE ) Opinion Term Extraction ( OTE ) tasks . strategy improves computational efﬁ- ciency also distinguishes opinion target spans properly . framework si- multaneously achieves strong performance ASTE well ATE OTE tasks . particular , analysis shows span- level approach achieves signiﬁcant im- provements baselines triplets multi-word targets opinions .     Introduction Aspect-Based Sentiment Analysis ( ABSA ) ( Liu ,      ; Pontiki et al. ,      ) aggregation sev- eral ﬁne-grained sentiment analysis tasks , various subtasks designed aspect tar- get fundamental item . example ∗ Equal contribution . Lu Xu Yew Ken Chia Joint PhD Program Alibaba Singapore University Technology Design .  We make code publicly available https : // github.com/chiayewken/Span-ASTE . Figure   : example ASTE . spans highlighted orange target terms , span blue opin- ion term . “ - ” top target terms indicates nega- tive sentiment . Figure   , aspect targets “ Windows   ” “ touchscreen functions ” . Aspect Sentiment Classi- ﬁcation ( ASC ) ( Dong et al. ,      ; Zhang et al. ,      ; Yang et al. ,      ; Li et al. ,     a ; Tang et al. ,      ) one well-explored subtasks ABSA aims predict sentiment polarity given aspect target . However , always practical assume aspect target pro- vided . Aspect Term Extraction ( ATE ) ( Yin et al. ,      ; Li et al. ,     b ; et al. ,      ) focuses extracting aspect targets , Opinion Term Extraction ( OTE ) ( Yang Cardie ,      ; Klinger Cimiano ,      ; Yang Cardie ,      ) aims extract opinion terms largely determine sentiment polarity sentence cor- responding target term . Aspect Sentiment Triplet Extraction ( ASTE ) ( Peng et al. ,      ) recently proposed subtask ABSA , forms complete picture sentiment informa- tion triplet aspect target term , corresponding opinion term , expressed sen- timent . example Figure   , two triplets : ( “ Windows   ” , “ enjoy ” , Negative ) ( “ touchscreen functions ” , “ enjoy ” , Negative ) . initial approach ASTE ( Peng et al. ,      ) two-stage pipeline . ﬁrst stage extracts target terms sentiments via joint labeling scheme   , well opinion terms stan-  For example , joint tag “ B-POS ” denotes beginning -- dard BIOES   tags . second stage couples extracted target opinion terms determine paired sentiment relation . know ABSA , aspect sentiment mostly determined opinion terms expressed aspect target ( Qiu et al. ,      ; Yang Cardie ,      ) . How- ever , pipeline approach breaks interaction within triplet structure . Moreover , pipeline ap- proaches usually suffer error propagation problem . Recent end-to-end approaches ( Wu et al. ,      ; Xu et al. ,     b ; Zhang et al. ,      ) jointly extract target opinion terms classify sentiment relation . One drawback heavily rely word-to-word interactions pre- dict sentiment relation target-opinion pair . Note common aspect tar- gets opinions contain multiple words , accounts roughly one-third triplets benchmark datasets . However , previous meth- ods ( Wu et al. ,      ; Zhang et al. ,      ) predict sentiment polarity word-word pair inde- pendently , guarantee sentiment consistency forming triplet . result , prediction limitation triplets contain multi-word targets opinions inevitably hurts overall ASTE performance . example Figure   , considering word-to-word in- teractions , easy wrongly predict “ enjoy ” expresses positive sentiment “ Windows ” . Xu et al . (     b ) proposed position-aware tagging scheme allow model couple word target span possible opinion spans , i.e. , aspect word opinion span interactions ( vice versa , aspect span opinion word interactions ) . However , still directly model span-to- span interactions whole target spans opinion spans . paper , propose span-based model ASTE ( Span-ASTE ) , ﬁrst time di- rectly captures span-to-span interactions predicting sentiment relation aspect tar- get opinion pair . course , also con- sider single-word aspects opinions properly . model explicitly generates span representa- tions possible target opinion spans , paired sentiment relation independently predicted possible target opinion pairs . Span-based methods shown encouraging per- target span positive sentiment polarity .  A common tagging scheme sequence labeling , denot- ing “ begin , inside , outside , end single ” respectively . formance tasks , coreference res- olution ( Lee et al. ,      ) , semantic role labeling ( et al. ,     a ) , relation extraction ( Luan et al. ,      ; Wadden et al. ,      ) . However , directly applied ASTE task due different task-speciﬁc characteristics . contribution summarized follows : • tailor span-level approach explic- itly consider span-to-span interactions ASTE task conduct extensive analy- sis demonstrate effectiveness . ap- proach signiﬁcantly improves performance , especially triplets contain multi- word targets opinions . • propose dual-channel span pruning strat- egy incorporating explicit supervision ATE OTE tasks ease high com- putational cost caused span enumeration maximize chances pairing valid target opinion candidates together . • proposed Span-ASTE model outperforms previous methods signiﬁcantly ASTE task , also ATE OTE tasks four benchmark datasets BiLSTM BERT encoders .   Span-based ASTE  .  Task Formulation Let X = { x  , x  , ... , xn } denote sentence n tokens , let = { s ,  , s ,  , ... , si , j , ... , sn , n } set possible enumerated spans X , j indicating start end positions span sentence . limit span length   ≤ j − ≤ L. objective ASTE task extract possible triplets X . sentiment triplet deﬁned ( target , opinion , sentiment ) sentiment ∈ { P ositive , N egative , N eutral } .  .  Model Architecture shown Figure   , Span-ASTE consists three modules : sentence encoding , mention module , triplet module . given example , sentence ﬁrst input sentence encoding module obtain token-level representation , derive span-level representation enumerated span , “ enjoy ” , “ Win- dows   ” . adopt ATE OTE tasks supervise proposed dual-channel span prun- ing strategy obtains pruned target Figure   : Span-ASTE model structure . opinion candidates , “ Windows   ” “ enjoy ” respectively . Finally , target candidate opinion candidate coupled determine sentiment relation .  . .  Sentence Encoding explore two encoding methods obtain contextualized representation word sentence : BiLSTM BERT . BiLSTM ﬁrst obtain word repre- sentations { e  , e  , ... , ei , ... , en }    - dimension pre-trained GloVe ( Pennington et al. ,      ) embeddings contextualized bidirectional LSTM ( Hochreiter Schmid- huber ,      ) layer . ith token represented : hi = [ −→ hi ; ←− hi ] (   ) −→ hi  ward backward LSTMs respectively . ←− hi hidden states for- BERT alternative encoding method use pre-trained language model BERT ( De- vlin et al. ,      ) obtain contextualized word representations x = [ x  , x  , ... , xn ] . words tokenized multiple word pieces , use mean pooling aggregate representations . Span Representation deﬁne span rep- resentation si , j ∈ : si , j = ( cid:   ) [ hi ; hj ; fwidth ( , j ) ] [ xi ; xj ; fwidth ( , j ) ] BiLSTM BERT (   ) fwidth ( , j ) produces trainable feature em- bedding representing span width ( i.e. , j − +   ) . Besides concatenation start token , end token , width representations , span repre- sentation si , j also formed max-pooling mean-pooling across token representations span position j . experimental results found ablation study .  . .  Mention Module ATE & OTE Tasks employ ABSA sub- tasks ATE OTE guide dual-channel span pruning strategy scores predicted opinion target span . Note target terms opinion terms yet paired together stage . mention mod- ule takes representation enumerated span si , j input predicts mention types ∈ { arget , Opinion , Invalid } . P ( m|si , j ) = softmax ( FFNNm ( si , j ) ) (   ) FFNN denotes feed-forward neural net- work non-linear activation . Pruned Target Opinion sentence X length n , number enumerated spans ( n  ) , number possible pairs opinion target candidate spans ( n  ) later stage ( i.e. , triplet module ) . , computationally practical consider possible pairwise interactions using span- based approach . Previous works ( Luan et al. ,      ; Wadden et al. ,      ) employ pruning strategy reduce number spans , prune spans single pool mix different mention types . strategy fully consider PP-Span Enumerator-All consecutive subsequences < =   words-Span Extractor-Concat ( start token (     ) , end token (     ) , width (    ) ) - [ bs , num_spans ,      ] -Mention Head-Feedforward (      - >     - >     - >   ) { opinion , target , null } -CrossEntropyLoss-Relation Head-Pruner-Feedforward (      - >     - >     - >   ) -Keep top-k span embeds-K = sequence_length *  . - [ bs , top_k ,      ] -Pairwise embeds : -Concat ( span_a (      ) , span_b (      ) , multiply (      ) ) - [ bs , top_k , top_k ,      ] -Feedforward (      - >     - >     - >   ) { pos , neg , neutral , null } -CrossEntropyLoss-New Relation Head-Opinion Pruner - [ bs , top_k ,      ] -Mention loss-Target Pruner- [ bs , top_k ,      ] -Mention loss-Pairwise embeds : -Restrict opinion-target pairsSpan EnumeratorBERTSpan Embed ExtractorMentionHeadRelation HeadFinal LossInput Sequence [ bs=  , seq_len ] [ bs , seq_len ,     ] [ bs , num_spans ,   ] [ bs , num_spans ,      ] weight= . weight= . Opinion StartOpinion EndTargetStartTarget End+Pruned TargetsPruned OpinionsΦtripledfddfdLegend++++The Koreandishesaretastybutcostly.The Korean dishestastycostly.+++++Contextual Encoding ( Eq .   ) Span EnumerationThe Korean dishes++are tasty costlyThe , tastyThe , costlyKorean dishes , tastyKorean dishes , costlyTarget & Opinion Pruning ( Eq .   ) < Opinion > < Opinion > < Null > < Null > Auxiliary Span Classification ( Eq .    ) < Null > < Pos > < Neg > < Null > Pair EnumerationSentiment Triplet Classification ( Eq .   ,   ,   ) < Null > < Target > < Null > Loss ( Eq .    ) Element-wise addition mention scoreElement-wise addition mention scoreConcatenate OperationLegend+Distance EmbeddingFFNNFFNN < Null > < Opinion > < Target > Koreandishesaretastybutcostly.The Korean dishestastycostly.The Korean dishesare tasty costly+++++++FFNNThecostlytastyKorean dishesKorean dishes , tastyKorean dishes , costlyKorean dishes , tastyKorean dishes , costlyThe ++++The KoreandishesaretastyandcheapKorean dishestastycheap++++Contextual Encoding ( Eq .   ) Span Enumerationdishes++are tasty cheapdishes , tastydishes , cheapKorean dishes , tastyKorean dishes , cheapTwo-Way Span Pruning ( Eq .   ) < Positive > < Negative > < Neutral > Opinion-Target Pair EnumerationSentiment Triplet Classification ( Eq .   ,   ,   ) Loss ( Eq .    ) Concatenate OperationLegend+Distance EmbeddingFFNNFFNNdishesKorean dishes < Invalid > tastycheapTheare tasty cheapTop Target CandidatesTop Opinion CandidatesInvalid CandidatesSupervision Opinion Target Co-Extraction SubtasksSpan Boundary EmbeddingContextual EmbeddingSpan Width EmbeddingdishesKorean dishestastycheapTheare tasty cheapTop Target CandidatesTop Opinion CandidatesInvalid Candidates++++++++KKK+LegendFFNNSpan Width EmbeddingConcatenate Operation+Start-End EmbeddingContextual EmbeddingDistance EmbeddingTop-k Select OperationFFNN+++++++++++++FFFFFFFFFFFFFP+F Dataset Rest    Lap    Rest    Rest    # , # + , #   , # - , # SW # MW # , # + , #   , # - , # SW # MW # , # + , #   , # - , # SW # MW # , # + , #   , # - , # SW # MW Train                        Dev         Test                                                                                                                                                                                                                                                        Table   : Statistics datasets . # denotes number sentences . # + , #   , # - denote numbers positive , neutral , negative sentiment triplets respectively . # SW denotes number triplets target opinion terms single-word spans . # MW denotes number triplets least one target opinion terms multi-word spans . structure aspect sentiment triplet recognize fundamental difference target opinion term . Hence , propose use dual-channel pruning strategy results two separate pruned pools aspects opin- ions . minimizes computational costs maximizing chance pairing valid opinion target spans together . opinion target candidates selected based scores mention types span based Equation   : Φtarget ( si , j ) = P ( = target|si , j ) Φopinion ( si , j ) = P ( = opinion|si , j ) (   ) use mention scores Φtarget Φopinion select top candidates enumer- ated spans obtain target candidate pool St = { ... , st , b , ... } opinion candidate pool = { ... , c , , ... } respectively . consider pro- portionate number candidates sentence , number selected spans pruned target opinion candidates nz , n sen- tence length z threshold hyper-parameter . Note although pruning operation prevents gradient ﬂow back FFNN mention module , already receiving supervision ATE OTE tasks . Hence , model trained end-to-end without issue instability .  . .  Triplet Module Target Opinion Pair Representation ob- tain target-opinion pair representation cou- , b ∈ St pling target candidate representation st opinion candidate representation c , ∈ : gst , b , c , = [ st , b ; c , ; fdistance ( , b , c , ) ] (   ) fdistance ( , b , c , ) produces trainable feature embedding based distance ( i.e. , min ( |b − c| , |a − d| ) ) target opin- ion spans , following ( Lee et al. ,      ; et al. ,     a ; Xu et al. ,     b ) . span pair representation gst Sentiment Relation Classiﬁer , in-  put feed-forward neural network determine relation r ∈ probability sentiment R = { P ositive , N egative , N eutral , Invalid } target st , b opinion , b , c , : c , P ( r|st , b , c , ) = softmax ( FFNNr ( gst ) ) (   ) Invalid indicates target opinion pair valid sentiment relationship . , b , c ,  .  Training training objective deﬁned sum negative log-likelihood mention mod- ule triplet module . L = − ( cid:   ) si , j ∈S log P ( m∗ , j|si , j ) ( cid:   ) − st , b∈St , c , d∈So log P ( r∗|st , b , c , ) (   ) m∗ , j gold mention type span si , j , r∗ gold sentiment relation target opinion span pair ( st c , ) . indicates enumerated span pool ; St pruned target opinion span candidates . , b ,   Experiment  .  Datasets proposed Span-ASTE model evaluated four ASTE datasets released Xu et al . (     b ) , include three datasets restaurant do- main one dataset laptop domain . ﬁrst version ASTE datasets released Peng et al . (      ) . However , found triplets explicitly annotated ( Xu et al. ,     b ; Wu et al. ,      ) . Xu et al . (     b ) reﬁned datasets missing triplets removed triplets conﬂicting sentiments . Note Model    L B  CMLA+ ( Wang et al. ,      ) † RINANTE+ ( Dai Song ,      ) † Li-uniﬁed-R ( Li et al. ,      ) † Peng et al . (      ) † Zhang et al . (      ) ∗ GTS ( Wu et al. ,      ) ∗ JETo =  ( Xu et al. ,     b ) † Span-ASTE ( ) GTS ( Wu et al. ,      ) ∗ R E B =  ( Xu et al. ,     b ) † JETo Span-ASTE ( ) Rest    Lap    Rest    Rest    P . R .   .     .     .     .     .     .     .     .     .     .     .     .     .     .   F    .     .     .     .     .     .     .   P . R .   .     .     .     .     .     .     .     .     .     .     .     .     .     .   F    .     .     .     .     .     .     .   P . R .   .     .     .     .     .     .     .     .     .     .     .     .     .     .   F    .     .     .     .     .     .     .   P . R .   .     .     .     .     .     .     .     .     .     .     .     .     .     .   F    .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .   Table   : Results test set ASTE task . † : results retrieved Xu et al . (     b ) . ∗ : fair comparison , reproduce results using released implementation code conﬁguration ASTE datasets released Xu et al . (     b ) . four benchmark datasets derived Se- mEval Challenge ( Pontiki et al. ,      ,      ,      ) , opinion terms retrieved ( Fan et al. ,      ) . Table   shows detailed statistics .  .  Experiment Settings using BiLSTM encoder , pre-trained GloVe word embeddings trainable . hid- den size BiLSTM encoder     dropout rate  .  . second setting , ﬁne- tune pre-trained BERT ( Devlin et al. ,      ) encode sentence . Speciﬁcally , use un- cased version BERTbase . model trained    epochs linear warmup    % training steps followed linear decay learning rate   . employ AdamW optimizer maximum learning rate  e-  transformer weights weight decay  e-  . parameter groups , use learning rate  e-  weight decay . maximum span length L set   . span pruning threshold z set  .  . select best model weights based F  scores development set reported results average   runs different random seeds .    .  Baselines baselines summarized two groups : pipeline methods end-to-end methods . opinion terms BIOES tags ﬁrst stage . second stage , extracted targets opinions paired determine form valid triplet . Note approaches employ different methods obtain features ﬁrst stage . CMLA+ ( Wang et al. ,      ) employs attention mechanism consider interaction as- pect terms opinion terms . RINANTE+ ( Dai Song ,      ) adopts BiLSTM-CRF model mined rules capture dependency rela- tions . Li-uniﬁed-R ( Li et al. ,      ) uses uniﬁed tagging scheme jointly extract aspect term associated sentiment . Peng et al . (      ) in- cludes dependency relation information con- sidering interaction aspect opinion terms . End-to-end end-to-end methods aim jointly extract full triplets single stage . Pre- vious work Zhang et al . (      ) Wu et al . (      ) independently predict sentiment relation possible word-word pairs , hence require decoding heuristics determine overall senti- ment polarity triplet . JET ( Xu et al. ,     b ) models ASTE task structured prediction problem position-aware tagging scheme capture interaction three elements triplet .  .  Experiment Results Pipeline pipeline approaches listed be- low , modiﬁed Peng et al . (      ) extract aspect terms together asso- ciated sentiments via joint labeling scheme ,  See Appendix experimental settings , also dev results four datasets . Table   compares Span-ASTE previous mod- els terms Precision ( P. ) , Recall ( R. ) , F  scores four datasets . F  metric , model consistently outperforms previous works BiLSTM BERT sentence en- coders . cases , model signiﬁcantly out- performs end-to-end methods preci- sion recall . also observe two strong pipeline methods ( Li et al. ,      ; Peng et al. ,      ) achieved competitive recall results , overall performance much worse due low preci- sion . Speciﬁcally , using BiLSTM encoder GloVe embedding , model outperforms best pipeline model ( Peng et al. ,      )   .   ,  .   ,  .   ,   .   F  points four datasets . result indicates end-to-end approach ef- fectively encode interaction target opinion spans , also alleviates error propa- gation . general , end-to-end methods also competitive pipeline meth- ods . However , due limitations relying word-level interactions , performances less encouraging cases , results Lap    Rest    . BERT encoder , three end-to-end models achieve much stronger per- formance LSTM-based versions , consistent previous ﬁndings ( Devlin et al. ,      ) . approach outperforms previous best results GTS ( Wu et al. ,      )  .   ,  .   ,  .   ,  .   F  points four datasets .  .  Additional Experiments mentioned Section  . .  , employ ABSA subtasks ATE OTE guide span pruning strategy . examine Span-ASTE effectively extract target spans opinion spans , also evaluate model ATE OTE tasks four datasets . Table   shows com- parisons approach previous method GTS ( Wu et al. ,      ) .   Without additional retrain- ing tuning , model directly address ATE OTE tasks , signiﬁcant performance improvement GTS terms F  scores tasks . Even though GTS shows better re- call score Rest    dataset , low precision score results worse F  performance . better overall performance indicates span-level method beneﬁts sentiment triplet ex- traction , also improves extraction target opinion terms considering semantics whole span rather relying decoding heuristics tagging-based methods .  See Appendix target opinion data statistics . Note JET model ( Xu et al. ,     b ) able directly solve ATE OTE tasks unless evaluation conducted based triplet predictions . include comparisons Appendix . Dataset Model Rest    Lap    Rest    Rest    GTS  GTS  GTS  GTS  ATE R .   .     .     .     .     .     .     .     .   P .   .     .     .     .     .     .     .     .   OTE P . R .   .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .   F    .     .     .     .     .     .     .     .   F    .     .     .     .     .     .     .     .   Table   : Test results ATE OTE tasks BERT encoder . reference , include results RACL framework ( Chen Qian ,      ) Appendix . RACL current state-of-the-art method tasks . However , framework con- sider pairing relation target opin- ion , therefore difﬁcult completely fair comparison .   Analysis  .  Comparison Single-word Multi-word Spans compare performance Span-ASTE previous model GTS ( Wu et al. ,      ) following two settings Table   : Single-Word : target opinion terms triplet single- word spans , Multi-Word : least one target opinion terms triplet multi-word span . single-word setting , method shows con- sistent improvement terms precision recall score four datasets , re- sults improvement F  score . compare evaluations multi-word triplets , model achieves signiﬁcant improvements F  scores . Compared precision , recall shows greater improvement GTS approach . GTS heavily relies word-pair interactions ex- tract triplets , methods explicitly consider span-to-span interactions . span enumera- tion also naturally beneﬁts recall multi-word spans . GTS model , multi-word triplets pose challenges F  results drop    points , even    points Rest    . shown Table   , comparing single-word triplets , multi-word triplets com- mon account one-third even half datasets . Therefore , promising direction fu- ture work improve model ’ perfor- mance difﬁcult triplets . identify areas improvement , analyze results ASTE task based whether sentiment triplet contains multi- Mode Model  R E B Single-Word Multi-Word GTS  ∆ GTS  ∆ Rest    Lap    Rest    Rest    P . R . F  P . R . F  P . R . F  P . R . F    .     .     .     .     .     .     .     .   + .   + .   + .   + .   + .   + .   + .     .     .     .     .     .     .     .     .   + .     .     .     .     .     .     .   + .   + .   + .   + .     .     .     .     .     .     .     .     .   + .   + .   + .   + .   + .   + .   + .   +  .   + .   + .   + .   + .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .   Table   : Analysis different evaluation modes ASTE task . Dataset Model Rest    Lap    Rest    Rest    GTS  GTS  GTS  GTS  Multi-word Target Multi-word Opinion P . R .   .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .   F    .     .     .     .     .     .     .     .   P . R .   .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .   F    .     .     .     .     .     .     .     .   Table   : comparison test results model GTS based triplets multi-word targets opinions ASTE task . word target multi-word opinion term . Ta- ble   , results show performance lower triplet contains multi-word opinion term . trend attributed imbalanced data distribution triplets contain multi-word target opinion terms .  .  Pruning Efﬁciency demonstrate efﬁciency proposed dual-channel pruning strategy , also compare simpler strategy , denoted Single-Channel ( SC ) distinguish opin- ion target candidates . Figure   shows comparisons . Note mention module strategy explicitly solve ATE OTE tasks predicts mention label ∈ { V alid , Invalid } , V alid means span either target opinion span Invalid means span belong two groups . Given sentence length n pruning threshold z , number candidates limited nz , hence computational cost scales number pairwise interactions , n z  . dual-channel strategy considers target-opinion pair pruned target opinion candidate pools nz spans . Note possible two pools share candidates . com- parison , single-channel strategy considers    ) % (   F       Dual-Channel Single-Channel ( SC ) SC-Adjusted  .         .       .     .     .  Figure   : Dev results respect pruning threshold z intuitively refers number candidate spans keep per word sentence . target-opinion pair target opinion candidates drawn single pool nz spans . order consider least many target opinion candidates dual-channel strategy , single-channel strategy scale threshold z two , leads   times pairs computational cost . denote set- ting Figure   SC-Adjusted . controlling computational efﬁciency , signiﬁcant performance difference Dual-Channel Single-Channel F  score , especially lower values z . Although performance gap narrows increasing z , practical high values . According experimental results , select dual-channel pruning strategy z =  .  reported model .  .  Qualitative Analysis illustrate differences models , present sample sentences ASTE test set gold labels well predictions GTS ( Wu et al. ,      ) Span-ASTE Figure   . ﬁrst example , GTS correctly extracts tar- get term “ Windows   ” paired opinion term “ enjoy ” , sentiment incorrectly pre- dicted positive . forming triplet , decoding heuristic considers sentiment inde- Figure   : Qualitative analysis . target opinion terms highlighted orange blue respectively . arc indicates pairing relation target opinion terms . sentiment polarity triplet indicated target terms . pendently word-word pair : { ( “ Windows ” , “ ” , Neutral ) , ( “   ” , “ ” , Neutral ) , ( “ Windows ” , “ enjoy ” , Positive ) , ( “   ” , “ enjoy ” , Positive ) } . heuristic votes overall sentiment polarity frequent label among pairs . case tie (   neutral   positive ) , heuristic predeﬁned bias assign sentiment polar- ity positive . Similarly , word-level method fails capture negative sentiment expressed “ enjoy ” target term “ touchscreen functions ” . second example , incompletely extracts target term “ Korean dishes ” , resulting wrong triplet . examples , method able accurately extract target-opinion pairs determine overall sentiment even term multiple words .  .  Ablation Study conduct ablation study examine per- formance different modules span representa- tion methods , results shown Table   . average F  denotes average dev results Span-ASTE four benchmark datasets   runs . Similar observation coreference resolution ( Lee et al. ,      ) , ﬁnd ASTE performance reduced removing span width distance embedding . indicates positional information still useful ASTE task targets opinions far apart long less likely form valid span pair . mentioned Section  . .  , explore two methods ( i.e. , max pooling mean pooling ) form span representations instead concatenating span boundary token represen- tations . negative results suggest using pooling aggregate span representation dis- advantageous due loss information useful distinguishing valid invalid spans . Model Full model W/O width & distance embedding max pooling mean pooling Average F  ∆F    .     .     .     .   - .   - .   - .   Table   : Ablation study development sets .   Related Work Sentiment Analysis major Natural Language Understanding ( NLU ) task ( Wang et al. ,      ) extensively studied classiﬁcation problem sentence level ( Raffel et al. ,      ; Lan et al. ,      ; Yang et al. ,      ) . Aspect-Based Sentiment Analysis ( ABSA ) ( Pontiki et al. ,      ) addresses various sentiment analysis tasks ﬁne- grained level . mentioned Section   , subtasks mainly include ASC ( Dong et al. ,      ; Zhang et al. ,      ; Chen et al. ,      ; et al. ,     b ; Li et al. ,     a ; Peng et al. ,      ; Wang Lu ,      ; et al. ,      ; Li Lu ,      ; Xu et al. ,     a ) , ATE ( Qiu et al. ,      ; Yin et al. ,      ; Li et al. ,     b ; et al. ,      ) , OTE ( Hu Liu ,      ; Yang Cardie ,      ; Klinger Cimiano ,      ; Yang Cardie ,      ) . also another subtask named Target-oriented Opin- ion Words Extraction ( TOWE ) ( Fan et al. ,      ) , aim extract corresponding opinion words given target term . Another line research focuses addressing different subtasks together . Aspect Opinion Term Co-Extraction ( AOTE ) aiming extract aspect opinion terms together ( Wang et al. ,      ; et al. ,      ; Dai Song ,      ) often treated se- quence labeling problem . Note AOTE consider paired sentiment relationship be- tween target opinion term . End-to-End ABSA ( Li Lu ,      ; et al. ,      ; Li et al. ,      ; et al. ,      ) jointly extracts aspect +++ -- -- + term associated sentiment end-to-end manner . methods recently pro- posed jointly solve three subtasks ABSA . Chen Qian (      ) proposed relation aware collaborative learning framework unify three fundamental subtasks achieved strong performance subtask combined task . Wan et al . (      ) focused aspect category related subtasks , Aspect Category Extraction Aspect Category Target Joint Extraction . ASTE ( Peng et al. ,      ; Wu et al. ,      ; Xu et al. ,     b ; Zhang et al. ,      ) recent development ABSA aim extract form aspect term , associated sentiment , corresponding opinion term triplet .   Conclusions work , propose span-level approach - Span-ASTE learn interactions target spans opinion spans ASTE task . address limitation existing approaches consider word-to-word interactions . also propose include ATE OTE tasks supervision dual-channel pruning strat- egy reduce number enumerated target opinion candidates increase computational ef- ﬁciency maximize chances pairing valid target opinion candidates together . method signiﬁcantly outperforms previous methods ASTE well ATE OTE tasks analy- sis demonstrates effectiveness approach . achieve strong performance ASTE task , performance mostly attributed improvement multi-word triplets . discussed Section  .  , still signiﬁcant performance gap single-word multi- word triplets , potential area future work . References Peng Chen , Zhongqian Sun , Lidong Bing , Wei Yang .      . Recurrent attention network mem- Proc . ory aspect sentiment analysis . EMNLP . Jacob Devlin , Ming-Wei Chang , Kenton Lee , Kristina Toutanova .      . BERT : Pre-training deep bidirectional transformers language under- standing . Proc . NAACL . Li Dong , Furu Wei , Chuanqi Tan , Duyu Tang , Ming Zhou , Ke Xu .      . Adaptive recursive neural network target-dependent twitter sentiment clas- siﬁcation . Proc . ACL . Zhifang Fan , Zhen Wu , Xinyu Dai , Shujian Huang , Jiajun Chen .      . Target-oriented opinion words extraction target-fused neural sequence label- ing . Porc . NAACL . Matt Gardner , Joel Grus , Mark Neumann , Oyvind Tafjord , Pradeep Dasigi , Nelson F. Liu , Matthew Peters , Michael Schmitz , Luke S. Zettlemoyer .      . Allennlp : deep semantic natural language processing platform . Luheng , Kenton Lee , Omer Levy , Luke Zettle- moyer .     a . Jointly predicting predicates ar- Proc . guments neural semantic role labeling . ACL . Ruidan , Wee Sun Lee , Hwee Tou Ng , Daniel Dahlmeier .     b . Effective attention modeling Proc . aspect-level sentiment classiﬁcation . COLING . Ruidan , Wee Sun Lee , Hwee Tou Ng , Daniel Dahlmeier .      . interactive multi-task learning network end-to-end aspect-based sentiment anal- ysis . Proc . ACL . Sepp Hochreiter J¨urgen Schmidhuber .      . Long short-term memory . Neural computation ,  :    –    . Minqing Hu Bing Liu .      . Mining summa- rizing customer reviews . Proc . ACM SIGKDD . R. Klinger P. Cimiano .      . Joint pipeline probabilistic models ﬁne-grained sentiment anal- ysis : Extracting aspects , subjective phrases relations .      IEEE   th International Confer- ence Data Mining Workshops . Zhenzhong Lan , Mingda Chen , Sebastian Goodman , Kevin Gimpel , Piyush Sharma , Radu Soricut .      . Albert : lite bert self-supervised learn- ing language representations . Proc . ICLR . Kenton Lee , Luheng , Mike Lewis , Luke Zettle- moyer .      . End-to-end neural coreference resolu- tion . Proc . EMNLP . Zhuang Chen Tieyun Qian .      . Relation-aware collaborative learning uniﬁed aspect-based senti- ment analysis . Proc . ACL . Hao Li Wei Lu .      . Learning latent sentiment scopes entity-level sentiment analysis . Proc . AAAI . Hongliang Dai Yangqiu Song .      . Neural as- pect opinion term extraction mined rules weak supervision . Proc . ACL . Hao Li Wei Lu .      . Learning explicit im- plicit structures targeted sentiment analysis . Proc . EMNLP . Xin Li , Lidong Bing , Wai Lam , Bei Shi .     a . Transformation networks target-oriented senti- ment classiﬁcation . Proc . ACL . Xin Li , Lidong Bing , Piji Li , Wai Lam .      . uniﬁed model opinion target extraction target sentiment prediction . Proc . AAAI . Xin Li , Lidong Bing , Piji Li , Wai Lam , Zhimou Yang .     b . Aspect term extraction history Proc . attention selective transformation . IJCAI . Bing Liu .      . Sentiment analysis opinion min- ing . Synthesis lectures human language technolo- gies ,   (   ) : –    . Colin Raffel , Noam Shazeer , Adam Roberts , Kather- ine Lee , Sharan Narang , Michael Matena , Yanqi Zhou , Wei Li , Peter J. Liu .      . Exploring limits transfer learning uniﬁed text-to- text transformer . Journal Machine Learning Re- search ,    (     ) : –   . Jialong Tang , Ziyao Lu , Jinsong Su , Yubin Ge , Linfeng Song , Le Sun , Jiebo Luo .      . Progressive self- supervised attention learning aspect-level senti- ment analysis . Proc . ACL . David Wadden , Ulme Wennberg , Yi Luan , Han- naneh Hajishirzi .      . Entity , relation , event extraction contextualized span representations . Proc . EMNLP . Yi Luan , Dave Wadden , Luheng , Amy Shah , Mari Ostendorf , Hannaneh Hajishirzi .      . gen- eral framework information extraction using dy- namic span graphs . Proc . NAACL . Hai Wan , Yufei Yang , Jianfeng Du , Yanan Liu , Kunxun Qi , Jeff Z. Pan .      . Target-aspect-sentiment joint detection aspect-based sentiment analysis . Proc . AAAI . Dehong , Sujian Li , Houfeng Wang .      . Joint learning targeted sentiment analysis . Proc . EMNLP . Dehong , Sujian Li , Fangzhao Wu , Xing Xie , Houfeng Wang .      . Exploring sequence-to- sequence learning aspect term extraction . Proc . ACL . Haiyun Peng , Yukun , Yang Li , Erik Cam- bria .      . Learning multi-grained aspect target se- quence chinese sentiment analysis . Knowledge- Based Systems ,    :   –    . Haiyun Peng , Lu Xu , Lidong Bing , Fei Huang , Wei Lu , Luo Si .      . Knowing , : near complete solution aspect-based sentiment analysis . Proc . AAAI . Jeffrey Pennington , Richard Socher , Christopher . Manning .      . Glove : Global vectors word rep- resentation . Proc . EMNLP . Maria Pontiki , Dimitris Galanis , Haris Papageor- giou , Ion Androutsopoulos , Suresh Manandhar , Mo- hammed AL-Smadi , Mahmoud Al-Ayyoub , Yanyan Zhao , Bing Qin , Orph´ee De Clercq , et al .      . Semeval-     task   : Aspect based sentiment anal- ysis . Proc . SemEval . Maria Pontiki , Dimitris Galanis , Haris Papageorgiou , Suresh Manandhar , Ion Androutsopoulos .      . SemEval-     task    : Aspect based sentiment analysis . Proc . SemEval . Alex Wang , Amanpreet Singh , Julian Michael , Fe- lix Hill , Omer Levy , Samuel Bowman .      . GLUE : multi-task benchmark analysis plat- Proc . form natural language understanding . ICLR . Bailin Wang Wei Lu .      . Learning latent opin-  ions aspect-level sentiment classiﬁcation . Proc . AAAI . Wenya Wang , Sinno Jialin Pan , Daniel Dahlmeier , Xiaokui Xiao .      . Coupled multi-layer attentions  co-extraction aspect opinion terms . Proc . AAAI . Zhen Wu , Chengcan Ying , Fei Zhao , Zhifang Fan , Xinyu Dai , Rui Xia .      . Grid tagging scheme aspect-oriented ﬁne-grained opinion extraction . Findings EMNLP . Lu Xu , Lidong Bing , Wei Lu , Fei Huang .     a . Aspect sentiment classiﬁcation aspect-speciﬁc opinion spans . Proc . EMNLP . Lu Xu , Hao Li , W. Lu , Lidong Bing .     b . Position-aware tagging aspect sentiment triplet extraction . Proc . EMNLP . Bishan Yang Claire Cardie .      . Extracting opin- ion expressions semi-Markov conditional ran- dom ﬁelds . Proc . EMNLP . Bishan Yang Claire Cardie .      . Joint inference ﬁne-grained opinion extraction . Proc . ACL . Maria Pontiki , Dimitris Galanis , John Pavlopoulos , Ion Androutsopoulos , Harris Papageorgiou , Suresh Manandhar .      . Semeval-     task   : As- pect based sentiment analysis . Proc . SemEval . Min Yang , Wenting Tu , Jingxuan Wang , Fei Xu , Xiaojun Chen .      . Attention based lstm tar- get dependent sentiment classiﬁcation . Proc . AAAI . Guang Qiu , Bing Liu , Jiajun Bu , Chun Chen .      . Opinion word expansion target extrac- tion double propagation . Computational Linguistics ,    (   ) : –   . Zhilin Yang , Zihang Dai , Yiming Yang , Jaime Car- bonell , Ruslan Salakhutdinov , Quoc V. Le .      . Xlnet : Generalized autoregressive pretraining language understanding . Proc . NeurIPS . Yichun Yin , Furu Wei , Li Dong , Kaimeng Xu , Ming Zhang , Ming Zhou .      . Unsupervised word dependency path embeddings aspect term ex- traction . Proc . IJCAI . Chen Zhang , Qiuchi Li , Dawei Song , Benyou Wang .      . multi-task learning framework opinion triplet extraction . Findings EMNLP . Meishan Zhang , Yue Zhang , Duy-Tin Vo .      . Gated neural networks targeted sentiment anal- ysis . Proc . AAAI . Additional Experimental Settings run model experiments Nvidia Tesla V    GPU , CUDA version   .  PyTorch version  . .  . average run time BERT- based model     sec/epoch ,     sec/epoch ,    sec/epoch ,     sec/epoch Rest    , Lap    , Rest    , Rest    respectively . total number parameters  .  M GloVe used ,    M BERT base used . feed-forward neural networks mention module triplet module   hidden layers hidden size     . use ReLU activation dropout  .  hidden layer . use Xavier Normal weight initialization feed-forward parameters . span width distance embeddings        dimensions respectively . input values bucketed ( Gardner et al. ,      ) fed embedding matrix lookup : [   ,   ,   ,   ,   ,  -  ,  -   ,   -   ,   -   ,   + ] . training , model parameters updated sentence results batch size   . input text sequence , restrict maximum     tokens . B Additional Data Statistics Table   shows number target terms opin- ion terms four datasets . C Dev Results Table    shows results model development datasets . Additional Comparisons mentioned footnote   Section  .  , can- make direct comparison JET model ( Xu et al. ,     b ) , able directly solve ATE OTE tasks unless evaluation conducted based triplet results . Table   shows comparisons . proposed method Dataset Model Rest    Lap    Rest    Rest    =  =  =  =  JETo GTS  JETo GTS  JETo GTS  JETo GTS  ATE R .   .     .     .     .     .     .     .     .     .     .     .     .   P .   .     .     .     .     .     .     .     .     .     .     .     .   OTE P . R .   .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .   F    .     .     .     .     .     .     .     .     .     .     .     .   F    .     .     .     .     .     .     .     .     .     .     .     .   Table   : Test results ATE OTE tasks sub-optimal evaluation method . method GTS ( Wu et al. ,      ) allow ATE OTE tasks predicted independently ASTE task . However , JETo =  ( Xu et al. ,     b ) . Hence , make another comparison extracting opinion target spans ASTE predictions . Dataset Model P . GTS   .   RACL   .     .    GTS   .   RACL   .     .    GTS   .   RACL   .     .    GTS   .   RACL   .     .    Rest    Lap    Rest    Rest    ATE R .   .     .     .     .     .     .     .     .     .     .     .     .   OTE P . R .   .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .   F    .     .     .     .     .     .     .     .     .     .     .     .   F    .     .     .     .     .     .     .     .     .     .     .     .   Table   : Additional comparison test results ATE OTE tasks . Note RACL ( Chen Qian ,      ) consider supervision target- opinion pairs , includes sentiment polarities target terms . generally outperforms previous two end-to-end approaches four datasets . mentioned Table   , challenging make fair comparison previous ABSA framework RACL ( Chen Qian ,      ) , also address ATE OTE tasks solving ABSA subtasks , approach well GTS ( Wu et al. ,      ) . mentioned approaches different task set- tings . RACL considers sentiment polarity target terms solving ATE OTE tasks , GTS method consider pairing relation target opinion terms . However , reference , Table   shows compar- Dataset Train Dev Test Rest    Lap    Rest    Rest    # Target # Opinion # Target # Opinion # Target # Opinion # Target # Opinion                                                                                                                         Table   : Additional statistics . # Target denotes number target terms . # Opinion denotes numbers opinion terms . Model Rest    Lap    Rest    Rest    P . R . F  P . R . F  P . R . F  P . R . F  ( BiLSTM ) ( BERT )   .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .     .   Table    : Results development datasets . isons three methods ATE OTE tasks datasets released Xu et al . (     b ) . 