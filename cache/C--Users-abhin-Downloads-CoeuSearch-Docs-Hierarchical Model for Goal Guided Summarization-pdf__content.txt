Hierarchical Model Goal Guided Summarization Annual Financial Reports Yash Agrawal yash.agrawal @ research.iiit.ac.in IIIT Hyderabad , India Arunachalam s_arunachalam @ isb.edu ISB Hyderabad , India Vivek Anand vivek.a @ research.iiit.ac.in IIIT Hyderabad , India Vasudeva Varma vv @ iiit.ac.in IIIT Hyderabad , India ABSTRACT Every year publicly listed companies file financial reports give insights activities . reports meant share- holders general public evaluate company ’ health decide whether buy sell stakes company . However , annual financial reports tend long , time-consuming go reports company . propose Goal Guided Summarization technique summary extracted . goal , case , decision buy sell com- pany ’ shares . use hierarchical neural models achieving goal extracting summaries . means intrinsic extrinsic evaluation observe summaries extracted approach model decision buying selling shares better compared summaries extracted summarization techniques well complete document . also observe summary extractor model used construct stock portfolios give better returns compared major stock index . CCS CONCEPTS • Computing methodologies → Information extraction . KEYWORDS Guided Summarization , Financial Report Summarization , Hierar- chical Neural Model ACM Reference Format : Yash Agrawal , Vivek Anand , Arunachalam , Vasudeva Varma .      . Hierarchical Model Goal Guided Summarization Annual Financial Reports . Companion Proceedings Web Conference      ( WWW ’    Companion ) , April   –   ,      , Ljubljana , Slovenia . ACM , New York , NY , USA ,   pages . https : //doi.org/  .    /       .          INTRODUCTION advances stock market prediction based textual data using NLP techniques . models use news articles [  –   ] tweets [    ] . show text-based financial models paper published Creative Commons Attribution  .  International ( CC-BY  .  ) license . Authors reserve rights disseminate work personal corporate Web sites appropriate attribution . WWW ’    Companion , April   –   ,      , Ljubljana , Slovenia ©      IW C  ( International World Wide Web Conference Committee ) , published Creative Commons CC-BY  .  License . ACM ISBN    - -    -    - /  /   . https : //doi.org/  .    /       .        ( ) Annual Report Matson Inc. fiscal year end Dec      . stock rose   .   % successive year . ( b ) Annual Report Magna-Lab Inc. fiscal year end Feb      . stock fell   .   % successive year . Figure   : Illustration sentences   -K Filing might affect future stock price movements . useful making short term stock movement predictions . However , models based short-term events occur within company may give big picture company ’ health . Thus , data news tweets might useful short term gains , long term , analyzing company ’ overall health important . Annual financial reports companies comprehensively present big picture . Analysts read annual reports publicly listed companies evaluate . One prime reasons evaluation make call buy sell stakes company . Annual reports tend lengthy thus time-consuming read . , large number publicly listed companies make difficult individual go every company ’ annual report make decisions . Therefore manual analysis annual reports costly terms skilled labor time .     WWW ’    Companion , April   –   ,      , Ljubljana , Slovenia Yash Agrawal , Vivek Anand , Arunachalam , Vasudeva Varma Additionally , publicly available dataset finan- cial report summarization , makes problem challenging . Current state-of-the-art summarization models like BertSum [    ] require large amounts annotated data train generally absent domain specific areas . design experiment different approach extracts summary based end goal use-case summary . propose Goal Guided Sum- marization technique annual financial reports . approach motivated fact consumers information want make call buying selling stakes company . Thus goal summary provide information making buy sell decisions . goal used guide summarization approach . Since documents tend long , use hierarchical neural models document classifier . case classification task considered stock movement classification task . hierar- chical models give attention weights sentences making predictions . sentence level attention weights used ranks extract summary . Note task stock movement pre- diction different stock price prediction . former type , interested predicting binary label stock price go rather predicting actual price stock . Hence formulated two-class classification problem regression problem . Transformers BERT based models current state-of- the-art NLP tasks [   ,    ] . leverage train hierar- chical BERT based model ( refer H-BERT ) stock movement prediction task . also experiment Hi- erarchical Attention Network ( HAN ) [    ] task . train hierarchical models predict stock price movements using Management ’ Discussion Analysis ( MD & ) section   -K annual filings . use trained models extract summaries based model ’ inference attention weights . Figure   illustrates snapshot MD & section   -K candidate summary sentence highlighted .   -K filings , required U.S. Securities Exchange Commission ( SEC ) , compre- hensive reports filed annually publicly traded companies financial performance .   -K report    sections ( called Items ) like Business , Consolidated Financial Data , Management ’ Discussion Analysis , etc . choose MD & section company ’ management discusses company ’ operations detail section . contains forward-looking statements useful predicting giving company ’ future aspects . Thus MD & section becomes relevant goal buying/selling company ’ shares . Evaluating summary extracted model challenging gold summary data financial domain . address , create gold standard summary    randomly sampled MD & documents evaluation set help experts . set used intrinsic assessment sum- marization system using ROUGE [    ] scores . also perform extrinsic evaluation better support results [    ] . use performance stock movement prediction task extrin- sically evaluating summaries . show goal achieved better accuracy using summaries generated method compared summarization techniques . also evaluate reliability summary extractor model constructing portfolios stocks using model ’ predictions . compare portfolio ’ returns major stock indexes ob- serve summary extractor model gives higher returns average , ensuring summaries practical usage . contribution paper two folds . First , propose Goal Guided Summarization framework financial report sum- marization . Second , show extracted summaries effective modeling stock movement prediction task compared summarization techniques using intrinsic extrinsic evaluation .   PROBLEM DESCRIPTION Let MCT document containing Management Discussion Analysis section   -K filing company C fiscal year . contains sentences [ sent  , sent  , ... , sentN ] , senti ith sentence document . task assign label yi ∈ {   ,   } sentence senti . label indicates whether sentence included summary . extracted summary gives important information make buy sell decision company C ′s stocks make profits year +   .   RELATED WORK Efficient Market Hypothesis [    ] states security prices reflect available information everyone certain degree access information . several arguments researchers tried different ways predict stock prices . Attempts made using historical price data using different indicators [   ,    ] . methods take account actual events taking place within company . address , researchers exploited NLP techniques textual data like news events extracted [  –  ] . Augmenting historical data tweets also explored stock movement prediction [    ] . Du Tanaka-Ishii [   ] introduced stock embeddings learned news price data better predict stock price . Apart events , sentiment language also explored make stock predictions [    ,    ] . prediction techniques work short term events consider long term future plans companies . use extra information intermediate linguistic process- ing major part summarization field term Guided Summarization . Ng et al . [    ] introduced category-specific importance ( CSI ) aid sentence selection extractive summariza- tion . Xiong Litman [    ] explored summarization online reviews using review helpfulness guide . Takase et al . [    ] incorporated abstract meaning representation ( AMR ) results ad- ditional information Attention-based Summarization model . Nallapati et al . [    ] presented work adapting neural encoders extractive summarization used lexical features like named entity POS tags enriching encoder . Cao et al . [   ] used existing summaries soft templates guide Seq Seq model summary generation task . Jin et al . [    ] used semantics depen- dency guide neural abstractive summarization . Recently , large pre-trained models like BERT [   ] given better results many NLP tasks . Liu Lapata [    ] one first work leverage pre-trained language models summarization task .     Hierarchical Model Goal Guided Summarization Annual Financial Reports WWW ’    Companion , April   –   ,      , Ljubljana , Slovenia research field unsupervised sum- marization well . LexRank [    ] unsupervised graph based approach inspired PageRank [    ] HITS ( hyperlink-induced topic search ) algorithms . uses lexical centrality based approach find salience among sentences eigenvector centrality graph representation sentences . TextRank [    ] inspired PageRank algorithm employs ranking scheme graph node represents sentence graph . algorithm converges , get scores sentence graph summary generated . Ozsoy et al . [    ] explored use Latent Semantic Analysis text summarization task . identifies semantically important sentences document matrix decomposition techniques . NLP community actively working Annual Financial Reports financial disclosures . attempts predict risk stock volatility MD & section [    ] using regression techniques . works detecting omitted risk annual financial reports [    ] . Researchers also tried forecast- ing event sequence multiple  -K reports firm [    ] . studies suggest text financial reports disclosures useful gain insights make predictions . Work closely relates FNS shared task [   ,    ] attempt summarize annual financial reports filed UK . ground-truth summaries task based extracting important sections financial report like Chairman ’ statement , CEO review , etc , via rule based system human . studies use existing summarization sys- tems annual financial reports [   ] study bias present management provided summaries . Authors conclude automat- ically extracted summaries neutral compared positive bias present management provided summaries , proving im- portance system automatic summarization annual financial reports . knowledge , existing work exploited use stock price movements guide extract summary annual report . combine two task stock prediction summarization get important information summarized annual reports help make buy/sell decisions company ’ stock .   DATA COLLECTION U.S. Securities Exchange Commission ( SEC ) indepen- dent federal government regulatory agency mandates publicly traded companies report financial performance com- prehensive   -K report . obtain   -K report filings SEC website  . collected total        reports period                 different companies .   -K fillings section called Management Discussion Analysis con- tains important forward looking statements essential task stock movement prediction . sections also long , summarising give better image companies future precise manner [    ] . write script filter MD & sections loosely matching strings section headers - Table   : Basic statistics dataset Total Number Documents Total Number Companies Avg . Documents Per Company Avg . Sentences Per Document Avg . Tokens Per Document             .      .       .   reports pass filter . many reasons happen ; one different reports tend address section different ways . present work , use reports pass fil- ter [    ] . get       MD & sections filtered      different publicly traded companies . use Yahoo Finance  get stock price companies respective years . Given   -K filing company fiscal year , compare company ’ stock price next fiscal year +   capture effect filing fiscal year . example , assume   -K filing Apple Inc. fiscal year-end September      affect stock price movements October      September      , i.e. , next fiscal year . compare closing price Apple Inc. first working day end September      year get price affected w.r.t . filing fiscal year September      . label   -K filing fiscal year-end “ Buy '' stock price greater fiscal year-end +   , “ Sell '' otherwise . Since labeling data comparing price difference year , captures long term effect last year ’ filling company ’ stock price next filing published . Table   gives basic stats complete dataset gathered . Figure   : Representation data splits used training evaluation . use dataset train summary extraction model evaluate extracted summaries . make fair comparison baselines , split complete dataset half . One half dataset used training summary extraction model . half used evaluating extracted summary well comparison different baselines . splitting dataset shown Figure   . procedure enables model extract summaries unseen data , thus makes fair comparison baselines . take random samples years split  https : //www.sec.gov/edgar.shtml  https : //finance.yahoo.com     WWW ’    Companion , April   –   ,      , Ljubljana , Slovenia Yash Agrawal , Vivek Anand , Arunachalam , Vasudeva Varma dataset temporally want model learn also predict different types major events happening across years . example , reports      market crash data must present train test set model learn crash , evaluated well .   MODEL approach motivated main goal summary well unavailability summary dataset annual fi- nancial reports . propose use hierarchical neural models encoding MD & section   -K filing predict stock price movements . take MD & section contains forward looking statements helpful task stock movement prediction . input text MD & section tends long , ex- plore use hierarchical models . Hierarchical models work two levels . first level , get representations sentence document using encoding mechanism . second level , get complete document representation using sentence level representations first level . final doc- ument representation used stock movement prediction task . use BERT Sentence Transformer [    ] get sentence rep- resentation words tokens , first level hierarchical model . second level , use Bi-LSTM encode sequential information sentences . Attention layer added top Bi-LSTM get complete document representation . Consider input MD & section n number sentences . Let sentence represented si , ∈ [   , n ] . passing Sentence BERT [    ] . sentences contains Li number words . Let words represented wit , ∈ [   , Li ] ith sentence . Note use pre-trained Sentence BERT fine-tune training . si = SBERT ( [ wit ] ) , ∈ [   , Li ] (   ) second level , use hierarchical model encode se- quential information among sentences . use Bi-LSTM concatenate hidden states directions get complete hidden states corresponding sentences . −→ hi = ←− hi = hi = [ −−−−→ LSTM ( si ) , ∈ [   , n ] , ←−−−− LSTM ( si ) , ∈ [   , n ] , −→ hi , ←− hi ] (   ) (   ) (   ) Finally apply sentence level attention get single fixed length document representation . ai = ui = tanh ( Wshi + bs ) exp ( uT exp ( uT aihi ( cid:    ) = ( cid:    ) us ) us ) (   ) (   ) (   )  Ws , bs us learnable weights . document representation passed though linear layer followed softmax layer get Figure   : Architecture H-BERT Goal Guided Summa- rization Annal Financial Reports . probability stock movement prediction classification . p = f tmax ( Wcd + bc ) (   ) case , p  D vector Buy Sell classes . use negative log-likelihood correct element ( P ) vector training loss . L = − ( cid:    )  loд ( PD ) (   ) way , model trained predict stock movements , i.e. , whether stock price go given MD & section   -K filing . done training split dataset discussed Section   . training split divided standard train , validation , test split    % ,    %    % , re- spectively . validation split used find logical point stop training epochs , model least validation loss considered final trained model . trained model outputs classification prediction stock movements , also get attention weights given sentence MD & section ai , ∈ [   , N ] . sort attention weights pick sentences top attention weights .     Hierarchical Model Goal Guided Summarization Annual Financial Reports WWW ’    Companion , April   –   ,      , Ljubljana , Slovenia intuition attended sentences respon- sible predicting stock movement thus carry important information present summary . model ’ training done stock movement predic- tion task , turn acts guide summarization task . goal summary ( concise version full document help make buy/sell decision ) also training objective , call Goal Guided Summarization . also experiment adding past market condition document representation Equation   . add returns & P      Index last year given annual report concate- nating returns single  -D feature document representation . intuition remove systematic risk factors involved . example , market crash last year , could many negative sentences document . hence , sentence even slight positive orientation important enough present summary . expect model factor systematic events providing index returns . comparison purpose , also experiment Hierarchical Attention Network ( HAN ) [    ] stock movement predictor . model similar , instead SBERT Equation   , Bi-LSTM layer attention used get sentence representations . Similar H-BERT , HAN also , pick sentences highest computed attention weights test time extract summaries .   EVALUATION existing dataset summarization financial do- main , create manually ground-truth dataset summaries serve gold standard . randomly sample    documents evaluation split discussed Section   . Two human experts field finance given task pick important sentences documents ( MD & sections   -K filings ) help make invest- ment ( buy/sell ) decision . annotators senior researchers field thus capable task . gold standard sum- maries used report ROUGE [    ] score compare meth- ods . ROUGE scores computed ROUGE  .  , Java pack- age developed Kavita Ganesan  [    ] . studied automatic evaluation may neces- sarily correlate human judgements [    ,    ] . Thus also perform extrinsic evaluation extracted summaries . introduce goal specific evaluation summaries . paper , assumed goal summaries able convey important information decide buying selling company ’ shares . setting , use evaluation split dataset discussed Section   evaluate summaries . extract summaries evaluation split using trained model discussed Section   . Note evaluation split ’ data points included training time hence unseen trained model . model ranks sentences according attention weights , decide extent summarization . extract    % top-ranked sentences get summary . also take    % summary ranking based baseline  S & P     Standard & Poor ’     Index stock market index measures stock performance     largest publicly-traded companies U.S .  https : //github.com/kavgan/ROUGE- .  method [    ,    ,    ] fair comparison . consider stock movement labels ( buy sell ) summaries split train , validation , test sets . train HAN document classifier start report test results . procedure applied baseline extracted summaries . intuition test useful summaries achieving goal , i.e . modeling stock price movements . convergence , observe test scores models . use standard Accuracy Matthews Correlation Coefficient ( MCC ) metrics comparison . two metrics used previous stock movement prediction works [  –  ,    ] . MCC useful measure even class imbalance considers four categories - true positives , false negatives , true negatives , false positives . given formula : MCC = tp ∗ tn − f p ∗ f n ( cid:    ) ( tp + f p ) ( tp + f n ) ( tn + f p ) ( tn + f n )   BASELINE BertSum [    ] uses document level encoder based Bert [   ] . BertSumExt model introduced inter-sentence transformer layer using [ CLS ] output finally linear classification layer clas- sify sentence include exclude summary . Bert- SumExtAbs model uses two-stage fine-tuning en- coder trained extractive summarization task trained model used train encoder-decoder framework abstractive summarization task . include BertSumExt BertSumExtAbs baselines . also include three unsupervised extractive summarization techniques baselines . LexRank [    ] unsupervised graph based approach inspired PageRank HITS ( hyperlink-induced topic search ) algorithms . uses lexical centrality approach find salience among sentences eigenvector centrality graph representation sentences . TextRank [    ] also inspired PageRank algorithm em- ploys ranking scheme graph node represents sentence graph . algorithm converges , scores sentence graph summary extracted . LSA [    ] explores use Latent Semantic Analysis text summarization task . identifies semantically important sentences document matrix decomposition techniques .   EXPERIMENTAL SETUP model implemented Pytorch  framework . Due comparatively small size dataset , model dimensions chosen relatively small . H-BERT model , SBERT gives sentence representation     dimensions . document level Bi-LSTM hidden state dimensions set    dimensions resulted document representation    dimensions . Note SBERT fine-tuned process . HAN model , used GloVe pre-trained embeddings    dimensions . Hidden state dimension word encoder sentence encoder module set       dimensions respectively . resulted sentence document level representation       dimensions , respec- tively . used learning rate  .     Adam optimizer [    ] .  https : //pytorch.org     WWW ’    Companion , April   –   ,      , Ljubljana , Slovenia Yash Agrawal , Vivek Anand , Arunachalam , Vasudeva Varma Table   : Intrinsic evaluation using ROUGE- metric . Table   : Test Accuracy Matthews Correlation Coefficient ( MCC ) different summarization methods along different ad- ditional features . Note HAN model , setting used training summary extraction module well evaluation ( Section   ) . cases , use validation set decide early stopping point training . take model gives least validation loss best model .   RESULTS DISCUSSION  .  Intrinsic Evaluation Table   gives results ROUGE-  , ROUGE-  ROUGE-L met- rics . scores computed average    human extracted gold summaries . observed supervised summarization methods like BertSumExtAbs BertSumExt give good preci- sion score low recall . fact trained CNN/DailyMail [    ] dataset . datasets small summaries average around  .    .   sen- tences [    ] . fact models trained extract sentences thus result poor recall score . Precision also boosted reason sentences extracted summaries . observe H-BERT , without Index History gives comparable results . could fact Index History feature useful improve stock movement prediction capabilities ( Section  .  ) much effect sentence pays attention . H-BERT along HAN gives better results compared summarization techniques indicating sentences paid attention good summary candidates .  .  Extrinsic Evaluation Table   compares results proposed method summa- rization baseline summarization methods . Results computed per evaluation methodology given Section   . WWW ’   Companion , April  –  ,     , Ljubljana , SloveniaYashAgrawal , VivekAnand , SArunachalam , andVasudevaVarmaTable  : IntrinsicevaluationusingROUGE-metric.ROUGE- ROUGE- ROUGE-LPRF PRF PRF BertSumExtAbs  .   .    .    .   .   .    .   .    .  BertSumExt  .    .    .    .   .   .    .   .    .  LexRank  .    .    .    .    .    .    .    .    .  TextRank  .    .    .    .    .    .    .    .    .  LSA  .    .    .    .    .    .    .    .    .  HAN  .    .    .    .    .    .    .    .    .  H-BERT  .    .    .    .    .    .    .    .    .  H-BERT+IndexHistory  .    .    .    .    .    .    .    .    .  Table  : TestAccuracyandMatthewsCorrelationCoefficient ( MCC ) fordifferentsummarizationmethodsalongdifferentad-ditionalfeatures. ( ) OnlySummariesMethodAccuracy ( % ) MCCFullMD & ASection  .   .    BertSumExtAbs  .   .    BertSumExt  .   .    LexRank  .   .    TextRank  .   .    LSA  .   .    HAN  .   .    H-BERT  .   .    H-BERT+IndexHistory  .   .     ( b ) Summaries+SICMethodAccuracy ( % ) MCCFullMD & ASection  .   .    BertSumExtAbs  .   .    BertSumExt  .   .    LexRank  .   .    TextRank  .   .    LSA  .   .    HAN  .   .    H-BERT  .   .    H-BERT+IndexHistory  .   .     ( c ) Summaries+IndexMethodAccuracy ( % ) MCCFullMD & ASection  .   .    BertSumExtAbs  .   .    BertSumExt  .   .    LexRank  .   .    TextRank  .   .    LSA  .   .    HAN  .   .    H-BERT  .   .    H-BERT+IndexHistory  .   .     ( ) Summaries+SIC+IndexMethodAccuracy ( % ) MCCFullMD & ASection  .   .    BertSumExtAbs  .   .    BertSumExt  .   .    LexRank  .   .    TextRank  .   .    LSA  .   .    HAN  .   .    H-BERT  .   .    H-BERT+IndexHistory  .   .    NotethatforHANmodel , samesettingwasusedfortrainingthesummaryextractionmoduleaswellasforevaluation ( Section  ) .Forallcases , weuseavalidationsettodecideanearlystoppingpointwhiletraining.Wetakethemodelthatgivestheleastvalidationlosstobethebestmodel. RESULTSANDDISCUSSION . IntrinsicEvaluationTable givestheresultsforROUGE-  , ROUGE- andROUGE-Lmet-rics.Thescoresarecomputedasaverageover  humanextractedgoldsummaries.ItisobservedthatthesupervisedsummarizationmethodslikeBertSumExtAbsandBertSumExtgivegoodpreci-sionscorebutlowrecall.ThisisbecauseofthefactthattheyaretrainedonCNN/DailyMail [    ] dataset.Thesedatasetsaresmallandhavesummarieswithanaverageofaround .  to .  sen-tences [    ] .Becauseofthisfactthemodelsaretrainedtoextractveryfewsentencesandthusresultinpoorrecallscore.Precisionisalsoboostedbecauseofthesamereasonthatfewsentencesareextractedassummaries.WeobservethatH-BERT , withandwithoutIndexHistorygivescomparableresults.ThiscouldbebecauseofthefactthattheIndexHistoryfeatureisusefultoimprovethestockmovementpredictioncapabilities ( Section .  ) butdoesnothavemucheffectonthesentenceitpaysattentionto.H-BERTalongwithHANgivesbetterresultscomparedtoothersummarizationtechniquesindicatingthatthesentencesthatarebeingpaidmoreattentiontoaregoodsummarycandidates. . ExtrinsicEvaluationTable comparestheresultsoftheproposedmethodofsumma-rizationwithotherbaselinesummarizationmethods.ResultsarecomputedaspertheevaluationmethodologygiveninSection .WWW ’   Companion , April  –  ,     , Ljubljana , SloveniaYashAgrawal , VivekAnand , SArunachalam , andVasudevaVarmaTable  : IntrinsicevaluationusingROUGE-metric.ROUGE- ROUGE- ROUGE-LPRF PRF PRF BertSumExtAbs  .   .    .    .   .   .    .   .    .  BertSumExt  .    .    .    .   .   .    .   .    .  LexRank  .    .    .    .    .    .    .    .    .  TextRank  .    .    .    .    .    .    .    .    .  LSA  .    .    .    .    .    .    .    .    .  HAN  .    .    .    .    .    .    .    .    .  H-BERT  .    .    .    .    .    .    .    .    .  H-BERT+IndexHistory  .    .    .    .    .    .    .    .    .  Table  : TestAccuracyandMatthewsCorrelationCoefficient ( MCC ) fordifferentsummarizationmethodsalongdifferentad-ditionalfeatures . ( ) OnlySummariesMethodAccuracy ( % ) MCCFullMD & ASection  .   .    BertSumExtAbs  .   .    BertSumExt  .   .    LexRank  .   .    TextRank  .   .    LSA  .   .    HAN  .   .    H-BERT  .   .    H-BERT+IndexHistory  .   .     ( b ) Summaries+SICMethodAccuracy ( % ) MCCFullMD & ASection  .   .    BertSumExtAbs  .   .    BertSumExt  .   .    LexRank  .   .    TextRank  .   .    LSA  .   .    HAN  .   .    H-BERT  .   .    H-BERT+IndexHistory  .   .     ( c ) Summaries+IndexMethodAccuracy ( % ) MCCFullMD & ASection  .   .    BertSumExtAbs  .   .    BertSumExt  .   .    LexRank  .   .    TextRank  .   .    LSA  .   .    HAN  .   .    H-BERT  .   .    H-BERT+IndexHistory  .   .     ( ) Summaries+SIC+IndexMethodAccuracy ( % ) MCCFullMD & ASection  .   .    BertSumExtAbs  .   .    BertSumExt  .   .    LexRank  .   .    TextRank  .   .    LSA  .   .    HAN  .   .    H-BERT  .   .    H-BERT+IndexHistory  .   .    NotethatforHANmodel , samesettingwasusedfortrainingthesummaryextractionmoduleaswellasforevaluation ( Section  ) .Forallcases , weuseavalidationsettodecideanearlystoppingpointwhiletraining.Wetakethemodelthatgivestheleastvalidationlosstobethebestmodel. RESULTSANDDISCUSSION . IntrinsicEvaluationTable givestheresultsforROUGE-  , ROUGE- andROUGE-Lmet-rics.Thescoresarecomputedasaverageover  humanextractedgoldsummaries.ItisobservedthatthesupervisedsummarizationmethodslikeBertSumExtAbsandBertSumExtgivegoodpreci-sionscorebutlowrecall.ThisisbecauseofthefactthattheyaretrainedonCNN/DailyMail [    ] dataset.Thesedatasetsaresmallandhavesummarieswithanaverageofaround .  to .  sen-tences [    ] .Becauseofthisfactthemodelsaretrainedtoextractveryfewsentencesandthusresultinpoorrecallscore.Precisionisalsoboostedbecauseofthesamereasonthatfewsentencesareextractedassummaries.WeobservethatH-BERT , withandwithoutIndexHistorygivescomparableresults.ThiscouldbebecauseofthefactthattheIndexHistoryfeatureisusefultoimprovethestockmovementpredictioncapabilities ( Section .  ) butdoesnothavemucheffectonthesentenceitpaysattentionto.H-BERTalongwithHANgivesbetterresultscomparedtoothersummarizationtechniquesindicatingthatthesentencesthatarebeingpaidmoreattentiontoaregoodsummarycandidates. . ExtrinsicEvaluationTable comparestheresultsoftheproposedmethodofsumma-rizationwithotherbaselinesummarizationmethods.ResultsarecomputedaspertheevaluationmethodologygiveninSection .    Hierarchical Model Goal Guided Summarization Annual Financial Reports WWW ’    Companion , April   –   ,      , Ljubljana , Slovenia Accuracy MCC test set stock movement predic- tion task shown summaries using different techniques used input . Results full MD & section also added reference compare summarization techniques . see summaries ,    % full MD & section data , model stock movement task similar even better results meaning model parameters learned way gives us useful summary task . Table  a gives results summaries ( extra features ) used stock movement prediction task . observe summaries extracted H-BERT without Index History better able model task stock movement prediction respect baseline methods . addition summary , could factors drive prediction task , sector ( industry ) index  company belongs . study phenomenon include different features along summaries train model stock movement prediction task observe results . append extra feature document representation vector ( Equation (   ) ) classify stock movement prediction . allows us evaluate adding extra information summaries useful prediction task . Table  b , test effect sector adding SIC code  features summaries .     unique SIC codes accommodate companies dataset , concatenate    -dimensional one- hot-vector document vector d. see improvements results general . indicates given summary sector , model better able predict stock movements . similar analysis done concatenating  -dimensional one-hot-vector document vector capture effect company belongs index . consider & P        stocks index companies others non-index companies . reason using information index companies tend stable less risky general , could helpful stock movement prediction . Table  c shows results case . observe proposed method performs best case , meaning model trained summaries extracted proposed method gives underrated overrated view companies . adding index feature might normalized bias towards neutrality . Finally , Table  d , show results concatenating SIC Index features vectors along sum- mary features . results fell case , could additional information may introducing noise deteriorating patterns present data , thus compromising evaluation scores . best result Table   given H-BERT cases H-BERT Index History . suggests considering sum- maries extracted models lead better predictability stock movement . Moreover gives similar better prediction capabilities compared full MD & section . implies similar better decisions made consuming less informa- tion ( summaries ) compared full document .  A market index hypothetical portfolio investment holdings represents segment financial market .  The Standard Industrial Classification ( SIC ) four-digit codes categorize com- panies various industries ( sectors ) w.r.t . business activities .  The & P      Standard & Poor ’      Index market-capitalization-weighted index      largest publicly-traded companies U.S. covering    % market capitalization . Table   : Comparison CAGR Average returns benchmark indexes portfolio constructed summary extractor model . Asset Class & P     & P     Russell      DJIA HAN ( P =    ) HAN ( P =    ) HAN ( P =    ) HAN ( P =    ) H-BERT ( P =    ) H-BERT ( P =    ) H-BERT ( P =    ) H-BERT ( P =    ) H-BERT + Index Hist . ( P =    ) H-BERT + Index Hist . ( P =    ) H-BERT + Index Hist . ( P =    ) H-BERT + Index Hist . ( P =    ) CAGR Average Returns  .   %  .   %  .   %  .   %  .   %   .   %   .   %   .   %   .   %   .   %   .   %   .   %   .   %   .   %   .   %   .   %  .   %  .   %  .   %  .   %   .   %   .   %   .   %   .   %   .   %   .   %   .   %   .   %   .   %   .   %   .   %   .   %  .  Model Prediction Reliability Analysis considered attention weights model extract summaries analyzed actual predictions made model . section , evaluate summary extractor model ’ prediction capabilities constructing portfolio stocks . compare returns constructed portfolio traditional stock market indexes span    (           ) years . consider complete evaluation split data sort prediction probabilities given trained model year . pick top P probable buy label model construct equal-weighted portfolio stocks . compare CAGR  Average Returns constructed portfolio major indexes US - & P     , & P     , Russell      DJIA . Table   , observe portfolio constructed summary extractor model gives better returns compared various market indexes long run . implies predictions made summary extractor model corresponding attention weights reliable .    CONCLUSION FUTURE WORK paper , address problem information overload financial domain . propose Goal Guided Summarization framework annual financial reports . summarization task guided goal making buy sell decisions company ’ stocks . accomplish task training hierarchical neural models predict stock movements extract summary ranking sentences using attention weights . using intrinsic extrinsic evaluation , observe summaries extracted proposed method help analysts decision making .  Compound annual growth rate ( CAGR ) rate return would required investment grow beginning balance ending balance , assuming profits reinvested end year investment ’ lifespan .     WWW ’    Companion , April   –   ,      , Ljubljana , Slovenia Yash Agrawal , Vivek Anand , Arunachalam , Vasudeva Varma also achieves goal stock movement prediction better existing summarization techniques full document . summarization framework useful different do- mains annotated dataset available , purpose summary consumption defined . , also conclude trained summary extractor model used portfo- lio construction , generated better returns benchmark market indexes implying model extracted summaries reliable practical . things explored future works . (   ) Proposed method extracts sum- maries w.r.t . analyst investor . Similarly , summaries w.r.t . stakeholders like legal teams managers extracted designing appropriate goals . (   ) also examine items   -K report like Risk Factors Legal Proceedings along aggregated news articles global events related companies specific window extract goal specific summaries . REFERENCES [   ] Torben G Andersen Tim Bollerslev .      . Intraday periodicity volatility persistence financial markets . Journal empirical finance   ,  -  (      ) ,    –     . [   ] Ziqiang Cao , Wenjie Li , Sujian Li , Furu Wei .      . Retrieve , rerank rewrite : Soft template based neural summarization . Proceedings   th Annual Meeting Association Computational Linguistics ( Volume   : Long Papers ) .    –    . [   ] Eddy Cardinaels , Stephan Hollander , Brian J White .      . Automatic sum- maries earnings releases : Attributes effects investors ’ judgments . Avail- able SSRN         (      ) . [   ] Jacob Devlin , Ming-Wei Chang , Kenton Lee , Kristina Toutanova .      . BERT : Pre-training Deep Bidirectional Transformers Language Understanding . Proceedings      Conference North American Chapter Association Computational Linguistics : Human Language Technologies , Volume   ( Long Short Papers ) . Association Computational Linguistics , Minneapolis , Minnesota ,     –     . https : //doi.org/  .     /v /N  -     [   ] Xiao Ding , Yue Zhang , Ting Liu , Junwen Duan .      . Using structured events predict stock price movement : empirical investigation . Proceedings      Conference Empirical Methods Natural Language Processing ( EMNLP ) .     –     . [   ] Xiao Ding , Yue Zhang , Ting Liu , Junwen Duan .      . Deep learning event-driven stock prediction . Twenty-fourth international joint conference artificial intelligence . [   ] Xiao Ding , Yue Zhang , Ting Liu , Junwen Duan .      . Knowledge-driven event embedding stock prediction . Proceedings coling      ,   th international conference computational linguistics : Technical papers .     –     . [   ] Xin Du Kumiko Tanaka-Ishii .      . Stock Embeddings Acquired News Articles Price History , Application Portfolio Optimization . Proceedings   th Annual Meeting Association Computational Linguistics .     –     . [   ] Mahmoud El-Haj .      . MultiLing      : Financial Narrative Summarisation . Proceedings Workshop MultiLing      : Summarization Across Languages , Genres Sources .  –   . [    ] Mahmoud El-Haj , Ahmed AbuRa ’ ed , Marina Litvak , Nikiforos Pittaras , George Giannakopoulos .      . Financial Narrative Summarisation Shared Task ( FNS      ) . Proceedings  st Joint Workshop Financial Narrative Processing MultiLing Financial Summarisation . COLING , Barcelona , Spain ( Online ) ,  –   . https : //www.aclweb.org/anthology/    .fnp- .  [    ] Günes Erkan Dragomir R Radev .      . Lexrank : Graph-based lexical centrality salience text summarization . Journal artificial intelligence research    (      ) ,    –    . [    ] Eugene F Fama .      . behavior stock-market prices . journal Business    ,   (      ) ,   –    . [    ] Kavita Ganesan .      . ROUGE  .  : Updated Improved Measures Evalua- tion Summarization Tasks . (      ) . [    ] Karl Moritz Hermann , Tomas Kocisky , Edward Grefenstette , Lasse Espeholt , Kay , Mustafa Suleyman , Phil Blunsom .      . Teaching Machines Read Comprehend . Advances Neural Information Processing Systems    . Curran Associates , Inc. ,     –     . [    ] Hanqi Jin , Tianming Wang , Xiaojun Wan .      . SemSUM : Semantic Depen- dency Guided Neural Abstractive Summarization .. AAAI .     –     . [    ] Diederik P. Kingma Jimmy Ba .      . Adam : Method Stochastic Opti- mization . arXiv:    .     [ cs.LG ] [    ] Shimon Kogan , Dimitry Levin , Bryan R. Routledge , Jacob S. Sagi , Noah . Smith .      . Predicting Risk Financial Reports Regression . Pro- ceedings Human Language Technologies :      Annual Conference North American Chapter Association Computational Linguistics . As- sociation Computational Linguistics , Boulder , Colorado ,    –    . https : //www.aclweb.org/anthology/N  -     [    ] Wojciech Kryscinski , Nitish Shirish Keskar , Bryan McCann , Caiming Xiong , Richard Socher .      . Neural Text Summarization : Critical Evaluation . Proceedings      Conference Empirical Methods Natural Language Pro- cessing  th International Joint Conference Natural Language Processing ( EMNLP-IJCNLP ) . Association Computational Linguistics , Hong Kong , China ,    –    . https : //doi.org/  .     /v /D  -     [    ] Chin-Yew Lin .      . Rouge : package automatic evaluation summaries . Text summarization branches .   –   . [    ] Yang Liu Mirella Lapata .      . Text Summarization Pretrained Encoders . Proceedings      Conference Empirical Methods Natural Language Processing  th International Joint Conference Natural Language Pro- cessing ( EMNLP-IJCNLP ) . Association Computational Linguistics , Hong Kong , China ,     –     . https : //doi.org/  .     /v /D  -     [    ] Corentin Masson Syrielle Montariol .      . Detecting Omissions Risk Factors Company Annual Reports . Proceedings Second Workshop Financial Technology Natural Language Processing . - , Kyoto , Japan ,   –   . https : //www.aclweb.org/anthology/    .finnlp- .  [    ] Rada Mihalcea Paul Tarau .      . Textrank : Bringing order text . Pro- ceedings      conference empirical methods natural language processing .    –    . [    ] Ramesh Nallapati , Feifei Zhai , Bowen Zhou .      . Summarunner : recurrent neural network based sequence model extractive summarization documents . Thirty-First AAAI Conference Artificial Intelligence . [    ] Jun Ping Ng , Praveen Bysani , Ziheng Lin , Min-Yen Kan , Chew Lim Tan .      . Exploiting category-specific information multi-document summarization . Proceedings COLING      .     –     . [    ] Jekaterina Novikova , Ondřej Dušek , Amanda Cercas Curry , Verena Rieser .      . Need New Evaluation Metrics NLG . Proceedings      Conference Empirical Methods Natural Language Processing . Association Computational Linguistics , Copenhagen , Denmark ,     –     . https : //doi.org/   .     /v /D  -     [    ] Makbule Gulcin Ozsoy , Ferda Nur Alpaslan , Ilyas Cicekli .      . Text sum- marization using Latent Semantic Analysis . Journal Information Science    ,   (      ) ,    –    . https : //doi.org/  .    /                 [    ] Lawrence Page , Sergey Brin , Rajeev Motwani , Terry Winograd .      . PageRank citation ranking : Bringing order web . Technical Report . Stanford InfoLab . [    ] Nils Reimers Iryna Gurevych .      . Sentence-BERT : Sentence Embeddings using Siamese BERT-Networks . Proceedings      Conference Em- pirical Methods Natural Language Processing . Association Computational Linguistics . https : //arxiv.org/abs/    .      [    ] Sho Takase , Jun Suzuki , Naoaki Okazaki , Tsutomu Hirao , Masaaki Nagata .      . Neural headline generation abstract meaning representation . Proceed- ings      conference empirical methods natural language processing .     –     . [    ] Stephen J Taylor .      . Modelling financial time series . world scientific . [    ] Paul C Tetlock .      . Giving content investor sentiment : role media stock market . Journal finance    ,   (      ) ,     –     . [    ] Paul C Tetlock , Maytal Saar-Tsechansky , Sofus Macskassy .      . words : Quantifying language measure firms ’ fundamentals . Journal Finance    ,   (      ) ,     –     . [    ] Ashish Vaswani , Noam Shazeer , Niki Parmar , Jakob Uszkoreit , Llion Jones , Aidan N Gomez , Łukasz Kaiser , Illia Polosukhin .      . Attention need . Advances neural information processing systems .     –     . [    ] Wenting Xiong Diane Litman .      . Empirical analysis exploiting review helpfulness extractive summarization online reviews . Proceedings coling      ,   th international conference computational linguistics : Technical papers .     –     . [    ] Yumo Xu Shay B. Cohen .      . Stock Movement Prediction Tweets Historical Prices . Proceedings   th Annual Meeting Association Computational Linguistics ( Volume   : Long Papers ) . Association Computational Linguistics , Melbourne , Australia ,     –     . [    ] Zichao Yang , Diyi Yang , Chris Dyer , Xiaodong , Alex Smola , Eduard Hovy .      . Hierarchical Attention Networks Document Classification . Proceedings      Conference North American Chapter Association Computational Linguistics : Human Language Technologies . Association Computational Linguistics , San Diego , California ,     –     . https : //www . aclweb.org/anthology/N  -     [    ] Shuang ( Sophie ) Zhai Zhu ( Drew ) Zhang .      . Forecasting Firm Material Events  -K Reports . Proceedings Second Workshop Economics Natural Language Processing . Association Computational Linguistics , Hong Kong ,   –   . https : //doi.org/  .     /v /D  -        