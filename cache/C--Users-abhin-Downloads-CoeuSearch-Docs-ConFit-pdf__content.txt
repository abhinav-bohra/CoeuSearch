CONFIT: Toward Faithful Dialogue Summarization with Linguistically-Informed Contrastive Fine-tuning Anonymous ACL submission 001 002 003 004 005 006 007 008 009 010 011 012 013 014 015 016 017 018 019 020 021 022 023 024 025 026 027 028 029 030 031 032 033 034 035 036 037 038 039 040 041 042 Abstract Factual inconsistencies in generated sum- maries severely limit the practical applications of abstractive dialogue summarization. Al- though signiﬁcant progress has been achieved by using pre-trained neural language models, substantial amounts of hallucinated content are found during the human evaluation. In this work, we ﬁrst devised a typology of fac- tual errors to better understand the types of hal- lucinations generated by current models and conducted human evaluation on popular dialog summarization dataset. We further propose a training strategy that improves the factual con- sistency and overall quality of summaries via a novel contrastive ﬁne-tuning, called CON- FIT. To tackle top factual errors from our an- notation, we introduce additional contrastive loss with carefully designed hard negative sam- ples and self-supervised dialogue-speciﬁc loss to capture the key information between speak- ers. We show that our model signiﬁcantly reduces all kinds of factual errors on both SAMSum dialogue summarization and AMI meeting summarization. On both datasets, we achieve signiﬁcant improvements over state- of-the-art baselines using both automatic met- rics, ROUGE and BARTScore, and human evaluation. 1 Introduction Text summarization is used to generate a concise and accurate summary of a long text while focusing on the sections that convey the most useful infor- mation (Gurevych and Strube, 2004). In recent years, the resurgence of dialogue summarization has attracted signiﬁcant research attentions (Mc- Cowan et al., 2005; Gliwa et al., 2019; Koay et al., 2020; Zhang et al., 2021; Zhong et al., 2021; Zhu et al., 2021; Chen et al., 2021a; Li et al., 2021; Chen et al., 2021c; Fabbri et al., 2021; Chen et al., 2021d). The goal of dialogue summarization is to condense the conversational input into brief sentences ver- sion but cover salient information (McCowan et al., 1 2005; Yuan and Yu, 2020). Signiﬁcant progress has been made recently on abstractive dialogue summa- rization with various pre-trained models. However, such pre-trained models are susceptible to generat- ing hallucinate content that is not supported by the source documents (Cao et al., 2018; Maynez et al., 2020; Kryscinski et al., 2020). To tackle the issue of factual inconsistency in dialogue summarization, recent works correctly encode the names of speak- ers (Zhu et al., 2020), explicitly incorporate coref- erence information (Liu et al., 2021b), and order the personal named entities (Liu and Chen, 2021). But it is still challenging to improve the quality of summaries generated by different models and decrease the hallucination at the same time. To better understand the types of hallucinations generated by the pre-trained models, we devised a linguistically motivated taxonomy of factual er- rors for dialogue summarization, instead of simply classifying the summary as faithful or not. Based on our typology, we deﬁned an annotation protocol for factuality evaluation of dialogue summariza- tion. We then conducted a human evaluation of several pre-trained abstractive summarizers, includ- ing BART (Lewis et al., 2020), Pegasus (Zhang et al., 2020), and T5 (Raffel et al., 2020), aiming at identifying the proportion of different types of factual errors and studying the weaknesses of the pre-trained models. Our typology and annotation helps us gain deeper insights into the causes of factual inconsistency. Unlike news summarization (Pagnoni et al., 2021), we found that the challenges posed by dialogue summarization are more related to dialogue ﬂow modeling, informal interactions between speakers, and complex coreference resolu- tion. Figure 1 shows a dialogue-summary pair with three speciﬁc errors. In order to tackle the top factual errors produced by existing models, we propose to replace the most commonly used ﬁne-tuning with a linguistically- informed contrastive ﬁne-tuning approach. For 043 044 045 046 047 048 049 050 051 052 053 054 055 056 057 058 059 060 061 062 063 064 065 066 067 068 069 070 071 072 073 074 075 076 077 078 079 080 081 082 083 084 085 086 087 088 089 090 091 092 093 094 095 096 097 098 099 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 Figure 1: Sample summary of a SAMSum dialogue (Gliwa et al., 2019). The summary is generated by BART (Lewis et al., 2020). Errors are highlighted. example, the reason for producing wrong refer- ence errors is that models cannot understand the role in the dialogue, which goes beyond the events. Our goal is to drive the model to pay attention to the grounds of speciﬁc errors during the ﬁne- tuning, and learn how to reduce the generation of such errors. To be more speciﬁc, CONFIT learns to distinguish whether there are factual errors in the summaries and capture the key information in the dialogue content, such as numbers and person names. Experiments on SAMSum (Gliwa et al., 2019) and AMI (McCowan et al., 2005) show the generalizability of CONFIT when it is applied to different pre-trained models and datasets. Further- more, we employ both automatic evaluation and human evaluation on faithfulness and show that CONFIT signiﬁcantly reduces all different factual errors and generates summaries that are more fac- tually consistent. Moreover, we analytically ﬁnd that optimizing the contrastive ﬁne-tuning is quite beneﬁcial for improving the robustness of models, which brings further beneﬁts. Our contributions are as follows: • We introduce the ﬁrst typology of factual er- rors for dialogue summarization and use it to conduct comprehensive annotation and fo- cused analysis. • Targeting different categories of factual errors in the annotations, we reduce occurrence of such errors generated by various pre-trained models with a novel linguistically-informed contrastive ﬁne-tuning CONFIT approach. • We validate our method on a widely used dia- logue summarization corpus, SAMSum, and extend it to a meeting summarization corpus AMI. Evaluations of output summaries on au- tomatic metrics like ROUGE, BARTScore as well as human evaluations show that CONFIT outperforms baseline pre-trained models. 2 New Taxonomy of Factuality Errors for Abstractive Dialogue Summarization In order to gain deeper insights into the types of factuality errors introduced by different abstractive dialogue summarization systems, we proposed a new taxonomy of factuality errors for abstractive dialogue summarization based on our empirical experiments and annotations of the performance of a set of representative baseline summarization models on the SAMSum dataset, which is a widely- used large-scale dialogue summarization dataset of chat message dialogues in English (see Section 4.1). Speciﬁcally, we generate summaries of SAM- Sum dialogues using state-of-the-art abstractive dialogue summarization models, including models ﬁne-tuned based on T5 (Raffel et al., 2020), Pe- gasus (Zhang et al., 2020), BART (Lewis et al., 2020), D-HGN (Xiachong et al., 2021), and S- BART (Chen and Yang, 2021b). We then man- ually annotate all different types of errors in these generated summaries that are inconsistent with the source dialogue, compute detailed statistics of all these factuality errors, and then classify them into different categories. Based on our annotation and analysis, we propose a new taxonomy of errors with the majority focusing on factuality error, which in- cludes the following 8 error types: 2 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 Hey, do you have Betty's number?Amanda: Lemme checkSorry, can't find it.Ask Larry.He called her last time we were at the park together.I don't know him well.Don't be shy, he's very nice.If you say so… I'd rather you texted him.Okay.Ijusttextedhim.Urgh.. Alright.Bye.Hannah:Amanda:Hannah:Amanda:Hannah:Amanda:Hannah:Hey, do you have Betty's number?Amanda: Lemme checkSorry, can't find it.Ask Larry.He called her last time we were at the park together.I don't know him well.Don't be shy, he's very nice.If you say so… I'd rather you texted him.Okay.Ijusttextedhim.Urgh.. Alright.Bye.Hannah:Amanda:Hannah:Amanda:Hannah:Amanda:Hannah:Dialogue (Copy 2)Hey, do you have Betty's number?Amanda: Lemme checkSorry, can't find it.Ask Larry.He called her last time wewere at the park together.I don't know him well.Don't be shy, he's very nice.If you say so… I'd rather you texted him.Okay.Ijusttextedhim.Urgh.. Alright.Bye.Hannah:Amanda:Hannah:Amanda:Hannah:Amanda:Hannah:Dialogue (Copy 1)Dialogue (Copy 3)Hannah needs Betty's numberbut Amanda doesn't have it. Amanda needs to contact Larry.Reference(c) MissingInformation(b) Modality&Tense Error(a) CoreferenceErrorAmanda can't find Betty’s number. Larry called her last time theywere at the park. Amanda willtextLarry.GeneratedSummary150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 Category 1 - Missing Information: The content of the generated summary is incomplete compared to the reference. Example: [Reference Summary] Justin likes books. [Model-Generated Summary] Justin does not like books. [Reference Summary] Williams invites Ms. Blair for a coffee. They will go to her favourite coffee place near the square in a side alley at 2 p.m. [Model-Generated Summary] Ms. Blair is going to a coffee place near the square in a side alley. Category 2 - Redundant Information: There is redundant content in the generated summary com- pared to the reference. Example: [Reference Summary] Paula helped Charlotte with correct pronunciation of "Natal Lily." [Model-Generated Summary] Charlotte asks Paula how to pronounce the name of the plant "Natal Lily." Paula conﬁrms that the stress on the second syllable is 2nd. Category 3 - Circumstantial Error: Circumstan- tial information (e.g., date, time, location) about the predicate doesn’t match the reference. Example: [Reference Summary] The USA was founded in 1776. [Model-Generated Summary] The USA was founded in 1767. Category 4 - Wrong Reference Error: A pro- noun is with an incorrect or nonexistent antecedent, or a personal named entity in the generated sum- mary is in the place of a different personal entity in the reference. Example: [Reference Summary] Mohit asked Dar- lene about the test. [Model-Generated Summary] Darlene asked Mohit about the test. Category 5 - Negation Error: This encompasses factual errors resulting from missing or erroneous negation in the generated summary compared to the reference. Example: Category 6 - Object Error: This covers factual errors resulting from incorrect direct or indirect ob- jects (for non-personal entities only; errors of this nature involving personal entities are designated as Wrong Reference Errors). Example: [Reference Summary] Tara raised her glass. [Model-Generated raised her spoon. Summary] Tara Category 7 - Tense Error: This encompasses fac- tual errors resulting from discrepancies in gram- matical tense between the generated summary and the reference. Example: [Reference Summary] The children will go to the library. [Model-Generated Summary] The chil- dren went to the library. Category 8 - Modality Error: This includes fac- tual errors resulting from modal discrepancies, such getting words like "may", "should", "could" wrong, between the generated summary and the reference. Example: [Reference Summary] School may be cancelled today. [Model-Generated Summary] School is cancelled today. 2.1 Annotation and Analysis Using our proposed taxonomy of factuality errors, we compute the proportion of each type of factual- ity errors across different summarization models. We then investigate the model generation behavior that is indicative of errors, which guides the design of our proposed model. We performed a human evaluation of four model outputs from 19 SAMSum dialogues in order to identify the limitations of abstractive summariza- tion models in dialogue summarization tasks. The four models used in this human evaluation are two BART models with different random seeds 3 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 24 22 33 28 The ﬁnal training objective J (θ) of the proposed framework is as follows: Missing Information Redundant Information 3 0 Wrong Reference Circumstance 4 Negation Object 0 Tense 2 Modality 6 5 8 8 8 8 20 21 16 16 18 12 12 13 12 S-BART D-HGN BART 0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 Figure 2: Percentage of error types in each model dur- ing preliminary human evaluation of 19 SAMSum dia- logues. (ROUGE-L 48 and 49) (Lewis et al., 2020), D- HGN (ROUGE-L 40) (Xiachong et al., 2021), and S-BART (ROUGE-L 48 (Chen and Yang, 2021b)). BART and S-BART are pre-trained models (PLM), and D-HGN is trained from scratch. Since we are focusing on the dialogue domain, most of the fac- tual errors in the model summaries are related to coreference, anaphora, and other dialogue-speciﬁc characteristics. In fact, approximately 45% of all er- rors fall into the categories of Missing Information and Wrong Reference. The distribution of these er- rors throughout these pre-existing models informs the limitations of each model. Our proposed CON- FIT model targets the top errors generated by the current state-of-the-art models to reduce factual inconsistency. 3 CONFIT Model Standard ﬁne-tuning parameterizes the probabil- ity pα of the generator on a task-speciﬁc labeled dataset by maximizing cross-entropy loss. (cid:88) L = − log P (˜tl|t<l, D) (1) However, the cross-entropy loss has several shortcomings that can lead to factual inconsistency in dialogue summarization due to its sub-optimal generalization and instability. We propose a more efﬁcient ﬁne-tuning method CONFIT for factual consistency driven by the intuition that good gen- eralization requires capturing the similarity in one class and contrasting them in other classes. In CONFIT, we introduce two additional losses: con- trastive loss and self-supervised loss. We use two weights, actually which is coefﬁcients, to adjust the ratio of Lcon and Lself in the total loss of CONFIT. 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 J (θ) = L + αLcon + βLself (2) Our linguistically-informed typology and anno- tation help us gain deeper insights into the causes of different factual errors. To help our models gener- ate more faithful summaries, the proposed CONFIT learns to concentrate on the essential elements of dialogue and capture the dynamic role information as illustrated in Figure 3. Figure 3: A demonstration of our model. 3.1 Contrastive Loss In order to reduce the occurrence of factual errors, we propose a contrastive loss that uses the follow- ing negative sample generation techniques to target each error type in our proposed taxonomy: • Swap the nouns in the reference summary with each other randomly. This aims to re- duce wrong reference and object errors by providing negative samples. • Swap the verbs in the reference summary with each other randomly. This aims to the model reduce circumstance (and, to a lesser extent, tense and modality) errors. • Mask numbers and years in the dialogue and then pass it into the model to generate a neg- ative sample summary. This aims to reduce circumstance errors. • Randomly delete 30% of the sentences in the dialogue and then pass it into the model to generate a negative sample summary. This aims to reduce missing information errors. 4 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 ClassifierDialogueReferenceSummaryEmma:WhenwillourbusarrivetoNY?Ben:Around4:30PM.Emma:Iwanttohaveanap.Ben:Goodidea.Sleepwell.Emma:Couldyouwakemeuparound4:15?Ben:Sure.EmmaisabouttotakeanapinbustoNewYork.BenandEmmawillbetherearound4:30.BenwillwakeEmmaup15minutespriortotheirarrival.Ben and Emma will be there around 1:23.Emma is about to take lunchin bus toParis.ClassifierEmmawill wake Benup 12minutes prior to their arrival.Circumstantial 4:304:15ObjecterrornapwakeWrongreferenceIme305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 • Mask-and-ﬁll coreferent entities with BART in the dialogue and then pass it into the model to generate a negative sample summary. This aims to reduce wrong reference errors. This supplementary loss function helps CONFIT keep track of speaker information, thus improving the faithfulness of its summaries for dialogues that contain several ﬁrst-person references. Equation 3 demonstrates our contrastive loss function. During the ﬁne-tuning, we have the pos- itive samples, which is the reference summaries and another set of incorrect summaries, which is the negative samples. The contrastive objectives are learning representations that are invariant to different views of positive pairs; while maximizing the distance between negative pairs (Gunel et al., 2020). Our goal is to maximize the likelihoods of the positive samples and minimize the likelihoods of the negative samples as well. We use the follow- ing contrastive learning objective Lcon = − (cid:88) log yj (cid:54)=yi exp(cos(ci, cj)) exp(cos(ci, ck)) (cid:80) yk(cid:54)=yi (3) where yi and yj are positive summary pairs gen- erated by back translation technology and yk is from negative set of examples and ci ,cj, ck are their BART encoder representations. 3.2 Self-supervised Loss One unique challenge in abstractive dialogue sum- marization is the use of ﬁrst-person pronouns (such as "I" or "we") in speaker utterances, which the model has to correctly identify as being a reference to the speaker. This can lead to wrong reference errors in the summary, as the model cannot under- stand which participant is speaking and thus cannot accurately resolve ﬁrst-person references. To ad- dress this problem, we design a self-supervised loss that aims to determine whether two tokens belong to the same speaker. Based on these ﬁndings, we design a self-supervised loss to enable CONFIT to capture the dynamic roles in the dialogue. After the BART encoder, the input dialogue is encoded into hidden vectors C. Here, we ﬁrst ran- domly select k pairs of two tokens tm and tn from the input dialogue, with labels sm and sn denoting which speaker they are coming from. We also do the same for utterances. Given the concatenation of the encoder representation of dialogue, tm and tn, we use the following loss function to classify whether the two tokens or two utterances are from the same speaker. Lself = − k (cid:88) k (cid:88) m=1 n=1 log P (sm = sn|tm, tn, C) (4) 5 4 Experiments 4.1 Dataset We evaluate our new model on the popular SAM- Sum dialogue summarization dataset. Then, we extend our model to meeting summarization with the AMI Meeting Corpus. SAMSum (Gliwa et al., 2019) is a recently proposed large-scale dialogue summarization dataset consisting of 16,369 chat message dialogues in English written by linguists, and each message dialogue is annotated with a multi-sentence summary written by language ex- perts. 75% of the dialogues in the SAMSum dataset (Gliwa et al., 2019) are between two in- terlocutors, and the other 25% are among three or more interlocutors. The AMI Corpus is an- other well-known dialogue summarization dataset consisting of 137 multiparty meeting transcripts extracted from 100 hours of meeting recordings. Each meeting transcript in the dataset is also anno- tated with a generic abstractive summary. We use these two representative dialogue summarization datasets to empirically test our new model’s abstrac- tive summarization performance in the settings of both short conversation-style dialogues and long meeting-style dialogues. See Table 2 for detailed statistics of the two datasets. 4.2 Experiment Settings In our experiment using SAMSum, we trained BART for 3 epochs with a learning rate of 1e − 05, Pegasus for 20 epochs with a learning rate of 1e − 04, and T5 for 20 epochs with a learning rate of 1e − 05. In our experiment using AMI, we trained BART for 6,000 steps with a learning rate of 1e − 05, Pegasus for 24,000 steps with a learn- ing rate of 1e − 05, and T5 for 20,000 steps with a learning rate of 1e − 05. 4.3 Evaluation Metrics To evaluate our model, we use three metrics: ROUGE (Lin, 2004): ROUGE measures N- gram overlap between the reference and the au- tomatically generated summaries. BARTScore (Yuan et al., 2021): Because ROUGE scores only measure token overlap, other automated metrics (Rebuffel et al., 2021; Kryscin- ski et al., 2020; Wang et al., 2020; Scialom et al., 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 Model R-1 AMI R-2 R-L R-1 SAMSum R-2 R-L Extractive and Abstractive Models TextRank (Mihalcea and Tarau, 2004) Fast Abs RL (Chen and Bansal, 2018) PGN (See et al., 2017) PGN(DALL) (Feng et al., 2021b) T5 (Raffel et al., 2020) Pegasus (Zhang et al., 2020) BART (Lewis et al., 2020) Multi-view BART (Chen and Yang, 2020) 35.19* 38.76 48.34* 50.91* 6.13* 15.13 16.02* 17.75* Pre-trained Models 13.94 15.85 16.00 - 42.16 46.02 47.92 - Ours T5-ConFiT Pegasus-ConFiT BART-ConFiT 47.18 48.47 50.31 13.19 17.61 17.29 15.70* 35.18 23.49* 24.59* 29.27* 40.96 40.08* - 8.02* 17.18 15.28* - 28.78* 39.05 36.63* - 39.39 43.73 45.36 - 43.55 45.75 47.98 48.41 48.04 51.74 49.52 52.13 52.65 53.89 24.79 22.94 26.46 26.52 27.12 28.21 28.85 44.61 43.40 48.72 48.29 47.62 48.15 49.29 Table 1: Dialogue summarization ROUGE evaluation on the AMI (McCowan et al., 2005) and SAMSum (Gliwa et al., 2019) datasets. We adopt some results reported from the literature (Feng et al., 2021a) and implement the pre-trained models for a fair comparison. All results marked with an asterisk (*) are from Feng et al. (2021b). Dialogue Speakers Turns Length 5 Results Train Validation Test 14732 818 819 All 137 SAMSum 2.40 2.39 2.36 AMI 4 11.17 10.83 11.25 23.44 23.42 23.12 289 322 Table 2: Details about SAMSum and AMI. 2021) have been proposed to evaluate faithfulness more precisely. BARTScore is a transformer-based measure that scores a dialogue and the correspond- ing automatically generated summary and has been shown to be strongly correlated with human evalu- ations of faithfulness (Yuan et al., 2021). Human Evaluation: Finally, we conduct hu- man evaluations on 100 SAMSum (Gliwa et al., 2019) and 20 AMI (McCowan et al., 2005) dia- logues. Tang et al. (2021) found that Likert scales are a more consistent measure of factuality for ab- stractive dialogue summarization than Best-Worst Scaling. We have human evaluators directly rate the summaries on a scale from 1 to 10 correspond- ing to their faithfulness. In addition, using the error taxonomy proposed in Section 2, we have them mark whether each error type appeared in the given summary. We do this in a blinded fashion, so that the annotators do not see the corresponding model of the summary. Additionally, in order to prevent model information from leaking to the annotators, we randomly shufﬂe outputs within each dialogue before assigning them to annotators. Table 1 shows the ROUGE scores of our models, the baseline models they were ﬁne-tuned from, and a number of other abstractive summarization mod- els on the SAMSum and AMI datasets. Tables 5 and 6 show the average human faithfulness and BART scores respectively for each model’s outputs on 100 SAMSum and 20 AMI dialogues. We observe that for all three pretrained models CONFIT signiﬁcantly beat baselines on ROUGE- 1, ROUGE-L, and human faithfulness score for both datasets. For BARTScore, we note that, while performance increases on SAMSum for all mod- els, it decreases on AMI. However, given the fact that human evaluators rated the outputs of all three CONFIT models as more faithful than those of their corresponding baselines on both datasets, the decreases in BARTScore on AMI can likely be at- tributed to the imperfection of automated metrics at capturing faithfulness in text. 5.1 Error Analysis Tables 3 and 4 show the percentage of summaries that were labeled with each error type in our tax- onomy of factual errors (discussed in Section 2.) for both the baseline and CONFIT models on the SAMSum and AMI datasets respectively. We observe that on SAMSum, our ﬁne-tuning method greatly reduces missing information, re- dundant information, wrong reference, and circum- stance errors for all models. The largest reduction is on the "wrong reference" error type (20%, 7%, and 33% for BART, Pegasus, and T5 respectively), 6 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 Error Type Missing Information Redundant Information Wrong Reference Circumstance Negation Object Tense Modality BART 55% 12% 37% 14% 4% 10% 2% 6% BART-ConFiT Pegasus 44% 7% 17% 8% 1% 6% 1% 1% 56% 7% 25% 16% 7% 4% 3% 3% Pegasus-ConFiT 50% 4% 18% 10% 2% 7% 1% 5% T5 63% 7% 46% 8% 1% 2% 2% 5% T5-ConFiT 48% 4% 13% 9% 1% 7% 2% 8% Table 3: Percentage of autogenerated summaries containing each error type, according to our human evaluation of model outputs from 100 SAMSum dialogues. Note that a single summary can contain multiple error types, so they do not add up to 100%. Error Type Missing Information Redundant Information Wrong Reference Circumstance Negation Object Tense Modality BART 90% 10% 35% 35% 20% 45% 10% 10% BART-ConFiT Pegasus 85% 15% 30% 35% 15% 40% 10% 15% 80% 60% 35% 30% 5% 45% 0% 5% Pegasus-ConFiT 70% 25% 30% 30% 15% 25% 5% 5% T5 80% 0% 50% 40% 25% 55% 10% 20% T5-ConFiT 85% 25% 50% 35% 0% 55% 10% 10% Table 4: Percentage of autogenerated summaries containing each error type, according to our human evaluation of model outputs from 20 AMI dialogues. Note that a single summary can contain multiple error types, so they do not add up to 100%. Faithfulness Score SAMSum AMI BART BART-ConFiT Pegasus Pegasus-ConFiT T5 T5-ConFiT 5.540 7.250 6.260 6.770 5.422 6.920 4.850 5.600 5.250 5.895 4.150 4.950 Table 5: Average faithfulness score (on a scale of 1- 10) given to each model by human evaluators on 100 SAMSum and 20 AMI dialogues. Highest scores for each dataset have been bolded. BARTScore SAMSum AMI BART BART-ConFiT Pegasus Pegasus-ConFiT T5 T5-ConFiT -1.613 -1.468 -1.615 -1.608 -1.993 -1.677 -3.644 -3.669 -2.967 -3.369 -3.406 -3.798 Table 6: Average BARTScore for each model on 100 SAMSum and 20 AMI dialogues. Highest scores for each dataset have been bolded. 455 456 457 458 459 likely owing to the self-supervised loss function introduced in Section 3.2 that was designed to help the model more effectively capture speaker infor- mation. For AMI, however, our ﬁne-tuning method is not as consistent at reducing the frequency of 7 each error type across models. It is possible that this is due to sample size (20 AMI dialogues vs. 100 SAMSum dialogues). 5.2 Case Study Figure 4 shows the results of human annotation on the model outputs of a selected SAMSum dialogue. Note that all of the autogenerated summaries, both baseline and CONFIT, were marked as having miss- ing information errors by the annotator, likely due to the omission of Ernest’s relief upon hearing that the car that was crashed into did not belong to Mike. As a result, none of the models achieved a perfect factuality score on this dialogue; however, the scores for each CONFIT model were higher than those of their corresponding baselines. It can be observed that while baseline BART out- puts a summary with a circumstance error, mistak- enly asserting that Mike parked his car on Ernest’s street, the BART+CONFIT ﬁxes this error, cor- rectly asserting that Mike took his car to the garage today; as a result, the human annotator gave this summary a higher score than the predicted sum- mary from baseline BART. Baseline T5 outputs a summary with two coreference errors; speciﬁcally, it contains a missing subject in the ﬁrst sentence and incorrectly implies that the car that got crashed into belonged to Mike in the second sentence. The T5+CONFIT is able to ﬁx both of these errors, 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 Figure 4: Model outputs for selected SAMSum dialogue, along with the corresponding reference summary, human factuality scores, and errors. adding "Mike" to the beginning of the ﬁrst sentence and changing "his red Honda" to "a red Honda just like Mike’s" in the second sentence. Similarly, the output of baseline Pegasus contains a coreference error in the ﬁrst sentence, implying that Mike owns the car that was crashed into while the output of Pegasus+CONFIT does not. 6 Related Work Multi-party dialogues are especially challenging to summarize using automated models, given that they often contain pauses, false starts, reconﬁrmations, hesitations, and speaker interruptions (Sacks et al., 1978; Feng et al., 2021a; Chen and Yang, 2021a). Previous work in the ﬁeld has addressed these chal- lenges by incorporating semantic features, includ- ing keywords (Zhu et al., 2020), domain termi- nologies (Koay et al., 2020), topics (Zhao et al., 2020; Liu et al., 2021a), entailment knowledge (Li et al., 2018), and background knowledge (Feng et al., 2021c). Other works exploit personal named entities (Liu and Chen, 2021) and coreference infor- mation (Liu et al., 2021b) to learning to distinguish complex coreferent relationships expressed through personal pronouns (including the ﬁrst person "I") in the conversation (Lei et al., 2021). Researchers have also explored conversational structure (Zhao et al., 2021), utterance ﬂow modelling (Chen et al., 2021b), syntactic structure (Lee et al., 2021), gran- ularity control (Wu et al., 2021), but they have not yet converged to a simple and practical solution. Our proposed taxonomy of factual errors and annotations help us gain deeper insights into the causes of factual inconsistency in abstractive dia- logue summarization outputs. 7 Conclusion We presented CONFIT, a novel method to improve the faithfulness of abstractive dialogue summariza- tion models via contrastive and self-supervised ﬁne-tuning. By adapting the objective function during ﬁne-tuning to incorporate a contrastive loss that learns to distinguish positives from ex- amples with factual errors, and a self-supervised dialogue-speciﬁc loss that captures important di- alogue information ﬂow between multiple inter- locutors, CONFIT can signiﬁcantly improve the faithfulness of the abstractive summaries gener- ated by transformer-based sequence-to-sequence language models, and reduce multiple categories of factuality errors in the abstractive summaries by large margins. In our experiment on SAMSum and AMI, we demonstrated that CONFIT achieves better empirical performance compared to the base- line models ﬁne-tuned with the traditional cross- entropy loss, based on both automatic evaluation metrics and human evaluation. Our work provides new insights into improving the faithfulness of ab- stractive summarization systems using carefully designed novel objective functions for ﬁne-tuning that captures important structures and features of the text to summarize. 8 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 Ernest: hey Mike, did you park your car on our street?Mike: no, took it into garage todayErnest: ok goodMike: why?Ernest: someone just crashed into a red honda looking just like yoursMike: lol lucky meMike took his car into garage today. Ernest is relieved as someone had just crashed into a red Honda which looks like Mike’s.DialogueReference SummarySummarization ModelBARTBART ConFiTT5T5 ConFiTPegasusPegasus ConFiTMike parked his car on Ernest's street today. Mike crashed into a red honda.Mike took his car to the garage today. Someone crashed into his car.took his car into the garage today. Someone crashed into his red Honda.Mike took his car to the garage today. Someone crashed into a red Honda just like Mike's.Mike's car has been damaged beyond repair after being hit by another car.Mike took his car to the garage today because it had been hit by another car.Generated Summary3Score56824Types of Errors Circumstance Error Coreference Error Coreference Error Coreference Error N/A Circumstance Error Coreference Error Circumstance Error Removed Circumstance  Error and improved  faithfulness scoreRemoved Coreference  Error and improved  faithfulness scoreDiﬀerenceRemoved Coreference  Error and improved  faithfulness score548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 8 Ethics Statement References Human Evaluation We recruited seven volun- teer participants, requesting speakers of English. These annotators are participating voluntarily. Our participants are free to opt out of the study at any point in time. We have written four scripts for use in the annotation process: (1) the ﬁrst script gen- erates an annotation spreadsheet and a key spread- sheet from the model outputs. The annotation spreadsheet does not contain the model names; however, it contains an id that can be used to re- cover the model name from the key spreadsheet. For ease of annotation, summaries from the same dialogue are grouped together; however, they are randomly shufﬂed within each dialogue so that the annotators cannot guess from the ordering as to which model is which. (2) The second script splits an annotation spreadsheet into multiple spread- sheets so that the work can be distributed amongst annotators. (3) The third one merges these spread- sheets back together after the annotation process is ﬁnished. (4) The last script recovers the model names from the key spreadsheet and inserts them into the annotation spreadsheet. Each evaluator is asked to examine whether there is an error and the full context (dialogue, generated summaries, and reference) and give a score on a scale of 1 to 10 for each of the criteria. We only consider faithfulness, instead of general quality. E.g. 1: very poor, 3: poor, 5: neutral; 7: good; 10: very good. We asked each internal annotator to evaluate 300 samples. Other Ethical Issues (1) We did not use any per- sonally identiﬁable information in the experiments. (2) The goal of the project, improving the faithful- ness of automatically generated summaries, is to make the output of the summarization system more reliable and minimize confusion for the readers of the summaries. (3) We used existing summa- rization datasets that do not contain any sensitive information and are unlikely to cause any harm to the annotators. Ziqiang Cao, Furu Wei, Wenjie Li, and Sujian Li. 2018. Faithful to the original: Fact aware neural abstrac- In Proceedings of the AAAI tive summarization. Conference on Artiﬁcial Intelligence, volume 32. Jiaao Chen and Diyi Yang. 2020. Multi-view sequence- to-sequence models with conversational structure for abstractive dialogue summarization. In Proceed- ings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4106–4118, Online. Association for Computational Linguistics. Jiaao Chen and Diyi Yang. 2021a. Simple conver- sational data augmentation for semi-supervised ab- In Proceedings stractive dialogue summarization. of the Association for Computational Linguistics: EMNLP 2020, Online. Association for Computa- tional Linguistics. Jiaao Chen and Diyi Yang. 2021b. Structure-aware ab- stractive conversation summarization via discourse and action graphs. In Proceedings of the 2021 Con- ference of the North American Chapter of the Asso- ciation for Computational Linguistics: Human Lan- guage Technologies, pages 1380–1391, Online. As- sociation for Computational Linguistics. Mingda Chen, Zewei Chu, Sam Wiseman, and Kevin Gimpel. 2021a. Summscreen: A dataset for abstrac- tive screenplay summarization. Wang Chen, Piji Li, Hou Pong Chan, and Irwin King. 2021b. Dialogue summarization with supporting utterance ﬂow modelling and fact regularization. Knowledge-Based Systems, 229:107328. Yen-Chun Chen and Mohit Bansal. 2018. Fast abstrac- tive summarization with reinforce-selected sentence rewriting. In Proceedings of the 56th Annual Meet- ing of the Association for Computational Linguis- tics (Volume 1: Long Papers), pages 675–686, Mel- bourne, Australia. Association for Computational Linguistics. Yulong Chen, Yang Liu, Liang Chen, and Yue Zhang. 2021c. DialogSum: A real-life scenario dialogue In Findings of the Associ- summarization dataset. ation for Computational Linguistics: ACL-IJCNLP 2021, pages 5062–5074, Online. Association for Computational Linguistics. Yulong Chen, Yang Liu, and Yue Zhang. 2021d. Di- alogsum challenge: Summarizing real-life scenario dialogues. In Proceedings of the 14th International Conference on Natural Language Generation, pages 308–313. Alexander Fabbri, Faiaz Rahman, Imad Rizvi, Borui Wang, Haoran Li, Yashar Mehdad, and Dragomir Radev. 2021. ConvoSumm: Conversation summa- rization benchmark and improved abstractive sum- marization with argument mining. In Proceedings of 9 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 the 59th Annual Meeting of the Association for Com- putational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 6866–6880, Online. Association for Computational Linguistics. Xiachong Feng, Xiaocheng Feng, and Bing Qin. 2021a. A survey on dialogue summarization: Re- arXiv preprint cent advances and new frontiers. arXiv:2107.03175. Xiachong Feng, Xiaocheng Feng, Libo Qin, Bing Qin, and Ting Liu. 2021b. Language model as an annota- tor: Exploring dialogpt for dialogue summarization. arXiv preprint arXiv:2105.12544. Xiachong Feng, Xiaocheng Feng, Libo Qin, Bing Qin, and Ting Liu. 2021c. Language model as an an- notator: Exploring DialoGPT for dialogue summa- In Proceedings of the 59th Annual Meet- rization. ing of the Association for Computational Linguistics and the 11th International Joint Conference on Nat- ural Language Processing (Volume 1: Long Papers), pages 1479–1491, Online. Association for Computa- tional Linguistics. Bogdan Gliwa, Iwona Mochol, Maciej Biesek, and Aleksander Wawer. 2019. SAMSum corpus: A human-annotated dialogue dataset for abstractive summarization. In Proceedings of the 2nd Workshop on New Frontiers in Summarization, pages 70–79, Hong Kong, China. Association for Computational Linguistics. Beliz Gunel, Jingfei Du, Alexis Conneau, and Ves Stoy- anov. 2020. Supervised contrastive learning for pre- trained language model ﬁne-tuning. arXiv preprint arXiv:2011.01403. Iryna Gurevych and Michael Strube. 2004. Semantic similarity applied to spoken dialogue summarization. In COLING 2004: Proceedings of the 20th Inter- national Conference on Computational Linguistics, pages 764–770, Geneva, Switzerland. COLING. Jia Jin Koay, Alexander Roustai, Xiaojin Dai, Dillon Burns, Alec Kerrigan, and Fei Liu. 2020. How domain terminology affects meeting summarization In Proceedings of the 28th Inter- performance. national Conference on Computational Linguistics, pages 5689–5695, Barcelona, Spain (Online). Inter- national Committee on Computational Linguistics. Wojciech Kryscinski, Bryan McCann, Caiming Xiong, and Richard Socher. 2020. Evaluating the factual consistency of abstractive text summarization. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 9332–9346, Online. Association for Computa- tional Linguistics. Seolhwa Lee, Kisu Yang, Chanjun Park, João Sedoc, and Heuiseok Lim. 2021. Who says like a style of vitamin: Towards syntax-aware dialoguesumma- rization using multi-task learning. arXiv preprint arXiv:2109.14199. Yuejie Lei, Yuanmeng Yan, Zhiyuan Zeng, Keqing He, Ximing Zhang, and Weiran Xu. 2021. Hier- archical speaker-aware sequence-to-sequence model In ICASSP 2021- for dialogue summarization. 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 7823–7827. IEEE. Mike Lewis, Yinhan Liu, Naman Goyal, Mar- jan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising sequence-to-sequence pre- training for natural language generation, translation, and comprehension. In Proceedings of the 58th An- nual Meeting of the Association for Computational Linguistics, pages 7871–7880, Online. Association for Computational Linguistics. Daniel Li, Thomas Chen, Albert Tung, and Lydia Chilton. 2021. Hierarchical summarization for long- form spoken dialog. Haoran Li, Junnan Zhu, Jiajun Zhang, and Chengqing Zong. 2018. Ensure the correctness of the summary: Incorporate entailment knowledge into abstractive sentence summarization. In Proceedings of the 27th International Conference on Computational Linguis- tics, pages 1430–1441, Santa Fe, New Mexico, USA. Association for Computational Linguistics. Chin-Yew Lin. 2004. ROUGE: A package for auto- matic evaluation of summaries. In Text Summariza- tion Branches Out, pages 74–81, Barcelona, Spain. Association for Computational Linguistics. Junpeng Liu, Yanyan Zou, Hainan Zhang, Hongshen Chen, Zhuoye Ding, Caixia Yuan, and Xiaojie Wang. 2021a. Topic-aware contrastive learning for abstrac- tive dialogue summarization. In Proceedings of the 2020 Conference on Empirical Methods in Natu- ral Language Processing (EMNLP), Online. Asso- ciation for Computational Linguistics. Zhengyuan Liu and Nancy F. Chen. 2021. Controllable neural dialogue summarization with personal named entity planning. In Proceedings of the Association for Computational Linguistics: EMNLP 2020, On- line. Association for Computational Linguistics. Zhengyuan Liu, Ke Shi, and Nancy Chen. 2021b. Coreference-aware dialogue summarization. In Pro- ceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 509–519, Singapore and Online. Association for Computational Linguistics. Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. 2020. On faithfulness and factu- ality in abstractive summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1906–1919, On- line. Association for Computational Linguistics. Iain McCowan, Jean Carletta, Wessel Kraaij, Simone Ashby, S Bourban, M Flynn, M Guillemot, Thomas 10 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 Hain, J Kadlec, Vasilis Karaiskos, et al. 2005. The ami meeting corpus. In Proceedings of the 5th inter- national conference on methods and techniques in behavioral research, volume 88, page 100. Citeseer. Rada Mihalcea and Paul Tarau. 2004. TextRank: Bringing order into text. In Proc. of EMNLP, pages 404–411, Barcelona, Spain. Association for Compu- tational Linguistics. Artidoro Pagnoni, Vidhisha Balachandran, and Yulia Tsvetkov. 2021. Understanding factuality in abstrac- tive summarization with FRANK: A benchmark for factuality metrics. In Proceedings of the 2021 Con- ference of the North American Chapter of the Asso- ciation for Computational Linguistics: Human Lan- guage Technologies, pages 4812–4829, Online. As- sociation for Computational Linguistics. Colin Raffel, Noam Shazeer, Adam Roberts, Kather- ine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a uniﬁed text-to- text transformer. Journal of Machine Learning Re- search, 21(140):1–67. Clément Rebuffel, Thomas Scialom, Laure Soulier, Benjamin Piwowarski, Sylvain Lamprier, Jacopo Staiano, Geoffrey Scoutheeten, and Patrick Galli- nari. 2021. Data-questeval: A referenceless metric for data to text semantic evaluation. arXiv preprint arXiv:2104.07555. Harvey Sacks, Emanuel A Schegloff, and Gail Jeffer- son. 1978. A simplest systematics for the organiza- tion of turn taking for conversation. In Studies in the organization of conversational interaction, pages 7– 55. Elsevier. Thomas Scialom, Paul-Alexis Dray, Patrick Gallinari, Sylvain Lamprier, Benjamin Piwowarski, Jacopo Staiano, and Alex Wang. 2021. Questeval: Summa- rization asks for fact-based evaluation. Abigail See, Peter J. Liu, and Christopher D. Manning. 2017. Get to the point: Summarization with pointer- generator networks. In Proc. of ACL, pages 1073– 1083, Vancouver, Canada. Association for Computa- tional Linguistics. Xiangru Tang, Alexander R. Fabbri, Ziming Mao, Grif- ﬁn Adams, Borui Wang, Haoran Li, Yashar Mehdad, and Dragomir Radev. 2021. Investigating crowd- sourcing protocols for evaluating the factual consis- tency of summaries. Alex Wang, Kyunghyun Cho, and Mike Lewis. 2020. Asking and answering questions to evaluate the fac- In Proceedings of tual consistency of summaries. the 58th Annual Meeting of the Association for Com- putational Linguistics, pages 5008–5020, Online. Association for Computational Linguistics. Chien-Sheng Wu, Linqing Liu, Wenhao Liu, Pontus Stenetorp, and Caiming Xiong. 2021. Controllable abstractive dialogue summarization with sketch su- pervision. In Findings of the Association for Com- putational Linguistics: ACL-IJCNLP 2021, pages 5108–5122, Online. Association for Computational Linguistics. Feng Xiachong, Feng Xiaocheng, and Qin Bing. 2021. Incorporating commonsense knowledge into ab- stractive dialogue summarization via heterogeneous graph networks. In Proceedings of the 20th Chinese National Conference on Computational Linguistics, pages 964–975, Huhhot, China. Chinese Informa- tion Processing Society of China. Lin Yuan and Zhou Yu. 2020. Abstractive dialog sum- marization with semantic scaffolds. Weizhe Yuan, Graham Neubig, and Pengfei Liu. 2021. Bartscore: Evaluating generated text as text genera- tion. arXiv preprint arXiv:2106.11520. Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Pe- ter Liu. 2020. Pegasus: Pre-training with extracted gap-sentences for abstractive summarization. In In- ternational Conference on Machine Learning, pages 11328–11339. PMLR. Shiyue Zhang, Asli Celikyilmaz, Jianfeng Gao, and Mohit Bansal. 2021. EmailSum: Abstractive email In Proceedings of the 59th thread summarization. Annual Meeting of the Association for Computa- tional Linguistics and the 11th International Joint Conference on Natural Language Processing (Vol- ume 1: Long Papers), pages 6895–6909, Online. As- sociation for Computational Linguistics. Lulu Zhao, Weiran Xu, and Jun Guo. 2020. Improving abstractive dialogue summarization with graph struc- In Proceedings of the 28th tures and topic words. International Conference on Computational Linguis- tics, pages 437–449, Barcelona, Spain (Online). In- ternational Committee on Computational Linguis- tics. Lulu Zhao, Zeyuan Yang, Weiran Xu, Sheng Gao, and Jun Guo. 2021. Improving abstractive dialogue sum- marization with conversational structure and factual knowledge. Ming Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia Mutuma, Rahul Jha, Ahmed Hassan Awadallah, Asli Celikyilmaz, Yang Liu, Xipeng Qiu, and Dragomir Radev. 2021. QMSum: A new benchmark for query- based multi-domain meeting summarization. In Pro- ceedings of the 2021 Conference of the North Amer- ican Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5905–5921, Online. Association for Computational Linguistics. Chenguang Zhu, Yang Liu, Jie Mei, and Michael Zeng. 2021. Mediasum: A large-scale media interview dataset for dialogue summarization. arXiv preprint arXiv:2103.06410. 11 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 Chenguang Zhu, Ruochen Xu, Michael Zeng, and Xue- dong Huang. 2020. A hierarchical network for ab- stractive meeting summarization with cross-domain pretraining. In Findings of the Association for Com- putational Linguistics: EMNLP 2020, pages 194– 203, Online. Association for Computational Linguis- tics. 12 