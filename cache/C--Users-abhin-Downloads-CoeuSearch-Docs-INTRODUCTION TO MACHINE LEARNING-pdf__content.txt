INTRODUCTION MACHINE LEARNING Introduction Machine Learning Alex Smola S.V.N . Vishwanathan Yahoo ! Labs Santa Clara –and– Departments Statistics Computer Science Purdue University –and– College Engineering Computer Science Australian National University p u b l h e b h e p r e n c e f h e u n v e r f c b r g e Pitt Building , Trumpington Street , Cambridge , United Kingdom c b r g e u n v e r p r e Edinburgh Building , Cambridge CB   RU , UK    West   th Street , New York , NY      –     , USA     Williamstown Road , Port Melbourne , VIC      , Australia Ruiz de Alarc´on    ,       Madrid , Spain Dock House , Waterfront , Cape Town      , South Africa http : //www.cambridge.org c ( cid:   ) Cambridge University Press      book copyright . Subject statutory exception provisions relevant collective licensing agreements , reproduction part may take place without written permission Cambridge University Press . First published      Printed United Kingdom University Press , Cambridge Typeface Monotype Times   /  pt System LATEX  ε Vishwanathan ] [ Alexander J. Smola S.V.N . catalogue record book available British Library Library Congress Cataloguing Publication data available ISBN               hardback Author : vishy Revision :     Timestamp : October   ,      URL : svn : //smola @ repos.stat.purdue.edu/thebook/trunk/Book/thebook.tex Contents Preface   Introduction  .  Taste Machine Learning  . .  Applications  . .  Data  . .  Problems  .  Probability Theory  . .  Random Variables  . .  Distributions  . .  Mean Variance  . .  Marginalization , Independence , Conditioning , Bayes Rule  .  Basic Algorithms  . .  Naive Bayes  . .  Nearest Neighbor Estimators  . .  Simple Classiﬁer  . .  Perceptron  . .  K-Means   Density Estimation Limit Theorems  .   . .  Fundamental Laws  . .  Characteristic Function  . .  Tail Bounds  . .  Example  .  Parzen Windows  . .  Discrete Density Estimation  . .  Smoothing Kernel  . .  Parameter Estimation  . .  Silverman ’ Rule  . .  Watson-Nadaraya Estimator  .  Exponential Families  . .  Basics page                                                                                        v vi    . .  Examples  .  Estimation  . .  Maximum Likelihood Estimation  . .  Bias , Variance Consistency  . .  Bayesian Approach  . .  Example Sampling  . .   . .  Rejection Sampler Inverse Transformation  .    Contents                            Optimization  .  Preliminaries  .  Unconstrained Smooth Convex Minimization           . .  Convex Sets     . .  Convex Functions     . .  Subgradients  . .  Strongly Convex Functions     . .  Convex Functions Lipschitz Continous Gradient        . .  Fenchel Duality      . .  Bregman Divergence                                                                      . .  Minimizing One-Dimensional Convex Function  . .  Coordinate Descent  . .  Gradient Descent  . .  Mirror Descent  . .  Conjugate Gradient  . .  Higher Order Methods  . .  Bundle Methods  . .  Projection Based Methods  . .  Lagrange Duality  . .  Linear Quadratic Programs Stochastic Optimization  . .  Stochastic Gradient Descent  . .  Concave-Convex Procedure Practical Advice  .   .   .  Nonconvex Optimization  .  Constrained Optimization   Online Learning Boosting  .  Halving Algorithm  .  Weighted Majority             Contents   Conditional Densities  .  Logistic Regression  .  Regression  . .  Conditionally Normal Models  . .  Posterior Distribution  . .  Heteroscedastic Estimation  .  Multiclass Classiﬁcation  . .  Conditionally Multinomial Models  .  CRF ?  . .  Linear Chain CRFs  . .  Higher Order CRFs  . .  Kernelized CRFs  .  Optimization Strategies  . .  Getting Started  . .  Optimization Algorithms  . .  Handling Higher order CRFs  .  Hidden Markov Models  .  Reading  . .  Optimization   Kernels Function Spaces  .  Basics  . .  Examples  .  Kernels  . .  Feature Maps  . .  Kernel Trick  . .  Examples Kernels  .  Algorithms  . .  Kernel Perceptron  . .  Trivial Classiﬁer  . .  Kernel Principal Component Analysis  .  Reproducing Kernel Hilbert Spaces  . .  Hilbert Spaces  . .  Theoretical Properties  . .  Regularization  .  Banach Spaces  . .  Properties  . .  Norms Convex Sets   Linear Models  .  Support Vector Classiﬁcation vii                                                                                                                                                             viii   Contents  . .  Regularized Risk Minimization Viewpoint  . .  Exponential Family Interpretation  . .  Specialized Algorithms Training SVMs  .  Extensions  . .  ν trick  . .  Squared Hinge Loss  . .  Ramp Loss Support Vector Regression  . .   . .  Incorporating General Loss Functions Incorporating ν Trick  .   .  Novelty Detection  .  Margins Probability  .  Beyond Binary Classiﬁcation  .   . .  Multiclass Classiﬁcation  . .  Multilabel Classiﬁcation  . .  Ordinal Regression Ranking Large Margin Classiﬁers Structure  . .  Margin  . .  Penalized Margin  . .  Nonconvex Losses  .  Applications  . .  Sequence Annotation  . .  Matching  . .  Ranking  . .  Shortest Path Planning Image Annotation  . .   . .  Contingency Table Loss  .  Optimization  . .  Column Generation  . .  Bundle Methods  . .  Overrelaxation Dual  .   CRFs vs Structured Large Margin Models  .  .  Loss Function  .  .  Dual Connections  .  .  Optimization Appendix   Linear Algebra Functional Analysis Appendix   Conjugate Distributions Appendix   Loss Functions Bibliography                                                                                                                                                             Preface Since textbook biased selection references towards easily accessible work rather original references . may interest inventors concepts , greatly simpliﬁes access topics . Hence encourage reader follow references cited works interested ﬁnding may claim intellectual ownership certain key ideas .     Structure Book   Preface Canberra , August      IntroductionDensity EstimationGraphical ModelsKernelsOptimizationConditional DensitiesConditional Random FieldsLinear ModelsStructured EstimationDuality EstimationMomentMethodsReinforcement LearningIntroductionDensity EstimationGraphical ModelsKernelsOptimizationConditional DensitiesConditional Random FieldsLinear ModelsStructured EstimationDuality EstimationMomentMethodsReinforcement LearningIntroductionDensity EstimationGraphical ModelsKernelsOptimizationConditional DensitiesConditional Random FieldsLinear ModelsStructured EstimationDuality EstimationMomentMethodsReinforcement Learning   Introduction past two decades Machine Learning become one main- stays information technology , rather central , albeit usually hidden , part life . ever increasing amounts data becoming available good reason believe smart data analysis become even pervasive necessary ingredient technological progress . purpose chapter provide reader overview vast range applications heart machine learning problem bring degree order zoo problems . , discuss basic tools statistics probability theory , since form language many machine learning problems must phrased become amenable solving . Finally , outline set fairly basic yet eﬀective algorithms solve important problem , namely classiﬁcation . sophisticated tools , discussion general problems detailed analysis follow later parts book .  .  Taste Machine Learning Machine learning appear many guises . discuss number applications , types data deal , ﬁnally , formalize problems somewhat stylized fashion . latter key want avoid reinventing wheel every new application . Instead , much art machine learning reduce range fairly disparate problems set fairly narrow prototypes . Much science machine learning solve problems provide good guarantees solutions .  . .  Applications readers familiar concept web page ranking . , process submitting query search engine , ﬁnds webpages relevant query returns order relevance . See e.g . Figure  .  example query results “ ma- chine learning ” . , search engine returns sorted list webpages given query . achieve goal , search engine needs ‘ know ’       Introduction Fig .  .  .   top scoring webpages query “ machine learning ” pages relevant pages match query . knowledge gained several sources : link structure webpages , content , frequency users follow suggested links query , examples queries combination manually ranked webpages . Increasingly machine learning rather guesswork clever engineering used automate process designing good search engine [ RPB   ] . rather related application collaborative ﬁltering . Internet book- stores Amazon , video rental sites Netﬂix use informa- tion extensively entice users purchase additional goods ( rent movies ) . problem quite similar one web page ranking . , want obtain sorted list ( case articles ) . key dif- ference explicit query missing instead use past purchase viewing decisions user predict future viewing purchase habits . key side information decisions made similar users , hence collaborative nature process . See Figure  .  example . clearly desirable automatic system solve problem , thereby avoiding guesswork time [ BK   ] . equally ill-deﬁned problem automatic translation doc- uments . one extreme , could aim fully understanding text translating using curated set rules crafted computational linguist well versed two languages would like translate . rather arduous task , particular given text always grammatically cor- rect , document understanding part trivial one . Instead , could simply use examples translated documents , proceedings Canadian parliament multilingual entities ( United Nations , European Union , Switzerland ) learn translate two Web Images Maps News Shopping Gmail ! Sponsored LinksMachine LearningGoogle Sydney needs machinelearning experts . Apply today ! www.google.com.au/jobsSign Search Advanced Search Preferences Web Scholar Results   -      ,   ,    machine learning . (  .   seconds ) Machine learning - Wikipedia , free encyclopediaAs broad subfield artificial intelligence , machine learning concerned designand development algorithms techniques allow ... en.wikipedia.org/wiki/Machine_learning -   k - Cached - Similar pagesMachine Learning textbookMachine Learning study computer algorithms improve automatically throughexperience . Applications range datamining programs ... www.cs.cmu.edu/~tom/mlbook.html -  k - Cached - Similar pagesmachine learningwww.aaai.org/AITopics/html/machine.html - Similar pagesMachine LearningA list links papers resources machine learning.www.machinelearning.net/ -   k - Cached - Similar pagesIntroduction Machine LearningThis page pointers draft book Machine Learning individualchapters . downloaded Adobe Acrobat format . ... ai.stanford.edu/~nilsson/mlbook.html -   k - Cached - Similar pagesMachine Learning - Artificial Intelligence ( incl . Robotics ... Machine Learning - Artificial Intelligence . Machine Learning international forum forresearch computational approaches learning.www.springer.com/computer/artificial/journal/      -   k - Cached - Similar pagesMachine Learning ( Theory ) Graduating students Statistics appear substantial handicap compared tograduating students Machine Learning , despite substantially ... hunch.net/ -   k - Cached - Similar pagesAmazon.com : Machine Learning : Tom M. Mitchell : BooksAmazon.com : Machine Learning : Tom M. Mitchell : Books.www.amazon.com/Machine-Learning-Tom-M-Mitchell/dp/           -    k -Cached - Similar pagesMachine Learning JournalMachine Learning publishes articles mechanisms intelligent systemsimprove performance time . invite authors submit ... pages.stern.nyu.edu/~fprovost/MLJ/ -  k - Cached - Similar pagesCS     : Machine LearningSTANFORD . CS    Machine Learning Autumn      . Announcements . Final reports fromthis year 's class projects posted . ... cs   .stanford.edu/ -   k - Cached - Similar pages           Next SearchSearch within results | Language Tools | Search Tips | Dissatisfied ? Help us improve | Try Google Experimental©     Google - Google Home - Advertising Programs - Business Solutions - Googlemachine learningmachine learningGoogle  .  Taste Machine Learning   languages . words , could use examples translations learn translate . machine learning approach proved quite successful [ ? ] . Many security applications , e.g . access control , use face recognition one components . , given photo ( video recording ) person , recognize person . words , system needs classify faces one many categories ( Alice , Bob , Charlie , . . . ) decide unknown face . similar , yet conceptually quite diﬀerent problem veriﬁcation . goal verify whether person question claims . Note diﬀerently , yes/no question . deal diﬀerent lighting conditions , facial expressions , whether person wearing glasses , hairstyle , etc. , desirable system learns features relevant identifying person . Another application learning helps problem named entity recognition ( see Figure  .  ) . , problem identifying entities , places , titles , names , actions , etc . documents . steps crucial automatic digestion understanding documents . modern e-mail clients , Apple ’ Mail.app nowadays ship ability identify addresses mails ﬁling automatically address book . systems using hand-crafted rules lead satisfac- tory results , far eﬃcient use examples marked-up documents learn dependencies automatically , particular want de- ploy system many languages . instance , ’ bush ’ ’ rice ’ Fig .  .  . Books recommended Amazon.com viewing Tom Mitchell ’ Ma- chine Learning Book [ Mit   ] . desirable vendor recommend relevant books user might purchase . Fig .  .  .    Pictures person taken Yale face recognition database . challenge recognize dealing per- son    cases . Amazon.com Today 's DealsGifts & Wish Lists Gift Cards Account | HelpAdvertise Amazon  star : (    )   star : (   )   star : (   )   star : (   )   star : (   ) Quantity :   orSign turn  -Click ordering . Buying Choices   used & new $   .  Have one sell ? Share customer imagesSearch inside another edition bookAre Author orPublisher ? Find publishyour Kindle Books Hello . Sign get personalized recommendations . New customer ? Start . Books BooksAdvanced SearchBrowse SubjectsHot New ReleasesBestsellersThe New York Times® Best SellersLibros En EspañolBargain BooksTextbooksJoin Amazon Prime ship Two-Day free Overnight $  .   . Already member ? Sign in.Machine Learning ( Mcgraw-Hill International Edit ) ( Paperback ) Thomas Mitchell ( Author ) `` Ever since computers invented , wondered whetherthey might made learn ... '' ( ) (    customer reviews ) List Price : $   .  Price : $   .   & item ships FREE Super Saver Shipping.DetailsAvailability : Usually ships within     weeks . Ships sold Amazon.com . Gift-wrap available.   used & new available $   .  Also Available : List Price : Price : Offers : Hardcover (   ) $    .   $    .     used & new $   .   Better TogetherBuy book Introduction Machine Learning ( Adaptive Computation Machine Learning ) Ethem Alpaydin today ! Buy Together Today : $    .  Customers Bought Item Also BoughtPattern Recognition andMachine Learning ( Information Science andStatistics ) ChristopherM . Bishop (    ) $   .  Artificial Intelligence : AModern Approach (  ndEdition ) ( Prentice HallSeries ArtificialIntelligence ) StuartRussell (    ) $    .  The Elements StatisticalLearning T. Hastie (    ) $   .  Pattern Classification (  ndEdition ) Richard O.Duda (    ) $    .  Data Mining : PracticalMachine Learning Toolsand Techniques , SecondEdition ( Morgan KaufmannSeries DataManagement Systems ) byIan H. Witten (    ) $   .  › Explore similar items : Books (    ) Editorial ReviewsBook DescriptionThis exciting addition McGraw-Hill Series Computer Science focuses concepts techniques contribute rapidlychanging field machine learning -- including probability statistics , artificial intelligence , neural networks -- unifying logicaland coherent manner . Machine Learning serves useful reference tool software developers researchers , well outstanding textfor college students . -- text refers Hardcover edition . Book InfoPresents key algorithms theory form core machine learning . Discusses theoretical issues learningperformance vary number training examples presented ? learning algorithms appropriate various types oflearning tasks ? DLC : Computer algorithms . -- text refers Hardcover edition.Product DetailsPaperback :     pagesPublisher : McGraw-Hill Education ( ISE Editions ) ;  st edition ( October   ,      ) Language : EnglishISBN-   :           ISBN-   :    -          Product Dimensions :   x  .  x  .  inchesShipping Weight :  .  pounds ( View shipping rates policies ) Average Customer Review : (    customer reviews ) Amazon.com Sales Rank : #    ,    Books ( See Bestsellers Books ) Popular category : ( 's ? ) #    Books > Computers & Internet > Computer Science > Artificial Intelligence > Machine Learning ( Publishers authors : Improve Sales ) In-Print Editions : Hardcover (   ) | Editions Would like update product info give feedback images ? ( 'll ask sign get back ) Inside Book ( learn ) Browse search another edition book.First Sentence : Ever since computers invented , wondered whether might made learn . Read first pageBrowse Sample Pages : Front Cover | Copyright | Table Contents | Excerpt | Index | Back Cover | Surprise ! Search Inside Book : Customers viewing page may interested Sponsored Links ( 's ? ) Online Law Degreehttp : //www.edu-onlinedegree.org Juris Doctor JD & LLM Masters Low tuition , Free Textbooks Learning CDswww.mindperk.com Save powerful mind-boosting CDs & DVDs . Huge Selection Video Edit Magicwww.deskshare.com/download Video Editing Software trim , modify color , merge video Tags Customers Associate Product ( 's ? ) Click tag find related items , discussions , people.machine learning (   ) artificial intelligence (   ) computer science (   ) pattern recognition (   ) tags : Add first tagHelp others find product - tag Amazon searchNo one tagged product Amazon search yet . first tosuggest search appear ? Search Products Tagged publisher author ? Learn Amazon help make book eBook . publisher author hold digital rights book , make available eBook Amazon.com . Learn moreRate Item Improve RecommendationsI itNot ratedYour ratingDo n't like < > love ! Save yourrating ?       Customer Reviews   Reviews Average Customer Review (    customer reviews ) Share thoughts customers : Helpful Customer Reviews       people found following review helpful : excellent overview adv . undergrad beg . grad , September    ,     By Todd Ebert ( Long Beach California ) - See reviewsThis review : Machine Learning ( Hardcover ) agree previous reviews criticize book lack ofdepth , believe asset rather liability given targetaudience ( seniors beginning grad . students ) . average college senior typicallyknows little subjects like neural networks , genetic algorithms , Baysiannetworks , book goes long way demystifying subjects veryclear , concise , understandable way . Moreover , first-year grad . student isinterested possibly research field needs overview todive deeply one many branches entire books written aboutthem . one books one find diverse areas oflearning ( e.g . analytical , reinforcment , Bayesian , neural-network , genetic-algorithmic ) within cover.But encyclopedic introduction , author makes number ofconnections different paradigms . example , explains thatassociated paradigm notion inductive-learning bias , i.e . theunderlying assumptions lend validity given learning approach . end-of-chapter discussions bias seem interesting unique book.Finally , used book part reading material intro . AI class , andreceived much positive feedback students , although find thepresentation bit abstract undergraduate tastes Comment | Permalink | review helpful ? ( Report )       people found following review helpful : Great compilation , May    ,     By Steven Burns ( - ) - See reviewsThis review : Machine Learning ( Hardcover ) book completely worth price , worth hardcover take care it.The main chapters book independent , read order.The way explains different learning approaches beautiful :   ) itexplains nicely   ) gives examples   ) presents pseudocode summaries ofthe algorithms . software developer , else could possibly ask ? Comment | Permalink | review helpful ? ( Report )       people found following review helpful : Venerable , senses , April   ,     By eldil ( Albuquerque NM ) - See reviewsThis review : Machine Learning ( Hardcover ) 's pretty well done , covers theory core areas - maybe thestate field written - found unsatisfyingly un-synthesized , unconnected , short detail ( subjective ) . found  nd edition ofRussell Norvig better introduction covers topic , whichit everything think , except VC dimension.The book sorely needs update , written      field movedfast . comparison Mitchell 's current course ( materials generously availableonline ) shows  /  topics taught arisen since book waspublished ; Boosting , Support Vector Machines Hidden Markov Models namethe best-known . book also cover statistical data mining methods.Despite subjective complaint lack depth give theoreticalroots many fundamental techniques decently readably . many purposesthough may superceded R & N  nd ed . Comment | Permalink | review helpful ? ( Report ) Share thoughts customers : › See    customer reviews ... Recent Customer Reviews OutstandingI read book   years ago inthe PhD program Stanford University . Iconsider book bestMachine Learning book , one bestbooks ... Read morePublished   months ago Husam Abu-Haimed Great Start Machine LearningI used book mastersand found extremely helpful anda gentle introduction thick thingsof machine learning applications.Read morePublished   months ago Subrat Nanda Best book 've seen topicI book listed one bestand interesting 've ever read . lovedthe book much loved coursewe used . Read morePublished    months ago Lars Kristensson expensive would saygreat book wan na start sth anywherein machine learning , tooooooexpensive.Published    months ago X. Wu Excellent book , concise andreadableThis great book 're starting outwith machine learning . 's rare comeacross book like wellwritten technical depth . Read morePublished    months ago Part Time Reader great bookThis great book focuses onmachine learning techniques . beenused textbook class.Published November    ,      Jay Great introduction book forstudents data mining machinelearning classAlthough text book required inmy data mining class , found veryhelpful study . Read morePublished October    ,      Thanh Doan Excellently writtenI using textbook MachineLearning class . professor isexcellent , must say book awelcome addition class . Read morePublished October    ,      Gregor Kronenberger brief introduction ML ... First , statistical part machinelearning real subset ofmathematical statisitcs , whatever Bayesianor frequentist . Read morePublished September    ,      supercutepig Excellent reference bookI liked book . think author mustprovide figures book like Dudaand Hart 's Pattern Classification book.Read morePublished December    ,      Fatih NarSearch Customer Reviews search product 's reviews› See    customer reviews ... Customer Discussions Beta ( 's ? ) New ! See recommended Discussions YouThis product 's forum (   discussions ) DiscussionRepliesLatest PostNo discussions yetAsk questions , Share opinions , Gain insightStart new discussionTopic : Related forumsmachine learning ( start discussion ) artificial intelligence (   discussion ) Product Information Amapedia Community Beta ( 's ? ) first person add article item Amapedia.com . › See featured Amapedia.com articles Listmania ! Machine Learning Graphs : list J. Chan `` PhD Student ( ComputerScience ) '' Bayesian Network Books : list Tincture Iodine `` TOI '' Books Algorithms variety topics : list calvinnme `` Texan refugee '' Create Listmania ! listSearch Listmania ! 'd Like ... Learn Advanced Mathematics : guide Gal Gross `` Wir müssenwissen , wir werden wissen . - David Hilbert '' Learn Artificial Intelligence ( AI ) Games : guide John Funge study curriculum B.S . computer science ( honors mode ) : guide '' josie_roberts '' Create guideSearch GuidesLook Similar Items CategoryComputers & Internet > Computer Science > Artificial Intelligence > Machine LearningLook Similar Items Subject Machine learning Computer Books : GeneralFind books matching checked subjects i.e. , book must subject   subject   ... Harry Potter StoreOur HarryPotterStorefeaturesall thingsHarry , includingbooks , audio CDs andcassettes , DVDs , soundtracks , . Got Neti Pot ? Give yoursinuses abath withone many netipots Health & Personal Care Store.›See Drop Like It'sWaterproofAndshockproof , crushproof , andfreezeproof . , inaddition  -megapixelresolution BrightCapture technology , makes OlympusStylus    SW theperfect vacationcompanion . Plus , 's nowavailable $    .   fromAmazon.com . Editors ' Faves inBooksSave   % TheSignificant   , favoritepicks month . Feedback need help question Customer Service , contact us . Would like update product info give feedback images ? ( 'll ask sign get back ) feedback would like provide ? Click hereWhere 's Stuff ? Track recent orders.View change orders Account.Shipping & ReturnsSee shipping rates & policies.Return item ( 's Returns Policy ) .Need Help ? Forgot password ? Click here.Redeem buy gift certificate/card.Visit Help department.Search Amazon.com Recent History ( 's ? ) Recently Viewed ProductsAfter viewing product detail pages search results , look find easy way navigate back pages interested in.Look right column find helpful suggestions shopping session.› View & edit Browsing History Amazon.com Home | Directory StoresInternational Sites : Canada | United Kingdom | Germany | Japan | France | ChinaHelp | View Cart | Account | Sell Items |  -Click Settings     Introduction HAVANA ( Reuters ) - European Union ’ top development aid official left Cuba Sunday convinced EU diplomatic sanctions communist island dropped Fidel Castro ’ retirement , main aide said . < TYPE= '' ORGANIZATION '' > HAVANA < / > ( < TYPE= '' ORGANIZATION '' > Reuters < / > ) - < TYPE= '' ORGANIZATION '' > European Union < / > ’ top development aid official left < TYPE= '' ORGANIZATION '' > Cuba < / > Sunday convinced EU diplomatic sanctions communist < TYPE= '' LOCATION '' > island < / > dropped < TYPE= '' PERSON '' > Fidel Castro < / > ’ retirement , main aide said . Fig .  .  . Named entity tagging news article ( using LingPipe ) . relevant locations , organizations persons tagged information extraction . clearly terms agriculture , equally clear context contemporary politics refer members Republican Party . applications take advantage learning speech recog- nition ( annotate audio sequence text , system shipping Microsoft Vista ) , recognition handwriting ( annotate sequence strokes text , feature common many PDAs ) , trackpads com- puters ( e.g . Synaptics , major manufacturer pads derives name synapses neural network ) , detection failure jet en- gines , avatar behavior computer games ( e.g . Black White ) , direct marketing ( companies use past purchase behavior guesstimate whether might willing purchase even ) ﬂoor cleaning robots ( iRobot ’ Roomba ) . overarching theme learning problems exists nontrivial dependence observations , commonly refer x desired response , refer , simple set deterministic rules known . using learning infer dependency x systematic fashion . conclude section discussing problem classiﬁcation , since serve prototypical problem signiﬁcant part book . occurs frequently practice : instance , performing spam ﬁltering , interested yes/no answer whether e-mail con- tains relevant information . Note issue quite user depen- dent : frequent traveller e-mails airline informing recent discounts might prove valuable information , whereas many recipients might prove nuisance ( e.g . e-mail relates products available overseas ) . Moreover , nature annoying e- mails might change time , e.g . availability new products ( Viagra , Cialis , Levitra , . . . ) , diﬀerent opportunities fraud ( Nigerian     scam took new twist Iraq war ) , diﬀerent data types ( e.g . spam consists mainly images ) . combat problems  .  Taste Machine Learning   Fig .  .  . Binary classiﬁcation ; separate stars diamonds . example able drawing straight line separates sets . see later important example called linear classiﬁer . want build system able learn classify new e-mails . seemingly unrelated problem , cancer diagnosis shares common structure : given histological data ( e.g . microarray analysis pa- tient ’ tissue ) infer whether patient healthy . , asked generate yes/no answer given set observations . See Figure  .  example .  . .  Data useful characterize learning problems according type data use . great help encountering new challenges , since quite often problems similar data types solved similar tech- niques . instance natural language processing bioinformatics use similar tools strings natural language text DNA sequences . Vectors constitute basic entity might encounter work . instance , life insurance company might interesting obtaining vector variables ( blood pressure , heart rate , height , weight , cholesterol level , smoker , gender ) infer life expectancy potential customer . farmer might interested determining ripeness fruit based ( size , weight , spectral data ) . engineer might want ﬁnd dependencies ( voltage , current ) pairs . Likewise one might want represent documents vector counts describe occurrence words . latter commonly referred bag words features . One challenges dealing vectors scales units diﬀerent coordinates may vary widely . instance , could measure height kilograms , pounds , grams , tons , stones , would amount multiplicative changes . Likewise , representing temperatures , full class aﬃne transformations , depending whether rep- resent terms Celsius , Kelvin Farenheit . One way dealing     Introduction issues automatic fashion normalize data . discuss means automatic fashion . Lists : cases vectors obtain may contain variable number features . instance , physician might necessarily decide perform full battery diagnostic tests patient appears healthy . Sets may appear learning problems whenever large number potential causes eﬀect , well determined . instance , relatively easy obtain data concerning toxicity mushrooms . would desirable use data infer toxicity new mushroom given information chemical compounds . However , mushrooms contain cocktail compounds one may toxic . Consequently need infer properties object given set features , whose composition number may vary considerably . Matrices convenient means representing pairwise relationships . instance , collaborative ﬁltering applications rows matrix may represent users whereas columns correspond products . cases knowledge given ( user , product ) combina- tion , rating product user . related situation occurs whenever similarity information observations , implemented semi-empirical distance mea- sure . homology searches bioinformatics , e.g . variants BLAST [ AGML   ] , return similarity score necessarily satisfy requirements metric . Images could thought two dimensional arrays numbers , , matrices . representation crude , though , since exhibit spa- tial coherence ( lines , shapes ) ( natural images exhibit ) multiresolution structure . , downsampling image leads object similar statistics original image . Computer vision psychooptics created raft tools describing phenomena . Video adds temporal dimension images . , could represent three dimensional array . Good algorithms , however , take tem- poral coherence image sequence account . Trees Graphs often used describe relations collec- tions objects . instance ontology webpages DMOZ project ( www.dmoz.org ) form tree topics becoming increasingly reﬁned traverse root one leaves ( Arts → Animation → Anime → General Fan Pages → Oﬃcial Sites ) . case gene ontol- ogy relationships form directed acyclic graph , also referred GO-DAG [ ABB+   ] . examples describe estimation problems observations  .  Taste Machine Learning   vertices tree graph . However , graphs may observations . instance , DOM-tree webpage , call-graph computer program , protein-protein interaction networks may form basis upon may want perform inference . Strings occur frequently , mainly area bioinformatics natural language processing . may input estimation problems , e.g . classifying e-mail spam , attempting locate names persons organizations text , modeling topic structure document . Equally well may constitute output system . instance , may want perform document summarization , automatic translation , attempt answer natural language queries . Compound structures commonly occurring object . , situations structured mix diﬀerent data types . instance , webpage might contain images , text , tables , turn contain numbers , lists , might constitute nodes graph webpages linked among . Good statistical modelling takes de- pendencies structures account order tailor suﬃciently ﬂexible models .  . .  Problems range learning problems clearly large , saw discussing applications . said , researchers identiﬁed ever growing number templates used address large set situations . templates make deployment machine learning practice easy discussion largely focus choice set problems . give means complete list templates . Binary Classiﬁcation probably frequently studied problem machine learning led large number important algorithmic theoretic developments past century . simplest form reduces question : given pattern x drawn domain X , estimate value associated binary random variable ∈ { ±  } assume . instance , given pictures apples oranges , might want state whether object question apple orange . Equally well , might want predict whether home owner might default loan , given income data , credit history , whether given e-mail spam ham . ability solve basic problem already allows us address large variety practical settings . many variants exist regard protocol required make estimation :      Introduction Fig .  .  . Left : binary classiﬁcation . Right :  -class classiﬁcation . Note latter case much degree ambiguity . instance , able distinguish stars diamonds may suﬃce identify either correctly , since also need distinguish triangles . • might see sequence ( xi , yi ) pairs yi needs estimated instantaneous online fashion . commonly referred online learning . • might observe collection X : = { x  , . . . xm } : = { y  , . . . ym } pairs ( xi , yi ) used estimate ( set ) so-far unseen X ( cid:   ) = ( cid:  ) x ( cid:   ) ( cid:  ) . commonly referred batch learning .   , . . . , x ( cid:   ) ( cid:   ) • might allowed know X ( cid:   ) already time constructing model . commonly referred transduction . • might allowed choose X purpose model building . known active learning . • might full information X , e.g . coordinates xi might missing , leading problem estimation missing variables . • sets X X ( cid:   ) might come diﬀerent data sources , leading problem covariate shift correction . • might given observations stemming two problems time side information problems somehow related . known co-training . • Mistakes estimation might penalized diﬀerently depending type error , e.g . trying distinguish diamonds rocks asymmetric loss applies . Multiclass Classiﬁcation logical extension binary classiﬁca- tion . main diﬀerence ∈ {   , . . . , n } may assume range diﬀerent values . instance , might want classify document ac- cording language written ( English , French , German , Spanish , Hindi , Japanese , Chinese , . . . ) . See Figure  .  example . main dif- ference cost error may heavily depend type  .  Taste Machine Learning    Fig .  .  . Regression estimation . given number instances ( indicated black dots ) would like ﬁnd function f mapping observations X R f ( x ) close observed values . error make . instance , problem assessing risk cancer , makes signiﬁcant diﬀerence whether mis-classify early stage can- cer healthy ( case patient likely die ) advanced stage cancer ( case patient likely inconvenienced overly aggressive treatment ) . Structured Estimation goes beyond simple multiclass estimation assuming labels additional structure used estimation process . instance , might path ontology , attempting classify webpages , might permutation , attempting match objects , perform collaborative ﬁltering , rank documents retrieval setting . Equally well , might annotation text , performing named entity recognition . problems properties terms set might consider admissible , search space . discuss number problems Chapter ? ? . Regression another prototypical application . goal esti- mate real-valued variable ∈ R given pattern x ( see e.g . Figure  .  ) . instance , might want estimate value stock next day , yield semiconductor fab given current process , iron content ore given mass spectroscopy measurements , heart rate athlete , given accelerometer data . One key issues regression problems diﬀer choice loss . instance , estimating stock values loss put option decidedly one-sided . hand , hobby athlete might care estimate heart rate matches actual average . Novelty Detection rather ill-deﬁned problem . describes issue determining “ unusual ” observations given set past measurements . Clearly , choice considered unusual subjective . commonly accepted notion unusual events occur rarely . Hence possible goal design system assigns observation rating      Introduction Fig .  .  . Left : typical digits contained database US Postal Service . Right : unusual digits found novelty detection algorithm [ SPST+   ] ( description algorithm see Section  .  ) . score digits indicates degree novelty . numbers lower right indicate class associated digit . novel . Readers familiar density estimation might contend latter would reasonable solution . However , neither need score sums   entire domain , care particularly much novelty scores typical observations . later see somewhat easier goal achieved directly . Figure  .  example novelty detection applied optical character recognition database .  .  Probability Theory order deal instances machine learning used , need develop adequate language able describe problems concisely . begin fairly informal overview probability theory . details gentle detailed discussion see excellent book [ BT   ] .  . .  Random Variables Assume cast dice would like know chances whether would see   rather another digit . dice fair six outcomes X = {   , . . . ,   } equally likely occur , hence would see   roughly     cases . Probability theory allows us model uncertainty out- come experiments . Formally state   occurs probability     . many experiments , roll dice , outcomes numerical nature handle easily . cases , outcomes may numerical , e.g. , toss coin observe heads tails . cases , useful associate numerical values outcomes . done via random variable . instance , let random variable  .  Probability Theory    X take value +  whenever coin lands heads value −  otherwise . notational convention use uppercase letters , e.g. , X , etc denote random variables lower case letters , e.g. , x , etc denote values take . Fig .  .  . random variable ξ maps set outcomes experiment ( denoted X ) real numbers . illustration X consists patients physician might encounter , mapped via ξ weight height .  . .  Distributions Perhaps important way characterize random variable associate probabilities values take . random variable discrete , i.e. , takes ﬁnite number values , assignment probabilities called probability mass function PMF short . PMF must , deﬁnition , non-negative must sum one . instance , coin fair , i.e. , heads tails equally likely , random variable X described takes values +  −  probability  .  . written P r ( X = +  ) =  .  P r ( X = −  ) =  .  . (  .  ) danger confusion use slightly informal no- tation p ( x ) : = P r ( X = x ) . case continuous random variable assignment probabilities results probability density function PDF short . abuse terminology , keeping line convention , often use density distribution instead probability density function . case PMF , PDF must also non-negative integrate one . Figure  .   shows two distributions : uniform distribution p ( x ) = ( cid:   )   b−a   x ∈ [ , b ] otherwise , (  .  ) Xweightheightξ ( x ) x      Introduction Fig .  .   . Two common densities . Left : uniform distribution interval [ −  ,   ] . Right : Normal distribution zero mean unit variance . Gaussian distribution ( also called normal distribution ) p ( x ) = √    πσ  ( cid:   ) exp − ( x − µ )    σ  ( cid:   ) . (  .  ) Closely associated PDF indeﬁnite integral p. com- monly referred cumulative distribution function ( CDF ) . Deﬁnition  .  ( Cumulative Distribution Function ) real valued random variable X PDF p associated Cumulative Distribution Func- tion F given F ( x ( cid:   ) ) : = Pr ( cid:  ) X ≤ x ( cid:   ) ( cid:  ) = ( cid:   ) x ( cid:   ) −∞ dp ( x ) . (  .  ) CDF F ( x ( cid:   ) ) allows us perform range queries p eﬃciently . instance , integral calculus obtain Pr ( ≤ X ≤ b ) = ( cid:   ) b  dp ( x ) = F ( b ) − F ( ) . (  .  ) values x ( cid:   ) F ( x ( cid:   ) ) assumes speciﬁc value ,  .   .  special name . called quantiles distribution p . Deﬁnition  .  ( Quantiles ) Let q ∈ (   ,   ) . value x ( cid:   ) Pr ( X < x ( cid:   ) ) ≤ q Pr ( X > x ( cid:   ) ) ≤   − q q-quantile distribution p. Moreover , value x ( cid:   ) associated q =  .  called median . - -     .  .  .  .  .  . - -     .  .  .  .  .  .   .  Probability Theory    Fig .  .   . Quantiles distribution correspond area integral density p ( x ) integral takes pre-speciﬁed value . Illustrated  .  ,  .   .  quantiles respectively .  . .  Mean Variance common question ask random variable expected value might . instance , measuring voltage device , might ask typical values might . deciding whether ad- minister growth hormone child doctor might ask sensible range height . purposes need deﬁne expectations related quantities distributions . Deﬁnition  .  ( Mean ) deﬁne mean random variable X ( cid:   ) E [ X ] : = xdp ( x ) (  .  ) generally , f : R → R function , f ( X ) also random variable . mean mean given ( cid:   ) E [ f ( X ) ] : = f ( x ) dp ( x ) . (  .  ) Whenever X discrete random variable integral (  .  ) re- placed summation : E [ X ] = xp ( x ) . ( cid:   ) x (  .  ) instance , case dice equal probabilities  /    possible outcomes . easy see translates mean (   +   +   +   +   +   ) /  =  .  . mean random variable useful assessing expected losses beneﬁts . instance , stock broker might interested ex- pected value investment year ’ time . addition , however , also might want investigate risk investment . , likely value investment might deviate expecta- tion since might relevant decisions . means p ( x )      Introduction need variable quantify risk inherent random variable . One measure variance random variable . Deﬁnition  .  ( Variance ) deﬁne variance random variable X Var [ X ] : = E ( cid:    ) ( X − E [ X ] )   ( cid:    ) . (  .  ) , f : R → R function , variance f ( X ) given Var [ f ( X ) ] : = E ( cid:    ) ( f ( X ) − E [ f ( X ) ] )   ( cid:    ) . (  .   ) variance measures much average f ( X ) deviates ex- pected value . shall see Section  .  , upper bound variance used give guarantees probability f ( X ) within ( cid:   ) expected value . one reasons variance often associated risk random variable . Note often one discusses properties random variable terms standard deviation , deﬁned square root variance .  . .  Marginalization , Independence , Conditioning , Bayes Rule Given two random variables X , one write joint density p ( x , ) . Given joint density , one recover p ( x ) integrating . operation called marginalization : p ( x ) = ( cid:   )  dp ( x , ) . (  .   ) discrete random variable , replace integration summation : p ( x ) = ( cid:   )  p ( x , ) . (  .   ) say X independent , i.e. , values X takes depend values takes whenever p ( x , ) = p ( x ) p ( ) . (  .   ) Independence useful comes dealing large numbers ran- dom variables whose behavior want estimate jointly . instance , whenever perform repeated measurements quantity ,  .  Probability Theory    Fig .  .   . Left : sample two dependent random variables . Knowing ﬁrst coordinate allows us improve guess second coordinate . Right : sample drawn two independent random variables , obtained randomly permuting dependent sample . measuring voltage device , typically assume individ- ual measurements drawn distribution independent . , measured voltage number times aﬀect value next measurement . call random variables independently identically distributed , short , iid random variables . See Figure  .   example pair random variables drawn dependent independent distributions respectively . Conversely , dependence vital classiﬁcation regression prob- lems . instance , traﬃc lights intersection dependent . allows driver perform inference lights green direction traﬃc crossing path , i.e . lights indeed red . Likewise , whenever given picture x digit , hope dependence x label . Especially case dependent random variables , interested conditional probabilities , i.e. , probability X takes particular value given value . Clearly P r ( X = rain|Y = cloudy ) higher P r ( X = rain|Y = sunny ) . words , knowledge value signiﬁcantly inﬂuences distribution X . captured via conditional probabilities : p ( x|y ) : = p ( x , ) p ( ) . (  .   ) Equation  .   leads one key tools statistical inference . Theorem  .  ( Bayes Rule ) Denote X random variables - .  .  .  .  .  . - .  .  .  .  .  . - .  .  .  .  .  . - .  .  .  .  .  .     following holds p ( y|x ) = p ( x|y ) p ( ) p ( x ) .   Introduction (  .   ) follows fact p ( x , ) = p ( x|y ) p ( ) = p ( y|x ) p ( x ) . key consequence (  .   ) may reverse conditioning pair random variables .  . . .  Example illustrate reasoning means simple example — inference using AIDS test . Assume patient would like test carried . physician recommends test guaranteed detect HIV-positive whenever patient infected . hand , healthy patients   % error rate . , probability  .   diagnoses patient HIV-positive even , fact , HIV-negative . Moreover , assume  .   % population infected . assume patient test carried test re- turns ’ HIV-negative ’ . case , logic implies healthy , since test     % detection rate . converse case things quite straightforward . Denote X random variables associated health status patient outcome test respectively . interested p ( X = HIV+|T = HIV+ ) . Bayes rule may write p ( X = HIV+|T = HIV+ ) = p ( = HIV+|X = HIV+ ) p ( X = HIV+ ) p ( = HIV+ ) know terms numerator , p ( = HIV+ ) unknown . said , computed via p ( = HIV+ ) = ( cid:   ) p ( = HIV+ , x ) x∈ { HIV+ , HIV- } ( cid:   ) = x∈ { HIV+ , HIV- } p ( = HIV+|x ) p ( x ) =  .  ·  .     +  .   ·  .     . Substituting back conditional expression yields p ( X = HIV+|T = HIV+ ) =  .  ·  .      .  ·  .     +  .   ·  .     =  .     . words , even though test quite reliable , low prior probability infected AIDS much evidence accept hypothesis even test .  .  Probability Theory    Fig .  .   . graphical description HIV testing scenario . Knowing age patient inﬂuences prior whether patient HIV positive ( random variable X ) . outcomes tests     independent given status X . observe shaded random variables ( age , test   , test   ) would like infer un-shaded random variable X . special case graphical model discuss Chapter ? ? . Let us think could improve diagnosis . One way ob- tain information patient use diagnosis . instance , information age quite useful . Suppose patient    years old . case would want compute p ( X = HIV+|T = HIV+ , =    ) random variable denotes age . corre- sponding expression yields : p ( = HIV+|X = HIV+ , ) p ( X = HIV+|A ) p ( = HIV+|A ) simply conditioned random variables order take addi- tional information account . may assume test independent age patient , i.e . p ( t|x , ) = p ( t|x ) . remains therefore p ( X = HIV+|A ) . Recent US census data pegs number approximately  .  % . Plugging data back conditional expression yields  · .   + .  · .    =  .   . happened including additional observed random variables estimate become reliable . Combination evidence powerful tool . case helped us make classiﬁcation problem whether patient HIV- positive reliable .  · .    second tool arsenal use multiple measurements . ﬁrst test physician likely carry second test conﬁrm diagnosis . denote T  T  ( t  , t  respectively ) two tests . Obviously , want T  give us “ independent ” second opinion situation . words , want ensure T  make mistakes T  . instance , probably bad idea repeat T  without changes , since might perform diagnostic agextest  test        Introduction mistake . want diagnosis T  independent T  given health status X patient . expressed p ( t  , t |x ) = p ( t |x ) p ( t |x ) . (  .   ) See Figure  .   graphical illustration setting . Random variables satisfying condition (  .   ) commonly referred conditionally independent . shorthand write T  , T  ⊥⊥ X . sake argument assume statistics T  given p ( t |x ) x = HIV- x = HIV+  .   t  = HIV- t  = HIV+  .    .    .   Clearly test less reliable ﬁrst one . However , may combine estimates obtain reliable estimate based combination events . instance , t  = t  = HIV+ p ( X = HIV+|T  = HIV+ , T  = HIV+ ) =  .  ·  .   ·  .     .  ·  .   ·  .    +  .   ·  .   ·  .    =  .   . words , combining two tests conﬁrm high conﬁdence patient indeed diseased . carried combination evidence . Strong experimental evidence two positive tests eﬀectively overcame initially strong prior suggested patient might healthy . Tests example discussed fairly common . instance , might need decide manufacturing procedure prefer- able , choice parameters give better results regression es- timator , whether administer certain drug . Note often tests may conditionally independent would need take account .  .  Basic Algorithms conclude introduction machine learning discussing four simple algorithms , namely Naive Bayes , Nearest Neighbors , Mean Classiﬁer , Perceptron , used solve binary classiﬁcation prob- lem described Figure  .  . also introduce K-means algorithm employed labeled data available . algorithms readily usable easily implemented scratch basic form . sake concreteness assume interested spam ﬁlter- ing . , given set e-mails xi , denoted X : = { x  , . . . , xm }  .  Basic Algorithms    : `` LucindaParkison       '' < LucindaParkison       @ hotmail.com > : < kargr @ earthlink.net > Subject : think ACGU next winner Date : Mon ,    Feb        :  :   -     MIME-Version :  .  X-OriginalArrivalTime :    Feb        :  :  .     ( UTC ) FILETIME= [  A      :  C    B ] Return-Path : lucindaparkison       @ hotmail.com ( ACGU ) .       .  % think ( ACGU ) ’ current levels looks extremely attractive . Asset Capital Group , Inc. , ( ACGU ) announced expanding marketing bio-remediation fluids cleaning equipment . recent acquisition interest American Bio-Clean Corporation    News expected released next week growing company could drive price even higher . Buy ( ACGU ) Monday open . believe involved stage could enjoy nice ride . Fig .  .   . Example spam e-mail x  : quick brown fox jumped lazy dog . x  : dog hunts fox .  quick brown fox jumped  lazy dog hunts x  x                                           Fig .  .   . Vector space representation strings . associated labels yi , denoted : = { y  , . . . , ym } . labels sat- isfy yi ∈ { spam , ham } . key assumption make pairs ( xi , yi ) drawn jointly distribution p ( x , ) represents e-mail generating process user . Moreover , assume suﬃciently strong dependence x able estimate given x set labeled instances X , . need address fact e-mails Figure  .   text , whereas three algorithms present require data represented vectorial fashion . One way converting text vector using so-called bag words representation [ Mar   , Lew   ] . simplest version works follows : Assume list possible words occurring X , dictionary , able assign unique number words ( e.g . position dictionary ) . may simply count document xi number times given word j occurring . used value j-th coordinate xi . Figure  .   gives example representation . latter easy compute distances , similarities , statistics directly vectorial representation .     . .  Naive Bayes   Introduction example AIDS test used outcomes test infer whether patient diseased . context spam ﬁltering actual text e-mail x corresponds test label equivalent diagnosis . Recall Bayes Rule (  .   ) . could use latter infer p ( y|x ) = p ( x|y ) p ( ) p ( x ) . may good estimate p ( ) , , probability receiving spam ham mail . Denote mham mspam number ham spam e-mails X . case estimate p ( ham ) ≈ mham  p ( spam ) ≈ mspam  . key problem , however , know p ( x|y ) p ( x ) . may dispose requirement knowing p ( x ) settling likelihood ratio L ( x ) : = p ( spam|x ) p ( ham|x ) = p ( x|spam ) p ( spam ) p ( x|ham ) p ( ham ) . (  .   ) Whenever L ( x ) exceeds given threshold c decide x spam consequently reject e-mail . c large algorithm conservative classiﬁes email spam p ( spam|x ) ( cid:   ) p ( ham|x ) . hand , c small algorithm aggressively classiﬁes emails spam . key obstacle access p ( x|y ) . make key approximation . Recall Figure  .   . order model distribution test outcomes T  T  made assumption conditionally independent given diagnosis . Analogously , may treat occurrence word document separate test combine outcomes naive fashion assuming p ( x|y ) = # words x ( cid:   ) p ( wj|y ) , j=  (  .   ) wj denotes j-th word document x . amounts as- sumption probability occurrence word document independent words given category document . Even though assumption hold general – instance , word “ York ” much likely word “ New ” – suﬃces purposes ( see Figure  .   ) . assumption reduces diﬃculty knowing p ( x|y ) esti- mating probabilities occurrence individual words w. Estimates  .  Basic Algorithms    Fig .  .   . Naive Bayes model . occurrence individual words independent , given category text . instance , word Viagra fairly frequent = spam considerably less frequent = ham , except considering mailbox Pﬁzer sales representative . p ( w|y ) obtained , instance , simply counting frequency oc- currence word within documents given class . , estimate ( cid:   ) i=  p ( w|spam ) ≈ ( cid:   ) # words xi ( cid:    ) yi = spam wj = w ( cid:    ) ( cid:   ) # words xi j=  { yi = spam } j=  ( cid:   ) i=  ( cid:    ) ( cid:    ) yi = spam wj = w  equals   xi labeled spam w occurs j-th word xi . denominator simply total number words spam documents . Similarly one compute p ( w|ham ) . principle could perform summation whenever see new document x . would terribly ineﬃcient , since computation requires full pass X . Instead , perform single pass X store resulting statistics good estimate conditional probabilities . Algorithm  .  details implementation . Note performed number optimizations : Firstly , normaliza- tion m−  ham respectively independent x , hence incor- porate ﬁxed oﬀset . Secondly , since computing product large number factors numbers might lead numerical overﬂow underﬂow . addressed summing logarithm terms rather computing products . Thirdly , need address issue estimating p ( w|y ) words w might seen . One way dealing increment counts   . method commonly referred Laplace smoothing . encounter theoretical justiﬁcation heuristic Section  .  . spam m−  simple algorithm known perform surprisingly well , variants found modern spam ﬁlters . amounts commonly known “ Bayesian spam ﬁltering ” . Obviously , may apply problems document categorization , . yword  word   ... word nword        Introduction Algorithm  .  Naive Bayes Train ( X , ) { reads documents X labels } Compute dictionary X n words . Compute , mham mspam . Initialize b : = log c + log mham − log mspam oﬀset rejection threshold Initialize p ∈ R ×n pij =   , wspam = n , wham = n . { Count occurrence word } { xj =   denotes number times word j occurs document xi } yi = spam j =   n p  , j ← p  , j + xj  wspam ← wspam + xj  end else j =   n p  , j ← p  , j + xj  wham ← wham + xj  end end end { Normalize counts yield word probabilities } j =   n p  , j ← p  , j/wspam p  , j ← p  , j/wham end Classify ( x ) { classiﬁes document x } Initialize score threshold = −b j =   n ← + xj ( log p  , j − log p  , j ) end >   return spam else return ham  . .  Nearest Neighbor Estimators even simpler estimator Naive Bayes nearest neighbors . basic form assigns label nearest neighbor observation x ( see Figure  .   ) . Hence , need implement distance measure ( x , x ( cid:   ) ) pairs observations . Note distance need even symmetric . means nearest neighbor classiﬁers extremely  .  Basic Algorithms    Fig .  .   .   nearest neighbor classiﬁer . Depending whether query point x closest star , diamond triangles , uses one three labels . Fig .  .  . k-Nearest neighbor classiﬁers using Euclidean distances . Left : decision boundaries obtained  -nearest neighbor classiﬁer . Middle : color-coded sets number red / blue points ranges     . Right : decision boundary determining blue red dots majority . ﬂexible . instance , could use string edit distances compare two documents information theory based measures . However , problem nearest neighbor classiﬁcation esti- mates noisy whenever data noisy . instance , spam email erroneously labeled nonspam emails similar email share fate . See Figure  .   example . case beneﬁcial pool together number neighbors , say k-nearest neighbors x use majority vote decide class membership x. Algorithm  .  description algorithm . Note nearest neighbor algorithms yield excellent performance used good distance measure . instance , technology underlying Netﬂix progress prize [ BK   ] essentially nearest neighbours based . Note trivial extend algorithm regression . need change Algorithm  .  return average values yi instead majority vote . Figure  .   example . Note distance computation ( xi , x ) observations be-      Introduction Algorithm  .  k-Nearest Neighbor Classiﬁcation Classify ( X , , x ) { reads documents X , labels query x } =   Compute distance ( xi , x ) end Compute set containing indices k smallest distances ( xi , x ) . return majority label { yi ∈ } . Fig .  .  . k-Nearest neighbor regression estimator using Euclidean distances . Left : points ( x , ) drawn joint distribution . Middle :  -nearest neighbour classiﬁer . Right :  -nearest neighbour classiﬁer . Note regression estimate much smooth . come extremely costly , particular whenever number observations large whenever observations xi live high dimensional space . Random projections technique alleviate high computa- tional cost Nearest Neighbor classiﬁers . celebrated lemma Johnson Lindenstrauss [ DG   ] asserts set points high dimensional Euclidean space projected ( log m/ ( cid:   )   ) dimensional Euclidean space distance two points changes fac- tor (   ± ( cid:   ) ) . Since Euclidean distances preserved , running Nearest Neighbor classiﬁer mapped data yields results lower computational cost [ GIM   ] . surprising fact projection relies simple randomized algorithm : obtain d-dimensional representation n-dimensional ran- dom observations pick matrix R ∈ Rd×n element drawn independently normal distribution n−     variance zero mean . Multiplying x projection matrix shown achieve prop- erty high probability . details see [ DG   ] .  .  Basic Algorithms    Fig .  .   . trivial classiﬁer . Classiﬁcation carried accordance two means µ− µ+ closer test point x . Note sets positive negative labels respectively form half space .  . .  Simple Classiﬁer use geometry design another simple classiﬁcation algorithm [ SS   ] problem . simplicity assume observations x ∈ Rd , bag-of-words representation e-mails . deﬁne means µ+ µ− correspond classes ∈ { ±  } via µ− : =   m− ( cid:   ) yi=−  xi µ+ : =   m+ ( cid:   ) yi=  xi . used m− m+ denote number observations label yi = −  yi = +  respectively . even simpler approach using nearest neighbor classiﬁer would use class label corresponds mean closest new query x , described Figure  .   . Euclidean distances ( cid:    ) µ− − x ( cid:    )   = ( cid:    ) µ− ( cid:    )   + ( cid:    ) x ( cid:    )   −   ( cid:    ) µ− , x ( cid:    ) ( cid:    ) µ+ − x ( cid:    )   = ( cid:    ) µ+ ( cid:    )   + ( cid:    ) x ( cid:    )   −   ( cid:    ) µ+ , x ( cid:    ) . (  .   ) (  .   ) ( cid:    ) · , · ( cid:    ) denotes standard dot product vectors . Taking diﬀer- ences two distances yields f ( x ) : = ( cid:    ) µ+ − x ( cid:    )   − ( cid:    ) µ− − x ( cid:    )   =   ( cid:    ) µ− − µ+ , x ( cid:    ) + ( cid:    ) µ− ( cid:    )   − ( cid:    ) µ+ ( cid:    )   . (  .   ) linear function x sign corresponds labels esti- mate x . algorithm sports important property : classiﬁcation rule expressed via dot products . follows ( cid:    ) µ+ ( cid:    )   = ( cid:    ) µ+ , µ+ ( cid:    ) = m−  + ( cid:   ) yi=yj =  ( cid:    ) xi , xj ( cid:    ) ( cid:    ) µ+ , x ( cid:    ) = m−  + ( cid:   ) yi=  ( cid:    ) xi , x ( cid:    ) . wμ-μ+x      Introduction Fig .  .   . feature map φ maps observations x X feature space H . map φ convenient way encoding pre-processing steps systematically . Analogous expressions computed µ− . Consequently may ex- press classiﬁcation rule (  .   ) f ( x ) =  ( cid:   ) i=  αi ( cid:    ) xi , x ( cid:    ) + b (  .   ) ( cid:   ) ( cid:   ) b = m−  − yi=yj =−  ( cid:    ) xi , xj ( cid:    ) − m−  yi=yj =  ( cid:    ) xi , xj ( cid:    ) αi = yi/myi . + oﬀers number interesting extensions . Recall dealing documents needed perform pre-processing map e-mails vector space . general , may pick arbitrary maps φ : X → H mapping space observations feature space H , long latter endowed dot product ( see Figure  .   ) . means instead dealing ( cid:    ) x , x ( cid:   ) ( cid:    ) dealing ( cid:    ) φ ( x ) , φ ( x ( cid:   ) ) ( cid:    ) . see Chapter   , whenever H so-called Reproducing Kernel Hilbert Space , inner product abbreviated form kernel function k ( x , x ( cid:   ) ) satisﬁes k ( x , x ( cid:   ) ) : = ( cid:   ) φ ( x ) , φ ( x ( cid:   ) ) ( cid:   ) . (  .   ) small modiﬁcation leads number powerful algorithm foundation area research called kernel methods . encounter number algorithms regression , classiﬁcation , segmentation , density estimation course book . Examples suitable k polynomial kernel k ( x , x ( cid:   ) ) = ( cid:    ) x , x ( cid:   ) ( cid:    ) ∈ N Gaussian RBF kernel k ( x , x ( cid:   ) ) = e−γ ( cid:    ) x−x ( cid:   ) ( cid:    )   γ >   . upshot (  .   ) basic algorithm kernelized . , may rewrite (  .   ) f ( x ) =  ( cid:   ) i=  αik ( xi , x ) + b (  .   ) αi = yi/myi oﬀset b computed analogously . Xφ ( x ) xH  .  Basic Algorithms    Algorithm  .  Perceptron Perceptron ( X , ) { reads stream observations ( xi , yi ) } Initialize w =   b =   exists ( xi , yi ) yi ( ( cid:    ) w , xi ( cid:    ) + b ) ≤   w ← w + yixi b ← b + yi end Algorithm  .  Kernel Perceptron KernelPerceptron ( X , ) { reads stream observations ( xi , yi ) } Initialize f =   exists ( xi , yi ) yif ( xi ) ≤   f ← f + yik ( xi , · ) + yi end consequence moved fairly simple pedestrian lin- ear classiﬁer one yields nonlinear function f ( x ) rather nontrivial decision boundary .  . .  Perceptron previous sections assumed classiﬁer access train- ing set spam non-spam emails . real life , set might diﬃcult obtain . Instead , user might want instant results when- ever new e-mail arrives would like system learn immediately corrections mistakes system makes . overcome diﬃculties one could envisage working following protocol : emails arrive algorithm classiﬁes spam non-spam , user provides feedback whether classiﬁcation correct incorrect . feedback used improve performance classiﬁer period time . intuition formalized follows : classiﬁer maintains parameter vector . t-th time instance receives data point xt , assigns label ˆyt using current parameter vector . true label yt revealed , used update parameter vector classiﬁer . algorithms said online . describe perhaps simplest classiﬁer kind namely Perceptron [ Heb   , Ros   ] . Let us assume data points xt ∈ Rd , labels yt ∈ { ±  } . represent email bag-of-words vector assign +  spam emails −  non-spam emails . Perceptron maintains weight      Introduction Fig .  .   . Perceptron without bias . Left : time weight vector wt denoted dashed arrow corresponding separating plane ( also dashed ) . reference include linear separator w∗ separating plane ( denoted solid line ) . new observation xt arrives happens mis-classiﬁed current weight vector wt perform update . Also note margin point xt separating hyperplane deﬁned w∗ . Right : leads weight vector wt+  aligned w∗ . vector w ∈ Rd classiﬁes xt according rule ˆyt : = sign { ( cid:    ) w , xt ( cid:    ) + b } , (  .   ) ( cid:    ) w , xt ( cid:    ) denotes usual Euclidean dot product b oﬀset . Note similarity (  .   ) (  .   ) simple classiﬁer . latter , Perceptron linear classiﬁer separates domain Rd two halfspaces , namely { x| ( cid:    ) w , x ( cid:    ) + b >   } complement . ˆyt = yt updates made . hand , ˆyt ( cid:   ) = yt weight vector updated w ← w + ytxt b ← b + yt . (  .   ) Figure  .   shows update step Perceptron algorithm . simplicity illustrate case without bias , , b =   remains unchanged . detailed description algorithm given Algorithm  .  . important property algorithm performs updates w multiples observations xi makes mistake . Hence may express w w = ( cid:   ) i∈Error yixi . , replace xi x φ ( xi ) φ ( x ) obtain kernelized version Perceptron algorithm [ FS   ] ( Algorithm  .  ) . dataset ( X , ) linearly separable , Perceptron algorithm w * wtw * wt+ xtxt  .  Basic Algorithms    eventually converges correctly classiﬁes points X . rate convergence however depends margin . Roughly speaking , margin quantiﬁes linearly separable dataset , hence easy solve given classiﬁcation problem . Deﬁnition  .  ( Margin ) Let w ∈ Rd weight vector let b ∈ R oﬀset . margin observation x ∈ Rd associated label γ ( x , ) : = ( ( cid:    ) w , x ( cid:    ) + b ) . (  .   ) Moreover , margin entire set observations X labels γ ( X , ) : = min  γ ( xi , yi ) . (  .   ) Geometrically speaking ( see Figure  .   ) margin measures distance x hyperplane deﬁned { x| ( cid:    ) w , x ( cid:    ) + b =   } . Larger margin , well separated data hence easier ﬁnd hyperplane correctly classiﬁes dataset . following theorem asserts exists linear classiﬁer classify dataset large mar- gin , Perceptron also correctly classify dataset making small number mistakes . Theorem  .  ( Novikoﬀ ’ theorem ) Let ( X , ) dataset least one example labeled +  one example labeled −  . Let R : = maxt ( cid:    ) xt ( cid:    ) , assume exists ( w∗ , b∗ ) ( cid:    ) w∗ ( cid:    ) =   γt : = yt ( ( cid:    ) w∗ , xt ( cid:    ) + b∗ ) ≥ γ t. , Perceptron make (  +R  ) (  + ( b∗ )   ) mistakes . γ  result remarkable since depend dimensionality problem . Instead , depends geometry setting , quantiﬁed via margin γ radius R ball enclosing observations . Interestingly , similar bound shown Support Vector Machines [ Vap   ] discussing Chapter   . Proof safely ignore iterations mistakes made hence updates carried . Therefore , without loss generality assume t-th update made seeing t-th observation let wt denote weight vector update . Furthermore , simplicity assume algorithm started w  =   b  =   . update equation (  .   ) ( cid:    ) wt , w∗ ( cid:    ) + btb∗ = ( cid:    ) wt−  , w∗ ( cid:    ) + bt− b∗ + yt ( ( cid:    ) xt , w∗ ( cid:    ) + b∗ ) ≥ ( cid:    ) wt−  , w∗ ( cid:    ) + bt− b∗ + γ .      Introduction induction follows ( cid:    ) wt , w∗ ( cid:    ) +btb∗ ≥ tγ . hand made update yt ( ( cid:    ) xt , wt−  ( cid:    ) + bt−  ) <   . using ytyt =   , ( cid:    ) wt ( cid:    )   + b  ( cid:    ) xt ( cid:    )   +   +  yt ( ( cid:    ) wt−  , xt ( cid:    ) + bt−  ) = ( cid:    ) wt−  ( cid:    )   + b  ≤ ( cid:    ) wt−  ( cid:    )   + b  t−  + y  t−  + ( cid:    ) xt ( cid:    )   +   Since ( cid:    ) xt ( cid:    )   = R  apply induction conclude ( cid:    ) wt ( cid:    )  +b  ≤ ( cid:  ) R  +   ( cid:  ) . Combining upper lower bounds , using Cauchy- Schwartz inequality , ( cid:    ) w∗ ( cid:    ) =   yields ( cid:   ) ( cid:   ) wt bt ( cid:    ) tγ ≤ ( cid:    ) wt , w∗ ( cid:    ) + btb∗ = ( cid:   ) w∗ b∗ ( cid:   ) ( cid:   ) ( cid:   ) , ( cid:    ) wt ( cid:    )   + b   ( cid:    )   + ( b∗ )   ( cid:   ) ( cid:   ) wt ( cid:   ) ( cid:   ) bt ( cid:   ) ( cid:    ) ≤ ≤ ( cid:   ) ( cid:   ) ( cid:   ) ( cid:   ) ( cid:   ) ( cid:   ) ( cid:   ) w∗ ( cid:   ) ( cid:   ) b∗ ( cid:   ) ( cid:   ) ( cid:   ) ( cid:   ) ( cid:   ) ( cid:   ) = ( R  +   ) ( cid:    )   + ( b∗ )   . Squaring sides inequality rearranging terms yields upper bound number updates hence number mistakes . Perceptron building block research Neural Networks [ Hay   , Bis   ] . key insight combine large numbers net- works , often cascading fashion , larger objects fashion opti- mization algorithms would lead classiﬁers desirable properties . book take complementary route . Instead increasing number nodes investigate happens increasing com- plexity feature map φ associated kernel k. advantage reap beneﬁts convex analysis linear models , possibly expense slightly costly function evaluation .  . .  K-Means algorithms discussed far supervised , , assume labeled training data available . many applications much hope ; labeling may expensive , error prone , sometimes impossi- ble . instance , easy crawl collect every page within www.purdue.edu domain , rather time consuming assign topic page based contents . cases , one resort unsuper- vised learning . prototypical unsupervised learning algorithm K-means , clustering algorithm . Given X = { x  , . . . , xm } goal K-means partition k clusters point cluster similar points cluster points cluster .  .  Basic Algorithms    Towards end , deﬁne prototype vectors µ  , . . . , µk indicator vector rij   , , xi assigned cluster j . cluster dataset minimize following distortion measure , minimizes distance point prototype vector : J ( r , µ ) : =      ( cid:   ) k ( cid:   ) i=  j=  rij ( cid:    ) xi − µj ( cid:    )   , (  .   ) r = { rij } , µ = { µj } , ( cid:    ) · ( cid:    )   denotes usual Euclidean square norm . goal ﬁnd r µ , since easy jointly minimize J respect r µ , adapt two stage strategy : Stage   Keep µ ﬁxed determine r. case , easy see minimization decomposes independent problems . solution i-th data point xi found setting : rij =   j = argmin j ( cid:   ) ( cid:    ) xi − µj ( cid:   ) ( cid:    )   , (  .   )   otherwise . Stage   Keep r ﬁxed determine µ . Since r ’ ﬁxed , J quadratic function µ . minimized setting derivative respect µj   :  ( cid:   ) i=  Rearranging obtains rij ( xi − µj ) =   j . (  .   ) µj = ( cid:   ) rijxi ( cid:   ) rij Since ( cid:   ) rij counts number points assigned cluster j , essentially setting µj sample mean points assigned cluster j . (  .   ) . algorithm stops cluster assignments change signiﬁ- cantly . Detailed pseudo-code found Algorithm  .  . Two issues K-Means worth noting . First , sensitive choice initial cluster centers µ . number practical heuristics developed . instance , one could randomly choose k points given dataset cluster centers . methods try pick k points X farthest away . Second , makes hard assignment every point cluster center . Variants encounter later      Introduction Algorithm  .  K-Means Cluster ( X ) { Cluster dataset X } Initialize cluster centers µj j =   , . . . , k randomly repeat =   Compute j ( cid:   ) = argminj=  , ... , k ( xi , µj ) Set rij ( cid:   ) =   rij =   j ( cid:   ) ( cid:   ) = j end j =   k Compute µj = end ( cid:   ) rij xi ( cid:   ) rij Cluster assignments rij unchanged return { µ  , . . . , µk } rij book relax . Instead letting rij ∈ {   ,   } soft variants replace probability given xi belongs cluster j . K-Means algorithm concludes discussion set basic machine learning methods classiﬁcation regression . provide useful starting point aspiring machine learning researcher . book see many algorithms well connections basic algorithms advanced counterparts . Problems Problem  .  ( Eyewitness ) Assume eyewitness    % certain given person committed crime bar . Moreover , assume    people restaurant time crime . posterior probability person actually committed crime . Problem  .  ( DNA Test ) Assume police DNA library    million records . Moreover , assume false recognition probability  .      % per record . Suppose match found database search individual . chances identiﬁcation correct ? assume total population     million people . Hint : compute probability match occurring ﬁrst . Problem  .  ( Bomb Threat ) Suppose probability one thousand passengers plane bomb   :   ,     ,     . Assuming probability bomb evenly distributed among passengers ,  .  Basic Algorithms    probability two passengers bomb roughly equal   −   . Therefore , one might decide take bomb plane decrease chances somebody else bomb . wrong argument ? Problem  .  ( Monty-Hall Problem ) Assume TV show candidate given choice three doors . Behind two doors pencil behind one grand prize , car . candi- date chooses one door . , showmaster opens another door behind pencil . candidate switch doors ? probability winning car ? Problem  .  ( Mean Variance Random Variables ) Denote Xi random variables . Prove case EX  , ... XN ( cid:   ) ( cid:   ) ( cid:   ) xi =  ( cid:   )  EXi [ xi ] VarX  , ... XN ( cid:   ) ( cid:   ) ( cid:   ) xi =  ( cid:   )  VarXi [ xi ] show second equality assume independence Xi . Problem  .  ( Two Dices ) Assume game uses max- imum two dices . Compute probability seeing events {   , . . . ,   } . Hint : prove ﬁrst cumulative distribution function maximum pair random variables square original cumu- lative distribution function . Problem  .  ( Matching Coins ) Consider following game : two play- ers bring coin . ﬁrst player bets tossing coins match second one bets match . Show even one players bring tainted coin , game still would fair . Show interest player bring fair coin game . Hint : assume second player knows ﬁrst coin favors heads tails . Problem  .  ( Randomized Maximization ) many observations need draw distribution ensure maximum larger    % observations least    % probability ? Hint : generalize result Problem  .  maximum n random vari- ables . Application : Assume      computers performing MapReduce [ DG   ] Reducers wait      Mappers ﬁnished job . Compute quantile typical time completion .      Introduction Problem  .  Prove Normal distribution (  .  ) mean µ variance σ  . Hint : exploit fact p symmetric around µ . Problem  .   ( Cauchy Distribution ) Prove density p ( x ) =   π (   + x  ) (  .   ) mean variance undeﬁned . Hint : show integral diverges . Problem  .   ( Quantiles ) Find distribution mean ex- ceeds median . Hint : mean depends value high-quantile terms , whereas median . Problem  .   ( Multicategory Naive Bayes ) Prove multicate- gory Naive Bayes optimal decision given y∗ ( x ) : = argmax p ( )  n ( cid:   ) i=  p ( [ x ] i|y ) (  .   ) ∈ class label observation x . Problem  .   ( Bayes Optimal Decisions ) Denote y∗ ( x ) = argmaxy p ( y|x ) label associated largest conditional class probability . Prove y∗ ( x ) probability choosing wrong label given l ( x ) : =   − p ( y∗ ( x ) |x ) . Moreover , show y∗ ( x ) label incurring smallest misclassiﬁcation error . Problem  .   ( Nearest Neighbor Loss ) Show expected loss in- curred nearest neighbor classiﬁer exceed twice loss Bayes optimal decision .   Density Estimation  .  Limit Theorems Assume gambler go casino play game dice . happens , unlucky day among     times toss dice , see ’   ’ eleven times . fair dice know face occur equal probability     . Hence expected value     draws       ≈    , considerably eleven times observed . crying foul decide mathematical analysis order . probability seeing particular sequence trials n m−n . Moreover , ( cid:  ) ’   ’ given   n ! ( m−n ) ! diﬀerent n   sequences ’   ’ ’   ’ proportions n m−n respectively . Hence may compute probability seeing ’   ’    less via ( cid:  ) = n     ! Pr ( X ≤    ) =    ( cid:   ) i=  p ( ) =    ( cid:   ) i=  ( cid:   )      ( cid:   ) ( cid:   )     ( cid:   ) ( cid:   )     ( cid:   )    −i ≈  .  % (  .  ) looking ﬁgure decide things probably reasonable . , fact , consistent convergence behavior sim- ulated dice Figure  .  . computing (  .  ) learned something useful : expansion special case binomial series . ﬁrst term Fig .  .  . Convergence empirical means expectations . left right : em- pirical frequencies occurrence obtained casting dice    ,    ,    ,     ,     ,     times respectively . Note    throws still observed single ( cid:  )    ’   ’ , event occurs ( cid:  )   ≈  .  % probability .             .  .  .  . m=         .  .  .  . m=         .  .  .  . m=         .  .  .  . m=          .  .  .  . m=          .  .  .  . m=         Density Estimation counts number conﬁgurations could observe times ’   ’ sequence     dice throws . second third term probabilities seeing one particular instance sequence . Note general may lucky , since may con- siderably less information setting studying . instance , might know actual probabilities face dice , would likely assumption gambling casino questionable reputation . Often outcomes system dealing may continuous valued random variables rather binary ones , possibly even unknown range . instance , trying determine average wage questionnaire need determine many people need ask order obtain certain level conﬁdence . answer questions need discuss limit theorems . tell us much averages set observations may deviate corresponding expectations many observations need draw estimate number probabilities reliably . completeness present proofs fundamental theorems Section  . .  . useful albeit non-essential understanding remainder book may omitted .  . .  Fundamental Laws Law Large Numbers developed Bernoulli      one fundamental building blocks statistical analysis . states averages number observations converge expectations given suﬃ- ciently large number observations given certain assumptions independence observations . comes two ﬂavors : weak strong law . Theorem  .  ( Weak Law Large Numbers ) Denote X  , . . . , Xm random variables drawn p ( x ) mean µ = EXi [ xi ] . Moreover let ¯Xm : =     ( cid:   ) i=  Xi (  .  ) empirical average random variables Xi . ( cid:   ) >   following holds Pr ( cid:  ) ( cid:   ) ( cid:   ) ¯Xm − µ ( cid:   ) ( cid:   ) ≤ ( cid:   ) ( cid:  ) =   . lim m→∞ (  .  )  .  Limit Theorems    Fig .  .  . mean number casts dice . horizontal straight line denotes mean  .  . uneven solid line denotes actual mean ¯Xn function number draws , given semilogarithmic plot . crosses denote outcomes dice . Note ¯Xn ever closely approaches mean  .  obtain increasing number observations . establishes , indeed , large enough sample sizes , average converge expectation . strong law strengthens follows : Theorem  .  ( Strong Law Large Numbers ) conditions Theorem  .  Pr ( cid:  ) limm→∞ ¯Xm = µ ( cid:  ) =   . strong law implies almost surely ( measure theoretic sense ) ¯Xm converges µ , whereas weak law states every ( cid:   ) random variable ¯Xm within interval [ µ− ( cid:   ) , µ+ ( cid:   ) ] . Clearly strong implies weak law since measure events ¯Xm = µ converges   , hence ( cid:   ) -ball around µ would capture . laws justify may take sample averages , e.g . number events outcomes dice use latter estimate means , probabilities ( treat indicator variable event {   ;   } -valued random variable ) , variances related quantities . postpone proof Section  . .  , since eﬀective way proving Theo- rem  .  relies theory characteristic functions discuss next section . moment , give pictorial illustration Figure  .  . established random variable ¯Xm = m−  ( cid:   ) i=  Xi con- verges mean µ , natural second question establish quickly converges properties limiting distribution ¯Xm −µ . Note Figure  .  initial deviation mean large whereas observe data empirical mean approaches true one .                      Density Estimation Fig .  .  . Five instantiations running average outcomes toss dice . Note converge mean  .  . Moreover note well contained within upper lower envelopes given µ ± ( cid:    ) VarX [ x ] /m . central limit theorem answers question exactly addressing slightly general question , namely whether sum number independent random variables arises diﬀerent distribution might also well behaved limiting distribution . case long variance random variables bounded . limiting distribution sum Gaussian . aﬃrms pivotal role Gaussian distribution . Theorem  .  ( Central Limit Theorem ) Denote Xi independent ran- dom variables means µi standard deviation σi . Zm : = ( cid:   ) ( cid:   ) i=  σ   ( cid:   ) −     ( cid:   ) ( cid:   ) ( cid:   ) Xi − µi i=  (  .  ) converges Normal Distribution zero mean unit variance . Note like law large numbers central limit theorem ( CLT ) asymptotic result . , limit inﬁnite number observations become exact . said , often provides excellent approximation even ﬁnite numbers observations , illustrated Fig- ure  .  . fact , central limit theorem related limit theorems build foundation known asymptotic statistics . Example  .  ( Dice ) interested computing mean values returned dice may apply CLT sum variables                  .  Limit Theorems    mean µ =  .  variance ( see Problem  .  ) VarX [ x ] = EX [ x  ] − EX [ x ]   = (   +   +   +    +    +    ) /  −  .   ≈  .   . study random variable Wm : = m−  ( cid:   ) i=  [ Xi −  .  ] . Since terms sum zero mean , also Wm ’ mean vanishes . Moreover , Wm multiple Zm (  .  ) . Hence Wm converges normal distribution zero mean standard deviation  .  m−     . Consequently average tosses dice yields random vari- able mean  .  approach normal distribution variance m−      .   . words , empirical mean converges average rate ( m−     ) . Figure  .  gives illustration quality bounds implied CLT . One remarkable property functions random variables many conditions convergence properties random variables bestowed upon functions , . manifest following two results : variant Slutsky ’ theorem so-called delta method . former deals limit behavior whereas latter deals extension central limit theorem . Theorem  .  ( Slutsky ’ Theorem ) Denote Xi , Yi sequences ran- dom variables Xi → X Yi → c c ∈ R probability . Moreover , denote g ( x , ) function continuous ( x , c ) . case random variable g ( Xi , Yi ) converges probability g ( X , c ) . proof see e.g . [ Bil   ] . Theorem  .  often referred continuous mapping theorem ( Slutsky proved result aﬃne functions ) . means functions random variables possible pull limiting procedure function . device useful trying prove asymptotic normality order obtain characterizations limiting distribution . Theorem  .  ( Delta Method ) Assume Xn ∈ Rd asymptotically normal a−  n →   . Moreover , assume g : Rd → Rl mapping continuously diﬀerentiable b . case random variable g ( Xn ) converges n ( Xn − b ) → N (   , Σ ) a  n ( g ( Xn ) − g ( b ) ) → N (   , [ ∇xg ( b ) ] Σ [ ∇xg ( b ) ] ( cid:   ) ) . a−  Proof Via Taylor expansion see n [ g ( Xn ) − g ( b ) ] = [ ∇xg ( ξn ) ] ( cid:   ) a−  a−  n ( Xn − b ) (  .  ) (  .  )      Density Estimation ξn lies line segment [ b , Xn ] . Since Xn → b ξn → b , . Since g continuously diﬀerentiable b may apply Slutsky ’ the- orem see a−  n [ g ( Xn ) − g ( b ) ] → [ ∇xg ( b ) ] ( cid:   ) a−  n ( Xn − b ) . con- sequence , transformed random variable asymptotically normal covariance [ ∇xg ( b ) ] Σ [ ∇xg ( b ) ] ( cid:   ) . use delta method comes investigating properties maximum likelihood estimators exponential families . g play role mapping expectations natural parametrization distribution .  . .  Characteristic Function Fourier transform plays crucial role many areas mathematical analysis engineering . equally true statistics . historic rea- sons applications distributions called characteristic function , discuss section . foundations lie standard tools functional analysis signal processing [ Rud   , Pap   ] . begin recalling basic properties : Deﬁnition  .  ( Fourier Transform ) Denote f : Rn → C function deﬁned d-dimensional Euclidean space . Moreover , let x , ω ∈ Rn . Fourier transform F inverse F −  given F [ f ] ( ω ) : = (  π ) −   F −  [ g ] ( x ) : = (  π ) −   ( cid:   ) Rn ( cid:   ) Rn f ( x ) exp ( −i ( cid:    ) ω , x ( cid:    ) ) dx g ( ω ) exp ( ( cid:    ) ω , x ( cid:    ) ) dω . (  .  ) (  .  ) key insight F −  ◦ F = F ◦ F −  = Id . words , F F −  inverses functions L  integrable Rd , includes probability distributions . One key advantages Fourier transforms derivatives convolutions f translate  multiplications . F [ f ◦ g ] = (  π )   F [ f ] · F [ g ] . rule applies inverse transform , i.e . F −  [ f ◦ g ] = (  π )   F −  [ f ] F −  [ g ] .  beneﬁt statistical analysis often problems easily expressed Fourier domain easier prove convergence results . results carry original domain . exploiting fact proof law large numbers central limit theorem . Note deﬁnition Fourier transforms extended general domains groups . See e.g . [ BCR   ] details .  .  Limit Theorems    next introduce notion characteristic function distribution.  Deﬁnition  .  ( Characteristic Function ) Denote p ( x ) distribution random variable X ∈ Rd . characteristic function φX ( ω ) ω ∈ Rd given φX ( ω ) : = (  π )    F −  [ p ( x ) ] = ( cid:   ) exp ( ( cid:    ) ω , x ( cid:    ) ) dp ( x ) . (  .  ) words , φX ( ω ) inverse Fourier transform applied prob- ability measure p ( x ) . Consequently φX ( ω ) uniquely characterizes p ( x ) moreover , p ( x ) recovered φX ( ω ) via forward Fourier trans- form . One key utilities characteristic functions allow us deal easy ways sums random variables . Theorem  .  ( Sums random variables convolutions ) Denote X , ∈ R two independent random variables . Moreover , denote Z : = X + sum random variables . distribution Z sat- isﬁes p ( z ) = p ( x ) ◦ p ( ) . Moreover , characteristic function yields : φZ ( ω ) = φX ( ω ) φY ( ω ) . (  .   ) Proof Z given Z = X + . Hence , given Z = z freedom choose X = x freely provided = z − x . terms distributions means joint distribution p ( z , x ) given p ( z , x ) = p ( = z − x ) p ( x ) hence p ( z ) = ( cid:   ) p ( = z − x ) dp ( x ) = [ p ( x ) ◦ p ( ) ] ( z ) . result characteristic functions follows form property Fourier transform . sums several random variables characteristic function prod- uct individual characteristic functions . allows us prove weak law large numbers central limit theorem ( see Figure  .  illustration ) proving convergence Fourier domain . Proof [ Weak Law Large Numbers ] heart analysis lies Taylor expansion exponential exp ( iwx ) =   + ( cid:    ) w , x ( cid:    ) + ( |w| ) hence φX ( ω ) =   + iwEX [ x ] + ( |w| ) .   Chapter ? ? discuss general descriptions distributions φX special case . particular , replace exponential exp ( ( cid:    ) ω , x ( cid:    ) ) kernel function k ( x , x ( cid:   ) ) .      Density Estimation Fig .  .  . working example central limit theorem . top row contains distributions sums uniformly distributed random variables interval [  .  ,  .  ] . left right sums   ,   ,   ,      random variables . , bottom row contains distribution means rescaled number observations . Note distribution converges increasingly normal distribution . √ Given random variables Xi mean EX [ x ] = µ means average ¯Xm : =   i=  Xi characteristic function  ( cid:   ) ( cid:   ) ( cid:   ) φ ¯Xm ( ω ) =   + wµ + ( m−  |w| ) (  .   )   limit → ∞ converges exp ( iwµ ) , characteristic func- tion constant distribution mean µ . proves claim large sample limit ¯Xm essentially constant mean µ . Proof [ Central Limit Theorem ] use idea prove CLT . main diﬀerence , though , need assume second moments random variables Xi exist . avoid clutter prove case constant mean EXi [ xi ] = µ variance VarXi [ xi ] = σ  . -    .  .  . -    .  .  . -    .  .  . -    .  .  . -    .  .  . -    .  .  .  . -    .  .  .  . -    .  .  .  . -    .  .  .  . -    .  .  .  .   .  Limit Theorems    ( cid:   ) mσ  Let Zm : =  √ i=  ( Xi − µ ) . proof relies showing convergence characteristic function Zm , i.e . φZm normally dis- tributed random variable W zero mean unit variance . Expanding exponential second order yields : exp ( iwx ) =   + iwx − w x  + ( |w|  )     hence φX ( ω ) =   + iwEX [ x ] −     w VarX [ x ] + ( |w|  ) Since mean Zm vanishes centering ( Xi − µ ) variance per variable m−  may write characteristic function Zm via ( cid:   ) φZm ( ω ) =   −    m w  + ( m−  |w|  ) ( cid:   ) , taking limits → ∞ yields exponential function . limm→∞ φZm ( ω ) = exp ( −     ω  ) characteristic function normal distribution zero mean variance   . Since character- istic function transform injective proves claim . Note characteristic function number useful properties . instance , also used moment generating function via identity : ∇n ωφX (   ) = i−nEX [ xn ] . (  .   ) proof left exercise . See Problem  .  details . connection also implies ( subject regularity conditions ) know moments distribution able reconstruct directly since allows us reconstruct characteristic function . idea exploited density estimation [ Cra   ] form Edgeworth Gram-Charlier expansions [ Hal   ] .  . .  Tail Bounds practice never access inﬁnite number observations . Hence central limit theorem apply approximation real situation . instance , case dice , might want state worst case bounds ﬁnite sums random variables determine much empirical mean may deviate expectation . bounds useful simple averages quantify behavior sophisticated estimators based set observations . bounds discuss diﬀer amount knowledge assume random variables question . instance , might      Density Estimation know mean . leads Gauss-Markov inequality . know mean variance able state stronger bound , Chebyshev inequality . even stronger setting , know variable bounded range , able state Chernoﬀ bound . bounds progressively tight also diﬃcult prove . state order technical sophistication . Theorem  .  ( Gauss-Markov ) Denote X ≥   random variable let µ mean . ( cid:   ) >   Pr ( X ≥ ( cid:   ) ) ≤ µ ( cid:   ) . (  .   ) Proof use fact nonnegative random variables Pr ( X ≥ ( cid:   ) ) = ( cid:   ) ∞ ( cid:   ) dp ( x ) ≤ ( cid:   ) ∞ ( cid:   ) x ( cid:   ) dp ( x ) ≤ ( cid:   ) −  ( cid:   ) ∞   xdp ( x ) = µ ( cid:   ) . means random variables small mean , proportion samples large value small . Consequently deviations mean ( ( cid:   ) −  ) . However , note bound depend number observations . useful application Gauss-Markov inequality Chebyshev ’ inequality . statement range random variables using variance . Theorem  .   ( Chebyshev ) Denote X random variable mean µ variance σ  . following holds ( cid:   ) >   : Pr ( |x − µ| ≥ ( cid:   ) ) ≤ σ  ( cid:   )   . (  .   ) Proof Denote : = |X − µ|  random variable quantifying deviation X mean µ . construction know EY [ ] = σ  . Next let γ : = ( cid:   )   . Applying Theorem  .  γ yields Pr ( > γ ) ≤ σ /γ proves claim . Note improvement Gauss-Markov inequality . bounds whose conﬁdence improved ( ( cid:   ) −  ) state ( ( cid:   ) −  ) bounds deviations mean . Example  .  ( Chebyshev bound ) Assume ¯Xm : = m−  ( cid:   ) i=  Xi average random variables mean µ variance σ  . Hence ¯Xm also mean µ . variance given Var ¯Xm [ ¯xm ] =  ( cid:   ) i=  m− VarXi [ xi ] = m− σ  .  .  Limit Theorems    Applying Chebyshev ’ inequality yields probability deviation ( cid:   ) mean µ bounded σ  ( cid:   )   . ﬁxed failure probability δ = Pr ( | ¯Xm − µ| > ( cid:   ) ) δ ≤ σ m−  ( cid:   ) −  equivalently ( cid:   ) ≤ σ/ mδ . √ bound quite reasonable large δ means high levels conﬁdence need huge number observations . Much stronger results obtained able bound range random variables . Using latter , reap exponential improve- ment quality bounds form McDiarmid [ McD   ] inequality . state latter without proof : Theorem  .   ( McDiarmid ) Denote f : Xm → R function X let Xi independent random variables . case following holds : Pr ( |f ( x  , . . . , xm ) − EX  , ... , Xm [ f ( x  , . . . , xm ) ] | > ( cid:   ) ) ≤   exp ( cid:  ) −  ( cid:   )  C−  ( cid:  ) . constant C  given C  = ( cid:   ) i=  c  ( cid:   ) ( cid:   ) f ( x  , . . . , xi , . . . , xm ) − f ( x  , . . . , x ( cid:   )  , . . . , xm ) ( cid:   ) ( cid:   ) ≤ ci x  , . . . , xm , x ( cid:   ) . bound used averages number observations computed according algorithm long latter encoded f . particular , following bound [ Hoe   ] : Theorem  .   ( Hoeﬀding ) Denote Xi iid random variables bounded range Xi ∈ [ , b ] mean µ . Let ¯Xm : = m−  ( cid:   ) following bound holds : i=  Xi average . Pr ( cid:  ) ( cid:   ) ( cid:   ) ¯Xm − µ ( cid:   ) ( cid:   ) > ( cid:   ) ( cid:  ) ≤   exp ( cid:   ) −  m ( cid:   )   ( b − )   ( cid:   ) . (  .   ) Proof corollary Theorem  .   . ¯Xm individual random variable range [ a/m , b/m ] set f ( X  , . . . , Xm ) : = ¯Xm . Straight- forward algebra shows C  = m−  ( b − )   . Plugging back McDiarmid ’ theorem proves claim . Note (  .   ) exponentially better previous bounds . increasing sample size conﬁdence level also increases exponentially . Example  .  ( Hoeﬀding bound ) example  .  assume Xi iid random variables let ¯Xm average . Moreover , assume      Density Estimation Xi ∈ [ , b ] . want obtain guarantees probability | ¯Xm − µ| > ( cid:   ) . given level conﬁdence   − δ need solve δ ≤   exp ( cid:   ) −  m ( cid:   )   ( b−a )   ( cid:   ) (  .   ) ( cid:   ) . Straightforward algebra shows case ( cid:   ) needs satisfy ( cid:   ) ≥ |b − a| ( cid:    ) [ log   − log δ ] / m (  .   ) words , conﬁdence level enters logarithmically inequality , sample size improves conﬁdence ( cid:   ) = ( m−     ) . , order improve conﬁdence interval ( cid:   ) =  .  ( cid:   ) =  .   need     times many observations . bound tight ( see Problem  .  details ) , possible ob- tain better bounds know additional information . particular knowing bound variance random variable addition knowing bounded range would allow us strengthen statement considerably . Bernstein inequality captures connection . details see [ BBL   ] works empirical process theory [ vdVW   , SW   , Vap   ] .  . .  Example probably easiest illustrate various bounds using concrete exam- ple . semiconductor fab processors produced wafer . typical    mm wafer holds     chips . large number processing steps required produce ﬁnished microprocessor often impossible assess eﬀect design decision ﬁnished product produced . Assume production manager wants change step process ’ ’ process ’ B ’ . goal increase yield process , , number chips     potential chips wafer sold . Unfortunately number random variable , i.e . number working chips per wafer vary widely diﬀerent wafers . Since process ’ ’ running factory long time may assume yield well known , say µA =         processors average . goal determine whether process ’ B ’ better yield may . Obviously , since production runs expensive want able determine number quickly possible , i.e . using wafers possible . production manager risk averse wants ensure new process really better . Hence requires conﬁdence level    % change production .  .  Limit Theorems    ﬁrst step formalize problem . Since know process ’ ’ exactly need concern ’ B ’ . associate random variable Xi wafer . reasonable ( somewhat simplifying ) assumption posit Xi independent identically distributed Xi mean µB . Obviously know µB — otherwise would reason testing ! denote ¯Xm average yields wafers using process ’ B ’ . interested accuracy ( cid:   ) probability δ = Pr ( | ¯Xm − µB| > ( cid:   ) ) satisﬁes δ ≤  .   . Let us discuss various bounds behave . sake argument assume µB − µA =    , i.e . new process produces average    additional usable chips . Chebyshev order apply Chebyshev inequality need bound variance random variables Xi . worst possible variance would occur Xi ∈ {   ;     } events occur equal probability . words , equal probability wafer fully usable entirely broken . amounts σ  =  .  (     −   )   +  .  (     −     )   =    ,     . Since Chebyshev bounds δ ≤ σ m−  ( cid:   ) −  (  .   ) solve = σ /δ ( cid:   )   =    ,    / (  .   ·     ) =    ,     . words , would typically need   ,    wafers assess reasonable conﬁdence whether process ’ B ’ better process ’ ’ . completely unrealistic . Slightly better bounds obtained able make better assumptions variance . instance , sure yield process ’ B ’ least     , largest possible variance  .   (     −   )   +  .   (     −     )   =    ,     , leading minimum   ,    wafers much better . Hoeﬀding Since yields interval {   , . . . ,     } ex- plicit bound range observations . Recall inequality (  .   ) bounds failure probably δ =  .   exponential term . Solving yields ≥  . |b − a|  ( cid:   ) −  log (  /δ ) ≈    .  (  .   ) words , need lest     wafers determine whether process ’ B ’ better . signiﬁcant improvement almost two orders magnitude , still seems wasteful would like better .      Density Estimation Central Limit Theorem central limit theorem approximation . means reasoning accurate . said , large enough sample sizes , approximation good enough use practical predictions . Assume moment knew variance σ  exactly . case know ¯Xm approximately normal mean µB variance m− σ  . interested interval [ µ − ( cid:   ) , µ + ( cid:   ) ] contains    % probability mass normal distribution . , need solve integral    πσ  ( cid:   ) µ+ ( cid:   ) µ− ( cid:   ) ( cid:   ) exp − ( cid:   ) ( x − µ )    σ  dx =  .   (  .   ) solved eﬃciently using cumulative distribution function normal distribution ( see Problem  .  details ) . One check (  .   ) solved ( cid:   ) =  .  σ . words , interval ± .  σ contains    % probability mass normal distribution . number observations therefore determined ( cid:   ) =  .  σ/ √ hence =  .   σ  ( cid:   )   (  .   ) , problem know variance distribution . Using worst-case bound variance , i.e . σ  =    ,     would lead requirement least =     wafers testing . However , know variance , may estimate along mean use empirical estimate , possibly plus small constant ensure underestimate variance , instead upper bound . Assuming ﬂuctuations turn order    processors , i.e . σ  =      , able reduce requirement approximately    wafers . probably acceptable number practical test . Rates Constants astute reader noticed three conﬁdence bounds scaling behavior = ( ( cid:   ) −  ) . , cases number observations fairly ill behaved function amount conﬁdence required . interested convergence per se , statement like Chebyshev inequality would entirely suﬃcient . various laws bounds often used obtain con- siderably better constants statistical conﬁdence guarantees . complex estimators , methods classify , rank , annotate data , reasoning one become highly nontrivial . See e.g . [ MYA   , Vap   ] details .  .  Parzen Windows  .  Parzen Windows  . .  Discrete Density Estimation    convergence theorems discussed far mean use empir- ical observations purpose density estimation . Recall case Naive Bayes classiﬁer Section  . .  . One key ingredients ability use information word counts diﬀerent document classes estimate probability p ( wj|y ) , wj denoted number occurrences word j document x , given labeled . following discuss extremely simple crude method estimating probabilities . relies fact random variables Xi drawn distribution p ( x ) discrete values Xi ∈ X lim m→∞ ˆpX ( x ) = p ( x ) ˆpX ( x ) : = m−   ( cid:   ) i=  { xi = x } x ∈ X . (  .   ) (  .   ) Let us discuss concrete case . assume    documents would like estimate probability occurrence word ’ dog ’ . raw data : Document ID Occurrences ‘ dog ’                                                    means word ‘ dog ’ occurs following number times : Occurrences ‘ dog ’ Number documents                             Something unusual happening : reason never observed   instances word dog documents ,   less , alter- natively   times .   times ? reasonable assume corresponding value   either . Maybe sample enough . One possible strategy add pseudo-counts observations . amounts following estimate : ˆpX ( x ) : = ( + |X| ) −  ( cid:    )   +  ( cid:   ) i=  { xi = x } = p ( x ) ( cid:    ) (  .   ) Clearly limit → ∞ still p ( x ) . Hence , asymptotically lose anything . prescription used Algorithm  .  used method called Laplace smoothing . contrast two methods :      Density Estimation Occurrences ‘ dog ’                  .    .   Number documents   Frequency occurrence  .   Laplace smoothing  .   problem method |X| increases need increasingly observations obtain even modicum precision . average , need least one observation every x ∈ X . infeasible large domains following example shows .    .     .      .     .      .    .      .    .        .   Example  .  ( Curse Dimensionality ) Assume X = {   ,   } , i.e . x consists binary bit vectors dimensionality d. increases size X increases exponentially , requiring exponential number observations perform density estimation . instance , work images ,     ×     black white picture would require order        observations model fairly low-resolution images accurately . clearly utterly infeasible — number particles known universe order      . Bellman [ Bel   ] one ﬁrst formalize dilemma coining term ’ curse dimensionality ’ . example clearly shows need better tools deal high- dimensional data . present one tools next section .  . .  Smoothing Kernel proceed proper density estimation . Assume want estimate distribution weights population . Sample data population might look follows : X = {    ,    ,    ,    ,    ,    ,    ,    ,    ,    ,    ,    ,     ,    ,     ,    ,     ,     } . could use perform density estimate placing discrete components locations xi ∈ X weight  /|X| done Figure  .  . reason believe weights quantized kilograms , grams , miligrams ( pounds stones ) . even , would expect similar weights would similar densities associated . Indeed , right diagram Figure  .  shows , corresponding density continuous . key question arising may transform X realistic estimate density p ( x ) . Starting ’ density estimate ’ discrete terms ˆp ( x ) =     ( cid:   ) i=  δ ( x − xi ) (  .   )  .  Parzen Windows    may choose smooth smoothing kernel h ( x ) probability mass becomes somewhat spread . density estimate X ⊆ Rd achieved ˆp ( x ) =     ( cid:   ) i=  r−dh ( cid:  ) x−xi r ( cid:  ) . (  .   ) expansion commonly known Parzen windows estimate . Note obviously h must chosen h ( x ) ≥   x ∈ X moreover ( cid:   ) h ( x ) dx =   order ensure (  .   ) proper prob- ability distribution . formally justify smoothing . Let R small region q = ( cid:   ) R p ( x ) dx . samples drawn p ( x ) , probability k fall region R given binomial distribution ( cid:   ) ( cid:   ) k qk (   − q ) m−k . expected fraction points falling inside region easily com- puted expected value Binomial distribution : E [ k/m ] = q . Similarly , variance computed Var [ k/m ] = q (   − q ) /m . → ∞ variance goes   hence estimate peaks around expectation . therefore set k ≈ mq . assume R small p ( x ) constant R , V volume R. Rearranging obtain q ≈ p ( x ) · V , p ( x ) ≈ k mV . (  .   ) Let us set R cube side length r , deﬁne function h ( u ) = ( cid:   )   |ui| ≤       otherwise . Observe h ( cid:  ) x−xi r ( cid:  )   xi lies inside cube size r centered   Density Estimation    around x . let k =  ( cid:   ) i=  ( cid:   ) x − xi r ( cid:   ) , h one use (  .   ) estimate p via ˆp ( x ) =     ( cid:   ) i=  r−dh ( cid:   ) x − xi r ( cid:   ) , rd volume hypercube size r dimensions . symme- try , interpret equation sum cubes centered around data points xn . replace cube smooth kernel function h ( · ) recovers (  .   ) . exists large variety diﬀerent kernels used kernel density estimate . [ Sil   ] detailed description properties number kernels . Popular choices   x    e−   h ( x ) = (  π ) −     e−|x| h ( x ) =     max (   ,   − x  ) h ( x ) =   h ( x ) =     χ [ − ,  ] ( x ) Gaussian kernel Laplace kernel Epanechnikov kernel Uniform kernel h ( x ) = max (   ,   − |x| ) Triangle kernel . (  .   ) (  .   ) (  .   ) (  .   ) (  .   ) kernels triweight quartic kernel basically powers Epanechnikov kernel . practical purposes Gaussian ker- nel (  .   ) Epanechnikov kernel (  .   ) suitable . particular , latter attractive property compact support . means given density estimate location x need evaluate terms h ( xi − x ) distance ( cid:    ) xi − x ( cid:    ) less r. expan- sions computationally much cheaper , particular make use fast nearest neighbor search algorithms [ GIM   , IM   ] . Figure  .  examples kernels .  . .  Parameter Estimation far discussed issue parameter selection . evident Figure  .  , though , quite crucial choose good kernel width . Clearly , kernel overly wide oversmooth ﬁne detail might density . hand , narrow kernel useful , since able make statements locations actually observed data .  .  Parzen Windows    Fig .  .  . Left : naive density estimate given sample weight    persons . Right : underlying weight distribution . Fig .  .  . Parzen windows density estimate associated    observations Figure . left right : Gaussian kernel density estimate kernel width  .  ,   ,   ,    respectively . Fig .  .  . kernels Parzen windows density estimation . left right : Gaussian kernel , Laplace kernel , Epanechikov kernel , uniform density . Moreover , issue choosing suitable kernel function . fact large variety exists might suggest crucial issue . practice , turns case instead , choice suitable kernel width much vital good estimates . words , size matters , shape secondary . problem know kernel width best data . problem one-dimensional , might hope able eyeball size r. Obviously , higher dimensions approach fails . second                    .   .   .                     .   .   .   .   .   .            .    .    .             .    .    .             .    .    .             .    .    .   - -     .  .  . - -     .  .  . - -     .  .  . - -     .  .  .       Density Estimation option would choose r log-likelihood data maximized . given log  ( cid:   ) i=  p ( xi ) = −m log +  ( cid:   ) log  ( cid:   ) i=  j=  r−dh ( cid:   ) ( cid:   ) xi−xj r (  .   ) Remark  .   ( Log-likelihood ) consider logarithm likeli- hood reasons computational stability prevent numerical underﬂow . term p ( xi ) might within suitable range , say   −  , prod- uct      terms easily exceed exponent ﬂoating point representations computer . Summing logarithm , hand , perfectly feasible even large numbers observations . Unfortunately computing log-likelihood equally infeasible : decreas- ing r surviving terms (  .   ) functions h ( ( xi − xi ) /r ) = h (   ) , since arguments kernel functions diverge . words , log-likelihood maximized p ( x ) peaked exactly locations observed data . graph left Figure  .  shows happens situation . experienced case overﬁtting model ﬂexible . led situation model able explain observed data “ unreasonably well ” , simply able adjust parameters given data . encounter situation throughout book . exist number ways address problem . Validation Set : could use subset set observations estimate log-likelihood . , could partition obser- vations X : = { x  , . . . , xn } X ( cid:   ) : = { xn+  , . . . , xm } use second part likelihood score according (  .   ) . second set typically called validation set . n-fold Cross-validation : Taking idea , note particular reason given xi belong X X ( cid:   ) respec- tively . fact , could use splits observations sets X X ( cid:   ) infer quality estimate . compu- tationally infeasible , could decide split observations n equally sized subsets , say X  , . . . , Xn use validation set time remainder used generate density estimate . Typically n chosen    , case procedure  .  Parzen Windows    referred   -fold cross-validation . computationally at- tractive procedure insofar require us change basic estimation algorithm . Nonetheless , computation costly . Leave-one-out Estimator : extreme end cross-validation could choose n = m. , remove single observation time use remainder data estimate . Using average likelihood scores provides us even ﬁne-grained estimate . Denote pi ( x ) density estimate obtained using X : = { x  , . . . , xm } without xi . Parzen windows estimate given pi ( xi ) = ( −   ) −  ( cid:   ) r−dh ( cid:   ) ( cid:   ) xi−xj r = m−  ( cid:    ) ( cid:    ) p ( xi ) − r−dh (   ) . j ( cid:   ) =i (  .   ) Note precisely term r−dh (   ) removed estimate . term led divergent estimates r →   . means leave-one-out log-likelihood estimate computed easily via L ( X ) = log m−  +  ( cid:   ) i=  ( cid:    ) ( cid:    ) p ( xi ) − r−dh (   ) . log (  .   ) choose r L ( X ) maximized . strategy robust whenever implemented computationally eﬃcient manner , reliable performing model selection . alternative , probably theoretical interest , choose scale r priori based amount data disposition . Intuitively , need scheme ensures r →   number observations increases → ∞ . However , need ensure happens slowly enough number observations within range r keeps increasing order ensure good statistical performance . details refer reader [ Sil   ] . Chapter ? ? discusses issues model selection estimators general considerably detail .  . .  Silverman ’ Rule Assume aspiring demographer wishes estimate popu- lation density country , say Australia . might access limited census , random portion population determines live . consequence obtain relatively high number samples      Density Estimation Fig .  .  . Nonuniform density . Left : original density samples drawn distribution . Middle : density estimate uniform kernel . Right : density estimate using Silverman ’ adjustment . city dwellers , whereas number people living countryside likely small . attempt perform density estimation using Parzen windows , encounter interesting dilemma : regions high density ( i.e . cities ) want choose narrow kernel width allow us model variations population density accurately . Conversely , outback , wide kernel preferable , since population low . Unfortunately , information exactly density estimator could tell us . words chicken egg situation good density estimate seems necessary come good density estimate . Fortunately situation addressed realizing actually need know density rather rough estimate latter . obtained using information average distance k nearest neighbors point . One Silverman ’ rules thumb [ Sil   ] choose ri ri = c k ( cid:   ) ( cid:    ) x − xi ( cid:    ) . x∈kN N ( xi ) (  .   ) Typically c chosen  .  k small , e.g . k =   ensure estimate computationally eﬃcient . density estimate given p ( x ) =     ( cid:   ) i=  r−d h ( cid:   ) x−xi ri ( cid:   ) . (  .   ) Figure  .  shows example density estimate . clear locality dependent kernel width better choosing uniformly constant kernel density estimate . However , note increases computational complexity performing density estimate , since ﬁrst k nearest neigh- bors need found density estimate carried .  .  Parzen Windows     . .  Watson-Nadaraya Estimator able perform density estimation may use perform classiﬁcation regression . leads us eﬀective method non- parametric data analysis , Watson-Nadaraya estimator [ Wat   , Nad   ] . basic idea simple : assume binary classiﬁcation problem , i.e . need distinguish two classes . Provided able compute density estimates p ( x ) given set observations X could appeal Bayes rule obtain p ( y|x ) = p ( x|y ) p ( ) p ( x ) =  ·       : yi=y r−dh ( cid:  ) xi−x ( cid:   ) i=  r−dh ( cid:  ) xi−x ( cid:  ) r r ( cid:   ) ( cid:  ) . (  .   ) take sum xi label yi = numerator . advantage approach cheap design estimator . , need compute sums . downside , similar k-nearest neighbor classiﬁer may require sums ( search ) large number observations . , evaluation (  .   ) potentially ( ) operation . Fast tree based representations used accelerate [ BKL   , KM   ] , however behavior depends signiﬁ- cantly dimensionality data . encounter computationally attractive methods later stage . binary classiﬁcation (  .   ) simpliﬁed considerably . Assume ∈ { ±  } . p ( =  |x ) >  .  choose estimate =   converse case would estimate = −  . Taking diﬀerence twice numerator denominator see function yih ( cid:  ) xi−x ( cid:   ) h ( cid:  ) xi−x ( cid:   ) h ( cid:  ) xi−x ( cid:  ) r h ( cid:  ) xi−x ( cid:  ) ( cid:  ) = yiwi ( x ) f ( x ) = ( cid:  ) = : (  .   ) ( cid:   ) ( cid:   ) ( cid:   ) yi r r r   used achieve goal since f ( x ) >   ⇐⇒ p ( =  |x ) >  .  . Note f ( x ) weighted combination labels yi associated weights wi ( x ) depend proximity x observation xi . words , (  .   ) smoothed-out version k-nearest neighbor classiﬁer Section  . .  . Instead drawing hard boundary k closest observation use soft weighting scheme weights wi ( x ) depending observations closest . Note furthermore numerator (  .   ) similar simple classiﬁer Section  . .  . fact , kernels k ( x , x ( cid:   ) ) Gaussian RBF kernel , also kernels sense Parzen windows den- ( cid:   ) x−x ( cid:   ) sity estimate , i.e . k ( x , x ( cid:   ) ) = r−dh two terms identical . r ( cid:   )      Density Estimation Fig .  .  . Watson Nadaraya estimate . Left : binary classiﬁer . optimal solution would straight line since classes drawn normal distribution variance . Right : regression estimator . data generated sinusoid additive noise . regression tracks sinusoid reasonably well . means Watson Nadaraya estimator provides us alternative explanation (  .   ) leads usable classiﬁer . fashion Watson Nadaraya classiﬁer extends k- nearest neighbor classiﬁer also may construct Watson Nadaraya re- gression estimator replacing binary labels yi real-valued values yi ∈ R obtain regression estimator ( cid:   ) yiwi ( x ) . Figure  .  ex- ample workings regression estimator classiﬁer . easy use work well moderately dimensional data .  .  Exponential Families Distributions exponential family versatile tools statistical inference . Gaussians , Poisson , Gamma Wishart dis- tributions form part exponential family . play key role dealing graphical models , classiﬁcation , regression conditional ran- dom ﬁelds encounter later parts book . reasons popularity lead convex optimization prob- lems allow us describe probability distributions linear models .  . .  Basics Densities exponential family deﬁned p ( x ; θ ) : = p  ( x ) exp ( ( cid:    ) φ ( x ) , θ ( cid:    ) − g ( θ ) ) . (  .   )  .  Exponential Families    p  ( x ) density X often called base measure , φ ( x ) map x suﬃcient statistics φ ( x ) . θ commonly referred natural parameter . lives space dual φ ( x ) . Moreover , g ( θ ) normalization constant ensures p ( x ) properly normalized . g often referred log-partition function . name stems physics Z = eg ( θ ) denotes number states physical ensemble . g computed follows : g ( θ ) = log ( cid:   ) X exp ( ( cid:    ) φ ( x ) , θ ( cid:    ) ) dx . (  .   ) Example  .  ( Binary Model ) Assume X = {   ;   } φ ( x ) = x . case g ( θ ) = log ( cid:  ) e  + eθ ( cid:  ) = log ( cid:  )   + eθ ( cid:  ) . follows p ( x =   ; θ ) =    +eθ . words , choosing diﬀerent values θ one recover diﬀerent Bernoulli distributions .  +eθ p ( x =   ; θ ) = eθ One convenient properties exponential families log- partition function g used generate moments distribution simply taking derivatives . Theorem  .   ( Log partition function ) function g ( θ ) convex . Moreover , distribution p ( x ; θ ) satisﬁes ∇θg ( θ ) = Ex [ φ ( x ) ] ∇  θg ( θ ) = Varx [ φ ( x ) ] . (  .   ) Proof Note ∇  covariance matrix positive semideﬁnite . show (  .   ) expand θg ( θ ) = Varx [ φ ( x ) ] implies g convex , since ∇θg ( θ ) = ( cid:   ) X φ ( x ) exp ( cid:    ) φ ( x ) , θ ( cid:    ) dx X exp ( cid:    ) φ ( x ) , θ ( cid:    ) ( cid:   ) ( cid:   ) = φ ( x ) p ( x ; θ ) dx = Ex [ φ ( x ) ] . (  .   ) Next take second derivative obtain ∇  θg ( θ ) = ( cid:   ) X = Ex φ ( x ) [ φ ( x ) − ∇θg ( θ ) ] ( cid:   ) p ( x ; θ ) dx ( cid:    ) φ ( x ) φ ( x ) ( cid:   ) ( cid:    ) − Ex [ φ ( x ) ] Ex [ φ ( x ) ] ( cid:   ) (  .   ) (  .   ) proves claim . ﬁrst equality used (  .   ) . second line used deﬁnition variance . One may show higher derivatives ∇n θ g ( θ ) generate higher order cu- mulants φ ( x ) p ( x ; θ ) . g often also referred cumulant-generating function . Note general , computation g ( θ )      Density Estimation nontrivial since involves solving highdimensional integral . many cases , fact , computation NP hard , instance X do- main permutations [ FJ   ] . Throughout book discuss number approximation techniques applied case . Let us brieﬂy illustrate (  .   ) using binary model Example  .  . ∇θ = eθ θ = eθ (  +eθ )   . exactly would obtained direct computation mean p ( x =   ; θ ) variance p ( x =   ; θ ) − p ( x =   ; θ )   subject distribution p ( x ; θ ) .  +eθ ∇   . .  Examples large number densities members exponential family . Note , however , statistics common express dot product formulation historic reasons reasons notational com- pactness . discuss number common densities show written terms exponential family . detailed description commonly occurring types given table . Gaussian Let x , µ ∈ Rd let Σ ∈ Rd×d Σ ( cid:   )   , , Σ positive deﬁnite matrix . case normal distribution expressed via p ( x ) = (  π ) −   |Σ|−     exp ( cid:   ) − ( cid:   ) = exp x ( cid:   ) ( cid:  ) Σ− µ ( cid:  ) + tr     ( cid:   ) ( cid:   ) ( x − µ ) ( cid:   ) Σ−  ( x − µ ) ( cid:   ) ( cid:   ) xx ( cid:   ) ( cid:  ) Σ−  ( cid:  ) ( cid:   ) − c ( µ , Σ )     − (  .   ) ( cid:   )   log  π +     µ ( cid:   ) Σ− µ + c ( µ , Σ ) =     log |Σ| . combining terms x φ ( x ) : = ( x , −     xx ( cid:   ) ) obtain suﬃcient statistics x . corresponding linear coeﬃcients ( Σ− µ , Σ−  ) constitute natural parameter θ . remains done express p ( x ) terms (  .   ) rewrite g ( θ ) terms c ( µ , Σ ) . summary table following page contains details . Multinomial Another popular distribution one k discrete events . case X = {   , . . . , k } completely generic terms p ( x ) = πx πx ≥   ( cid:   ) x πx =   . denote ex ∈ Rk x-th unit vector canonical basis , ( cid:    ) ex , ex ( cid:   ) ( cid:    ) =   x = x ( cid:   )   otherwise . case may rewrite p ( x ) via p ( x ) = πx = exp ( ( cid:    ) ex , log π ( cid:    ) ) (  .   ) log π = ( log π  , . . . , log πk ) . words , succeeded  .  Exponential Families    rewriting distribution member exponential family φ ( x ) = ex θ = log π . Note deﬁnition θ restricted k −  dimensional manifold ( k dimensional prob- ability simplex ) . relax constraints need ensure p ( x ) remains normalized . Details given summary table . Poisson distribution often used model distributions discrete events . instance , number raindrops fall given surface area given amount time , number stars given volume space , number Prussian soldiers killed horse-kicks Prussian cavalry follow distribution . given p ( x ) = e−λλx x ! =   x ! exp ( x log λ − λ ) x ∈ N  . (  .   ) deﬁning φ ( x ) = x obtain exponential families model . Note things bit less trivial since   nonuniform x ! counting measure N  . case uniform measure leads exponential distribution discussed Problem  .   . reason many discrete processes follow Poisson distri- bution seen limit average large number Bernoulli draws : denote z ∈ {   ,   } random variable p ( z =   ) = α . Moreover , denote zn sum n draws random variable . case zn follows multinomial ( cid:  ) αk (   − α ) n−k . assume distribution p ( zn = k ) = ( cid:  ) n k let n → ∞ expected value zn remains constant . , rescale α = λ n . case p ( zn = k ) = = ( cid:   ) λk nk ( cid:   ) n ( cid:   ) n ! ( n − k ) ! k ! ( cid:   ) λk k !   − λ n   − ( cid:   ) n−k λ n (  .   ) n ! nk ( n − k ) ! ( cid:   )   − λ n ( cid:   ) k ( cid:   ) n → ∞ second term converges e−λ . third term con- verges   , since product  k terms , converge   . Using exponential families notation may check E [ x ] = λ moreover also Var [ x ] = λ . Beta distribution unit interval X = [   ,   ] versatile comes modelling unimodal bimodal distri-      Density Estimation Fig .  .   . Left : Poisson distributions λ = {   ,   ,    } . Right : Beta distributions =   b ∈ {   ,   ,   ,   ,   } . Note increasing b distribution becomes peaked close origin . butions . given p ( x ) = xa−  (   − x ) b−  Γ ( + b ) Γ ( ) Γ ( b ) . (  .   ) Taking logarithms see , , exponential families distribution , since p ( x ) = exp ( ( −   ) log x + ( b −   ) log (   − x ) + log Γ ( + b ) − log Γ ( ) − log Γ ( b ) ) . Figure  .   graphical description Poisson distribution Beta distribution . comprehensive list exponential family dis- tributions see table [ Fel   , FT   , MN   ] . principle map φ ( x ) , domain X underlying measure µ suitable , long log-partition function g ( θ ) computed eﬃciently . Theorem  .   ( Convex feasible domain ) domain deﬁnition Θ g ( θ ) convex . Proof construction g convex diﬀerentiable everywhere . Hence below-sets values c { x|g ( x ) ≤ c } exist . Consequently domain deﬁnition convex . convex function valuable comes parameter infer- ence since convex minimization problems unique minimum values global minima . discuss notion detail designing maximum likelihood estimators .              .   .   .   .   .   .   .   .   .   .  .  .  .  .  .  .  .  .  .  .  .  .  .   .  Exponential Families    Θ n      )   , ∞ − ( R ) ∞ ,   ( × R )   , ∞ − ( N R R n C × n R   ) ∞ ,   (   R   ) ∞ ,   ( n C × R n ) + R ( ) ∞ ,   ( )   , ∞ − (   θ   −   θ ( cid:   )   θ       θ   θ + |   θ | g  l    +   θ g  l    − π   g  l − π   g  l   θ g  l    −   θ   θ   − π g  l    √    n       θ g  l   θ − )   θ ( Γ g  l )   θ ( Γ )   θ ( Γ )   θ +   θ ( Γ g  l ( cid:  ) θ e −   ( cid:  )  θ e   = Ni ( cid:   ) e +   g  l g  l g  l − θ e θ g  l ( cid:  )    −   +   θ ( cid:  ) Γ g  l   = ni   g  l n   θ + |   θ | g  l ( cid:   ) +   θ − ) ) x −   ( g  l , x g  l ( ( cid:  ) x    − , | x | g  l ( cid:  ) ) x − , x g  l ( ( cid:  ) ( cid:   ) x x    ( cid:  )   x    ( cid:  )  x − − , x − , x , x − ( cid:  ) ( cid:  ) ( cid:  ) x x e x x x g n   n u  C g n   n u  C g n   n u  C !  x e u g  e b e L e u g  e b e L e u g  e b e L   ) x −   ( x    − x     + n − X | |  x )  θ   = ni ( cid:   ) ( Γ g  l − )  θ ( Γ g  l   = ni ( cid:   ) ) n x g  l , . . . ,   x g  l (   − )  x   = ni ( cid:   ) ( )   − θ ( g  l +   g  l )   − θ ( ) ) θ e −   ( g  l − ( g  l x g  l − x x     − e  x c  r e n e g ) ) θ ( g − , θ ( e u g  e b e L } N . .   { }   ,   { ) ∞ ,   [ +  N +  N R n R l     n   l u  l    n e n  p x E  l l u  n r e B n      P e c  l p  L n     u  G ) ∞ ,   [ l   r  N e  r e v n  ) ∞ ,   [ ]   ,   [ n C n  + R N Θ     G  r  h   W   e B   χ e  r e v n   e l h c  r   c   h   r  g  L e   g u j n  C ( cid:  ) θ ( cid:  ) ) θ ( g ) x ( φ e r u   e  X n      e   N . n × n R n   e c  r    e   n ﬁ e    e  e v      p f  e n  c e h    n C .  n    n e    n n  x e l p       l  b  b  r p e h   e   n e  n      .  Estimation   Density Estimation many statistical problems challenge estimate parameters in- terest . instance , context exponential families , may want estimate parameter ˆθ close “ true ” parameter θ∗ distribution . problem fully general , describe relevant steps obtaining estimates special case exponential family . done two reasons — ﬁrstly , exponential families important special case encounter slightly complex variants reasoning later chapters book . Secondly , suﬃ- ciently simple form able show range diﬀerent techniques . advanced applications small subset methods may practically feasible . Hence exponential families provide us working example based compare consequences number diﬀerent techniques .  . .  Maximum Likelihood Estimation Whenever distribution p ( x ; θ ) parametrized parameter θ may use data ﬁnd value θ maximizes likelihood data would generated distribution choice parameter . instance , assume observe set temperature measurements X = { x  , . . . , xm } . case , could try ﬁnding normal distribution likelihood p ( X ; θ ) data assumption normal distribution maximized . Note imply way temperature measurements actually drawn normal distribution . Instead , means attempting ﬁnd Gaussian ﬁts data best fashion . distinction may appear subtle , critical : assume model accurately reﬂects reality . Instead , simply try best possible job modeling data given speciﬁed model class . Later encounter alternative approaches estimation , namely Bayesian methods , make assumption model ought able describe data accurately . Deﬁnition  .   ( Maximum Likelihood Estimator ) model p ( · ; θ ) parametrized θ observations X maximum likelihood estimator ( MLE ) ˆθML [ X ] : = argmax θ p ( X ; θ ) . (  .   )  .  Estimation    context exponential families leads following procedure : given observations drawn iid distribution , express joint likelihood p ( X ; θ ) =  ( cid:   ) i=  p ( xi ; θ ) =  ( cid:   ) i=  exp ( ( cid:    ) φ ( xi ) , θ ( cid:    ) − g ( θ ) ) = exp ( ( ( cid:    ) µ [ X ] , θ ( cid:    ) − g ( θ ) ) ) µ [ X ] : =     ( cid:   ) i=  φ ( xi ) . (  .   ) (  .   ) (  .   ) µ [ X ] empirical average map φ ( x ) . Maximization p ( X ; θ ) equivalent minimizing negative log-likelihood − log p ( X ; θ ) . latter common practical choice since independently drawn data , product probabilities decomposes sum logarithms individual likelihoods . leads following objective function minimized − log p ( X ; θ ) = [ g ( θ ) − ( cid:    ) θ , µ [ X ] ( cid:    ) ] (  .   ) Since g ( θ ) convex ( cid:    ) θ , µ [ X ] ( cid:    ) linear θ , follows minimization (  .   ) convex optimization problem . Using Theorem  .   ﬁrst order optimality condition ∇θg ( θ ) = µ [ X ] (  .   ) implies θ = [ ∇θg ] −  ( µ [ X ] ) equivalently Ex∼p ( x ; θ ) [ φ ( x ) ] = ∇θg ( θ ) = µ [ X ] . (  .   ) Put another way , conditions state aim ﬁnd distribu- tion p ( x ; θ ) expected value φ ( x ) observed empirically via µ [ X ] . mild technical conditions solution (  .   ) exists . general , (  .   ) solved analytically . certain special cases , though , easily possible . discuss two choices following : Multinomial Poisson distributions . Example  .  ( Poisson Distribution ) Poisson distribution  p ( x ; θ ) =   x ! exp ( θx − eθ ) follows g ( θ ) = eθ φ ( x ) = x . allows   Often Poisson distribution speciﬁed using λ : = log θ rate parameter . case p ( x ; λ ) = λxe−λ/x ! parametrization . advantage natural parametrization using θ directly take advantage properties log-partition function generating cumulants x .      Density Estimation us solve (  .   ) closed form using ∇θg ( θ ) = eθ =     ( cid:   ) i=  xi hence θ = log  ( cid:   ) i=  xi − log . (  .   ) Example  .  ( Multinomial Distribution ) multinomial distri- bution log-partition function given g ( θ ) = log ( cid:   ) N i=  eθi , hence  ∇ig ( θ ) = eθi j=  eθj ( cid:   ) N =     ( cid:   ) j=  { xj = } . (  .   ) easy check (  .   ) satisﬁed eθi = ( cid:   ) j=  { xj = } . words , MLE discrete distribution simply given empirical frequencies occurrence . multinomial setting also exhibits two rather important aspects ex- ponential families : ﬁrstly , choosing θi = c + log ( cid:   ) i=  { xj = } c ∈ R lead equivalent distribution . case since suﬃcient statistic φ ( x ) minimal . context means coordinates φ ( x ) linearly dependent — x ( cid:   ) j [ φ ( x ) ] j =   , hence could eliminate one dimension . precisely additional degree freedom reﬂected scaling freedom θ . ( cid:    ) j=  { xj = } Secondly , data events occur , expression ( cid:    ) ( cid:   ) = log   ill deﬁned . due fact log particular set counts occurs boundary convex set within natural parameters θ well deﬁned . see diﬀerent types priors alleviate issue . Using MLE without problems . saw Figure  .  , conver- gence slow , since using side information . latter provide us problems numerically better conditioned show better convergence , provided assumptions ac- curate . discussing Bayesian approach estimation , let us discuss basic statistical properties estimator .  . .  Bias , Variance Consistency designing estimator ˆθ ( X ) would like obtain number desirable properties : general biased towards particular solution unless good reason believe solution preferred . Instead , would like estimator recover , least  .  Estimation    average , “ correct ” parameter , exist . formalized notion unbiased estimator . Secondly , would like , even correct parameter found , e.g . trying ﬁt Gaussian distribution data normally distributed , converge best possible parameter choice obtain data . understood consistency . Finally , would like estimator achieve low bias near-optimal estimates quickly possible . latter measured eﬃciency estimator . context encounter Cram´er-Rao bound controls best possible rate estimator achieve goal . Figure  .   gives pictorial description . Fig .  .   . Left : unbiased estimator ; estimates , denoted circles mean true parameter , denoted star . Middle : consistent estimator . true model within class consider ( denoted ellipsoid ) , estimates converge white star best model within class approximates true model , denoted solid star . Right : diﬀerent estimators diﬀerent regions uncertainty , made explicit ellipses around true parameter ( solid star ) . Deﬁnition  .   ( Unbiased Estimator ) estimator ˆθ [ X ] unbiased θ X ∼ p ( X ; θ ) EX [ ˆθ [ X ] ] = θ . words , expectation parameter estimate matches true pa- rameter . Note makes sense true parameter actually exists . instance , data Poisson distributed attempt modeling Gaussian obviously obtain unbiased estimates . ﬁnite sample sizes MLE often biased . instance , normal distribution variance estimates carry bias ( m−  ) . See problem  .   details . general , fairly mild conditions , MLE asymptotically unbiased [ DGL   ] . prove exponential families . general settings proof depends dimensionality smoothness family densities disposition .      Density Estimation Theorem  .   ( MLE Exponential Families ) Assume X m-sample drawn iid p ( x ; θ ) . estimate ˆθ [ X ] = g−  ( µ [ X ] ) asymp- totically normal m−     [ ˆθ [ X ] − θ ] → N (   , ( cid:  ) ∇  θg ( θ ) ( cid:  ) −  ) . (  .   ) words , estimate ˆθ [ X ] asymptotically normal , converges true parameter θ , moreover , variance correct parameter given inverse covariance matrix data , given second derivative log-partition function ∇  Proof Denote µ = ∇θg ( θ ) true mean . Moreover , note ∇  θg ( θ ) covariance data drawn p ( x ; θ ) . central limit theorem ( Theorem  .  ) n−     [ µ [ X ] − µ ] → N (   , ∇  θg ( θ ) . θg ( θ ) ) . note ˆθ [ X ] = [ ∇θg ] −  ( µ [ X ] ) . Therefore , delta method ( Theorem  .  ) know ˆθ [ X ] also asymptotically normal . Moreover , inverse function theorem Jacobian g−  satisﬁes ∇µ [ ∇θg ] −  ( µ ) = θg ( θ ) ( cid:  ) −  . Applying Slutsky ’ theorem ( Theorem  .  ) proves claim . ( cid:  ) ∇  established asymptotic properties MLE exponen- tial families natural ask much variation one may expect ˆθ [ X ] performing estimation . Cramer-Rao bound governs . Theorem  .   ( Cram´er Rao [ Rao   ] ) Assume X drawn p ( X ; θ ) let ˆθ [ X ] asymptotically unbiased estimator . Denote Fisher information matrix B variance ˆθ [ X ] : = Cov [ ∇θ log p ( X ; θ ) ] B : = Var ( cid:    ) ˆθ [ X ] ( cid:    ) . (  .   ) case det IB ≥   estimators ˆθ [ X ] . Proof prove claim scalar case . extension matrices straightforward . Using Cauchy-Schwarz inequality Cov  ( cid:    ) ∇θ log p ( X ; θ ) , ˆθ [ X ] ( cid:    ) ≤ Var [ ∇θ log p ( X ; θ ) ] Var ( cid:    ) ˆθ [ X ] ( cid:    ) = IB . (  .   ) Note true parameter expected log-likelihood score vanishes EX [ ∇θ log p ( X ; θ ) ] = ∇θ ( cid:   ) p ( X ; θ ) dX = ∇θ  =   . (  .   )  .  Estimation    Hence may simplify covariance formula dropping means via Cov ( cid:    ) ( cid:    ) ∇θ log p ( X ; θ ) , ˆθ [ X ] ( cid:    ) ( cid:    ) ∇θ log p ( X ; θ ) ˆθ [ X ] = EX ( cid:   ) = = ∇θ p ( X ; θ ) ˆθ ( X ) ∇θ log p ( X ; θ ) dθ ( cid:   ) p ( X ; θ ) ˆθ ( X ) dX = ∇θθ =   . last equality follows since may interchange integration X derivative respect θ . Cram´er-Rao theorem implies limit well may estimate parameter given ﬁnite amounts data . also yardstick may measure eﬃciently estimator uses data . Formally , deﬁne eﬃciency quotient actual performance Cram´er-Rao bound via e : =  /det IB . (  .   ) closer e   , lower variance corresponding estimator ˆθ ( X ) . Theorem  .   implies exponential families MLE asymptot- ically eﬃcient . turns generally true . Theorem  .   ( Eﬃciency MLE [ Cra   , GW   , Ber   ] ) max- imum likelihood estimator asymptotically eﬃcient ( e =   ) . far discussed behavior ˆθ [ X ] whenever exists true θ generating p ( θ ; X ) . true , need settle less : well ˆθ [ X ] approaches best possible choice within given model class . behavior referred consistency . Note possible deﬁne consistency per se . instance , may ask whether ˆθ converges optimal parameter θ∗ , whether p ( x ; ˆθ ) converges optimal density p ( x ; θ∗ ) , respect norm . fairly general conditions turns true ﬁnite-dimensional parameters smoothly parametrized densities . See [ DGL   , vdG   ] proofs details .  . .  Bayesian Approach analysis Maximum Likelihood method might suggest in- ference solved problem . , limit , MLE unbiased exhibits small variance possible . Empirical results using ﬁnite amount data , present Figure  .  indicate otherwise . making assumptions lead interesting general      Density Estimation theorems , ignores fact practice almost always idea expect solution . would foolish ignore additional information . instance , trying determine voltage battery , reasonable expect measurement order  . V less . Consequently prior knowledge incorporated estimation process . fact , use side information guide estimation turns tool building estimators work well high dimensions . Recall Bayes ’ rule (  .   ) states p ( θ|x ) = p ( x|θ ) p ( θ ) . con- text means interested posterior probability θ assuming particular value , may obtain using likelihood ( often referred evidence ) x generated θ via p ( x|θ ) prior belief p ( θ ) θ might chosen distribution generating x . Observe subtle important diﬀerence MLE : instead treating θ parameter density model , treat θ unobserved random variable may attempt infer given observations X . p ( x ) done number diﬀerent purposes : might want infer likely value parameter given posterior distribution p ( θ|X ) . achieved ˆθMAP ( X ) : = argmax − log p ( X|θ ) − log p ( θ ) . p ( θ|X ) = argmin (  .   ) θ θ second equality follows since p ( X ) depend θ . estimator also referred Maximum Posteriori , MAP estimator . diﬀers maximum likelihood estimator adding negative log-prior optimization problem . reason sometimes also referred Penalized MLE . Eﬀectively penalizing unlikely choices θ via − log p ( θ ) . Note using ˆθMAP ( X ) parameter choice quite accurate . , infer distribution θ general guarantee posterior indeed concentrated around mode . accurate treatment use distribution p ( θ|X ) directly via ( cid:   ) p ( x|X ) = p ( x|θ ) p ( θ|X ) dθ . (  .   ) words , integrate unknown parameter θ obtain density estimate directly . see , generally impossible solve (  .   ) exactly , important exception conjugate priors . cases one may resort sampling posterior distribution approx- imate integral . possible design wide variety prior distributions , book  .  Estimation    focuses two important families : norm-constrained prior conjugate priors . encounter throughout , former sometimes guise regularization Gaussian Processes , latter context exchangeable models Dirichlet Process . Norm-constrained priors take form p ( θ ) ∝ exp ( −λ ( cid:    ) θ − θ  ( cid:    ) p ) p , ≥   λ >   . (  .   ) , restrict deviation parameter value θ guess θ  . intuition extreme values θ much less likely moderate choices θ lead smooth even distributions p ( x|θ ) . popular choice Gaussian prior obtain p = =   λ =  / σ  . Typically one sets θ  =   case . Note (  .   ) spell normalization p ( θ ) — context MAP estimation needed since simply becomes constant oﬀset optimization problem (  .   ) . ˆθMAP [ X ] = argmin θ [ g ( θ ) − ( cid:    ) θ , µ [ X ] ( cid:    ) ] + λ ( cid:    ) θ − θ  ( cid:    ) p (  .   ) , p ≥   λ ≥   resulting optimization problem convex unique solution . Moreover , eﬃcient algorithms exist solve problem . discuss detail Chapter   . Figure  .   shows regions equal prior probability range diﬀerent norm-constrained priors . seen diagram , choice norm profound consequences solution . said , show Chapter ? ? , estimate ˆθMAP well concentrated converges optimal solution fairly general conditions . alternative norm-constrained priors conjugate priors . designed posterior p ( θ|X ) functional form prior p ( θ ) . exponential families priors deﬁned via p ( θ|n , ν ) = exp ( ( cid:    ) nν , θ ( cid:    ) − ng ( θ ) − h ( ν , n ) ) ( cid:   ) h ( ν , n ) = log exp ( ( cid:    ) nν , θ ( cid:    ) − ng ( θ ) ) dθ . (  .   ) (  .   ) Note p ( θ|n , ν ) member exponential family feature map φ ( θ ) = ( θ , −g ( θ ) ) . Hence h ( ν , n ) convex ( nν , n ) . Moreover , posterior distribution form p ( θ|X ) ∝ p ( X|θ ) p ( θ|n , ν ) ∝ exp ( ( cid:    ) mµ [ X ] + nν , θ ( cid:    ) − ( + n ) g ( θ ) ) . (  .   )      Density Estimation Fig .  .   . left right : regions equal prior probability R  priors using ( cid:   )   , ( cid:   )   ( cid:   ) ∞ norm . Note ( cid:   )   norm invariant regard coordinate system . shall see later , ( cid:   )   norm prior leads solutions small number coordinates nonzero . m+n , posterior distribution form conjugate prior parameters mµ [ X ] +nν + n. words , n acts like phantom sample size ν corresponding mean parameter . interpreta- tion reasonable given desire design prior , combined likelihood remains model class : treat prior knowl- edge observed virtual data beforehand added actual set observations . sense data prior become completely equivalent — obtain knowledge either actual observations virtual observations describe belief data gen- eration process supposed behave . Eq . (  .   ) added beneﬁt allowing us provide exact nor- malized version posterior . Using (  .   ) obtain p ( θ|X ) = exp ( cid:   ) ( cid:    ) mµ [ X ] + nν , θ ( cid:    ) − ( + n ) g ( θ ) − h ( cid:   ) mµ [ X ] +nν m+n , + n ( cid:   ) ( cid:   ) . main remaining challenge compute normalization h range important conjugate distributions . table following page pro- vides details . Besides attractive algebraic properties , conjugate priors also second advantage — integral (  .   ) solved exactly : ( cid:   ) p ( x|X ) = exp ( ( cid:    ) φ ( x ) , θ ( cid:    ) − g ( θ ) ) × ( cid:   ) exp ( cid:    ) mµ [ X ] + nν , θ ( cid:    ) − ( + n ) g ( θ ) − h ( cid:   ) mµ [ X ] +nν m+n , + n ( cid:   ) ( cid:   ) dθ Combining terms one may check integrand amounts normal-  .  Estimation    ization conjugate distribution , albeit φ ( x ) added . yields p ( x|X ) = exp ( cid:   ) h ( cid:   ) mµ [ X ] +nν+φ ( x ) m+n+  , + n +   ( cid:   ) − h ( cid:   ) mµ [ X ] +nν m+n ( cid:   ) ( cid:   ) , + n expansion useful whenever would like draw x p ( x|X ) without need obtain instantiation latent variable θ . provide explicit expansions appendix   . [ GS   ] use fact θ integrated obtain called collapsed Gibbs sampler topic models [ BNJ   ] .  . .  Example Assume would like build language model based available doc- uments . instance , linguist might interested estimating fre- quency words Shakespeare ’ collected works , one might want compare change respect collection webpages . mod- els describing documents treating bags words obtained independently exceedingly simple , valuable quick-and-dirty content ﬁltering categorization , e.g . spam ﬁlter mail server content ﬁlter webpages . Hence model document multinomial distribution : denote wi ∈ {   , . . . , md } words d. Moreover , denote p ( w|θ ) probability occurrence word w , assumption words independently drawn , md ( cid:   ) p ( d|θ ) = p ( wi|θ ) . (  .   ) i=  goal ﬁnd parameters θ p ( d|θ ) accurate . given collection documents denote mw number counts word w entire collection . Moreover , denote total number words entire collection . case p ( D|θ ) = p ( di|θ ) = ( cid:   )  ( cid:   ) w p ( w|θ ) mw . (  .   ) Finding suitable parameters θ given proceeds follows : maximum likelihood model set p ( w|θ ) = mw  . (  .   ) words , use empirical frequency occurrence best guess suﬃcient statistic φ ( w ) = ew , ew denotes unit vector nonzero “ coordinate ” w. Hence µ [ ] w = mw .      Density Estimation know conjugate prior multinomial model Dirichlet model . follows (  .   ) posterior mode obtained replacing µ [ ] mµ [ ] +nν . Denote nw : = νw · n pseudo-counts arising conjugate prior parameters ( ν , n ) . case estimate probability word w m+n p ( w|θ ) = mw + nw + n = mw + nνw + n . (  .   ) words , add pseudo counts nw actual word counts mw . particularly useful document dealing brief , , whenever little data : quite unreasonable infer webpage approximately      words words occurring page zero probability . exactly mitigated means conjugate prior ( ν , n ) . Finally , let us consider norm-constrained priors form (  .   ) . case , integral required ( cid:   ) ( cid:   ) p ( ) = ∝ p ( D|θ ) p ( θ ) dθ ( cid:   ) exp −λ ( cid:    ) θ − θ  ( cid:    ) p + ( cid:    ) µ [ ] , θ ( cid:    ) − mg ( θ ) ( cid:   ) dθ intractable need resort approximation . popular choice replace integral p ( D|θ∗ ) θ∗ maximizes integrand . precisely MAP approximation (  .   ) . Hence , order perform estimation need solve minimize θ g ( θ ) − ( cid:    ) µ [ ] , θ ( cid:    ) + λ  ( cid:    ) θ − θ  ( cid:    ) p . (  .   ) simple strategy minimizing (  .   ) gradient descent . given value θ compute gradient objective function take ﬁxed step towards minimum . simplicity assume = p =   λ =  / σ  , , assume θ normally distributed variance σ  mean θ  . gradient given ∇θ [ − log p ( , θ ) ] = Ex∼p ( x|θ ) [ φ ( x ) ] − µ [ ] +   mσ  [ θ − θ  ] (  .   ) words , depends discrepancy mean φ ( x ) respect current model empirical average µ [ X ] , diﬀerence θ prior mean θ  . Unfortunately , convergence procedure θ ← θ − η∇θ [ . . . ] usually slow , even adjust steplength η eﬃciently . reason gradient need point towards minimum space likely  .  Sampling    distorted . better strategy use Newton ’ method ( see Chapter   detailed discussion convergence proof ) . relies second order Taylor approximation − log p ( , θ + δ ) ≈ − log p ( , θ ) + ( cid:    ) δ , G ( cid:    ) + δ ( cid:   ) Hδ     (  .   ) G H ﬁrst second derivatives − log p ( , θ ) respect θ . quadratic expression minimized respect δ choosing δ = −H − G fashion update algorithm letting θ ← θ −H − G . One may show ( see Chapter   ) Algorithm  .  quadratically convergent . Note prior θ ensures H well conditioned even case variance φ ( x ) . practice means prior ensures fast convergence optimization algorithm . Algorithm  .  Newton method MAP estimation NewtonMAP ( ) Initialize θ = θ  converged Compute G = Ex∼p ( x|θ ) [ φ ( x ) ] − µ [ ] +   Compute H = Varx∼p ( x|θ ) [ φ ( x ) ] +   Update θ ← θ − H − G mσ    mσ  [ θ − θ  ] end return θ  .  Sampling far considered problem estimating underlying probability density , given set samples drawn density . let us turn converse problem , , generate random variables given underlying probability density . words , want design random variable generator . useful number reasons : may encounter probability distributions optimization suit- able model parameters essentially impossible equally im- possible obtain closed form expression distribution . cases may still possible perform sampling draw examples kind data expect see model . Chapter ? ? discusses number graphical models problem arises . Secondly , assume interested testing performance network router diﬀerent load conditions . Instead introducing under-development router live network wreaking havoc , one could      Density Estimation estimate probability density network traﬃc various load conditions build model . behavior network simulated using probabilistic model . involves drawing random variables estimated probability distribution . Carrying , suppose generate data packets sampling see anomalous behavior router . order reproduce debug problem one needs access set random packets caused problem ﬁrst place . words , often convenient random variable generator reproducible ; ﬁrst blush seems like contradiction . , random number generator supposed generate random variables . less contradiction consider random numbers generated computer — given particular initialization ( typically depends state system , e.g . time , disk size , bios checksum , etc . ) random number algorithm produces sequence numbers , practical purposes , treated iid . simple method linear congruential generator [ PTVF   ] xi+  = ( axi + b ) mod c . performance iterations depends signiﬁcantly choice constants , b , c. instance , GNU C compiler uses =            , b =       c =     . general b c need relatively prime −   needs divisible prime factors c   . much advisable attempt implementing generators one ’ unless absolutely necessary . Useful desiderata pseudo random number generator ( PRNG ) practical purposes statistically indistinguishable sequence iid data . , applying number statistical tests , accept null-hypothesis random variables iid . See Chapter ? ? detailed discussion statistical testing procedures random variables . following assume access uniform RNG U [   ,   ] draws random numbers uniformly range [   ,   ] .  . .  Inverse Transformation consider scenario would like draw dis- tinctively non-uniform distribution . Whenever latter relatively simple achieved applying inverse transform : Theorem  .   z ∼ p ( z ) z ∈ Z injective transformation φ : Z → X inverse transform φ−  φ ( Z ) follows random  .  Sampling    Discrete Probability Distribution Cumulative Density Function  .   .   .                 .   .   .   .                Fig .  .   . Left : discrete probability distribution   possible outcomes . Right : associated cumulative distribution function . sampling , draw x uniformly random U [   ,   ] compute inverse F . variable x : = φ ( z ) drawn ( cid:   ) denotes determinant Jacobian φ−  . ( cid:   ) ∇xφ−  ( x ) ( cid:   ) ( cid:   ) · p ( φ−  ( x ) ) . ( cid:   ) ( cid:   ) ∇xφ−  ( x ) ( cid:   ) ( cid:   ) follows immediately applying variable transformation mea- sure , i.e . change dp ( z ) dp ( φ−  ( x ) ) ( cid:   ) ( cid:   ) . conversion strat- egy particularly useful univariate distributions . ( cid:   ) ∇xφ−  ( x ) ( cid:   ) Corollary  .   Denote p ( x ) distribution R cumulative distri- bution function F ( x ( cid:   ) ) = ( cid:   ) x ( cid:   ) −∞ dp ( x ) . transformation x = φ ( z ) = F −  ( z ) converts samples z ∼ U [   ,   ] samples drawn p ( x ) . apply strategy number univariate distributions . One common cases sampling discrete distribution . Example  .  ( Discrete Distribution ) case discrete distribu- tion {   , . . . , k } cumulative distribution function step-function steps {   , . . . , k } height step given corre- sponding probability event . implementation works follows : denote p ∈ [   ,   ] k vector probabilities denote f ∈ [   ,   ] k fi = fi−  + pi f  = p  steps cumulative distribution function . random variable z drawn U [   ,   ] obtain x = φ ( z ) : = argmini { fi ≥ z } . See Figure  .   example distribution   events .       .   .   .   .  Exponential Distribution Cumulative Distribution Function   Density Estimation    .   .   .   .                                Fig .  .   . Left : Exponential distribution λ =   . Right : associated cumulative distribution function . sampling , draw x uniformly random U [   ,   ] compute inverse . Example  .  ( Exponential Distribution ) density Exponential- distributed random variable given p ( x|λ ) = λ exp ( −λx ) λ >   x ≥   . (  .   ) allows us compute cdf F ( x|λ ) =   − exp ( −λx ) λ >   x ≥   . (  .   ) Therefore generate Exponential random variable draw z ∼ U [   ,   ] solve x = φ ( z ) = F −  ( z|λ ) = −λ−  log (   − z ) . Since z   − z drawn U [   ,   ] simplify x = −λ−  log z . could apply reasoning normal distribution order draw Gaussian random variables . Unfortunately , cumulative distribution function Gaussian available closed form would need resort rather nontrivial numerical techniques . turns exists much elegant algorithm roots Gauss ’ proof normalization constant Normal distribution . technique known Box-M¨uller transform . Example  .   ( Box-M¨uller Transform ) Denote X , independent Gaus- sian random variables zero mean unit variance .   x    √  π   √  π p ( x , ) =   ( x +y  ) (  .   )    π e−   e−   e−     y  =  .  Sampling    Fig .  .   . Red : true density standard normal distribution ( red line ) con- trasted histogram   ,    random variables generated Box-M¨uller transform . key observation joint distribution p ( x , ) radially symmet- ric , i.e . depends radius r  = x  + y  . Hence may perform variable substitution polar coordinates via map φ x = r cos θ = r sin θ hence ( x , ) = φ−  ( r , θ ) . (  .   ) allows us express density terms ( r , θ ) via p ( r , θ ) = p ( φ−  ( r , θ ) ) ( cid:   ) ( cid:   ) ∇r , θφ−  ( r , θ ) ( cid:   ) ( cid:   ) =    π e−     r  ( cid:   ) ( cid:   ) ( cid:   ) ( cid:   ) ( cid:   ) cos θ −r sin θ sin θ r cos θ ( cid:   ) ( cid:   ) ( cid:   ) ( cid:   ) ( cid:   ) = r  π e−     r  . fact p ( r , θ ) constant θ means easily sample θ ∈ [   ,  π ] drawing random variable , say zθ U [   ,   ] rescaling  π . obtain sampler r need compute cumulative distribution function p ( r ) = re−     r  : F ( r ( cid:   ) ) = ( cid:   ) r ( cid:   )   re−     r  dr =   − e−     r ( cid:   )   hence r = F −  ( z ) = ( cid:    ) −  log (   − z ) . (  .   ) Observing z ∼ U [   ,   ] implies   − z ∼ U [   ,   ] yields following sampler : draw zθ , zr ∼ U [   ,   ] compute x x = ( cid:    ) −  log zr cos  πzθ = ( cid:    ) −  log zr sin  πzθ . Note Box-M¨uller transform yields two independent Gaussian ran- dom variables . See Figure  .   example sampler .            .   .   .   .   .   .   .   .   .   .        Density Estimation Example  .   ( Uniform distribution disc ) similar strategy employed sampling unit disc . case closed- form expression distribution simply given p ( x , ) = ( cid:   )   π   x  + y  ≤   otherwise Using variable transform (  .   ) yields p ( r , θ ) = p ( φ−  ( r , θ ) ) ( cid:   ) ( cid:   ) ∇r , θφ−  ( r , θ ) ( cid:   ) ( cid:   ) = ( cid:   ) r π   r ≤   otherwise (  .   ) (  .   ) Integrating θ yields p ( r ) =  r r ∈ [   ,   ] corresponding CDF F ( r ) = r  r ∈ [   ,   ] . Hence sampler draws zr , zθ ∼ U [   ,   ] √ computes x = zr cos  πzθ = zr sin  πzθ . √  . .  Rejection Sampler methods random variable generation looked far re- quire intimate knowledge pdf distribution . describe general purpose method , used generate samples arbitrary distribution . Let us begin sampling set : Example  .   ( Rejection Sampler ) Denote X ⊆ X set let p density X . sampler drawing pX ( x ) ∝ p ( x ) x ∈ X pX ( x ) =   x ( cid:   ) ∈ X , , pX ( x ) = p ( x|x ∈ X ) obtained procedure : repeat draw x ∼ p ( x ) x ∈ X return x , algorithm keeps drawing p random variable contained X . probability occurs clearly p ( X ) . Hence larger p ( X ) higher eﬃciency sampler . See Figure  .   . Example  .   ( Uniform distribution disc ) procedure works trivially follows : draw x , ∼ U [   ,   ] . Accept (  x −   )   + (  y −   )   ≤   return sample (  x −   ,  y −   ) . sampler eﬃciency   π since surface ratio unit square unit ball . Note time need carry sophisticated measure  .  Sampling    Fig .  .   . Rejection sampler . Left : samples drawn uniform distribution [   ,   ]   . Middle : samples drawn uniform distribution unit disc points grey shaded area . Right : procedure allows us sample uniformly arbitrary sets . Fig .  .   . Accept reject sampling Beta (   ,   ) distribution . Left : Samples generated uniformly blue rectangle ( shaded area ) . samples fall red curve Beta (   ,   ) distribution ( darkly shaded area ) accepted . Right : true density Beta (   ,   ) distribution ( red line ) contrasted histogram   ,    samples drawn rejection sampler . transform . mathematical convenience came expense slightly less eﬃcient sampler —    % samples rejected . reasoning used obtain hard accept/reject procedure used considerably sophisticated rejection sampler . basic idea , given distribution p ﬁnd another distribution q , rescaling , becomes upper envelope p , use q sample reject depending ratio q p . Theorem  .   ( Rejection Sampler ) Denote p q distributions X let c constant cq ( x ) ≥ p ( x ) x ∈ X .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .       Density Estimation algorithm draws p acceptance probability c−  . repeat draw x ∼ q ( x ) ∼ U [   ,   ] ct ≤ p ( x ) q ( x ) return x Proof Denote Z event sample drawn q accepted . Bayes rule probability Pr ( x|Z ) written follows Pr ( Z|x ) Pr ( x ) Pr ( Z ) used Pr ( Z ) = ( cid:   ) Pr ( Z|x ) q ( x ) dx = ( cid:   ) c− p ( x ) dx = c−  . Pr ( x|Z ) = = p ( x ) = p ( x ) cq ( x ) · q ( x ) c−  (  .   ) Note algorithm Example  .   special case rejection sampler — majorize pX uniform distribution rescaled   p ( X ) . Example  .   ( Beta distribution ) Recall Beta ( , b ) distribution , member Exponential Family suﬃcient statistics ( log x , log (  − x ) ) , given p ( x|a , b ) = Γ ( + b ) Γ ( ) Γ ( b ) xa−  (   − x ) b−  , (  .   ) given ( , b ) one verify ( problem  .   ) : = argmax p ( x|a , b ) = x −   + b −   . (  .   ) provided >   . Hence , use proposal distribution uniform distri- bution U [   ,   ] scaling factor c = p ( |a , b ) may apply Theorem  .   . illustrated Figure  .   , generate sample Beta ( , b ) ﬁrst generate pair ( x , ) , uniformly random shaded rectangle . sample retained ct ≤ p ( x|a , b ) , rejected otherwise . acceptance rate sampler   c . Example  .   ( Normal distribution ) may use Laplace distri- bution generate samples Normal distribution . , use q ( x|λ ) = λ   e−λ|x| (  .   ) proposal distribution . normal distribution p = N (   ,   ) zero  .  Sampling    mean unit variance turns choosing λ =   yields eﬃcient sampling scheme ( see Problem  .   ) p ( x ) ≤ ( cid:    )  e π q ( x|λ =   ) illustrated Figure  .   , ﬁrst generate x ∼ q ( x|λ =   ) using inverse transform method ( see Example  .  Problem  .   ) ∼ U [   ,   ] . ≤ ( cid:    )  e/πp ( x ) accept x , otherwise reject . eﬃciency scheme ( cid:    ) π  e . ( cid:    )  e π g ( x|  ,   ) p ( x )  .   .   .    −  −        Fig .  .   . Rejection sampling Normal distribution ( red curve ) . Samples generated uniformly Laplace distribution rescaled ( cid:    )  e/π . samples fall red curve standard normal distribution ( darkly shaded area ) accepted . rejection sampling fairly eﬃcient low dimensions eﬃciency unsatisfactory high dimensions . leads us instance curse dimensionality [ Bel   ] : pdf d-dimensional Gaussian random variable centered   variance σ    given p ( x|σ  ) = (  π ) −   σ−de−    σ  ( cid:    ) x ( cid:    )   suppose want draw p ( x|σ  ) sampling another Gaussian q slightly larger variance ρ  > σ  . case ratio distributions maximized   yields c = q (  |σ  ) p (  |ρ  ) = ( cid:    ) ( cid:    ) ρ σ      Density Estimation suppose ρ σ =  .   , =      , ﬁnd c ≈       . words , need generate approximately   ,    samples average q draw single sample p. discuss sophisticated sampling algorithms , namely Gibbs Sampling , Section ? ? . allows us draw rather nontrivial distributions long distributions small subsets random variables simple enough tackled directly . Problems Problem  .  ( Bias Variance Decomposition {   } ) Prove vari- ance VarX [ x ] random variable written EX [ x  ] − EX [ x ]   . Problem  .  ( Moment Generating Function {   } ) Prove char- acteristic function used generate moments given (  .   ) . Hint : use Taylor expansion exponential apply diﬀerential oper- ator expectation . Problem  .  ( Cumulative Error Function {   } ) erf ( x ) = ( cid:    )  /π ( cid:   ) x   e−x  dx . (  .   ) Problem  .  ( Weak Law Large Numbers {   } ) analogy proof central limit theorem prove weak law large numbers . Hint : use ﬁrst order Taylor expansion eiωt =   + iωt + ( ) compute approx- imation characteristic function . Next compute limit → ∞ φ ¯Xm . Finally , apply inverse Fourier transform associate constant distribution mean µ . Problem  .  ( Rates conﬁdence bounds {   } ) Show rate hoeﬀding tight — get bound central limit theorem compare hoeﬀding rate . Problem  .  ’ use chip wafer random variable ? Give counterexample . Give bounds actually allowed . Problem  .  ( Union Bound ) Work many bounds time . logarithmic penalty . Problem  .  ( Randomized Rounding {   } ) Solve linear system equations Ax = b integral x .  .  Sampling    Problem  .  ( Randomized Projections {   } ) Prove random- ized projections converge . Problem  .   ( Count-Min Sketch {   } ) Prove projection trick Problem  .   ( Parzen windows triangle kernels {   } ) Suppose given following data : X = {   ,   ,   ,   ,   } . Plot estimated den- sity using kernel density estimator following kernel : k ( u ) = ( cid:   )  .  −  .   ∗ |u| |u| ≤     otherwise . Problem  .   Gaussian process link Gaussian prior natural pa- rameters Problem  .   Optimization Gaussian regularization Problem  .   Conjugate prior ( student-t wishart ) . Problem  .   ( Multivariate Gaussian {   } ) Prove Σ ( cid:   )   nec- essary suﬃcient condition normal distribution well deﬁned . Problem  .   ( Discrete Exponential Distribution {   } ) φ ( x ) = x uniform measure . Problem  .   Exponential random graphs . Problem  .   ( Maximum Entropy Distribution ) Show exponen- tial families arise solution maximum entropy estimation prob- lem . Problem  .   ( Maximum Likelihood Estimates Normal Distributions ) Derive maximum likelihood estimates normal distribution , , show result ˆµ =     ( cid:   ) i=  xi ˆσ  =     ( cid:   ) i=  ( xi − ˆµ )   (  .   ) using exponential families parametrization . Next show mean estimate ˆµ unbiased , variance estimate slight bias (   ) . see , take expectation respect ˆσ  .      Density Estimation Problem  .   ( cdf Logistic random variable {   } ) Show cdf Logistic random variable ( ? ? ) given ( ? ? ) . Problem  .   ( Double-exponential ( Laplace ) distribution {   } ) Use inverse-transform method generate sample double-exponential ( Laplace ) distribution (  .   ) . Problem  .   ( Normal random variables polar coordinates {   } ) X  X  standard normal random variables let ( R , θ ) de- note polar coordinates pair ( X  , X  ) . Show R  ∼ χ    θ ∼ Unif [   ,  π ] . Problem  .   ( Monotonically increasing mappings {   } ) mapping : R → R one-to-one , , monotonically increasing , , x > implies ( x ) > ( ) . Problem  .   ( Monotonically increasing multi-maps {   } ) Let : Rn → Rn one-to-one . X ∼ pX ( x ) , show distribution pY ( ) = ( X ) obtained via ( ? ? ) . Problem  .   ( Argmax Beta ( , b ) distribution {   } ) Show mode Beta ( , b ) distribution given (  .   ) . Problem  .   ( Accept reject sampling unit disk {   } ) Give least TWO diﬀerent accept-reject based sampling schemes generate sam- ples uniformly random unit disk . Compute eﬃciency . Problem  .   ( Optimizing Laplace Standard Normal {   } ) Optimize ratio p ( x ) /g ( x|µ , σ ) , respect µ σ , p ( x ) standard normal distribution ( ? ? ) , g ( x|µ , σ ) Laplace distribution (  .   ) . Problem  .   ( Normal Random Variable Generation {   } ) aim problem write code generate standard normal random vari- ables ( ? ? ) using diﬀerent methods . generate U ∼ Unif [   ,   ] apply ( ) Box-Muller transformation outlined Section ? ? . ( ii ) use following approximation inverse CDF Φ−  ( α ) ≈ − a  + a t   + b t + b t  , (  .   )  .  Sampling t  = log ( α−  )    a  =  .      , a  =  .      , b  =  .      , b  =  .      ( iii ) use method outlined example  .   . Plot histogram samples generated conﬁrm nor- mally distributed . Compare diﬀerent methods terms time needed generate      random variables . Problem  .   ( Non-standard Normal random variables {   } ) Describe scheme based Box-Muller transform generate dimensional nor- mal random variables p ( x|  , ) . used generate arbitrary normal random variables p ( x|µ , Σ ) . Problem  .   ( Uniform samples disk {   } ) Show ideas described Section ? ? generalized draw samples uniformly ran- dom axis parallel ellipse : { ( x , ) : x    a  + x    b  ≤   } .   Optimization Optimization plays increasingly important role machine learning . instance , many machine learning algorithms minimize regularized risk functional : min f J ( f ) : = λΩ ( f ) + Remp ( f ) , empirical risk Remp ( f ) : =     ( cid:   ) i=  l ( f ( xi ) , yi ) . (  .  ) (  .  ) xi training instances yi corresponding labels . l loss function measures discrepancy predictions f ( xi ) . Finding optimal f involves solving optimization problem . chapter provides self-contained overview basic concepts tools optimization , especially geared towards solving machine learning problems . terms concepts , cover topics related convexity , duality , Lagrange multipliers . terms tools , cover variety optimization algorithms including gradient descent , stochastic gradient descent , Newton ’ method , Quasi-Newton methods . also look specialized algorithms tailored towards solving Linear Programming Quadratic Programming problems often arise machine learning problems .  .  Preliminaries Minimizing arbitrary function , general , diﬃcult , ob- jective function minimized convex things become considerably simpler . see shortly , key advantage dealing convex functions local optima also global optima . Therefore , well developed tools exist ﬁnd global minima convex function . Conse- quently , many machine learning algorithms formulated terms convex optimization problems . brieﬂy review concept convex sets functions section .         Optimization  . .  Convex Sets Deﬁnition  .  ( Convex Set ) subset C Rn said convex (   − λ ) x + λy ∈ C whenever x ∈ C , ∈ C   < λ <   . Intuitively , means line joining two points x set C lies inside C ( see Figure  .  ) . easy see ( Exercise  .  ) intersections convex sets also convex . Fig .  .  . convex set ( left ) contains line joining two points belong set . non-convex set ( right ) satisfy property . vector sum ( cid:   ) λixi called convex combination λi ≥   ( cid:   ) λi =   . Convex combinations helpful deﬁning convex hull : Deﬁnition  .  ( Convex Hull ) convex hull , conv ( X ) , ﬁnite sub- set X = { x  , . . . , xn } Rn consists convex combinations x  , . . . , xn .  . .  Convex Functions Let f real valued function deﬁned set X ⊂ Rn . set { ( x , µ ) : x ∈ X , µ ∈ R , µ ≥ f ( x ) } (  .  ) called epigraph f . function f deﬁned convex function epigraph convex set Rn+  . equivalent , commonly used , deﬁnition ( Exercise  .  ) follows ( see Figure  .  geometric intuition ) : Deﬁnition  .  ( Convex Function ) function f deﬁned set X called convex , x , x ( cid:   ) ∈ X   < λ <   λx + (   − λ ) x ( cid:   ) ∈ X , f ( λx + (   − λ ) x ( cid:   ) ) ≤ λf ( x ) + (   − λ ) f ( x ( cid:   ) ) . (  .  )  .  Preliminaries function f called strictly convex f ( λx + (   − λ ) x ( cid:   ) ) < λf ( x ) + (   − λ ) f ( x ( cid:   ) ) whenever x ( cid:   ) = x ( cid:   ) .    (  .  ) fact , deﬁnition extended show f convex function λi ≥   ( cid:   ) ( cid:   ) λi =   ( cid:   ) ( cid:   ) f ( cid:   ) λixi ≤ λif ( xi ) . (  .  ) inequality called Jensen ’ inequality ( problem ) .   Fig .  .  . convex function ( left ) satisﬁes (  .  ) ; shaded region denotes epi- graph . nonconvex function ( right ) satisfy (  .  ) . f : X → R diﬀerentiable , f convex , , f ( x ( cid:   ) ) ≥ f ( x ) + ( cid:   ) x ( cid:   ) − x , ∇f ( x ) ( cid:   ) x , x ( cid:   ) ∈ X . (  .  ) words , ﬁrst order Taylor approximation lower bounds convex function universally ( see Figure  .  ) . rest chapter ( cid:    ) x , ( cid:    ) denotes Euclidean dot product vectors x , , ( cid:    ) x , ( cid:    ) : = ( cid:   )  xiyi . (  .  ) f twice diﬀerentiable , f convex , , Hessian positive semi-deﬁnite , , ∇ f ( x ) ( cid:   )   . (  .  ) twice diﬀerentiable strictly convex functions , Hessian matrix pos- itive deﬁnite , , ∇ f ( x ) ( cid:   )   . brieﬂy summarize operations preserve convexity :        x                 f ( x )        x .  .  .  .  .  .  . f ( x )      Optimization Addition f  f  convex , f  + f  also convex . Scaling f convex , αf convex α >   . Aﬃne Transform f convex , g ( x ) = f ( Ax + b ) matrix vector b also convex . Adding Linear Function f convex , g ( x ) = f ( x ) + ( cid:    ) , x ( cid:    ) vector also convex . Subtracting Linear Function f convex , g ( x ) = f ( x ) − ( cid:    ) , x ( cid:    ) vector also convex . Pointwise Maximum fi convex , g ( x ) = maxi fi ( x ) also convex . Scalar Composition f ( x ) = h ( g ( x ) ) , f convex ) g convex , h convex , non-decreasing b ) g concave , h convex , non-increasing . Fig .  .  . Left : Convex Function two variables . Right : corresponding convex below-sets { x|f ( x ) ≤ c } , diﬀerent values c. also called contour plot . intimate relation convex functions convex sets . instance , following lemma show sets ( level sets ) convex functions , sets f ( x ) ≤ c , convex . Lemma  .  ( Below-Sets Convex Functions ) Denote f : X → R convex function . set Xc : = { x | x ∈ X f ( x ) ≤ c } , c ∈ R , (  .   ) convex . Proof x , x ( cid:   ) ∈ Xc , f ( x ) , f ( x ( cid:   ) ) ≤ c. Moreover , since f convex , also f ( λx + (   − λ ) x ( cid:   ) ) ≤ λf ( x ) + (   − λ ) f ( x ( cid:   ) ) ≤ c   < λ <   . (  .   ) Hence ,   < λ <   , ( λx + (   − λ ) x ( cid:   ) ) ∈ Xc , proves claim . Figure  .  depicts situation graphically . - - -         - - -                                  - - -         - - -           .  Preliminaries    hinted introduction chapter , minimizing arbitrary function ( possibly even compact ) set arguments diﬃcult task , likely exhibit many local minima . contrast , minimiza- tion convex objective function convex set exhibits exactly one global minimum . prove property . Theorem  .  ( Minima Convex Sets ) convex function f : X → R attains minimum , set x ∈ X , minimum value attained , convex set . Moreover , f strictly convex , set contains single element . Proof Denote c minimum f X . set Xc : = { x|x ∈ X f ( x ) ≤ c } clearly convex . f strictly convex , two distinct x , x ( cid:   ) ∈ Xc   < λ <   f ( λx + (   − λ ) x ( cid:   ) ) < λf ( x ) + (   − λ ) f ( x ( cid:   ) ) = λc + (   − λ ) c = c , contradicts assumption f attains minimum Xc . There- fore Xc must contain single element . following lemma shows , minimum point characterized precisely . Lemma  .  Let f : X → R diﬀerentiable convex function . x minimizer f , , , ( cid:   ) x ( cid:   ) − x , ∇f ( x ) ( cid:   ) ≥   x ( cid:   ) . (  .   ) Proof show forward implication , suppose x optimum (  .   ) hold , , exists x ( cid:   ) ( cid:   ) x ( cid:   ) − x , ∇f ( x ) ( cid:   ) <   . Consider line segment z ( λ ) = (   − λ ) x + λx ( cid:   ) ,   < λ <   . Since X convex , z ( λ ) lies X . hand ,  dλ ( cid:   ) ( cid:   ) f ( z ( λ ) ) ( cid:   ) ( cid:   ) λ=  = ( cid:   ) x ( cid:   ) − x , ∇f ( x ) ( cid:   ) <   , shows small values λ f ( z ( λ ) ) < f ( x ) , thus showing x optimal . reverse implication follows (  .  ) noting f ( x ( cid:   ) ) ≥ f ( x ) , whenever (  .   ) holds .      Optimization One way ensure (  .   ) holds set ∇f ( x ) =   . words , minimizing convex function equivalent ﬁnding x ∇f ( x ) =   . Therefore , ﬁrst order conditions necessary suﬃcient minimizing convex function .  . .  Subgradients far , worked diﬀerentiable convex functions . subgradient generalization gradients appropriate convex functions , including necessarily smooth . Deﬁnition  .  ( Subgradient ) Suppose x point convex func- tion f ﬁnite . subgradient normal vector tangential supporting hyperplane f x . Formally µ called subgradient f x , , f ( x ( cid:   ) ) ≥ f ( x ) + ( cid:   ) x ( cid:   ) − x , µ ( cid:   ) x ( cid:   ) . (  .   ) set subgradients point called subdiﬀerential , de- noted ∂xf ( x ) . set empty f said subdiﬀerentiable x . hand , set singleton , function said diﬀerentiable x . case use ∇f ( x ) denote gradient f . Convex functions subdiﬀerentiable everywhere domain . state simple rules subgradient calculus : Addition ∂x ( f  ( x ) + f  ( x ) ) = ∂xf  ( x ) + ∂xf  ( x ) ∂xαf ( x ) = α∂xf ( x ) , α >   Aﬃne Transform g ( x ) = f ( Ax + b ) matrix vector b , Scaling ∂xg ( x ) = ( cid:   ) ∂yf ( ) . Pointwise Maximum g ( x ) = maxi fi ( x ) ∂g ( x ) = conv ( ∂xfi ( cid:   ) ) ( cid:   ) ∈ argmaxi fi ( x ) . deﬁnition subgradient also understood geometrically . illustrated Figure  .  , diﬀerentiable convex function always lower bounded ﬁrst order Taylor approximation . concept ex- tended non-smooth functions via subgradients , Figure  .  shows . using involved concepts , proof Lemma  .  extended subgradients . case , minimizing convex nonsmooth function en- tails ﬁnding x   ∈ ∂f ( x ) .  .  Preliminaries     . .  Strongly Convex Functions analyzing optimization algorithms , sometimes easier work strongly convex functions , generalize deﬁnition convexity . Deﬁnition  .  ( Strongly Convex Function ) convex function f σ- strongly convex , , exists constant σ >   function f ( x ) − σ   ( cid:    ) x ( cid:    )   convex . constant σ called modulus strong convexity f . f twice diﬀerentiable , equivalent , perhaps easier , deﬁnition strong convexity : f strongly convex exists σ ∇ f ( x ) ( cid:   ) σI . (  .   ) words , smallest eigenvalue Hessian f uniformly lower bounded σ everywhere . important examples strongly con- vex functions include : Example  .  ( Squared Euclidean Norm ) function f ( x ) = λ λ-strongly convex .   ( cid:    ) x ( cid:    )   Example  .  ( Negative Entropy ) Let ∆n = { x s.t . ( cid:   ) n dimensional simplex , f : ∆n → R negative entropy : xi =   xi ≥   } f ( x ) = ( cid:   )  xi log xi . (  .   ) f  -strongly convex respect ( cid:    ) · ( cid:    )   norm simplex ( see Problem  .  ) . f σ-strongly convex function one show following prop- erties ( Exercise  .  ) . x , x ( cid:   ) arbitrary µ ∈ ∂f ( x ) µ ( cid:   ) ∈ ∂f ( x ( cid:   ) ) . f ( x ( cid:   ) ) ≥ f ( x ) + ( cid:   ) x ( cid:   ) − x , µ ( cid:   ) + f ( x ( cid:   ) ) ≤ f ( x ) + ( cid:   ) x ( cid:   ) − x , µ ( cid:   ) + σ ( cid:   ) x ( cid:   ) − x ( cid:   ) ( cid:   )   ( cid:   )     ( cid:   ) µ ( cid:   ) − µ ( cid:   ) ( cid:   )   ( cid:   )  σ ( cid:   ) x − x ( cid:   ) ( cid:   ) ( cid:   ) x − x ( cid:   ) , µ − µ ( cid:   ) ( cid:   ) ≥ σ ( cid:   )   ( cid:   )   ( cid:   ) µ − µ ( cid:   ) ( cid:   ) ( cid:   ) ( cid:   ) x − x ( cid:   ) , µ − µ ( cid:   ) ( cid:   ) ≤   . ( cid:   ) σ (  .   ) (  .   ) (  .   ) (  .   )      Optimization  . .  Convex Functions Lipschitz Continous Gradient somewhat symmetric concept strong convexity Lipschitz conti- nuity gradient . see later connected Fenchel duality . Deﬁnition  .  ( Lipschitz Continuous Gradient ) diﬀerentiable con- vex function f said Lipschitz continuous gradient , exists constant L >   , ( cid:   ) ∇f ( x ) − ∇f ( x ( cid:   ) ) ( cid:   ) ( cid:   ) ( cid:   ) ≤ L ( cid:   ) ( cid:   ) x − x ( cid:   ) ( cid:   ) ( cid:   ) ∀x , x ( cid:   ) . (  .   ) , f twice diﬀerentiable , equivalent , perhaps easier , deﬁnition Lipschitz continuity gradient : f Lipschitz continuous gradient strongly convex exists L LI ( cid:   ) ∇ f ( x ) . (  .   ) words , largest eigenvalue Hessian f uniformly upper bounded L everywhere . f Lipschitz continuous gradient modulus L , one show following properties ( Exercise  .  ) . f ( x ( cid:   ) ) ≤ f ( x ) + ( cid:   ) x ( cid:   ) − x , ∇f ( x ) ( cid:   ) + f ( x ( cid:   ) ) ≥ f ( x ) + ( cid:   ) x ( cid:   ) − x , ∇f ( x ) ( cid:   ) + L ( cid:   ) x − x ( cid:   ) ( cid:   ) ( cid:   )   ( cid:   )     ( cid:   ) ∇f ( x ) − ∇f ( x ( cid:   ) ) ( cid:   ) ( cid:   )   ( cid:   )  L ( cid:   ) x − x ( cid:   ) ( cid:   ) ( cid:   ) x − x ( cid:   ) , ∇f ( x ) − ∇f ( x ( cid:   ) ) ( cid:   ) ≤ L ( cid:   )   ( cid:   )   ( cid:   ) ∇f ( x ) − ∇f ( x ( cid:   ) ) ( cid:   ) ( cid:   ) ( cid:   ) x − x ( cid:   ) , ∇f ( x ) − ∇f ( x ( cid:   ) ) ( cid:   ) ≥   . ( cid:   ) L (  .   ) (  .   ) (  .   ) (  .   )  . .  Fenchel Duality Fenchel conjugate function f given f ∗ ( x∗ ) = sup x { ( cid:    ) x , x∗ ( cid:    ) − f ( x ) } . (  .   ) Even f convex , Fechel conjugate written supremum linear functions always convex . rules computing Fenchel duals summarized Table  . .  . f convex epigraph (  .  ) closed convex set , f ∗∗ = f . f f ∗ convex , satisfy so-called Fenchel-Young inequality f ( x ) + f ∗ ( x∗ ) ≥ ( cid:    ) x , x∗ ( cid:    ) x , x∗ . (  .   )  .  Preliminaries    Fig .  .  . convex function always lower bounded ﬁrst order Taylor ap- proximation . true even function diﬀerentiable ( see Figure  .  ) Fig .  .  . Geometric intuition subgradient . nonsmooth convex function ( solid blue ) subdiﬀerentiable “ kink ” points . illustrate two subgradients ( dashed green red lines ) “ kink ” point tangential function . normal vectors lines subgradients . Observe ﬁrst order Taylor approximations obtained using subgradients lower bounds convex function . inequality becomes equality whenever x∗ ∈ ∂f ( x ) , , f ( x ) + f ∗ ( x∗ ) = ( cid:    ) x , x∗ ( cid:    ) x x∗ ∈ ∂f ( x ) . (  .   ) Strong convexity ( Section  . .  ) Lipschitz continuity gradient                        Optimization Table  .  . Rules computing Fenchel Duals Scalar Addition g ( x ) = f ( x ) + α g∗ ( x∗ ) = f ∗ ( x∗ ) − α . Function Scaling Parameter Scaling α >   g ( x ) = αf ( x ) g∗ ( x∗ ) = αf ∗ ( x∗/α ) . α ( cid:   ) =   g ( x ) = f ( αx ) g∗ ( x∗ ) = f ∗ ( x∗/α ) Linear Transformation invertible matrix ( f ◦A ) ∗ = f ∗ ◦ ( A−  ) ∗ . g ( x ) = f ( x − x  ) g∗ ( x∗ ) = f ∗ ( x∗ ) + ( cid:    ) x∗ , x  ( cid:    ) . g ( x ) = f  ( x ) + f  ( x )   = x∗ } .   + x∗   ( x∗   ( x∗ Pointwise Inﬁmum g ( x ) = inf fi ( x ) g∗ ( x∗ ) = supi f ∗   ) s.t . x∗   ) + f ∗ inf { f ∗ ( x∗ ) . g∗ ( x∗ ) = Shift Sum ( Section  . .  ) related Fenchel duality according following lemma , state without proof . Lemma  .   ( Theorem  . .   . .  [ HUL   ] ) ( ) f σ-strongly convex , f ∗ Lipschitz continuous gradient modulus   σ . ( ii ) f convex Lipschitz continuous gradient modulus L , f ∗   L -strongly convex . Next describe convex functions Fenchel conjugates . Example  .  ( Squared Euclidean Norm ) Whenever f ( x ) =   f ∗ ( x∗ ) =   jugate .   ( cid:    ) x ( cid:    )     ( cid:    ) x∗ ( cid:    )   , , squared Euclidean norm con- Example  .  ( Negative Entropy ) Fenchel conjugate negative entropy (  .   ) f ∗ ( x∗ ) = log exp ( x∗ ) . ( cid:   )   . .  Bregman Divergence Let f diﬀerentiable convex function . Bregman divergence deﬁned f given ∆f ( x , x ( cid:   ) ) = f ( x ) − f ( x ( cid:   ) ) − ( cid:   ) x − x ( cid:   ) , ∇f ( x ( cid:   ) ) ( cid:   ) . (  .   ) Also see Figure  .  . well known examples . Example  .  ( Square Euclidean Norm ) Set f ( x ) =   ∇f ( x ) = x therefore   ( cid:    ) x ( cid:    )   . Clearly , ∆f ( x , x ( cid:   ) ) =     ( cid:    ) x ( cid:    )   −     ( cid:   ) x ( cid:   ) ( cid:   ) ( cid:   )   − ( cid:   ) x − x ( cid:   ) , x ( cid:   ) ( cid:   ) = ( cid:   ) ( cid:   ) x − x ( cid:   ) ( cid:   ) ( cid:   )   . ( cid:   )      .  Preliminaries     Fig .  . . f ( x ) value function x , f ( x ( cid:   ) ) + ( cid:    ) x − x ( cid:   ) , ∇f ( x ( cid:   ) ) ( cid:    ) denotes ﬁrst order Taylor expansion f around x ( cid:   ) , evaluated x . diﬀerence two quantities Bregman divergence , illustrated . Example  .  ( Relative Entropy ) Let f un-normalized entropy f ( x ) = ( cid:   )  ( xi log xi − xi ) . (  .   ) One calculate ∇f ( x ) = log x , log x component wise loga- rithm entries x , write Bregman divergence ( cid:   ) ( cid:   ) ( cid:   ) ( cid:   ) log x ( cid:   ) x ( cid:   ) + − ( cid:   ) x − x ( cid:   ) , log x ( cid:   ) ( cid:   ) x ( cid:   ) xi log xi − xi − ∆f ( x , x ( cid:   ) ) =  ( cid:   )  = ( cid:   ) xi log ( cid:   ) xi x ( cid:   )   ( cid:   )  + x ( cid:   ) − xi ( cid:   ) .  Example  .  ( p-norm ) Let f square p-norm f ( x ) =     ( cid:    ) x ( cid:    )   p =     ( cid:   ) ( cid:   ) xp   ( cid:   )  /p . (  .   ) f ( x  ) f ( x ) f ( x  ) +›x−x  , ∇f ( x  ) ﬁ∆f ( x , x  )       Optimization say q-norm dual p-norm whenever   verify ( Problem  .   ) i-th component gradient ∇f ( x ) q =   . One p +   ∇xif ( x ) = sign ( xi ) |xi|p−  ( cid:    ) x ( cid:    ) p−  p . corresponding Bregman divergence ∆f ( x , x ( cid:   ) ) =     ( cid:    ) x ( cid:    )   p −     ( cid:   ) x ( cid:   ) ( cid:   ) ( cid:   )   p − ( cid:   ) ( cid:   ) ( xi − x ( cid:   ) )  (  .   ) . i|p−  sign ( x ( cid:   ) ) |x ( cid:   ) ( cid:    ) x ( cid:   ) ( cid:    ) p−  p following properties Bregman divergence immediately follow : • ∆f ( x , x ( cid:   ) ) convex x . • ∆f ( x , x ( cid:   ) ) ≥   . • ∆f may symmetric , , general ∆f ( x , x ( cid:   ) ) ( cid:   ) = ∆f ( x ( cid:   ) , x ) . • ∇x∆f ( x , x ( cid:   ) ) = ∇f ( x ) − ∇f ( x ( cid:   ) ) . next lemma establishes another important property . Lemma  .   Bregman divergence (  .   ) deﬁned diﬀerentiable convex function f satisﬁes ∆f ( x , ) + ∆f ( , z ) − ∆f ( x , z ) = ( cid:    ) ∇f ( z ) − ∇f ( ) , x − ( cid:    ) . (  .   ) Proof ∆f ( x , ) + ∆f ( , z ) = f ( x ) − f ( ) − ( cid:    ) x − , ∇f ( ) ( cid:    ) + f ( ) − f ( z ) − ( cid:    ) − z , ∇f ( z ) ( cid:    ) = f ( x ) − f ( z ) − ( cid:    ) x − , ∇f ( ) ( cid:    ) − ( cid:    ) − z , ∇f ( z ) ( cid:    ) = ∆f ( x , z ) + ( cid:    ) ∇f ( z ) − ∇f ( ) , x − ( cid:    ) .  .  Unconstrained Smooth Convex Minimization section describe various methods minimize smooth convex objective function .  . .  Minimizing One-Dimensional Convex Function warm let us consider problem minimizing smooth one di- mensional convex function J : R → R interval [ L , U ] . seemingly  .  Unconstrained Smooth Convex Minimization     Algorithm  .  Interval Bisection   : Input : L , U , precision ( cid:   )   : Set =   , a  = L b  = U   : ( bt − ) · J ( cid:   ) ( U ) > ( cid:   )   : ) >   at+  = bt+  = at+bt J ( cid:   ) ( at+bt     at+  = at+bt   bt+  = bt   :   :   : else   : end = +     :    : end    : Return : at+bt   simple problem many applications . see later , many optimiza- tion methods ﬁnd direction descent minimize objective function along direction  ; subroutine called line search . Algorithm  .  depicts simple line search routine based interval bisection . show Algorithm  .  converges , let us ﬁrst derive im- portant property convex functions one variable . diﬀerentiable one-dimensional convex function J (  .  ) reduces J ( w ) ≥ J ( w ( cid:   ) ) + ( w − w ( cid:   ) ) · J ( cid:   ) ( w ( cid:   ) ) , (  .   ) J ( cid:   ) ( w ) denotes gradient J . Exchanging role w w ( cid:   ) (  .   ) , write J ( w ( cid:   ) ) ≥ J ( w ) + ( w ( cid:   ) − w ) · J ( cid:   ) ( w ) . Adding two equations yields ( w − w ( cid:   ) ) · ( J ( cid:   ) ( w ) − J ( cid:   ) ( w ( cid:   ) ) ) ≥   . (  .   ) (  .   ) w ≥ w ( cid:   ) , implies J ( cid:   ) ( w ) ≥ J ( cid:   ) ( w ( cid:   ) ) . words , gradient one dimensional convex function monotonically non-decreasing . Recall minimizing convex function equivalent ﬁnding w∗ J ( cid:   ) ( w∗ ) =   . Furthermore , easy see interval bisection maintains invariant J ( cid:   ) ( ) <   J ( cid:   ) ( bt ) >   . along monotonicity gradient suﬃces ensure w∗ ∈ ( , bt ) . Setting w = w∗ (  .   ) , using monotonicity gradient allows us   objective function convex , one dimensional function obtained restricting along search direction also convex ( Exercise  .   ) .     write w ( cid:   ) ∈ ( , bt )   Optimization J ( w ( cid:   ) ) − J ( w∗ ) ≤ ( w ( cid:   ) − w∗ ) · J ( cid:   ) ( w ( cid:   ) ) ≤ ( bt − ) · J ( cid:   ) ( U ) . (  .   ) Since halve interval ( , bt ) every iteration , follows ( bt−at ) = ( U − L ) / t . Therefore J ( w ( cid:   ) ) − J ( w∗ ) ≤ ( U − L ) · J ( cid:   ) ( U )  t , (  .   ) w ( cid:   ) ∈ ( , bt ) . words , ﬁnd ( cid:   ) -accurate solution , , J ( w ( cid:   ) ) − J ( w∗ ) ≤ ( cid:   ) need log ( U − L ) + log J ( cid:   ) ( U ) + log (  / ( cid:   ) ) < itera- tions . algorithm converges ( cid:   ) accurate solution ( log (  / ( cid:   ) ) ) iterations said linearly convergent . multi-dimensional objective functions , one rely mono- tonicity property gradient . Therefore , one needs sophisticated optimization algorithms , describe .  . .  Coordinate Descent Coordinate descent conceptually simplest algorithm minimizing multidimensional smooth convex function J : Rn → R. every iteration select coordinate , say , update wt+  = wt − ηtei . (  .   ) ei denotes i-th basis vector , , vector one i-th co- ordinate zeros everywhere else , ηt ∈ R non-negative scalar step size . One could , instance , minimize one dimensional convex function J ( wt − ηei ) obtain stepsize ηt . coordinates either selected cyclically , ,   ,   , . . . , n ,   ,   , . . . greedily , , coordinate yields maximum reduction function value . Even though coordinate descent shown converge J Lip- schitz continuous gradient [ LT   ] , practice quite slow . However , high precision solution required , case machine learning applications , coordinate descent often used ) cost per iteration low b ) speed convergence may acceptable especially variables loosely coupled .  . .  Gradient Descent Gradient descent ( also widely known steepest descent ) optimization technique minimizing multidimensional smooth convex objective func- tions form J : Rn → R. basic idea follows : Given location  .  Unconstrained Smooth Convex Minimization wt iteration , compute gradient ∇J ( wt ) , update wt+  = wt − ηt∇J ( wt ) ,     (  .   ) ηt scalar stepsize . See Algorithm  .  details . Diﬀerent variants gradient descent depend ηt chosen : Exact Line Search : Since J ( wt − η∇J ( wt ) ) one dimensional convex function η , one use Algorithm  .  compute : ηt = argmin η J ( wt − η∇J ( wt ) ) . (  .   ) Instead simple bisecting line search sophisticated line searches More-Thuente line search golden bisection rule also used speed convergence ( see [ NW   ] Chapter   extensive discussion ) . Inexact Line Search : Instead minimizing J ( wt − η∇J ( wt ) ) could simply look stepsize results suﬃcient decrease objective function value . One popular set suﬃcient decrease conditions Wolfe conditions J ( wt+  ) ≤ J ( wt ) + c ηt ( cid:    ) ∇J ( wt ) , wt+  − wt ( cid:    ) ( suﬃcient decrease ) ( cid:    ) ∇J ( wt+  ) , wt+  − wt ( cid:    ) ≥ c  ( cid:    ) ∇J ( wt ) , wt+  − wt ( cid:    ) ( curvature ) (  .   ) (  .   )   < c  < c  <   ( see Figure  .  ) . Wolfe conditions also called Armijio-Goldstein conditions . suﬃcient decrease (  .   ) alone enforced , called Armijio rule . Fig .  .  . suﬃcient decrease condition ( left ) places upper bound acceptable stepsizes curvature condition ( right ) places lower bound acceptable stepsizes . acceptable stepsizeacceptable stepsize       Optimization Algorithm  .  Gradient Descent   : Input : Initial point w  , gradient norm tolerance ( cid:   )   : Set =     : ( cid:    ) ∇J ( wt ) ( cid:    ) ≥ ( cid:   ) wt+  = wt − ηt∇J ( wt )   : = +     :   : end   : Return : wt Decaying Stepsize : Instead performing line search every itera- tion , one use stepsize decays according ﬁxed schedule , example , ηt =  / t. Section  . .  discuss decay schedule convergence rates generalized version gradient descent . √ Fixed Stepsize : Suppose J Lipschitz continuous gradient mod- ulus L. Using (  .   ) gradient descent update wt+  = wt − ηt∇J ( wt ) one write J ( wt+  ) ≤ J ( wt ) + ( cid:    ) ∇J ( wt ) , wt+  − wt ( cid:    ) + L   ( cid:    ) wt+  − wt ( cid:    ) (  .   ) = J ( wt ) − ηt ( cid:    ) ∇J ( wt ) ( cid:    )   + Lη     ( cid:    ) ∇J ( wt ) ( cid:    )   . (  .   ) Minimizing (  .   ) function ηt clearly shows upper bound J ( wt+  ) minimized set ηt =   L , ﬁxed stepsize rule . Theorem  .   Suppose J Lipschitz continuous gradient modu- lus L. Algorithm  .  ﬁxed stepsize ηt =   L return solution wt ( cid:    ) ∇J ( wt ) ( cid:    ) ≤ ( cid:   ) (  / ( cid:   )   ) iterations . Proof Plugging ηt =   L rearranging (  .   ) obtains    L Summing inequality ( cid:    ) ∇J ( wt ) ( cid:    )   ≤ J ( wt ) − J ( wt+  ) (  .   )    L  ( cid:   ) t=  ( cid:    ) ∇J ( wt ) ( cid:    )   ≤ J ( w  ) − J ( wT ) ≤ J ( w  ) − J ( w∗ ) , clearly shows ( cid:    ) ∇J ( wt ) ( cid:    ) →   → ∞ . Furthermore , write following simple inequality : ( cid:    ) ∇J ( wT ) ( cid:    ) ≤ ( cid:    )  L ( J ( w  ) − J ( w∗ ) ) +   .  .  Unconstrained Smooth Convex Minimization     Solving ( cid:    )  L ( J ( w  ) − J ( w∗ ) ) +   = ( cid:   ) shows (  / ( cid:   )   ) claimed . addition Lipschitz continuous gradient , J σ-strongly convex , said . First , one translate convergence ( cid:    ) ∇J ( wt ) ( cid:    ) convergence function values . Towards end , use (  .   ) write J ( wt ) ≤ J ( w∗ ) +    σ ( cid:    ) ∇J ( wt ) ( cid:    )   . Therefore , follows whenever ( cid:    ) ∇J ( wt ) ( cid:    ) < ( cid:   ) J ( wt ) − J ( w∗ ) < ( cid:   )  / σ . Furthermore , strengthen rates convergence . Theorem  .   Assume everything Theorem  .   . Moreover assume J σ-strongly convex , let c : =   − σ L . J ( wt ) − J ( w∗ ) ≤ ( cid:   )  log ( ( J ( w  ) − J ( w∗ ) ) / ( cid:   ) ) log (  /c ) (  .   ) iterations . Proof Combining (  .   ) ( cid:    ) ∇J ( wt ) ( cid:    )   ≥  σ ( J ( wt ) − J ( w∗ ) ) , using deﬁnition c one write c ( J ( wt ) − J ( w∗ ) ) ≥ J ( wt+  ) − J ( w∗ ) . Applying equation recursively cT ( J ( w  ) − J ( w∗ ) ) ≥ J ( wT ) − J ( w∗ ) . Solving ( cid:   ) = cT ( J ( w  ) − J ( w∗ ) ) rearranging yields (  .   ) . applied practical problems strongly convex gra- dient descent yields low accuracy solution within iterations . How- ever , iterations progress method “ stalls ” increase accuracy obtained (  / ( cid:   )   ) rates convergence . hand , function strongly convex , gradient descent converges linearly , , ( log (  / ( cid:   ) ) ) iterations . However , number       Optimization iterations depends inversely log (  /c ) . approximate log (  /c ) = − log (   − σ/L ) ≈ σ/L , shows convergence depends ratio L/σ . ratio called condition number problem . problem well conditioned , i.e. , σ ≈ L gradient descent converges extremely fast . contrast , σ ( cid:   ) L gradient descent requires many iterations . best illustrated example : Consider quadratic objective function J ( w ) =     w ( cid:   ) Aw − bw , (  .   ) ∈ Rn×n symmetric positive deﬁnite matrix , b ∈ Rn arbitrary vector . Recall twice diﬀerentiable function σ-strongly convex Lipschitz continuous gradient modulus L Hessian sat- isﬁes LI ( cid:   ) ∇ J ( w ) ( cid:   ) σI ( see (  .   ) (  .   ) ) . case quadratic function (  .   ) ∇ J ( w ) = hence σ = λmin L = λmax , λmin ( respectively λmax ) denotes minimum ( respectively maximum ) eigen- value . One thus change condition number problem varying eigen-spectrum matrix . instance , set n × n identity matrix , λmax = λmin =   hence problem well conditioned . case , gradient descent converges quickly optimal solution . illustrate behavior two dimensional quadratic function Figure  .  ( right ) . hand , choose λmax ( cid:   ) λmin problem (  .   ) becomes ill-conditioned . case gradient descent exhibits zigzagging slow convergence seen Figure  .  ( left ) . shortcomings , gradient descent widely used practice . number diﬀerent algorithms described understood explicitly implicitly changing condition number problem accelerate convergence .  . .  Mirror Descent One way motivate gradient descent use following quadratic ap- proximation objective function Qt ( w ) : = J ( wt ) + ( cid:    ) ∇J ( wt ) , w − wt ( cid:    ) +     ( w − wt ) ( cid:   ) ( w − wt ) , (  .   ) , previous section , ∇J ( · ) denotes gradient J. Mini- mizing quadratic model every iteration entails taking gradients  .  Unconstrained Smooth Convex Minimization     Fig .  .  . Convergence gradient descent exact line search two quadratic problems (  .   ) . problem left ill-conditioned , whereas problem right well-conditioned . plot contours objective function , steps taken gradient descent . seen gradient descent converges fast well conditioned problem , zigzags takes many iterations converge ill-conditioned problem . respect w setting zero , gives w − wt : = −∇J ( wt ) . (  .   ) Performing line search along direction −∇J ( wt ) recovers familiar gradient descent update wt+  = wt − ηt∇J ( wt ) . (  .   ) closely related mirror descent method replaces quadratic penalty (  .   ) Bregman divergence deﬁned convex function f yield Qt ( w ) : = J ( wt ) + ( cid:    ) ∇J ( wt ) , w − wt ( cid:    ) + ∆f ( w , wt ) . (  .   ) Computing gradient , setting zero , using ∇w∆f ( w , wt ) = ∇f ( w ) − ∇f ( wt ) , minimizer model written ∇f ( w ) − ∇f ( wt ) = −∇J ( wt ) . (  .   ) , using stepsize ηt resulting updates written wt+  = ∇f −  ( ∇f ( wt ) − ηt∇J ( wt ) ) . (  .   )   ( cid:    ) · ( cid:    )   recovers usual gradient easy verify choosing f ( · ) =   descent updates . hand choose f un-normalized entropy (  .   ) ∇f ( · ) = log therefore (  .   ) specializes wt+  = exp ( log ( wt ) − ηt∇J ( wt ) ) = wt exp ( −ηt∇J ( wt ) ) , (  .   ) sometimes called Exponentiated Gradient ( EG ) update .       Optimization Theorem  .   Let J convex function J ( w∗ ) denote minimum value . mirror descent updates (  .   ) σ-strongly convex function f satisfy ∆f ( w∗ , w  ) +    σ ( cid:   ) ηt ( cid:   ) η  ( cid:    ) ∇J ( wt ) ( cid:    )   ≥ min  J ( wt ) − J ( w∗ ) . Proof Using convexity J ( see (  .  ) ) (  .   ) write J ( w∗ ) ≥ J ( wt ) + ( cid:    ) w∗ − wt , ∇J ( wt ) ( cid:    )   ηt ≥ J ( wt ) − ( cid:    ) w∗ − wt , f ( wt+  ) − f ( wt ) ( cid:    ) . applying Lemma  .   rearranging ∆f ( w∗ , wt ) − ∆f ( w∗ , wt+  ) + ∆f ( wt , wt+  ) ≥ ηt ( J ( wt ) − J ( w∗ ) ) . Summing =   , . . . , ∆f ( w∗ , w  ) − ∆f ( w∗ , wT +  ) + ( cid:   )  ∆f ( wt , wt+  ) ≥ ( cid:   )  ηt ( J ( wt ) − J ( w∗ ) ) . Noting ∆f ( w∗ , wT +  ) ≥   , J ( wt ) − J ( w∗ ) ≥ mint J ( wt ) − J ( w∗ ) , rearranging follows ∆f ( w∗ , w  ) + ( cid:   ) ( cid:   ) ηt J ( wt ) − J ( w∗ ) . ∆f ( wt , wt+  ) ≥ min (  .   )  Using (  .   ) (  .   ) ∆f ( wt , wt+  ) ≤    σ ( cid:    ) ∇f ( wt ) − ∇f ( wt+  ) ( cid:    )   =    σ ( cid:    ) ∇J ( wt ) ( cid:    )   . η  (  .   ) proof completed plugging (  .   ) (  .   ) . Corollary  .   J Lipschitz continuous gradient modulus L , stepsizes ηt chosen ηt = ( cid:    )  σ∆f ( w∗ , w  ) L    √   ∆f ( w∗ , w  ) σ (  .   )   √  . min  ≤t≤T J ( wt ) − J ( w∗ ) ≤ L ( cid:    ) Proof Since ∇J Lipschitz continuous min  ≤t≤T J ( wt ) − J ( w∗ ) ≤ ∆f ( w∗ , w  ) +    σ ηt ( cid:   ) ( cid:   ) η  L  .  .  Unconstrained Smooth Convex Minimization     Plugging (  .   ) using Problem  .   min  ≤t≤T J ( wt ) − J ( w∗ ) ≤ L ( cid:    ) ∆f ( w∗ , w  )  σ   ) (   + ( cid:   )  ( cid:   )  √   ( cid:    ) ≤ L ∆f ( w∗ , w  )  σ   √  .  . .  Conjugate Gradient Let us revisit problem minimizing quadratic objective function (  .   ) . Since ∇J ( w ) = Aw − b , optimum ∇J ( w ) =   ( see Lemma  .  ) hence Aw = b . (  .   ) fact , Conjugate Gradient ( CG ) algorithm ﬁrst developed method solve linear system . already saw , updating w along negative gradient direction may lead zigzagging . Therefore CG uses so-called conjugate directions . Deﬁnition  .   ( Conjugate Directions ) Non zero vectors pt pt ( cid:   ) said conjugate respect symmetric positive deﬁnite matrix p ( cid:   ) ( cid:   ) Apt =   ( cid:   ) = ( cid:   ) . Conjugate directions { p  , . . . , pn−  } linearly independent form basis . see , suppose pt ’ linearly independent . exists non-zero coeﬃcients σt ( cid:   ) σtpt =   . pt ’ conjugate ( cid:   ) Apt ( cid:   ) =   ( cid:   ) . directions , therefore p ( cid:   ) Since positive deﬁnite implies σt ( cid:   ) =   ( cid:   ) , contradiction . turns , conjugate directions generated iteratively ( cid:   ) Apt = σt ( cid:   ) p ( cid:   ) σtpt ) = ( cid:   ) ( cid:   ) ( ( cid:   ) σtp ( cid:   ) follows : Starting w  ∈ Rn deﬁne p  = −g  = b − Aw  , set αt = − g ( cid:   ) pt p ( cid:   ) Apt wt+  = wt + αtpt gt+  = Awt+  − b g ( cid:   ) t+ Apt p ( cid:   ) Apt pt+  = −gt+  + βt+ pt βt+  = (  .  a ) (  .  b ) (  .  c ) (  .  d ) (  .  e )       Optimization following theorem asserts pt generated procedure indeed conjugate directions . Theorem  .   Suppose t-th iterate generated conjugate gradient method (  .   ) solution (  .   ) , following properties hold : span { g  , g  , . . . , gt } = span { g  , Ag  , . . . , Atg  } . span { p  , p  , . . . , pt } = span { g  , Ag  , . . . , Atg  } . p ( cid:   ) j gt =   j < p ( cid:   ) j Apt =   j < . (  .   ) (  .   ) (  .   ) (  .   ) Proof proof induction . induction hypothesis holds trivially =   . Assuming (  .   ) (  .   ) hold , prove continue hold +   . Step   : ﬁrst prove (  .   ) holds . Using (  .  c ) , (  .  b ) (  .  a ) j gt+  = p ( cid:   ) p ( cid:   ) = p ( cid:   ) j ( Awt+  − b ) j ( Awt + αtpt − b ) ( cid:   ) = p ( cid:   ) j Awt − ( cid:   ) Apt − b g ( cid:   ) pt p ( cid:   ) Apt p ( cid:   ) j Apt p ( cid:   ) Apt g ( cid:   ) pt . = p ( cid:   ) j gt − j = , terms cancel , j < terms vanish due induction hypothesis . Step   : Next prove (  .   ) holds . Using (  .  c ) (  .  b ) gt+  = Awt+  − b = Awt + αtApt − b = gt + αtApt . induction hypothesis , gt ∈ span { g  , Ag  , . . . , Atg  } , Apt ∈ span { Ag  , A g  , . . . , At+ g  } . Combining two conclude gt+  ∈ span { g  , Ag  , . . . , At+ g  } . hand , already showed gt+  orthogonal { p  , p  , . . . , pt } . Therefore , gt+  /∈ span { p  , p  , . . . , pt } . Thus induction assumption implies gt+  /∈ span { g  , Ag  , . . . , Atg  } . allows us conclude span { g  , g  , . . . , gt+  } = span { g  , Ag  , . . . , At+ g  } .  .  Unconstrained Smooth Convex Minimization     Step   prove (  .   ) holds . Using (  .  e ) t+ Apj = −g ( cid:   ) p ( cid:   ) t+ Apj + βt+ p ( cid:   ) Apj . deﬁnition βt+  (  .  d ) expression vanishes j = t. j < , ﬁrst term zero Apj ∈ span { p  , p  , . . . , pj+  } , subspace orthogonal gt+  already shown Step   . induction hypothesis guarantees second term zero . Step   Clearly , (  .   ) (  .  e ) imply (  .   ) . concludes proof . practical implementation (  .   ) requires two observations : First , using (  .  e ) (  .   ) −g ( cid:   ) pt = g ( cid:   ) gt − βtg ( cid:   ) pt−  = g ( cid:   ) gt . Therefore (  .  a ) simpliﬁes αt = g ( cid:   ) gt p ( cid:   ) Apt . (  .   ) Second , using (  .  c ) (  .  b ) gt+  − gt = ( wt+  − wt ) = αtApt . gt ∈ span { p  , . . . , pt } , subspace orthogonal gt+  (  .   ) . Therefore t+ Apt =   g ( cid:   ) t+ gt+  ) . Substituting back (  .  d ) using (  .   ) αt yields ( g ( cid:   ) βt+  = g ( cid:   ) t+ gt+  g ( cid:   ) gt . (  .   ) summarize CG algorithm Algorithm  .  . Unlike gradient descent whose convergence rates minimizing quadratic objective function (  .   ) depend upon condition number , following theorem shows , CG iterates converge n steps . Theorem  .   CG iterates (  .   ) converge minimizer (  .   ) n steps . Proof Let w denote minimizer (  .   ) . Since pt ’ form basis w − w  = σ p  + . . . + σn− pn−  , scalars σt . proof strategy show coeﬃcients       Optimization Algorithm  .  Conjugate Gradient   :   :   :   : Input : Initial point w  , residual norm tolerance ( cid:   )   : Set =   , g  = Aw  − b , p  = −g    : ( cid:    ) Awt − b ( cid:    ) ≥ ( cid:   ) αt = g ( cid:   ) gt p ( cid:   ) Apt wt+  = wt + αtpt gt+  = gt + αtApt g ( cid:   ) t+ gt+  βt+  = g ( cid:   ) gt pt+  = −gt+  + βt+ pt = +     :    : end    : Return : wt   :   : σt coincide αt deﬁned (  .  a ) . Towards end premultiply p ( cid:   ) use conjugacy obtain σt = p ( cid:   ) ( w − w  ) p ( cid:   ) Apt . (  .   ) hand , following iterative process (  .  b ) w  wt yields wt − w  = α p  + . . . + αt− pt−  . premultiplying p ( cid:   ) using conjugacy p ( cid:   ) ( wt − w  ) =   . Substituting (  .   ) (  .   ) produces σt = p ( cid:   ) ( w − wt ) p ( cid:   ) Apt = − g ( cid:   ) pt p ( cid:   ) Apt , (  .   ) (  .   ) thus showing σt = αt . Observe gt+  computed via (  .  c ) nothing gradient J ( wt+  ) . Furthermore , consider following one dimensional optimization problem : min α∈R φt ( α ) : = J ( wt + αpt ) . Diﬀerentiating φt respect α ( α ) = p ( cid:   ) φ ( cid:   ) ( Awt + αApt − b ) = p ( cid:   ) ( gt + αApt ) .  .  Unconstrained Smooth Convex Minimization gradient vanishes set α = − g ( cid:   ) pt , recovers (  .  a ) . p ( cid:   ) Apt words , every iteration CG minimizes J ( w ) along conjugate direction pt . Contrast gradient descent minimizes J ( w ) along negative gradient direction gt every iteration .     natural ask idea generating conjugate directions minimizing objective function along directions applied general convex functions . main diﬃculty Theorems  .    .   hold . spite , extensions CG eﬀective even setting . Basically update rules gt pt remain , parameters αt βt computed diﬀerently . Table  .  gives overview diﬀerent extensions . See [ NW   , Lue   ] details . Table  .  . Non-Quadratic modiﬁcations Conjugate Gradient Descent Generic Method Compute Hessian Kt : = ∇ J ( wt ) update αt βt αt = − g ( cid:   ) pt p ( cid:   ) Ktpt βt = − g ( cid:   ) t+ Ktpt p ( cid:   ) Ktpt Fletcher-Reeves Set αt = argminα J ( wt + αpt ) βt = g ( cid:   ) t+ gt+  g ( cid:   ) gt . Polak-Ribi ` ere Hestenes-Stiefel Set αt = argminα J ( wt + αpt ) , yt = gt+  − gt , βt = ( cid:   ) gt+  g ( cid:   ) gt practice , Polak-Ribi ` ere tends better Fletcher-Reeves . . Set αt = argminα J ( wt + αpt ) , yt = gt+  − gt , βt = ( cid:   ) gt+  ( cid:   ) pt .  . .  Higher Order Methods Recall motivation gradient descent minimizer quadratic model Qt ( w ) : = J ( wt ) + ( cid:    ) ∇J ( wt ) , w − wt ( cid:    ) +     ( w − wt ) ( cid:   ) ( w − wt ) , quadratic penalty equation uniformly penalizes deviation wt diﬀerent dimensions . function ill-conditioned one would intuitively want penalize deviations diﬀerent directions diﬀer- ently . One way achieve using Hessian , results       Optimization Algorithm  .  Newton ’ Method   : Input : Initial point w  , gradient norm tolerance ( cid:   )   : Set =     : ( cid:    ) ∇J ( wt ) ( cid:    ) > ( cid:   )   : Compute pt : = −∇ J ( wt ) − ∇J ( wt ) Compute ηt = argminη J ( wt + ηpt ) e.g. , via Algorithm  .  . wt+  = wt + ηtpt = +     :   : end   : Return : wt   :   : following second order Taylor approximation : Qt ( w ) : = J ( wt ) + ( cid:    ) ∇J ( wt ) , w − wt ( cid:    ) +     ( w − wt ) ( cid:   ) ∇ J ( wt ) ( w − wt ) . (  .   ) course , requires J twice diﬀerentiable . also assume J strictly convex hence Hessian positive deﬁnite in- vertible . Minimizing Qt taking gradients respect w setting zero obtains w − wt : = −∇ J ( wt ) − ∇J ( wt ) , (  .   ) Since minimizing model objective function , perform line search along descent direction (  .   ) compute stepsize ηt , yields next iterate : wt+  = wt − ηt∇ J ( wt ) − ∇J ( wt ) . (  .   ) Details found Algorithm  .  . Suppose w∗ denotes minimum J ( w ) . say algorithm exhibits quadratic convergence sequences iterates { wk } generated algorithm satisﬁes : ( cid:    ) wk+  − w∗ ( cid:    ) ≤ C ( cid:    ) wk − w∗ ( cid:    )   (  .   ) constant C >   . show Newton ’ method exhibits quadratic convergence close optimum . Theorem  .   ( Quadratic convergence Newton ’ Method ) Suppose J twice diﬀerentiable , strongly convex , Hessian J bounded Lipschitz continuous modulus neighborhood so- lution w∗ . Furthermore , assume ( cid:   ) ( cid:   ) ≤ N . iterations ( cid:   ) ∇ J ( w ) −  ( cid:   )  .  Unconstrained Smooth Convex Minimization     wt+  = wt − ∇ J ( wt ) − ∇J ( wt ) converge quadratically w∗ , minimizer J . Proof First notice ∇J ( wt ) − ∇J ( w∗ ) = ( cid:   )     ∇ J ( wt + ( w∗ − wt ) ) ( wt − w∗ ) dt . (  .   ) Next using fact ∇ J ( wt ) invertible gradient vanishes optimum ( ∇J ( w∗ ) =   ) , write wt+  − w∗ = wt − w∗ − ∇ J ( wt ) − ∇J ( wt ) = ∇ J ( wt ) −  [ ∇ J ( wt ) ( wt − w∗ ) − ( ∇J ( wt ) − ∇J ( w∗ ) ) ] . (  .   ) Using (  .   ) , (  .   ) , Lipschitz continuity ∇ J ( cid:   ) ∇J ( wt ) − ∇J ( w∗ ) − ∇ J ( wt ) ( wt − w∗ ) ( cid:   ) ( cid:   ) ( cid:   ) = ≤ ( cid:   )   ( cid:   ) ( cid:   ) ( cid:   ) ( cid:   )   ( cid:   )     [ ∇ J ( wt + ( wt − w∗ ) ) − ∇ J ( wt ) ] ( wt − w∗ ) dt ( cid:   ) ( cid:   ) ( cid:   ) ( cid:   ) ( cid:   ) [ ∇ J ( wt + ( wt − w∗ ) ) − ∇ J ( wt ) ] ( cid:   ) ( cid:   ) ( cid:   ) ( cid:    ) ( wt − w∗ ) ( cid:    ) dt ≤ ( cid:    ) wt − w∗ ( cid:    )   ( cid:   )     dt =    ( cid:    ) wt − w∗ ( cid:    )   . Finally use (  .   ) (  .   ) conclude (  .   ) ( cid:    ) wt+  − w∗ ( cid:    ) ≤    ( cid:   ) ( cid:   ) ∇ J ( wt ) −  ( cid:   ) ( cid:   ) ( cid:    ) wt − w∗ ( cid:    )   ≤ N   ( cid:    ) wt − w∗ ( cid:    )   . Newton ’ method described suﬀers two major problems . First , applies twice diﬀerentiable , strictly convex functions . Sec- ond , involves computing inverting n × n Hessian matrix every iteration , thus making computationally expensive . Although Newton ’ method extended deal positive semi-deﬁnite Hes- sian matrices , computational burden often makes unsuitable large scale applications . cases one resorts Quasi-Newton methods .  . . .  Quasi-Newton Methods Unlike Newton ’ method , computes Hessian objective func- tion every iteration , quasi-Newton methods never compute Hessian ; approximate past gradients . Since require ob- jective function twice diﬀerentiable , quasi-Newton methods much       Optimization Fig .  .  . blue solid line depicts one dimensional convex function J ( w ) = w  +   w  + w. green dotted-dashed line represents ﬁrst order Taylor approximation J ( w ) , red dashed line represents second order Taylor approximation , evaluated w =   . widely applicable . widely regarded workhorses smooth nonlinear optimization due combination computational ef- ﬁciency good asymptotic convergence . popular quasi-Newton algorithm BFGS , named discoverers Broyde , Fletcher , Goldfarb , Shanno . section describe BFGS limited memory counterpart LBFGS . Suppose given smooth ( necessarily strictly ) convex objective function J : Rn → R current iterate wt ∈ Rn . like Newton ’ method , BFGS forms local quadratic model objective function , J : Qt ( w ) : = J ( wt ) + ( cid:    ) ∇J ( wt ) , w − wt ( cid:    ) +     ( w − wt ) ( cid:   ) Ht ( w − wt ) . (  .   ) Unlike Newton ’ method uses Hessian build quadratic model (  .   ) , BFGS uses matrix Ht ( cid:   )   , positive-deﬁnite estimate Hessian . quasi-Newton direction descent found minimizing Qt ( w ) : w − wt = −H −  ∇J ( wt ) . (  .   ) stepsize ηt >   found line search obeying Wolfe conditions                                     .  Unconstrained Smooth Convex Minimization (  .   ) (  .   ) . ﬁnal update given wt+  = wt − ηtH −  ∇J ( wt ) . Given wt+  need update quadratic model (  .   )     (  .   ) Qt+  ( w ) : = J ( wt+  ) + ( cid:    ) ∇J ( wt+  ) , w − wt+  ( cid:    ) +     ( w − wt+  ) ( cid:   ) Ht+  ( w − wt+  ) . (  .   ) updating model reasonable expect gradient Qt+  match gradient J wt wt+  . Clearly , ∇Qt+  ( w ) = ∇J ( wt+  ) + Ht+  ( w − wt+  ) , (  .   ) implies ∇Qt+  ( wt+  ) = ∇J ( wt+  ) , hence second con- dition automatically satisﬁed . order satisfy ﬁrst condition , require ∇Qt+  ( wt ) = ∇J ( wt+  ) + Ht+  ( wt − wt+  ) = ∇J ( wt ) . (  .   ) rearranging , obtain so-called secant equation : Ht+ st = yt , (  .   ) st : = wt+  − wt yt : = ∇J ( wt+  ) − ∇J ( wt ) denote recent step along optimization trajectory parameter gradient space , respectively . Since Ht+  positive deﬁnite matrix , pre-multiplying secant equation st yields curvature condition ( cid:   ) yt >   . (  .   ) curvature condition satisﬁed , inﬁnite number matrices Ht+  satisfy secant equation ( secant equation represents n linear equations , symmetric matrix Ht+  n ( n+  ) /  degrees freedom ) . resolve issue choose closest matrix Ht satisﬁes secant equation . key insight BFGS comes observation descent direction computation (  .   ) involves inverse matrix Bt : = H −  . Therefore , choose matrix Bt+  : = H −  t+  close Bt also satisﬁes secant equation :  min B ( cid:    ) B − Bt ( cid:    ) s. t. B = B ( cid:   ) Byt = st . (  .   ) (  .   ) matrix norm ( cid:    ) · ( cid:    ) appropriately chosen [ NW   ] , shown  Bt+  = (   −ρtsty ( cid:   ) ) Bt (   −ρtyts ( cid:   ) ) + ρtsts ( cid:   ) , (  .   )     Algorithm  .  LBFGS   Optimization   :   : Input : Initial point w  , gradient norm tolerance ( cid:   ) >     : Set =   B  =   : ( cid:    ) ∇J ( wt ) ( cid:    ) > ( cid:   ) pt = −Bt∇J ( wt )   : Find ηt obeys (  .   ) (  .   ) st = ηtpt wt+  = wt + st yt : = ∇J ( wt+  ) − ∇J ( wt ) =   : Bt : = ( cid:   ) yt ( cid:   ) yt yt ) −  ρt = ( ( cid:   )   :   :   :   :  ) Bt ( − ρtyts ( cid:   ) ) + ρtsts ( cid:   )     :    : Bt+  = ( − ρtsty ( cid:   ) = +      :    : end    : Return : wt st ) −  . words , matrix Bt modiﬁed via ρt : = ( ( cid:   ) incremental rank-two update , eﬃcient compute , obtain Bt+  . exists interesting connection BFGS update (  .   ) Hestenes-Stiefel variant Conjugate gradient . see assume exact line search used compute wt+  , therefore ( cid:   )   . Furthermore , assume Bt =   , use (  .   ) write ∇J ( wt+  ) = pt+  = −Bt+ ∇J ( wt+  ) = −∇J ( wt+  ) + ( cid:   ) ∇J ( wt+  ) ( cid:   ) st st , (  .   ) recovers Hestenes-Stiefel update ( see (  .  e ) Table  .  ) . Limited-memory BFGS ( LBFGS ) variant BFGS designed solv- ing large-scale optimization problems ( d  ) cost storing updating Bt would prohibitively expensive . LBFGS approximates quasi-Newton direction (  .   ) directly last pairs st yt via matrix-free approach . reduces cost ( md ) space time per iteration , freely chosen . Details found Algorithm  .  .  . . .  Spectral Gradient Methods Although spectral gradient methods use Hessian explicitly , motivated arguments reminiscent Quasi-Newton methods . Recall update rule (  .   ) secant equation (  .   ) . Suppose want  .  Unconstrained Smooth Convex Minimization     simple matrix approximates Hessian . Speciﬁcally , want Ht+  = αt+ I (  .   ) αt+  scalar denotes identity matrix . secant equation (  .   ) becomes αt+ st = yt . (  .   ) general , equation solved . Therefore use αt+  minimizes ( cid:    ) αt+ st − yt ( cid:    )   yields Barzilai-Borwein ( BB ) step- size αt+  = ( cid:   ) yt ( cid:   ) st . (  .   ) turns , αt+  lies minimum maximum eigenvalue average Hessian direction st , hence name Spectral Gradient method . parameter update (  .   ) given wt+  = wt −   αt ∇J ( wt ) . (  .   ) practical implementation uses safeguards ensure stepsize αt+  neither small large . Given   < αmin < αmax < ∞ compute ( cid:   ) ( cid:   ) αt+  = min αmax , max αmin , ( cid:   ) ( cid:   ) . ( cid:   ) yt ( cid:   ) st (  .   ) One peculiar features spectral gradient methods use non-monotone line search . algorithms seen far , stepsize chosen objective function J decreases every iteration . contrast , non-monotone line searches employ parameter ≥   ensure objective function decreases every iterations . course , setting =   results usual monotone line search . Details found Algorithm  .  .  . .  Bundle Methods methods discussed applicable minimizing smooth , con- vex objective functions . regularized risk minimization problems involve non-smooth objective function . cases , one needs use bundle methods . order lay ground bundle methods ﬁrst describe precursor cutting plane method [ Kel   ] . Cutting plane method based simple observation : convex function bounded       Optimization Algorithm  .  Spectral Gradient Method   : Input : w  , ≥   , αmax > αmin >   , γ ∈ (   ,   ) ,   > σ  > σ  >   , α  ∈ [ αmin , αmax ] , ( cid:   ) >   λ =     : Initialize : =     : ( cid:    ) ∇J ( wt ) ( cid:    ) > ( cid:   )   :   : TRUE dt = −   ∇J ( wt )   : αt w+ = wt + λdt δ = ( cid:    ) dt , ∇J ( wt ) ( cid:    ) J ( w+ ) ≤ min ≤j≤min ( , −  ) J ( xt−j ) + γλδ   :   :   :    :    :    :    :    :    :    :    :    :    :    : wt+  = w+ st = wt+  − wt yt = ∇J ( wt+  ) − ∇J ( wt ) break else λtmp = −   λtmp > σ  λtmp < σ λ   λ δ/ ( J ( w+ ) − J ( wt ) − λδ ) λ = λtmp else λ = λ/  end    :    :    : end end αt+  = min ( αmax , max ( αmin , ( cid:   ) yt ( cid:   ) st = +      :    : end    : Return : wt ) ) linearization ( i.e. , ﬁrst order Taylor approximation ) . See Figures  .   .  geometric intuition , recall (  .  ) (  .   ) : J ( w ) ≥ J ( w ( cid:   ) ) + ( cid:   ) w − w ( cid:   ) , ( cid:   ) ( cid:   ) ∀w ( cid:   ) ∈ ∂J ( w ( cid:   ) ) . (  .   ) Given subgradients s  , s  , . . . , st evaluated locations w  , w  , . . . , wt−  , construct tighter ( piecewise linear ) lower bound J follows ( also see Figure  .   ) : J ( w ) ≥ J CP  ( w ) : = max  ≤i≤t { J ( wi−  ) + ( cid:    ) w − wi−  , si ( cid:    ) } . (  .   )  .  Unconstrained Smooth Convex Minimization Given iterates { wi } t−  next iterate wt : i=  , cutting plane method minimizes J CP  wt : = argmin w J CP  ( w ) .     obtain (  .   ) iteratively reﬁnes piecewise linear lower bound J CP allows us get close minimum J ( see Figure  .   illustration ) . w∗ denotes minimizer J , clearly J ( wi ) ≥ J ( w∗ ) fol- ( wt ) . words , J ( w∗ ) sandwiched ( wt ) ( see Figure  .   illustration ) . cutting hence min ≤i≤t J ( wi ) ≥ J ( w∗ ) . hand , since J ≥ J CP lows J ( w∗ ) ≥ J CP min ≤i≤t J ( wi ) J CP plane method monitors monotonically decreasing quantity    ( cid:   ) : = min  ≤i≤t J ( wi ) − J CP  ( wt ) , (  .   ) terminates whenever ( cid:   ) falls predeﬁned threshold ( cid:   ) . ensures solution J ( wt ) ( cid:   ) optimum , , J ( wt ) ≤ J ( w∗ ) + ( cid:   ) . Fig .  .   . convex function ( blue solid curve ) bounded lin- earizations ( dashed lines ) . gray area indicates piecewise linear lower bound obtained using linearizations . depict iterations cutting plane method . iteration piecewise linear lower bound minimized new linearization added minimizer ( red rectangle ) . seen , adding linearizations improves lower bound . Although cutting plane method shown convergent [ Kel   ] ,       Optimization Fig .  .   . convex function ( blue solid curve ) four linearizations evaluated four diﬀerent locations ( magenta circles ) . approximation gap ( cid:   )   end fourth iteration indicated height cyan horizontal band i.e. , diﬀerence lowest value J ( w ) evaluated far minimum J CP ( w ) ( red diamond ) .   well known ( see e.g. , [ LNN   , Bel   ] ) slow new iterates move far away previous ones ( i.e. , causing unstable “ zig-zag ” behavior iterates ) . fact , worst case cutting plane method might require exponentially many steps converge ( cid:   ) optimum solution .  Bundle methods stabilize CPM augmenting piecewise linear lower ( e.g. , J CP ( w ) (  .   ) ) prox-function ( i.e. , proximity control func- tion ) prevents overly large steps iterates [ Kiw   ] . Roughly speaking ,   popular types bundle methods , namely , proximal [ Kiw   ] , trust region [ SZ   ] , level set [ LNN   ] . three versions use   ( cid:    ) · ( cid:    )   prox-function , diﬀer way compute new   iterate : { proximal : wt : = argmin w ζt   ( cid:    ) w − ˆwt−  ( cid:    )   + J CP  ( w ) } , trust region : wt : = argmin w { level set : wt : = argmin w     ( cid:    ) w − ˆwt−  ( cid:    )   ≤ κt } , { J CP  ( w ) |     ( cid:    ) w − ˆwt−  ( cid:    )   | J CP  ( w ) ≤ τt } , (  .    ) (  .   ) (  .   ) ˆwt−  current prox-center , ζt , κt , τt positive trade- oﬀ parameters stabilization . Although (  .   ) shown equivalent (  .   ) appropriately chosen ζt κt , tuning ζt rather diﬃcult trust region approach used automatically tuning  .  Constrained Optimization     κt . Consequently trust region algorithm BT [ SZ   ] widely used practice .  .  Constrained Optimization far focus unconstrained optimization problems . Many ma- chine learning problems involve constraints , often written following canonical form : J ( w ) min w s. t. ci ( w ) ≤   ∈ ei ( w ) =   ∈ E (  .   a ) (  .   b ) (  .   c ) ci ei convex functions . say w feasible satisﬁes constraints , , ci ( w ) ≤   ∈ ei ( w ) =   ∈ E . Recall w minimizer unconstrained problem ( cid:    ) ∇J ( w ) ( cid:    ) =   ( see Lemma  .  ) . Unfortunately , constraints present one use simple characterization solution . instance , w ( cid:    ) ∇J ( w ) ( cid:    ) =   may feasible point . illustrate , consider following simple minimization problem ( see Figure  .   ) : min w w      s. t.   ≤ w ≤   . (  .   a ) (  .   b ) Clearly ,     w  minimized w =   , presence con- straints , minimum (  .    ) attained w =   ∇J ( w ) = w equal   . Therefore , need ways detect convergence . Section  . .  discuss general purpose algorithms based concept or- thogonal projection . Section  . .  discuss Lagrange duality , used characterize solutions constrained optimization problems .  . .  Projection Based Methods Suppose interested minimizing smooth convex function following form : J ( w ) , min w∈Ω (  .    )       Optimization Fig .  .   . unconstrained minimum quadratic function     w  attained w =   ( red circle ) . , enforce constraints   ≤ w ≤   ( illustrated shaded area ) minimizer attained w =   ( green diamond ) . Ω convex feasible region . instance , Ω may described convex functions ci ei (  .    ) . algorithms describe section applicable Ω relatively simple set onto compute orthogonal projection . Given point w ( cid:   ) feasible region Ω , orthogonal projection PΩ ( w ( cid:   ) ) w ( cid:   ) Ω deﬁned PΩ ( w ( cid:   ) ) : = argmin w∈Ω ( cid:   ) w ( cid:   ) − w ( cid:   ) ( cid:   )   . ( cid:   ) (  .    ) Geometrically speaking , PΩ ( w ( cid:   ) ) closest point w ( cid:   ) Ω . course , w ( cid:   ) ∈ Ω PΩ ( w ( cid:   ) ) = w ( cid:   ) . interested ﬁnding approximate solution (  .    ) , , w ∈ Ω J ( w ) − min w∈Ω J ( w ) = J ( w ) − J ∗ ≤ ( cid:   ) , (  .    ) pre-deﬁned tolerance ( cid:   ) >   . course , J ∗ unknown hence gap J ( w ) − J ∗ computed practice . Furthermore , showed Section  .  , constrained optimization problems ( cid:    ) ∇J ( w ) ( cid:    ) vanish optimal solution . Therefore , use following stopping        w           J ( w )  .  Constrained Optimization     Algorithm  .  Basic Projection Based Method   : Input : Initial point w  ∈ Ω , projected gradient norm tolerance ( cid:   ) >     : Initialize : =     : ( cid:    ) PΩ ( wt − ∇J ( wt ) ) − wt ( cid:    ) > ( cid:   )   :   : Find direction descent dt wt+  = PΩ ( wt + ηtdt ) = +     :   : end   : Return : wt criterion algorithms ( cid:    ) PΩ ( wt − ∇J ( wt ) ) − wt ( cid:    ) ≤ ( cid:   ) . (  .    ) intuition follows : wt − ∇J ( wt ) ∈ Ω PΩ ( wt − ∇J ( wt ) ) = wt , , ∇J ( wt ) =   , , wt global minimizer J ( w ) . hand , wt − ∇J ( wt ) /∈ Ω PΩ ( wt − ∇J ( wt ) ) = wt , constraints preventing us making progress along descent direction −∇J ( wt ) hence stop . basic projection based method described Algorithm  .  . unconstrained optimization algorithm used generate direction descent dt . line search used ﬁnd stepsize ηt . updated parameter wt − ηtdt projected onto Ω obtain wt+  . dt chosen negative gradient direction −∇J ( wt ) , resulting algorithm called projected gradient method . One show rates convergence gradient descent various line search schemes also preserved projected gradient descent .  . .  Lagrange Duality Lagrange duality plays central role constrained convex optimization . basic idea augment objective function (  .    ) weighted sum constraint functions deﬁning Lagrangian : L ( w , α , β ) = J ( w ) + ( cid:   ) αici ( w ) + ( cid:   ) βiei ( w ) (  .    ) i∈I αi ≥   βi ∈ R. sequel , refer α ( respectively β ) Lagrange multipliers associated inequality ( respectively equality ) constraints . Furthermore , call α β dual feasible i∈E       Optimization αi ≥   βi ∈ R. Lagrangian satisﬁes following fundamental property , makes extremely useful constrained optimization . Theorem  .   Lagrangian (  .    ) (  .    ) satisﬁes max α≥  , β L ( w , α , β ) = ( cid:   ) J ( w ) w feasible ∞ otherwise . particular , J ∗ denotes optimal value (  .    ) , J ∗ = min w max α≥  , β L ( w , α , β ) . Proof First assume w feasible , , ci ( w ) ≤   ∈ ei ( w ) =   ∈ E. Since αi ≥   ( cid:   ) ( cid:   ) αici ( w ) + βiei ( w ) ≤   , (  .    ) i∈I i∈E equality attained setting αi =   whenever ci ( w ) <   . Conse- quently , max α≥  , β L ( w , α , β ) = max α≥  , β J ( w ) + ( cid:   ) i∈I αici ( w ) + ( cid:   ) i∈E βiei ( w ) = J ( w ) whenever w feasible . hand , w feasible either ci ( cid:   ) ( w ) >   ei ( cid:   ) ( w ) ( cid:   ) =   ( cid:   ) . ﬁrst case simply let αi ( cid:   ) → ∞ see maxα≥  , β L ( w , α , β ) → ∞ . Similarly , ei ( cid:   ) ( w ) ( cid:   ) =   let βi ( cid:   ) → ∞ ei ( cid:   ) ( w ) >   βi ( cid:   ) → −∞ ei ( cid:   ) ( w ) <   arrive conclusion . deﬁne Lagrange dual function ( α , β ) = min w L ( w , α , β ) , (  .    ) α ≥   β , one prove following property , often called weak duality . Theorem  .   ( Weak Duality ) Lagrange dual function (  .    ) sat- isﬁes feasible w α ≥   β . particular ( α , β ) ≤ J ( w ) D∗ : = max α≥  , β min w L ( w , α , β ) ≤ min w max α≥  , β L ( w , α , β ) = J ∗ . (  .    )  .  Constrained Optimization     Proof , observe whenever w feasible ( cid:   ) i∈I αici ( w ) + ( cid:   ) i∈E βiei ( w ) ≤   . Therefore ( α , β ) = min w L ( w , α , β ) = min w J ( w ) + ( cid:   ) i∈I αici ( w ) + ( cid:   ) i∈E βiei ( w ) ≤ J ( w ) feasible w α ≥   β . particular , one choose w minimizer (  .    ) α ≥   β maximizers ( α , β ) obtain (  .    ) . Weak duality holds arbitrary function , not-necessarily convex . objective function constraints convex , certain technical con- ditions , also known Slater ’ conditions hold , say . Theorem  .   ( Strong Duality ) Supposed objective function f constraints ci ∈ ei ∈ E (  .    ) convex following constraint qualiﬁcation holds : exists w ci ( w ) <   ∈ . Lagrange dual function (  .    ) satisﬁes D∗ : = max α≥  , β min w L ( w , α , β ) = min w max α≥  , β L ( w , α , β ) = J ∗ . (  .    ) proof theorem quite technical found standard reference ( e.g. , [ BV   ] ) . Therefore omit proof proceed discuss various implications strong duality . First note min w max α≥  , β L ( w , α , β ) = max α≥  , β min w L ( w , α , β ) . (  .    ) words , one switch order minimization w max- imization α β . called saddle point property convex functions . Suppose strong duality holds . Given α ≥   β ( α , β ) > −∞ feasible w immediately write duality gap J ( w ) − J ∗ = J ( w ) − D∗ ≤ J ( w ) − ( α , β ) , J ∗ D∗ deﬁned (  .    ) . show w∗ primal optimal ( α∗ , β∗ ) dual optimal J ( w∗ ) − ( α∗ , β∗ ) =   . provides non-heuristic stopping criterion constrained optimization : stop J ( w ) − ( α , β ) ≤ ( cid:   ) , ( cid:   ) pre-speciﬁed tolerance .       Optimization Suppose primal dual optimal values attained w∗ ( α∗ , β∗ ) respectively , consider following line argument : J ( w∗ ) = ( α∗ , β∗ ) = min w J ( w ) + ( cid:   ) i∈I ≤ J ( w∗ ) + ≤ J ( w∗ ) . ( cid:   ) α∗ ci ( w ) + ( cid:   ) β∗ ej ( w ) i∈I ci ( w∗ ) + α∗ ( cid:   ) i∈E ei ( w∗ ) β∗ i∈E (  .   a ) (  .   b ) (  .   c ) (  .   d ) write (  .   a ) used strong duality , (  .   c ) obtains setting w = w∗ (  .   c ) . Finally , obtain (  .   d ) used fact w∗ feasible hence (  .    ) holds . Since (  .    ) holds equality , one conclude following complementary slackness condition : ci ( w∗ ) + α∗ ( cid:   ) i∈I ( cid:   ) i∈E ei ( w∗ ) =   . β∗ words , α∗ =   whenever ci ( w ) <   . Furthermore , since w∗ minimizes L ( w , α∗ , β∗ ) w , follows gradient must vanish w∗ , , ci ( w∗ ) =   equivalently α∗ ∇J ( w∗ ) + ( cid:   ) i∈I ∇ci ( w∗ ) + α∗ ( cid:   ) i∈E ∇ei ( w∗ ) =   . β∗ Putting everything together , obtain ci ( w∗ ) ≤   ∀i ∈ ej ( w∗ ) =   ∀i ∈ E α∗ ≥   α∗ ci ( w∗ ) =   ∇ei ( w∗ ) =   . β∗ (  .   a ) (  .   b ) (  .   c ) (  .   d ) (  .   e ) ∇J ( w∗ ) + ( cid:   ) i∈I ∇ci ( w∗ ) + α∗ ( cid:   ) i∈E conditions called KKT conditions . primal problem convex , KKT conditions necessary suﬃcient . words , ˆw ( ˆα , ˆβ ) satisfy (  .    ) ˆw ( ˆα , ˆβ ) primal dual optimal zero duality gap . see note ﬁrst two conditions show ˆw feasible . Since αi ≥   , L ( w , α , β ) convex w. Finally last condition states ˆw minimizes L ( w , ˆα , ˆβ ) . Since ˆαici ( ˆw ) =    .  Constrained Optimization ej ( ˆw ) =   ,     ( ˆα , ˆβ ) = min w L ( w , ˆα , ˆβ ) n ( cid:   ) i=  = J ( ˆw ) + = J ( ˆw ) . ˆαici ( ˆw ) +  ( cid:   ) j=  ˆβjej ( ˆw )  . .  Linear Quadratic Programs far discussed general constrained optimization problems . Many ma- chine learning problems special structure exploited fur- ther . discuss implication duality two problems .  . . .  Linear Programming optimization problem linear objective function ( equality inequality ) linear constraints said linear program ( LP ) . canonical linear program following form : min w c ( cid:   ) w s. t. Aw = b , w ≥   . (  .   a ) (  .   b ) w c n dimensional vectors , b dimensional vector , × n matrix < n . Suppose given LP form : min w c ( cid:   ) w s. t. Aw ≥ b , (  .   a ) (  .   b ) transform canonical LP introducing non-negative slack variables c ( cid:   ) w min w , ξ s. t. Aw − ξ = b , ξ ≥   . (  .   a ) (  .   b ) Next , split w positive negative parts w+ w− respec- tively setting w+ = max (   , −wi ) . Using new = max (   , wi ) w−       Optimization variables rewrite (  .    ) min w+ , w− , ξ  ( cid:   )      c −c     w+ w− ξ s. t. ( cid:  ) −A −I ( cid:  )   w+ w− ξ    = b ,    ≥   , w+ w− ξ (  .   a ) (  .   b ) thus yielding canonical LP (  .    ) variables w+ , w− ξ . introducing non-negative Lagrange multipliers α β one write Lagrangian (  .    ) L ( w , β , ) = c ( cid:   ) w + β ( cid:   ) ( Aw − b ) − α ( cid:   ) w . (  .    ) Taking gradients respect primal dual variables setting zero obtains ( cid:   ) β − α = c Aw = b α ( cid:   ) w =   w ≥   α ≥   . (  .   a ) (  .   b ) (  .   c ) (  .   d ) (  .   e ) Condition (  .   c ) simpliﬁed noting w α con- strained non-negative , therefore α ( cid:   ) w =   , , αiwi =   =   , . . . , n . Using (  .   a ) , (  .   c ) , (  .   b ) write c ( cid:   ) w = ( ( cid:   ) β − α ) ( cid:   ) w = β ( cid:   ) Aw = β ( cid:   ) b . Substituting (  .    ) eliminating primal variable w yields following dual LP b ( cid:   ) β max α , β s.t . ( cid:   ) β − α = c , α ≥   . (  .   a ) (  .   b ) , let β+ = max ( β ,   ) β− = max (   , −β ) convert  .  Constrained Optimization     LP following canonical LP max α , β+ , β−  ( cid:   )      b −b     β+ β− α s.t . ( cid:  ) ( cid:   ) −A ( cid:   ) −I ( cid:  )   β+ β− α    = c ,    ≥   . β+ β− α (  .   a ) (  .   b ) easily veriﬁed primal-dual problem symmetric ; taking dual dual recover primal ( Problem  .   ) . One important thing note however primal (  .    ) involves n variables n + constraints , dual (  .    ) involves  m + n variables  m +  n constraints .  . . .  Quadratic Programming optimization problem convex quadratic objective function lin- ear constraints said convex quadratic program ( QP ) . canonical convex QP written follows : w ( cid:   ) Gx + w ( cid:   )   min   w s.t . ( cid:   ) w = bi ∈ E ( cid:   ) w ≤ bi ∈ (  .   a ) (  .   b ) (  .   c ) G ( cid:   )   n × n positive semi-deﬁnite matrix , E ﬁnite set indices , ai n dimensional vectors , bi scalars . warm let us consider arguably simpler equality constrained quadratic programs . case , stack ai matrix bi vector b write w ( cid:   ) Gw + w ( cid:   )   min   w s.t . Aw = b (  .   a ) (  .   b ) introducing non-negative Lagrange multipliers β Lagrangian optimization problem written L ( w , β ) =     w ( cid:   ) Gw + w ( cid:   ) + β ( Aw − b ) . (  .    ) ﬁnd saddle point Lagrangian take gradients respect       Optimization w β set zero . obtains Gw + + ( cid:   ) β =   Aw = b . Putting two conditions together yields following linear system equations ( cid:   ) G ( cid:   )   ( cid:   ) ( cid:   ) w β ( cid:   ) = ( cid:   ) −d b ( cid:   ) . (  .    ) matrix equation called KKT matrix , use characterize conditions (  .    ) unique solution . Theorem  .   Let Z n × ( n − ) matrix whose columns form basis null space , , AZ =   . full row rank , reduced-Hessian matrix Z ( cid:   ) GZ positive deﬁnite , exists unique pair ( w∗ , β∗ ) solves (  .    ) . Furthermore , w∗ also minimizes (  .    ) . Proof Note unique ( w∗ , β∗ ) exists whenever KKT matrix non-singular . Suppose case , exist non-zero vectors b ( cid:   ) G ( cid:   )   ( cid:   ) ( cid:   ) b ( cid:   ) =   . Since Aa =   implies lies null space hence exists u = Zu . Therefore ( cid:  ) Zu   ( cid:  ) ( cid:   ) G ( cid:   )   ( cid:   ) ( cid:   ) Zu   ( cid:   ) = u ( cid:   ) Z ( cid:   ) GZu =   . Positive deﬁniteness Z ( cid:   ) GZ implies u =   hence =   . hand , full row rank ( cid:   ) b =   implies b =   . summary , b zero , contradiction . Let w ( cid:   ) = w∗ feasible point ∆w = w∗ − w. Since Aw∗ = Aw = b A∆w =   . Hence , exists non-zero u ∆w = Zu . objective function J ( w ) written J ( w ) =     ( w∗ − ∆w ) ( cid:   ) G ( w∗ − ∆w ) + ( w∗ − ∆w ) ( cid:   ) = J ( w∗ ) +     ∆w ( cid:   ) G∆w − ( Gw∗ + ) ( cid:   ) ∆w . First note     u ( cid:   ) Z ( cid:   ) GZu >   positive deﬁniteness reduced Hessian . Second , since w∗ solves (  .    ) follows ( Gw∗ +   ∆w ( cid:   ) G∆w =    .  Stochastic Optimization     ) ( cid:   ) ∆w = β ( cid:   ) A∆w =   . Together two observations imply J ( w ) > J ( w∗ ) . technical conditions theorem met , solving equality constrained QP (  .    ) equivalent solving linear system (  .    ) . See [ NW   ] extensive discussion algorithms used task . Next turn attention general QP (  .    ) also contains inequality constraints . Lagrangian case written L ( w , β ) =     w ( cid:   ) Gw + w ( cid:   ) + ( cid:   ) i∈I αi ( ( cid:   ) w − bi ) + ( cid:   ) i∈E βi ( ( cid:   ) w − bi ) . (  .    ) Let w∗ denote minimizer (  .    ) . deﬁne active set ( w∗ ) ( w∗ ) = ( cid:    ) s.t . ∈ ( cid:   ) w∗ = bi ( cid:    ) , KKT conditions (  .    ) problem written ( cid:   ) w − bi <   ∀i ∈ \ ( w∗ ) w − bi =   ∀i ∈ E ∪ ( w∗ ) ( cid:   ) ≥   ∀i ∈ ( w∗ ) α∗ Gw∗ + + ( cid:   ) α∗ ai + i∈A ( w∗ ) ( cid:   ) i∈E βiai =   . (  .   a ) (  .   b ) (  .   c ) (  .   d ) Conceptually main diﬃculty solving (  .    ) identifying active set ( w∗ ) . α∗ =   ∈ \ ( w∗ ) . algorithms solving (  .    ) viewed diﬀerent ways identify active set . See [ NW   ] detailed discussion .  .  Stochastic Optimization Recall regularized risk minimization involves data-driven optimization problem objective function involves summation loss terms set data modeled : min f J ( f ) : = λΩ ( f ) +     ( cid:   ) i=  l ( f ( xi ) , yi ) . Classical optimization techniques must compute sum entirety evaluation objective , respectively gradient . available data sets grow ever larger , “ batch ” optimizers therefore become increasingly ineﬃcient . also ill-suited incremental setting , partial data must modeled arrives .       Optimization Stochastic gradient-based methods , contrast , work gradient esti- mates obtained small subsamples ( mini-batches ) training data . greatly reduce computational requirements : large , redundant data sets , simple stochastic gradient descent routinely outperforms sophisticated second-order batch methods orders magnitude . key idea J ( w ) replaced instantaneous estimate Jt computed mini-batch size k comprising subset points ( xt ) =   , . . . , k drawn dataset : , yt Jt ( w ) = λΩ ( w ) +   k k ( cid:   ) i=  l ( w , xt , yt ) . (  .    ) Setting k =   obtains algorithm processes data points arrive .  . .  Stochastic Gradient Descent Perhaps simplest stochastic optimization algorithm Stochastic Gradi- ent Descent ( SGD ) . parameter update SGD takes form : wt+  = wt − ηt∇Jt ( wt ) . (  .    ) Jt diﬀerentiable , one choose arbitrary subgradient ∂Jt ( wt ) compute update . shown SGD asymptotically converges true minimizer J ( w ) stepsize ηt decays (  / ) . instance , one could set √ ηt = ( cid:    ) τ τ + , (  .    ) τ >   tuning parameter . See Algorithm  .  details .  . . .  Practical Considerations One simple yet eﬀective rule thumb tune τ select small subset data , try various values τ subset , choose τ reduces objective function . cases letting ηt decay (  /t ) found eﬀective : ηt = τ τ + . (  .    ) free parameter τ >   tuned described . Ω ( w ) σ- strongly convex , dividing stepsize ηt σλ yields good practical performance .  .  Nonconvex Optimization     Algorithm  .  Stochastic Gradient Descent   : Input : Maximum iterations , batch size k , τ   : Set =   w  =     : <   :   :   : ( cid:    ) τ τ +t Choose subset k data points ( xt Compute stepsize ηt = wt+  = wt − ηt∇Jt ( wt ) = +     :   : end   : Return : wT , yt ) compute ∇Jt ( wt )  .  Nonconvex Optimization focus previous sections convex objective functions . Some- times non-convex objective functions also arise machine learning applica- tions . problems signiﬁcantly harder tools minimizing objective functions well developed . brieﬂy describe one algo- rithm applied whenever write objective function diﬀerence two convex functions .  . .  Concave-Convex Procedure function bounded Hessian decomposed diﬀerence two ( non-unique ) convex functions , , one write J ( w ) = f ( w ) − g ( w ) , (  .    ) f g convex functions . Clearly , J convex , exists reasonably simple algorithm namely Concave-Convex Procedure ( CCP ) ﬁnding local minima J . basic idea simple : tth iteration replace g ﬁrst order Taylor expansion wt , , g ( wt ) + ( cid:    ) w − wt , ∇g ( wt ) ( cid:    ) minimize Jt ( w ) = f ( w ) − g ( wt ) − ( cid:    ) w − wt , ∇g ( wt ) ( cid:    ) . (  .    ) Taking gradients setting zero shows Jt minimized setting ∇f ( wt+  ) = ∇g ( wt ) . (  .    ) iterations CCP toy minimization problem illustrated Figure  .   , complete algorithm listing found Algorithm  .  .       Optimization Fig .  .   . Given function left decompose diﬀerence two convex functions depicted right panel . CCP algorithm generates iterates matching points two convex curves tangent vectors . seen , iterates approach solution x =  .  . Algorithm  .  Concave-Convex Procedure   : Input : Initial point w  , maximum iterations , convex functions f , g   : Set =     : <   : Set wt+  = argminw f ( w ) − g ( wt ) − ( cid:    ) w − wt , ∇g ( wt ) ( cid:    ) = +     :   : end   : Return : wT Theorem  .   Let J function decomposed diﬀer- ence two convex functions e.g. , (  .    ) . iterates generated (  .    ) monotically decrease J . Furthermore , stationary point iterates local minima J . Proof Since f g convex f ( wt ) ≥ f ( wt+  ) + ( cid:    ) wt − wt+  , ∇f ( wt+  ) ( cid:    ) g ( wt+  ) ≥ g ( wt ) + ( cid:    ) wt+  − wt , ∇g ( wt ) ( cid:    ) . Adding two inequalities , rearranging , using (  .    ) shows J ( wt ) = f ( wt ) − g ( wt ) ≥ f ( wt+  ) − g ( wt+  ) = J ( wt+  ) , claimed . Let w∗ stationary point iterates . ∇f ( w∗ ) = ∇g ( w∗ ) , turn implies w∗ local minima J ∇J ( w∗ ) =   . number extensions CCP . mention passing . First , shown instances EM algorithm ( Sec- tion ? ? ) shown special cases CCP . Second , rate con-  .  .  .  .  .  .  .                  .  .  .  .  .  .  .                 .  Practical Advice     vergence CCP related eigenvalues positive semi-deﬁnite matrix ∇  ( f + g ) . Third , CCP also extended solve constrained problems form : f  ( w ) − g  ( w ) min w s.t . fi ( w ) − gi ( w ) ≤ ci =   , . . . , n . , , fi gi =   ,   , . . . , n assumed convex . every iteration , replace gi ﬁrst order Taylor approximation solve following constrained convex problem : f  ( w ) − g  ( wt ) + ( cid:    ) w − wt , ∇g  ( wt ) ( cid:    ) min w s.t . fi ( w ) − gi ( wt ) + ( cid:    ) w − wt , ∇gi ( wt ) ( cid:    ) ≤ ci =   , . . . , n .  .  Practical Advice range optimization algorithms presented chapter might somewhat intimidating beginner . simple rules thumb alleviate anxiety Code Reuse : Implementing eﬃcient optimization algorithm correctly time consuming error prone . Therefore , far possible use existing libraries . number high class optimization libraries com- mercial open source exist . Unconstrained Problems : unconstrained minimization smooth convex function LBFGS ( Section  . . .  algorithm choice . many practical situations spectral gradient method ( Section  . . .  ) also competitive . also added advantage easy imple- ment . function minimized non-smooth Bundle methods ( Section  . .  ) preferred . Amongst diﬀerent formulations , Bundle Trust algorithm tends quite robust . Constrained Problems : constrained problems important understand nature constraints . Simple equality ( Ax = b ) box ( l ≤ x ≤ u ) constraints easier handle general non-linear constraints . objective function smooth , constraint set Ω simple , orthogonal projections PΩ easy compute , spectral projected gradient ( Section  . .  ) method choice . optimization problem QP LP specialized solvers tend much faster general purpose solvers .       Optimization Large Scale Problems : parameter vector high dimensional consider coordinate descent ( Section  . .  ) especially one dimensional line search along coordinate carried eﬃciently . objective function made summation large number terms , consider stochastic gradient descent ( Section  . .  ) . Although algorithms guarantee accurate solution , practical experience shows large scale machine learning problems rarely necessary . Duality : Sometimes problems hard optimize primal may become simpler dual . instance , objective function strongly convex non-smooth , Fenchel conjugate smooth Lipschitz continuous gradient . Problems Problem  .  ( Intersection Convex Sets {   } ) C  C  con- vex sets , show C  ∩ C  also convex . Extend result show ( cid:   ) n i=  Ci convex Ci convex . Problem  .  ( Linear Transform Convex Sets {   } ) Given set C ⊂ Rn linear transform ∈ Rm×n , deﬁne AC : = { = Ax : x ∈ C } . C convex show AC also convex . Problem  .  ( Convex Combinations {   } ) Show subset Rn convex contains convex combination elements . Problem  .  ( Convex Hull {   } ) Show convex hull , conv ( X ) smallest convex set contains X . Problem  .  ( Epigraph Convex Function {   } ) Show func- tion satisﬁes Deﬁnition  .  , , epigraph convex . Problem  .  Prove Jensen ’ inequality (  .  ) . Problem  .  ( Strong convexity negative entropy {   } ) Show negative entropy (  .   )  -strongly convex respect ( cid:    ) · ( cid:    )   norm simplex . Hint : First show φ ( ) : = ( −   ) log −   ( t−  )   t+  ≥   ≥   . Next substitute = xi/yi show ( cid:   ) ( xi − yi ) log ≥ ( cid:    ) x − ( cid:    )     . xi yi   .  Practical Advice     Problem  .  ( Strongly Convex Functions {   } ) Prove  .   ,  .   ,  .    .   . Problem  .  ( Convex Functions Lipschitz Continuous Gradient {   } ) Prove  .   ,  .   ,  .    .   . Problem  .   ( One Dimensional Projection {   } ) f : Rd → R convex , show arbitrary x p Rd one dimensional function Φ ( η ) : = f ( x + ηp ) also convex . Problem  .   ( Quasi-Convex Functions {   } ) Section  .  showed below-sets convex function Xc : = { x | f ( x ) ≤ c } convex . Give counter-example show converse true , , exist non-convex functions whose below-sets convex . class functions called Quasi-Convex . Problem  .   ( Gradient p-norm {   } ) Show gradient p-norm (  .   ) given (  .   ) . Problem  .   Derive Fenchel conjugate following functions f ( x ) = ( cid:   )   x ∈ C ∞ otherwise . f ( x ) = ax + b C convex set x ( cid:   ) Ax positive deﬁnite matrix f ( x ) =     f ( x ) = − log ( x ) f ( x ) = exp ( x ) f ( x ) = x log ( x ) Problem  .   ( Convergence gradient descent {   } ) Suppose J Lipschitz continuous gradient modulus L. show Algorithm  .  inexact line search satisfying Wolfe conditions (  .   ) (  .   ) return solution wt ( cid:    ) ∇J ( wt ) ( cid:    ) ≤ ( cid:   ) (  / ( cid:   )   ) iter- ations . Problem  .   Show   + ( cid:   ) ( cid:   ) t=  t=   √     ≤   √        Optimization Problem  .   ( Coordinate Descent Quadratic Programming {   } ) Derive projection based method uses coordinate descent generate directions descent solving following box constrained QP :     w ( cid:   ) Qw + c ( cid:   ) w min w∈Rn s.t . l ≤ w ≤ u . may assume Q positive deﬁnite l u scalars . Problem  .   ( Dual LP {   } ) Show dual LP (  .    ) (  .    ) . words , recover primal computing dual dual .   Online Learning Boosting far learning algorithms considered assumed training data available building model predicting labels unseen data points . many modern applications data available streaming fashion , one needs predict labels ﬂy . describe concrete example , consider task spam ﬁltering . emails arrive learning algorithm needs classify spam ham . Tasks tackled via online learning . Online learning proceeds rounds . round training example revealed learning algorithm , uses current model predict label . true label revealed learner incurs loss updates model based feedback provided . protocol summarized Algorithm  .  . goal online learning minimize total loss incurred . appropriate choice labels loss functions , setting encompasses large number tasks classiﬁcation , regression , density estimation . spam detection example , email misclassiﬁed user provide feedback used update spam ﬁlter , goal minimize number misclassiﬁed emails .  .  Halving Algorithm halving algorithm conceptually simple , yet illustrates many concepts online learning . Suppose access set n experts , , functions fi map input space X output space = { ±  } . Furthermore , assume one experts consistent , , exists j ∈ {   , . . . , n } fj ( xt ) = yt =   , . . . , . halving algorithm maintains set Ct consistent experts time t. Initially C  = {   , . . . , n } , updated recursively Ct+  = { ∈ Ct s.t . fi ( xt+  ) = yt+  } . (  .  ) prediction new data point computed via majority vote amongst consistent experts : ˆyt = majority ( Ct ) . Lemma  .  Halving algorithm makes log  ( n ) mistakes .           Online Learning Boosting Algorithm  .  Protocol Online Learning   : =   , . . . ,   : Get training instance xt Predict label ˆyt   :   : Get true label yt   :   : Update model   : end Incur loss l ( ˆyt , xt , yt ) Proof Let denote total number mistakes . halving algorithm makes mistake iteration least half consistent experts Ct predict wrong label . turn implies |Ct+ | ≤ |Ct|   ≤ |C |  M = n  M . hand , since one experts consistent follows   ≤ |Ct+ | . Therefore ,  M ≤ n. Solving completes proof .  .  Weighted Majority turn scenario none experts consistent . There- fore , aim minimize number mistakes minimize regret . chapter consider online methods solving following optimization problem : J ( w ) J ( w ) = min w∈Ω  ( cid:   ) t=  ft ( w ) . (  .  ) Suppose access function ψ continuously diﬀerentiable strongly convex modulus strong convexity σ >   ( see Section  . .  deﬁnition strong convexity ) , deﬁne Bregman divergence (  .   ) corresponding ψ ∆ψ ( w , w ( cid:   ) ) = ψ ( w ) − ψ ( w ( cid:   ) ) − ( cid:   ) w − w ( cid:   ) , ∇ψ ( w ( cid:   ) ) ( cid:   ) . also generalize orthogonal projection (  .    ) replacing square Euclidean norm Bregman divergence : Pψ , Ω ( w ( cid:   ) ) = argmin w∈Ω ∆ψ ( w , w ( cid:   ) ) . (  .  )  .  Weighted Majority     Algorithm  .  Stochastic ( sub ) gradient Descent   : Input : Initial point x  , maximum iterations   : =   , . . . ,   : Compute ˆwt+  = ∇ψ∗ ( ∇ψ ( wt ) − ηtgt ) gt = ∂wft ( wt ) Set wt+  = Pψ , Ω ( ˆwt+  )   :   : end   : Return : wT +  Denote w∗ = Pψ , Ω ( w ( cid:   ) ) . like Euclidean distance non-expansive , Bregman projection also shown non-expansive following sense : ∆ψ ( w , w ( cid:   ) ) ≥ ∆ψ ( w , w∗ ) + ∆ψ ( w∗ , w ( cid:   ) ) w ∈ Ω . diameter Ω measured ∆ψ given diamψ ( Ω ) = max w , w ( cid:   ) ∈Ω ∆ψ ( w , w ( cid:   ) ) . (  .  ) (  .  ) rest chapter make following standard assumptions : • ft convex revealed time instance . • Ω closed convex subset Rn non-empty interior . • diameter diamψ ( Ω ) Ω bounded F < ∞ . • set optimal solutions (  .  ) denoted Ω∗ non-empty . • subgradient ∂wft ( w ) computed every w ∈ Ω . • Bregman projection (  .  ) computed every w ( cid:   ) ∈ Rn . • gradient ∇ψ , inverse ( ∇ψ ) −  = ∇ψ∗ computed . method employ solve (  .  ) given Algorithm  .  . analyzing performance algorithm would like discuss three special cases . First , Euclidean distance squared recovers projected stochastic gradient descent , second Entropy recovers Exponentiated gradient descent , third p-norms p >   recovers p-norm Perceptron . BUGBUG TODO . key result Lemma  .  given . found various guises diﬀerent places notably Lemma  .   .  [ ? ] , Theorem  .  Eq . (  .   ) (  .   ) [ ? ] , proof Theorem   [ ? ] , well Lemma   [ ? ] . prove slightly general variant ; allow projections arbitrary Bregman divergence also take account generalized version strong convexity ft . modiﬁcations allow us deal general settings within uniﬁed framework .       Online Learning Boosting Deﬁnition  .  say convex function f strongly convex respect another convex function ψ modulus λ f ( w ) − f ( w ( cid:   ) ) − ( cid:   ) w − w ( cid:   ) , µ ( cid:   ) ≥ λ∆ψ ( w , w ( cid:   ) ) µ ∈ ∂f ( w ( cid:   ) ) . usual notion strong convexity recovered setting ψ ( · ) =   (  .  )   ( cid:    ) · ( cid:    )   . Lemma  .  Let ft strongly convex respect ψ modulus λ ≥   t. w ∈ Ω sequences generated Algorithm  .  satisfy ∆ψ ( w , wt+  ) ≤ ∆ψ ( w , wt ) − ηt ( cid:    ) gt , wt − w ( cid:    ) + η    σ ( cid:    ) gt ( cid:    )   ≤ (   − ηtλ ) ∆ψ ( w , wt ) − ηt ( ft ( wt ) − ft ( w ) ) + (  .  ) η    σ ( cid:    ) gt ( cid:    )   . (  .  ) Proof prove result three steps . First upper bound ∆ψ ( w , wt+  ) ∆ψ ( w , ˆwt+  ) . consequence (  .  ) non-negativity Bregman divergence allows us write ∆ψ ( w , wt+  ) ≤ ∆ψ ( w , ˆwt+  ) . (  .  ) next step use Lemma  .   write ∆ψ ( w , wt ) + ∆ψ ( wt , ˆwt+  ) − ∆ψ ( w , ˆwt+  ) = ( cid:    ) ∇ψ ( ˆwt+  ) − ∇ψ ( wt ) , w − wt ( cid:    ) . Since ∇ψ∗ = ( ∇ψ ) −  , update step   Algorithm  .  equivalently written ∇ψ ( ˆwt+  ) − ∇ψ ( wt ) = −ηtgt . Plugging equation rearranging ∆ψ ( w , ˆwt+  ) = ∆ψ ( w , wt ) − ηt ( cid:    ) gt , wt − w ( cid:    ) + ∆ψ ( wt , ˆwt+  ) . (  .   ) Finally upper bound ∆ψ ( wt , ˆwt+  ) . need two observations :   ( cid:    ) ( cid:    )   x , ∈ Rn σ >   . Second , σ First , ( cid:    ) x , ( cid:    ) ≤     ( cid:    ) wt − ˆwt+  ( cid:    )   . strong convexity ψ allows us bound ∆ψ ( ˆwt+  , wt ) ≥ σ Using two observations  σ ( cid:    ) x ( cid:    )   + σ ∆ψ ( wt , ˆwt+  ) = ψ ( wt ) − ψ ( ˆwt+  ) − ( cid:    ) ∇ψ ( ˆwt+  ) , wt − ˆwt+  ( cid:    ) = − ( ψ ( ˆwt+  ) − ψ ( wt ) − ( cid:    ) ∇ψ ( wt ) , ˆwt+  − wt ( cid:    ) ) + ( cid:    ) ηtgt , wt − ˆwt+  ( cid:    ) = −∆ψ ( ˆwt+  , wt ) + ( cid:    ) ηtgt , wt − ˆwt+  ( cid:    ) σ   ( cid:    ) wt − ˆwt+  ( cid:    )   + ( cid:    ) wt − ˆwt+  ( cid:    )   ( cid:    ) gt ( cid:    )   + η    σ ≤ − = ( cid:    ) gt ( cid:    )   . (  .   ) σ   η    σ Inequality (  .  ) follows putting together (  .  ) , (  .   ) , (  .   ) , (  .  ) follows using (  .  ) f = ft w ( cid:   ) = wt substituting  .  Weighted Majority (  .  ) . ready prove regret bounds .     Lemma  .  Let w∗ ∈ Ω∗ denote best parameter chosen hindsight , let ( cid:    ) gt ( cid:    ) ≤ L t. regret Algorithm  .  bounded via  ( cid:   ) t=  ft ( wt ) − ft ( w∗ ) ≤ F ( cid:   ) − λ + ( cid:   )   ηT L   σ  ( cid:   ) t=  ηt . (  .   ) Proof Set w = w∗ rearrange (  .  ) obtain ft ( wt ) − ft ( w∗ ) ≤   ηt Summing ( (   − ληt ) ∆ψ ( w∗ , wt ) − ∆ψ ( w∗ , wt+  ) ) + ηt  σ ( cid:    ) gt ( cid:    )   .  ( cid:   ) t=  ft ( wt ) − ft ( w∗ ) ≤   ηt  ( cid:   ) t=  ( cid:    ) ( (   − ηtλ ) ∆ψ ( w∗ , wt ) − ∆ψ ( w∗ , wt+  ) ) + ( cid:    ) ( cid:    ) T  ( cid:    )  ( cid:   ) t=  ( cid:    ) Since diameter Ω bounded F ∆ψ non-negative ηt  σ ( cid:    ) ( cid:    ) T  ( cid:    ) gt ( cid:    )   . ( cid:    ) ∆ψ ( w∗ , w  ) −   ηT ∆ψ ( w∗ , wT +  ) +  ( cid:   ) t=  ∆ψ ( w∗ , wt ) ( cid:   )   ηt −   ηt−  ( cid:   ) − λ ∆ψ ( w∗ , w  ) +  ( cid:   ) t=  ∆ψ ( w∗ , wt ) ( cid:   )   ηt −   ηt−  ( cid:   ) − λ − λ − λ ( cid:   ) ( cid:   ) ( cid:   ) T  = ≤ ≤ ( cid:   )   η  ( cid:   )   η  ( cid:   )   η  − λ F +  ( cid:   ) t=  F ( cid:   )   ηt −   ηt−  ( cid:   ) − λ = F ( cid:   ) − λ . ( cid:   )   ηT hand , since subgradients Lipschitz continuous constant L follows T  ≤ L   σ  ( cid:   ) t=  ηt . Putting together bounds T  T  yields (  .   ) . Corollary  .  λ >   set ηt =   λt  ( cid:   ) t=  ft ( xt ) − ft ( x∗ ) ≤ L   σλ (   + log ( ) ) ,       Online Learning Boosting hand , λ =   , set ηt =  √    ( cid:   ) t=  ft ( xt ) − ft ( x∗ ) ≤ ( cid:   ) F + ( cid:   ) √ . L  σ Proof First consider λ >   ηt =   consequently (  .   ) specializes λt . case   ηT = λ ,  ( cid:   ) t=  ft ( wt ) − ft ( w∗ ) ≤ L   σλ  ( cid:   ) t=     ≤ L   σλ (   + log ( ) ) . λ =   , set ηt =  √  use problem  .  rewrite (  .   )  ( cid:   ) t=  ft ( wt ) − ft ( w∗ ) ≤ F √ + L  σ  ( cid:   ) t=    √    √ + ≤ F √ . L  σ Problems Problem  .  ( Generalized Cauchy-Schwartz {   } ) Show ( cid:    ) x , ( cid:    ) ≤  σ ( cid:    ) x ( cid:    )   + σ     ( cid:    ) ( cid:    )   x , ∈ Rn σ >   . Problem  .  ( Bounding sum series {   } ) Show ( cid:   ) b √ b − +   . Hint : Upper bound sum integral . t=a   √    ≤   Conditional Densities number machine learning algorithms derived using condi- tional exponential families distribution ( Section  .  ) . Assume training set { ( x  , y  ) , . . . , ( xm , ym ) } drawn iid underlying distribution . Using Bayes rule (  .   ) one write likelihood p ( θ|X , ) ∝ p ( θ ) p ( |X , θ ) = p ( θ )  ( cid:   ) i=  p ( yi|xi , θ ) , (  .  ) hence negative log-likelihood − log p ( θ|X , ) = −  ( cid:   ) i=  log p ( yi|xi , θ ) − log p ( θ ) + const . (  .  ) prior knowledge data , choose zero mean unit variance isotropic normal distribution p ( θ ) . yields − log p ( θ|X , ) =     ( cid:    ) θ ( cid:    )   −  ( cid:   ) i=  log p ( yi|xi , θ ) + const . (  .  ) Finally , assume conditional exponential family model p ( y|x , θ ) , , p ( y|x , θ ) = exp ( ( cid:    ) φ ( x , ) , θ ( cid:    ) − g ( θ|x ) ) , (  .  )  − log p ( θ|X , ) =     ( cid:    ) θ ( cid:    )   +  ( cid:   ) i=   g ( θ|xi ) − ( cid:    ) φ ( xi , yi ) , θ ( cid:    ) + const . (  .  ) g ( θ|x ) = log ( cid:   ) y∈Y exp ( ( cid:    ) φ ( x , ) , θ ( cid:    ) ) , (  .  ) log-partition function . Clearly , (  .  ) smooth convex objective function , algorithms unconstrained minimization Chapter             Conditional Densities used obtain maximum aposteriori ( MAP ) estimate θ . Given optimal θ , class label given x predicted using y∗ = argmax p ( y|x , θ ) .  (  .  ) chapter discuss number algorithms derived specializing setup . discussion uniﬁes seemingly disparate algorithms , often discussed separately literature .  .  Logistic Regression begin simplest case namely binary classiﬁcation  . key ob- servation labels ∈ { ±  } hence g ( θ|x ) = log ( exp ( ( cid:    ) φ ( x , +  ) , θ ( cid:    ) ) + exp ( ( cid:    ) φ ( x , −  ) , θ ( cid:    ) ) ) . (  .  ) Deﬁne ˆφ ( x ) : = φ ( x , +  ) − φ ( x , −  ) . Plugging (  .  ) (  .  ) , using deﬁnition ˆφ rearranging p ( = + |x , θ ) =   ( cid:   ) ( cid:   ) − ˆφ ( x ) , θ ( cid:   ) ( cid:   )   + exp p ( = − |x , θ ) =   ( cid:   ) ( cid:   ) ˆφ ( x ) , θ ( cid:   ) ( cid:   ) ,   + exp compactly p ( y|x , θ ) = ( cid:   ) ( cid:   )   −y ˆφ ( x ) , θ ( cid:   ) ( cid:   ) .   + exp (  .  ) Since p ( y|x , θ ) logistic function , hence name logistic regression . classiﬁcation rule (  .  ) case specializes follows : predict +  when- ever p ( = + |x , θ ) ≥ p ( = − |x , θ ) otherwise predict −  . However log p ( = + |x , θ ) p ( = − |x , θ ) ( cid:   ) ˆφ ( x ) , θ ( cid:   ) , = prediction func- therefore one equivalently use sign tion . Using (  .  ) write objective function logistic regression  ( cid:   ) ( cid:   ) ˆφ ( x ) , θ ( cid:   ) ( cid:   )     ( cid:    ) θ ( cid:    )   +  ( cid:   ) i=  ( cid:   ) log   + exp ( cid:   ) ( cid:   ) −yi ˆφ ( xi ) , θ ( cid:   ) ( cid:   ) ( cid:   )   name logistic regression misnomer !  .  Regression     minimize objective function ﬁrst compute gradient . ∇J ( θ ) = θ +  ( cid:   ) exp i=    + exp ( cid:   ) ( cid:   ) ( cid:   ) ( cid:   ) −yi ˆφ ( xi ) , θ ( cid:   ) ( cid:   ) −yi ˆφ ( xi ) , θ ( cid:   ) ( cid:   ) ( −yi ˆφ ( xi ) )  ( cid:   ) ( p ( yi|xi , θ ) −   ) yi ˆφ ( xi ) . = θ + i=  Notice second term gradient vanishes whenever p ( yi|xi , θ ) =   . Therefore , one way interpret logistic regression view method maximize p ( yi|xi , θ ) point ( xi , yi ) training set . Since objective function logistic regression twice diﬀerentiable one also compute Hessian ∇ J ( θ ) = −  ( cid:   ) i=  p ( yi|xi , θ ) (   − p ( yi|xi , θ ) ) ˆφ ( xi ) ˆφ ( xi ) ( cid:   ) , used y  ( Section  . .  ) obtain optimal parameter θ . =   . Hessian used Newton method  .  Regression  . .  Conditionally Normal Models ﬁxed variance  . .  Posterior Distribution integrating vs. Laplace approximation , eﬃcient estimation ( sparse greedy )  . .  Heteroscedastic Estimation explain two parameters . many details ( assignment ) .  .  Multiclass Classiﬁcation  . .  Conditionally Multinomial Models joint feature map      .  CRF ? • Motivation learning digit example • general deﬁnition • Gaussian process + structure = CRF   Conditional Densities  . .  Linear Chain CRFs • Graphical model • Applications • Optimization problem  . .  Higher Order CRFs •  -d CRFs applications vision • Skip chain CRFs • Hierarchical CRFs ( graph transducers , sutton et . al . JMLR etc )  . .  Kernelized CRFs • feature maps kernels • clique decomposition theorem • representer theorem • Optimization strategies kernelized CRFs  .  Optimization Strategies  . .  Getting Started • three things needed optimize – MAP estimate – log-partition function – gradient log-partition function • Worked example ( linear chain ? )  . .  Optimization Algorithms - Optimization algorithms ( LBFGS , SGD , EG ( Globerson et . al ) )  . .  Handling Higher order CRFs - things done higher order CRFs ( brieﬂy )  .  Hidden Markov Models  .  Hidden Markov Models     • Deﬁnition • Discuss modeling joint distribution p ( x , ) • way predict marginalizing x • wasteful CRFs generally outperform  .  Reading talk : • Details HMM optimization • CRFs applied predicting parse trees via matrix tree theorem ( collins , koo et al ) • CRFs graph matching problems • CRFs Gaussian distributions ( yes exist )  . .  Optimization issues optimization ( blows number classes ) . structure . better ? Problems Problem  .  Poisson models Problem  .  Bayes Committee Machine Problem  .  Newton / CG approach   Kernels Function Spaces Kernels measures similarity . Broadly speaking , machine learning al- gorithms rely dot product instances “ ker- nelized ” replacing instances ( cid:    ) x , x ( cid:   ) ( cid:    ) kernel function k ( x , x ( cid:   ) ) . saw examples algorithms Sections  . .   . .  see many examples Chapter   . Arguably , design good ker- nel underlies success machine learning many applications . chapter lay ground theoretical properties kernels present number examples . Algorithms use kernels found later chapters .  .  Basics Let X denote space inputs k : X × X → R function satisﬁes k ( x , x ( cid:   ) ) = ( cid:    ) Φ ( x ) , Φ ( x ) ( cid:    ) (  .  ) Φ feature map maps X dot product space H. words , kernels correspond dot products dot product space . main advantage using kernel similarity measure threefold : First , feature space rich enough , simple estimators hyperplanes half-spaces may suﬃcient . instance , classify points Figure BUGBUG , need nonlinear decision boundary , map points   dimensional space hyperplane suﬃces . Second , kernels allow us construct machine learning algorithms dot product space H without explicitly computing Φ ( x ) . Third , need make assumptions input space X set . see later chapter , allows us compute similarity discrete objects strings , trees , graphs . ﬁrst half chapter present examples kernels , discuss theoretical properties kernels second half .          . .  Examples   Kernels Function Spaces  . . .  Linear Kernel Linear kernels perhaps simplest kernels . assume x ∈ Rn deﬁne k ( x , x ( cid:   ) ) = ( cid:   ) x , x ( cid:   ) ( cid:   ) = ( cid:   ) xix ( cid:   ) .  x x ( cid:   ) dense computing kernel takes ( n ) time . hand , sparse vectors reduced ( |nnz ( x ) ∩ nnz ( x ( cid:   ) ) | ) , nnz ( · ) denotes set non-zero indices vector | · | de- notes size set . Linear kernels natural representation use vectorial data . also widely used text mining documents represented vector containing frequency occurrence words ( Recall encountered so-called bag words representation Chapter   ) . Instead simple bag words , one also map text set pairs words co-occur sentence richer representation .  . . .  Polynomial Kernel Given x ∈ Rn , compute feature map Φ taking d-th order products ( also called monomials ) entries x . illustrate concrete example , let us consider x = ( x  , x  ) =   , ( cid:  ) . Although tedious compute Φ ( x ) case Φ ( x ) = ( cid:  ) x  Φ ( x ( cid:   ) ) explicitly order compute k ( x , x ) , shortcut following proposition shows .   , x x  , x x    , x  Proposition  .  Let Φ ( x ) ( resp . Φ ( x ( cid:   ) ) ) denote vector whose entries possible d-th degree ordered products entries x ( resp . x ( cid:   ) ) .  k ( x , x ( cid:   ) ) = ( cid:   ) Φ ( x ) , Φ ( x ( cid:   ) ) ( cid:   ) = ( cid:  ) ( cid:   ) x , x ( cid:   ) ( cid:   ) ( cid:  ) . (  .  ) Proof direct computation ( cid:   ) Φ ( x ) , Φ ( x ( cid:   ) ) ( cid:   ) = ( cid:   ) . . . ( cid:   ) xj  . . . xjd · x ( cid:   ) j  . . . x ( cid:   ) jd j  jd ( cid:   ) = j  xj  · x ( cid:   ) j  . . . ( cid:   ) jd xjd · x ( cid:   ) jd =  xj · x ( cid:   ) j  ( cid:   ) j    = ( cid:  ) ( cid:   ) x , x ( cid:   ) ( cid:   ) ( cid:  )  .  Basics     kernel (  .  ) called polynomial kernel . useful extension inhomogeneous polynomial kernel k ( x , x ( cid:   ) ) = ( cid:  ) ( cid:   ) x , x ( cid:   ) ( cid:   ) + c ( cid:  ) , (  .  ) computes monomials degree ( problem  .  ) .  . . .  Radial Basis Function Kernels  . . .  Convolution Kernels framework convolution kernels general way extend notion kernels structured objects strings , trees , graphs . Let x ∈ X discrete object decomposed P parts xp ∈ Xp many diﬀerent ways . concrete example consider string x = abc split two sets substrings size two namely { , bc } { ab , c } . denote set decompositions R ( x ) , use build kernel X follows : [ k  ( cid:   ) . . . ( cid:   ) kP ] ( x , x ( cid:   ) ) = ( cid:   ) P ( cid:   ) ¯x∈R ( x ) , ¯x ( cid:   ) ∈R ( x ( cid:   ) ) p=  kp ( ¯xp , ¯x ( cid:   ) p ) . (  .  )   , . . . , ¯x ( cid:   ) , sum possible ways decompose x x ( cid:   ) ¯x  , . . . , ¯xP ¯x ( cid:   ) P respectively . cardinality R ( x ) ﬁnite , shown (  .  ) results valid kernel . Although convolution kernels provide abstract framework , speciﬁc instantiations idea lead rich set kernels discrete objects . discuss detail .  . . .  String Kernels basic idea behind string kernels simple : Compare strings means subsequences contain . number common sub- sequences , similar two strings . subsequences need equal weights . instance , weight subsequence may given inverse frequency occurrence . Similarly , ﬁrst last characters subsequence rather far apart , contribution kernel must down-weighted . Formally , string x composed characters ﬁnite alphabet Σ |x| denotes length . say subsequence x = x x  . . . x|x| = xi xi  . . . xi|s|   ≤ i  < i  < . . . < i|s| ≤ |x| . particular , ii+  = ii +   substring x . example , acb subsequence adbc abc subsequence adc substring . Assume exists function # ( x , ) returns number times subsequence       Kernels Function Spaces occurs x non-negative weighting function w ( ) ≥   returns weight associated s. basic string kernel written k ( x , x ( cid:   ) ) = ( cid:   )  # ( x , ) # ( x ( cid:   ) , ) w ( ) . (  .  ) Diﬀerent string kernels derived specializing equation : substrings kernel : restrict summation (  .  ) sub- strings [ VS   ] provide suﬃx tree based algorithm allows one compute arbitrary w ( ) kernel k ( x , x ( cid:   ) ) ( |x| + |x ( cid:   ) | ) time memory . k-Spectrum kernel : k-spectrum kernel obtained restricting summation (  .  ) substrings length k. slightly general variant considers substrings length k. k tuning parameter typically set small number ( e.g. ,   ) . simple trie based algorithm used compute k-spectrum kernel ( ( |x| + |x ( cid:   ) | ) k ) time ( problem  .  ) . Inexact substring kernel : Sometimes input strings might measurement errors therefore desirable take account inexact matches . done replacing # ( x , ) (  .  ) another function # ( x , , ( cid:   ) ) reports number approximate matches x . ( cid:   ) denotes number mismatches allowed , typically small number ( e.g. ,   ) . trading oﬀ computational complexity storage kernel computed eﬃciently . See [ LK   ] details . Mismatch kernel : Instead simply counting number occurrences substring use weighting scheme down-weights contribu- tions longer subsequences yields so-called mismatch kernel . Given index sequence = ( i  , . . . , ik )   ≤ i  < i  < . . . < ik ≤ |x| associate subsequence x ( ) = xi xi  . . . xik . Furthermore , deﬁne |I| = ik − i  +   . Clearly , |I| > k contiguous . Let λ ≤   decay factor . Redeﬁne # ( x , ) = ( cid:   ) λ|I| , s=x ( ) (  .  ) , count occurrences x weight associated subsequence depends length . illustrate , consider subsequence abc occurs string abcebc twice , namely , abcebc abcebc . ﬁrst occurrence counted weight λ  second occurrence counted weight λ  . turns , kernel computed dynamic programming algorithm ( problem BUGBUG ) ( |x| · |x ( cid:   ) | ) time .  .  Basics  . . .  Graph Kernels     two diﬀerent notions graph kernels . First , kernels graphs used compare nodes single graph . contrast , kernels graphs focus comparing two graphs . random walk ( continuous time limit , diﬀusion ) underlie types kernels . basic intuition two nodes similar number paths connect two graphs similar share many common paths . describe kernels formally need introduce notation . graph G consists ordered set n vertices V = { v  , v  , . . . , vn } , set directed edges E ⊂ V ×V . vertex vi said neighbor another vertex vj connected edge , i.e. , ( vi , vj ) ∈ E ; also denoted vi ∼ vj . adjacency matrix graph n × n matrix Aij =   vi ∼ vj ,   otherwise . walk length k G sequence indices i  , i  , . . . ik vir−  ∼ vir   ≤ r ≤ k . adjacency matrix normalized cousin , deﬁned ˜A : = D− A , property rows sums one , therefore serve transition matrix stochastic process . , diag- onal matrix node degrees , i.e. , Dii = di = ( cid:   ) j Aij . random walk G process generating sequences vertices vi  , vi  , vi  , . . . according P ( ik+ |i  , . . . ik ) = ˜Aik , ik+  . tth power ˜A thus describes t-length walks , i.e. , ( ˜At ) ij probability transition vertex vj vertex vi via walk length ( problem BUGBUG ) . p  initial probability dis- tribution vertices , probability distribution pt describing location random walker time pt = ˜Atp  . jth component pt denotes probability ﬁnishing t-length walk vertex vj . random walk need continue indeﬁnitely ; model , associate every node vik graph stopping probability qik . overall probability stopping steps given q ( cid:   ) pt . Given two graphs G ( V , E ) G ( cid:   ) ( V ( cid:   ) , E ( cid:   ) ) , direct product G× graph vertex set V× = { ( vi , v ( cid:   ) r ) : vi ∈ V , v ( cid:   ) r ∈ V ( cid:   ) } , edge set E× = { ( ( vi , v ( cid:   ) r ) , ( vj , v ( cid:   ) ) ) : ( vi , vj ) ∈ E ∧ ( v ( cid:   ) r , v ( cid:   ) ) ∈ E ( cid:   ) } . (  .  ) (  .  ) words , G× graph pairs vertices G G ( cid:   ) , two vertices G× neighbors corresponding vertices G G ( cid:   ) neighbors ; see Figure  .  illustration . ( cid:   ) respective adjacency matrices G G ( cid:   ) , adjacency       Kernels Function Spaces    ’    ’    ’       G  G    ’   ’   ’   ’    ’    ’    ’    ’    ’    ’    ’    ’ G×    ’ Fig .  .  . Two graphs ( G  & G  ) direct product ( G× ) . node direct product graph labeled pair nodes (  .  ) ; edge exists direct product corresponding nodes adjacent original graphs (  .  ) . instance , nodes    ( cid:   )    ( cid:   ) adjacent edge nodes     ﬁrst ,   ( cid:   )   ( cid:   ) second graph . matrix G× A× = ⊗ ( cid:   ) . Similarly , ˜A× = ˜A ⊗ ˜A ( cid:   ) . Performing random walk direct product graph equivalent performing simultaneous random walk G G ( cid:   ) . p p ( cid:   ) denote initial probability distributions vertices G G ( cid:   ) , corresponding initial probability distribution direct product graph p× : = p ⊗ p ( cid:   ) . Likewise , q q ( cid:   ) stopping probabilities ( , probability random walk ends given vertex ) , stopping probability direct product graph q× : = q ⊗ q ( cid:   ) . deﬁne kernel computes similarity G G ( cid:   ) , one ˜At natural idea simply sum q ( cid:   ) ×p× values t. However , × sum might converge , leaving kernel value undeﬁned . overcome problem , introduce appropriately chosen non-negative coeﬃcients µ ( ) , deﬁne kernel G G ( cid:   ) k ( G , G ( cid:   ) ) : = ∞ ( cid:   ) t=  µ ( ) q ( cid:   ) × ˜At ×p× . (  .  ) idea extended graphs whose nodes associated labels replacing matrix ˜A× matrix label similarities . appro- priate choices µ ( ) sum converges eﬃcient algorithms computing kernel devised . See [ ? ] details . turns , simple idea performing random walk prod-  .  Kernels     uct graph extended compute kernels Auto Regressive Moving Average ( ARMA ) models [ VSV   ] . Similarly , also used deﬁne kernels transducers . Connections so-called rational ker- nels transducers graph kernels deﬁned via (  .  ) made explicit [ ? ] .  .  Kernels  . .  Feature Maps give examples , linear classiﬁer , nonlinear ones r -r  map  . .  Kernel Trick  . .  Examples Kernels gaussian , polynomial , linear , texts , graphs - stress fact diﬀerence structure input space structure output space  .  Algorithms  . .  Kernel Perceptron  . .  Trivial Classiﬁer  . .  Kernel Principal Component Analysis  .  Reproducing Kernel Hilbert Spaces turns , class functions coincides class positive semi-deﬁnite functions . Intuitively , notion positive semi-deﬁnite function extension familiar notion positive semi-deﬁnite matrix ( also see Appendix BUGBUG ) : Deﬁnition  .  real n × n symmetric matrix K satisfying ( cid:   ) αiαjKi , j ≥   (  .   ) , j αi , αj ∈ R called positive semi-deﬁnite . equality (  .   ) occurs α  , . . . , αn =   , K said positive deﬁnite . Deﬁnition  .  Given set points x  , . . . , xn ∈ X function k , matrix Ki , j = k ( xi , xj ) (  .   )       Kernels Function Spaces called Gram matrix kernel matrix k respect x  , . . . , xn . Deﬁnition  .  Let X nonempty set , k : X × X → R function . k gives rise positive ( semi- ) deﬁnite Gram matrix x  , . . . , xn ∈ X n ∈ N k said positive ( semi- ) deﬁnite . Clearly , every kernel function k form (  .  ) positive semi-deﬁnite . see simply write ( cid:   ) , j αiαjk ( xi , xj ) = ( cid:   ) , j αiαj ( cid:    ) xi , xj ( cid:    ) = ( cid:   ) ( cid:   )  αixi , ( cid:   ) j ( cid:   ) αjxj ≥   . establish converse , , show every positive semi- deﬁnite kernel function written (  .  ) . Towards end , deﬁne map Φ X space functions mapping X R ( denoted RX ) via Φ ( x ) = k ( · , x ) . words , Φ ( x ) : X → R function assigns value k ( x ( cid:   ) , x ) x ( cid:   ) ∈ X . Next construct vector space taking possible linear combinations Φ ( x ) f ( · ) = n ( cid:   ) i=  αiΦ ( xi ) = n ( cid:   ) i=  αik ( · , xi ) , (  .   ) ∈ N , αi ∈ R , xi ∈ X arbitrary . space endowed natural dot product ( cid:    ) f , g ( cid:    ) = n ( cid:   ) n ( cid:   ) ( cid:   ) i=  j=  αiβjk ( xi , x ( cid:   ) j ) . (  .   ) see dot product well deﬁned even though contains expansion coeﬃcients ( need unique ) , note ( cid:    ) f , g ( cid:    ) = ( cid:   ) n ( cid:   ) j ) , independent αi . Similarly , g , note ( cid:    ) f , g ( cid:    ) = ( cid:   ) n j=  βjf ( x ( cid:   ) i=  αif ( xi ) , time independent βj . also shows ( cid:    ) f , g ( cid:    ) bilinear . Symme- try follows ( cid:    ) f , g ( cid:    ) = ( cid:    ) g , f ( cid:    ) , positive semi-deﬁniteness k implies ( cid:    ) f , f ( cid:    ) = ( cid:   ) , j αiαjk ( xi , xj ) ≥   . (  .   ) Applying (  .   ) shows functions (  .   ) particular ( cid:    ) f , k ( · , x ) ( cid:    ) = f ( x ) . ( cid:   ) k ( · , x ) , k ( · , x ( cid:   ) ) ( cid:   ) = k ( x , x ( cid:   ) ) . (  .   ) (  .   )  .  Reproducing Kernel Hilbert Spaces     view properties , k called reproducing kernel . using (  .   ) following property positive semi-deﬁnite functions ( problem  .  ) k ( x , x ( cid:   ) )   ≤ k ( x , x ) · k ( x ( cid:   ) , x ( cid:   ) ) (  .   ) write |f ( x ) |  = | ( cid:    ) f , k ( · , x ) ( cid:    ) | ≤ k ( x , x ) · ( cid:    ) f , f ( cid:    ) . (  .   ) inequality , f =   whenever ( cid:    ) f , f ( cid:    ) =   , thus establishing ( cid:    ) · , · ( cid:    ) valid dot product . fact , one complete space functions (  .   ) norm corresponding dot product (  .   ) , thus get Hilbert space H , called reproducing kernel Hilbert Space ( RKHS ) . alternate way deﬁne RKHS Hilbert space H functions input space X R property f ∈ H x ∈ X , point evaluations f → f ( x ) continuous ( particular , points values f ( x ) well deﬁned , already distinguishes RKHS many L  Hilbert spaces ) . Given point evaluation functional , one construct reproducing kernel using Riesz representation theorem . Moore-Aronszajn theorem states , every positive semi- deﬁnite kernel X × X , exists unique RKHS vice versa . ﬁnish section noting ( cid:    ) · , · ( cid:    ) positive semi-deﬁnite func- tion vector space functions (  .   ) . follows directly bilinearity dot product (  .   ) write functions f  , . . . , fp coeﬃcients γ  , . . . , γp ( cid:   ) ( cid:   )  j γiγj ( cid:    ) fi , fj ( cid:    ) = ( cid:   ) ( cid:   )  γifi , ( cid:   ) j ( cid:   ) γjfj ≥   . (  .   )  . .  Hilbert Spaces evaluation functionals , inner products  . .  Theoretical Properties Mercer ’ theorem , positive semideﬁniteness  . .  Regularization Representer theorem , regularization      .  Banach Spaces  . .  Properties  . .  Norms Convex Sets   Kernels Function Spaces - smoothest function ( L  ) - smallest coeﬃcients ( L  ) - structured priors ( CAP formalism ) Problems Problem  .  Show (  .   ) holds arbitrary positive semi-deﬁnite function k . Problem  .  Show inhomogeneous polynomial kernel (  .  ) valid kernel computes monomials degree . Problem  .  ( k-spectrum kernel {   } ) Given two strings x x ( cid:   ) show one compute k-spectrum kernel ( section  . . .  ) ( ( |x| + |x ( cid:   ) | ) k ) time . Hint : need use trie .   Linear Models hyperplane space H endowed dot product ( cid:    ) · , · ( cid:    ) described set { x ∈ H| ( cid:    ) w , x ( cid:    ) + b =   } (  .  ) w ∈ H b ∈ R. hyperplane naturally divides H two half-spaces : { x ∈ H| ( cid:    ) w , x ( cid:    ) + b ≥   } { x ∈ H| ( cid:    ) w , x ( cid:    ) + b <   } , hence used decision boundary binary classiﬁer . chapter study number algorithms employ linear decision boundaries . Although models look restrictive ﬁrst glance , combined kernels ( Chapter   ) yield large class useful algorithms . algorithms study chapter maximize margin . Given set X = { x  , . . . , xm } , margin distance closest point X hyperplane (  .  ) . Elementary geometric arguments ( Problem  .  ) show distance point xi hyperplane given | ( cid:    ) w , xi ( cid:    ) + b |/ ( cid:    ) w ( cid:    ) , hence margin simply min i=  , ... , | ( cid:    ) w , xi ( cid:    ) + b | ( cid:    ) w ( cid:    ) . (  .  ) Note parameterization hyperplane (  .  ) unique ; multiply w b non-zero constant , obtain hyperplane . One way resolve ambiguity set min i=  , ... | ( cid:    ) w , xi ( cid:    ) + b| =   . case , margin simply becomes  / ( cid:    ) w ( cid:    ) . postpone justiﬁcation margin maximization later jump straight ahead description various algorithms .  .  Support Vector Classiﬁcation Consider binary classiﬁcation task , given training set { ( x  , y  ) , . . . , ( xm , ym ) } xi ∈ H yi ∈ { ±  } . aim ﬁnd linear decision boundary parameterized ( w , b ) ( cid:    ) w , xi ( cid:    ) + b ≥             Linear Models { x | ( cid:    ) w , x ( cid:    ) + b = −  } { x | ( cid:    ) w , x ( cid:    ) + b =   } yi = +  x  x  yi = −  w { x | ( cid:    ) w , x ( cid:    ) + b =   } ( cid:    ) w , x  ( cid:    ) + b = +  ( cid:    ) w , x  ( cid:    ) + b = −  ( cid:    ) w , x  − x  ( cid:    ) =   ( cid:    ) w ( cid:    ) , x  − x  ( cid:   ) =   ( cid:    ) w ( cid:    ) ( cid:   ) w Fig .  .  . linearly separable toy binary classiﬁcation problem separating diamonds circles . normalize ( w , b ) ensure mini=  , ... | ( cid:    ) w , xi ( cid:    ) + b | =   . case , margin given   ( cid:    ) w ( cid:    ) calculation inset shows . whenever yi = +  ( cid:    ) w , xi ( cid:    ) +b <   whenever yi = −  . Furthermore , dis- cussed , ﬁx scaling w requiring mini=  , ... | ( cid:    ) w , xi ( cid:    ) +b | =   . compact way write desiderata require yi ( ( cid:    ) w , xi ( cid:    ) + b ) ≥   ( also see Figure  .  ) . problem maximizing margin therefore reduces equivalently max w , b s.t .   ( cid:    ) w ( cid:    ) yi ( ( cid:    ) w , xi ( cid:    ) + b ) ≥   , min w , b s.t . ( cid:    ) w ( cid:    )       yi ( ( cid:    ) w , xi ( cid:    ) + b ) ≥   . (  . a ) (  . b ) (  . a ) (  . b ) constrained convex optimization problem quadratic objec- tive function linear constraints ( see Section  .  ) . deriving (  .  ) implicitly assumed data linearly separable , , hyperplane correctly classiﬁes training data . classiﬁer called hard margin classiﬁer . data linearly separable , (  .  ) solution . deal situation introduce  .  Support Vector Classiﬁcation     non-negative slack variables ξi relax constraints : yi ( ( cid:    ) w , xi ( cid:    ) + b ) ≥   − ξi . Given w b constraints satisﬁed making ξi large enough . renders whole optimization problem useless . Therefore , one penalize large ξi . done via following modiﬁed optimization problem : min w , b , ξ s.t . ( cid:    ) w ( cid:    )   +     C   ( cid:   ) i=  ξi yi ( ( cid:    ) w , xi ( cid:    ) + b ) ≥   − ξi ξi ≥   , (  . a ) (  . b ) (  . c ) C >   penalty parameter . resultant classiﬁer said soft margin classiﬁer . introducing non-negative Lagrange multipliers αi βi one write Lagrangian ( see Section  .  ) L ( w , b , ξ , α , β ) = ( cid:    ) w ( cid:    )   +     C   ( cid:   ) i=  ξi +  ( cid:   ) i=  αi (   − ξi − yi ( ( cid:    ) w , xi ( cid:    ) + b ) ) −  ( cid:   ) i=  βiξi . Next take gradients respect w , b ξ set zero . ∇wL = w −  ( cid:   ) i=  αiyixi =   ∇bL = −  ( cid:   ) i=  αiyi =   ∇ξiL = C  − αi − βi =   . (  . a ) (  . b ) (  . c ) Substituting (  .  ) Lagrangian simplifying yields dual ob- jective function : −     ( cid:   ) , j yiyjαiαj ( cid:    ) xi , xj ( cid:    ) +  ( cid:   ) i=  αi , (  .  ) needs maximized respect α . notational convenience minimize negative (  .  ) . Next turn attention dual constraints . Recall αi ≥   βi ≥   , conjunc- tion (  . c ) immediately yields   ≤ αi ≤ C . Furthermore , (  . b ) ( cid:   ) i=  αiyi =   . Putting everything together , dual optimization problem     boils min α s.t .     ( cid:   ) , j yiyjαiαj ( cid:    ) xi , xj ( cid:    ) −  ( cid:   ) i=  αi  ( cid:   ) i=  αiyi =     ≤ αi ≤ C  .   Linear Models (  . a ) (  . b ) (  . c ) let H × matrix entries Hij = yiyj ( cid:    ) xi , xj ( cid:    ) , e , α , m-dimensional vectors whose i-th components one , αi , yi respectively , dual compactly written following Quadratic Program ( QP ) ( Section  . .  ) : α ( cid:   ) Hα − α ( cid:   ) e   min   α s.t . α ( cid:   ) =     ≤ αi ≤ C  . (  . a ) (  . b ) (  . c ) turning attention algorithms solving (  .  ) , number observations order . First , note computing H requires com- puting dot products training examples . map input data Reproducing Kernel Hilbert Space ( RKHS ) via feature map φ , still compute entries H solve optimal α . case , Hij = yiyj ( cid:    ) φ ( xi ) , φ ( xj ) ( cid:    ) = yiyjk ( xi , xj ) , k kernel associated RKHS . Given optimal α , one easily recover decision boundary . direct consequence (  . a ) , allows us write w linear combination training data : w =  ( cid:   ) i=  αiyiφ ( xi ) , hence decision boundary ( cid:    ) w , x ( cid:    ) + b =  ( cid:   ) i=  αiyik ( xi , x ) + b . (  .   ) KKT conditions ( Section  .  ) αi (   − ξi − yi ( ( cid:    ) w , xi ( cid:    ) + b ) ) =   βiξi =   . consider three cases yi ( ( cid:    ) w , xi ( cid:    ) + b ) implications KKT conditions ( see Figure  .  ) .  .  Support Vector Classiﬁcation     { x | ( cid:    ) w , x ( cid:    ) + b = −  } { x | ( cid:    ) w , x ( cid:    ) + b =   } Fig .  .  . picture depicts well classiﬁed points ( yi ( ( cid:    ) w , xi ( cid:    ) + b ) >   black , support vectors yi ( ( cid:    ) w , xi ( cid:    ) + b ) =   blue , margin errors yi ( ( cid:    ) w , xi ( cid:    ) + b ) <   red . yi ( ( cid:    ) w , xi ( cid:    ) + b ) <   : case , ξi >   , hence KKT conditions ( see (  . c ) ) . points imply βi =   . Consequently , αi = C said margin errors . yi ( ( cid:    ) w , xi ( cid:    ) + b ) >   : case , ξi =   , (  −ξi −yi ( ( cid:    ) w , xi ( cid:    ) +b ) ) <   , KKT conditions αi =   . points said well classiﬁed . easy see decision boundary (  .   ) change even points removed training set . yi ( ( cid:    ) w , xi ( cid:    ) + b ) =   : case ξi =   βi ≥   . Since αi non-negative satisﬁes (  . c ) follows   ≤ αi ≤ C . points said margin . also sometimes called support vectors . Since support vectors satisfy yi ( ( cid:    ) w , xi ( cid:    ) + b ) =   yi ∈ { ±  } follows b = yi − ( cid:    ) w , xi ( cid:    ) support vector xi . However , practice recover b average b = yi − ( cid:   )  ( cid:    ) w , xi ( cid:    ) . (  .   ) support vectors , , points xi   < αi < C . uses support vectors , overall algorithm called C-Support Vector classiﬁer C-SV classiﬁer short .       Linear Models  . .  Regularized Risk Minimization Viewpoint closer examination (  .  ) reveals ξi =   whenever yi ( ( cid:    ) w , xi ( cid:    ) +b ) >   . hand , ξi =   − yi ( ( cid:    ) w , xi ( cid:    ) + b ) whenever yi ( ( cid:    ) w , xi ( cid:    ) + b ) <   . short , ξi = max (   ,   − yi ( ( cid:    ) w , xi ( cid:    ) + b ) ) . Using observation one eliminate ξi (  .  ) , write following unconstrained optimization problem : min w , b     ( cid:    ) w ( cid:    )   + C   ( cid:   ) i=  max (   ,   − yi ( ( cid:    ) w , xi ( cid:    ) + b ) ) . (  .   ) Writing (  .  ) (  .   ) particularly revealing shows support vector classiﬁer nothing regularized risk minimizer . regularizer square norm decision hyperplane     ( cid:    ) w ( cid:    )   , loss function so-called binary hinge loss ( Figure  .  ) : l ( w , x , ) = max (   ,   − ( ( cid:    ) w , x ( cid:    ) + b ) ) . (  .   ) easy verify binary hinge loss (  .   ) convex non- diﬀerentiable ( see Figure  .  ) renders overall objective function (  .   ) convex non-smooth . two diﬀerent strategies minimize objective function . minimizing (  .   ) primal , one employ non-smooth convex optimizers bundle methods ( Section  . .  ) . yields dimensional problem dimension x . hand , since (  .   ) strongly convex presence     ( cid:    ) w ( cid:    )   term , Fenchel dual Lipschitz continuous gradient ( see Lemma  .   ) . dual problem dimensional contains linear constraints . strategy particularly attractive kernel trick used whenever ( cid:   ) m. fact , dual problem obtained via Fenchel duality related Quadratic programming problem (  .  ) obtained via Lagrange duality ( problem  .  ) .  . .  Exponential Family Interpretation motivating arguments deriving SVM algorithm largely geometric . show equally elegant probabilistic interpre- tation also exists . Assuming training set { ( x  , y  ) , . . . , ( xm , ym ) } drawn iid underlying distribution , using Bayes rule (  .   ) one write likelihood p ( θ|X , ) ∝ p ( θ ) p ( |X , θ ) = p ( θ )  ( cid:   ) i=  p ( yi|xi , θ ) , (  .   )  .  Support Vector Classiﬁcation     loss ( ( cid:    ) w , x ( cid:    ) + b ) Fig .  .  . binary hinge loss . Note loss convex non-diﬀerentiable kink point . Furthermore , increases linearly distance decision hyperplane ( ( cid:    ) w , x ( cid:    ) + b ) decreases . hence negative log-likelihood − log p ( θ|X , ) = −  ( cid:   ) i=  log p ( yi|xi , θ ) − log p ( θ ) + const . (  .   ) absence prior knowledge data , choose zero mean unit variance isotropic normal distribution p ( θ ) . yields − log p ( θ|X , ) =     ( cid:    ) θ ( cid:    )   −  ( cid:   ) i=  log p ( yi|xi , θ ) + const . (  .   ) maximum aposteriori ( MAP ) estimate θ obtained minimizing (  .   ) respect θ . Given optimal θ , predict class label given x via y∗ = argmax p ( y|x , θ ) .  (  .   ) course , aim maximize p ( yi|xi , θ ) also ensure p ( y|xi , θ ) small ( cid:   ) = yi . , instance , achieved requiring p ( yi|xi , θ ) p ( y|xi , θ ) ≥ η , ( cid:   ) = yi η ≥   . (  .   ) saw Section  .  exponential families distributions rather ﬂex- ible modeling tools . could , instance , model p ( yi|xi , θ ) conditional exponential family distribution . Recall deﬁnition : p ( y|x , θ ) = exp ( ( cid:    ) φ ( x , ) , θ ( cid:    ) − g ( θ|x ) ) . (  .   )       Linear Models φ ( x , ) joint feature map depends input data x label , g ( θ|x ) log-partition function . (  .   ) boils  p ( yi|xi , θ ) maxy ( cid:   ) =yi p ( y|xi , θ ) ( cid:   ) ( cid:   ) ( cid:   ) ( cid:   ) = exp φ ( xi , yi ) − max ( cid:   ) =yi φ ( xi , ) , θ ≥ η . (  .   ) choose η log η =   , set φ ( x , ) = ∈ { ±  } rewrite (  .   )   φ ( x ) , observe ( cid:   ) yi   φ ( xi ) − ( cid:   ) − ( cid:   ) yi   ( cid:   ) φ ( xi ) , θ = yi ( cid:    ) φ ( xi ) , θ ( cid:    ) ≥   . (  .   ) replacing − log p ( yi|xi , θ ) (  .   ) condition (  .   ) obtain following objective function : min θ s.t . ( cid:    ) θ ( cid:    )       yi ( cid:    ) φ ( xi ) , θ ( cid:    ) ≥   , (  .  a ) (  .  b ) recovers (  .  ) , without bias b . prediction function recovered noting (  .   ) specializes y∗ = argmax y∈ { ±  } ( cid:    ) φ ( x , ) , θ ( cid:    ) = argmax y∈ { ±  }    ( cid:    ) φ ( x ) , θ ( cid:    ) = sign ( ( cid:    ) φ ( x ) , θ ( cid:    ) ) . (  .   ) , replace (  .   ) linear penalty constraint viola- maxy ( cid:   ) =yi p ( y|xi , θ ) sometimes tion order recover (  .  ) . quantity log called log-odds ratio , discussion shows SVMs interpreted maximizing log-odds ratio exponential family . interpretation developed consider extensions SVMs tackle multiclass , multilabel , structured prediction problems . p ( yi|xi , θ )  . .  Specialized Algorithms Training SVMs main task training SVMs boils solving (  .  ) . × matrix H usually dense stored memory . Decomposition methods designed overcome diﬃculties . basic idea identify update small working set B solving small sub- problem every iteration . Formally , let B ⊂ {   , . . . , } working set αB corresponding sub-vector α. Deﬁne ¯B = {   , . . . , } \ B α ¯B analogously . order update αB need solve following     (  .  a ) (  .  b ) (  .  c )  .  Support Vector Classiﬁcation sub-problem (  .  ) obtained freezing α ¯B : ( cid:   ) HBB HB ¯B H ¯BB H ¯B ¯B ( cid:   ) ( cid:   ) ( cid:   ) αB α ¯B − ( cid:  ) α ( cid:   ) B α ( cid:   ) ¯B ( cid:  ) e ( cid:  ) α ( cid:   )   min   αB s.t . ( cid:  ) α ( cid:   ) ( cid:  ) B α ( cid:   ) ¯B ( cid:  ) =     ≤ αi ≤ B α ( cid:   ) ¯B C  ( cid:   ) HBB HB ¯B H ¯BB H ¯B ¯B ∈ B . ( cid:   ) , permutation matrix H. eliminating constant terms rearranging , one simplify problem BHBBαB + α ( cid:   ) α ( cid:   )   min   αB ByB = −α ( cid:   ) s.t . α ( cid:   ) ¯By ¯B B ( H ¯BBα ¯B − e )   ≤ αi ≤ C  ∈ B . (  .  a ) (  .  b ) (  .  c ) extreme case decomposition method Sequential Minimal Op- timization ( SMO ) algorithm Platt [ Pla   ] , updates two coef- ﬁcients per iteration . advantage strategy see resultant sub-problem solved analytically . Without loss generality let B = { , j } , deﬁne = yi/yj , ( cid:  ) ci ( cid:  ) = ( H ¯BBα ¯B − e ) ( cid:   ) = ( −α ( cid:   ) ¯By ¯B/yj ) . (  .   ) specializes cj min αi , αj     ( Hiiα  + Hjjα  j +  Hijαjαi ) + ciαi + cjαj s.t . sαi + αj =   ≤ αi , αj ≤ C  . QP two variables analytic solution . (  .  a ) (  .  b ) (  .  c ) Lemma  .  ( Analytic solution   variable QP ) Deﬁne bounds L = H = ( cid:   ) d− C max (   ,   max (   , ) ( cid:   ) min ( C , ) d− C min ( C ,   ) >   otherwise >   ) otherwise , (  .   ) (  .   )     auxiliary variables   Linear Models χ = ( Hii + Hjjs  −  sHij ) ρ = ( cjs − ci − Hijd + Hjjds ) . (  .   ) (  .   ) optimal value (  .   ) computed analytically follows : χ =    αi = ( cid:   ) L ρ <   H otherwise . χ >   , αi = max ( L , min ( H , ρ/χ ) ) . cases , αj = ( − sαi ) . Proof Eliminate equality constraint setting αj = ( − sαi ) . Due constraint   ≤ αj ≤ C follows sαi = − αj bounded via − C one write L ≤ αi ≤ H L H given (  .   ) (  .   ) respectively . ≤ sαi ≤ d. Combining   ≤ αi ≤ C Substituting αj = ( d−sαi ) objective function , dropping terms depend αi , simplifying substituting χ ρ yields following optimization problem αi :     α  χ − αiρ min αi s.t . L ≤ αi ≤ H . First consider case χ =   . case , αi = L ρ <   otherwise αi = H. hand , χ >   unconstrained optimum optimization problem given ρ/χ . constrained optimum obtained clipping appropriately : max ( L , min ( H , ρ/χ ) ) . concludes proof . complete description SMO need valid stopping criterion well scheme selecting working set every iteration . order derive stopping criterion use KKT gap , , extent KKT conditions violated . Towards end introduce non- negative Lagrange multipliers b ∈ R , λ ∈ Rm µ ∈ Rm write Lagrangian (  .  ) . L ( α , b , λ , µ ) = α ( cid:   ) Hα − α ( cid:   ) e + bα ( cid:   ) − λ ( cid:   ) α + µ ( cid:   ) ( α −     let J ( α ) =     α ( cid:   ) Hα − α ( cid:   ) e objective function ∇J ( α ) = Hα − e gradient , taking gradient Lagrangian respect α setting   shows (  .   ) C  e ) . ∇J ( α ) + = λ − µ . (  .   )  .  Support Vector Classiﬁcation Furthermore , KKT conditions λiαi =   µi ( C  − αi ) =   ,     (  .   ) λi ≥   µi ≥   . Equations (  .   ) (  .   ) compactly rewritten ∇J ( α ) + byi ≥   αi =   C  ∇J ( α ) + byi ≤   αi = ∇J ( α ) + byi =     < αi < C  . (  .  a ) (  .  b ) (  .  c ) Since yi ∈ { ±  } , rewrite (  .   ) −yi∇J ( α ) ≤ b ∈ Iup −yi∇J ( α ) ≥ b ∈ Idown , index sets Iup Idown deﬁned Iup = { : αi < Idown = { : αi < C  C  , yi =   αi >   , yi = −  } , yi = −  αi >   , yi =   } . (  .  a ) (  .  b ) summary , KKT conditions imply α solution (  .  )   ( α ) ≤ ( α ) ( α ) = max i∈Iup −yi∇J ( α ) ( α ) = min i∈Idown −yi∇J ( α ) . (  .   ) Therefore , natural stopping criterion stop KKT gap falls desired tolerance ( cid:   ) , , ( α ) ≤ ( α ) + ( cid:   ) . (  .   ) Finally , turn attention issue working set selection . ﬁrst order approximation objective function J ( α ) written J ( α + ) ≈ J ( α ) + ∇J ( α ) ( cid:   ) . Since interested updating coeﬃcients working set B B   ( cid:  ) , case rewrite ﬁrst order set ( cid:   ) = ( cid:  ) ( cid:   )     approximation   Linear Models ∇J ( α ) ( cid:   ) BdB ≈ J ( α + ) − J ( α ) . among possible directions dB wish choose one decreases objective function maintaining feasibility . best expressed following optimization problem : min dB s.t . ∇J ( α ) ( cid:   ) BdB ( cid:   ) BdB =   di ≥   αi =   ∈ B di ≤   αi = −   ≤ di ≤   . C  ∈ B (  .  a ) (  .  b ) (  .  c ) (  .  d ) (  .  e ) (  .  b ) comes ( cid:   ) ( α + ) =   ( cid:   ) α =   , (  .  c ) (  .  d ) comes   ≤ αi ≤ C . Finally , (  .  e ) prevents objective function diverging −∞ . specialize (  .   ) SMO , obtain min , j s.t . ∇J ( α ) idi + ∇J ( α ) jdj yidi + yjdj =   dk ≥   αk =   k ∈ { , j } C  dk ≤   αk = k ∈ { , j } −   ≤ dk ≤   k ∈ { , j } . (  .  a ) (  .  b ) (  .  c ) (  .  d ) (  .  e ) ﬁrst glance , seems choosing optimal j set {   , . . . , } × {   , . . . } requires ( m  ) eﬀort . show ( ) eﬀort suﬃces . Deﬁne new variables ˆdk = ykdk k ∈ { , j } , use observation yk ∈ { ±  } rewrite objective function ( −yi∇J ( α ) + yj∇J ( α ) j ) ˆdj . Consider case −∇J ( α ) iyi ≥ −∇J ( α ) jyj . constraints (  .  c ) (  .  d ) choose ∈ Iup j ∈ Idown , ˆdj = −  ˆdi =   feasible objective function attains negative value . choices j ( , j ∈ Iup ; , j ∈ Idown ; ∈ Idown j ∈ Iup ) objective function value   attained setting ˆdi = ˆdj =   . case −∇J ( α ) jyj ≥ −∇J ( α ) iyi analogous . summary , optimization  .  Extensions problem (  .   ) boils     min i∈Iup , j∈Idown yi∇J ( α ) − yj∇J ( α ) j = min i∈Iup yi∇J ( α ) − max j∈Idown yj∇J ( α ) j , clearly solved ( ) time . Comparison (  .   ) shows every iteration SMO choose update coeﬃcients αi αj maximally violate KKT conditions .  .  Extensions  . .  ν trick soft margin formulation parameter C trade-oﬀ two conﬂicting requirements namely maximizing margin minimizing training error . Unfortunately , parameter rather unintuitive hence diﬃcult tune . ν-SVM proposed address issue . Theorem  .  shows , ν controls number support vectors margin errors . primal problem ν-SVM written min w , b , ξ , ρ s.t . ( cid:    ) w ( cid:    )   − ρ +       νm  ( cid:   ) i=  ξi yi ( ( cid:    ) w , xi ( cid:    ) + b ) ≥ ρ − ξi ξi ≥   , ρ ≥   . (  .  a ) (  .  b ) (  .  c ) , write Lagrangian introducing non-negative Lagrange multipliers , take gradients respect primal variables set zero , substitute result back Lagrangian obtain following dual : min α s.t .     ( cid:   ) , j yiyjαiαj ( cid:    ) xi , xj ( cid:    )  ( cid:   ) i=   ( cid:   ) i=  αiyi =   αi ≥     ≤ αi ≤   νm . (  .  a ) (  .  b ) (  .  c ) (  .  d ) turns dual simpliﬁed via following lemma .       Linear Models Lemma  .  Let ν ∈ [   ,   ] (  .   ) feasible . least one solution α satisﬁes ( cid:   ) αi =   . Furthermore , ﬁnal objective value (  .   ) non-zero solutions satisfy ( cid:   ) αi =   . Proof feasible region (  .   ) bounded , therefore feasible exists optimal solution . Let α denote solution assume ( cid:   ) αi >   . case deﬁne ¯α =   j αj ( cid:   ) α , easily check ¯α also feasible . , let H denote × matrix Hij = yiyj ( cid:    ) xi , xj ( cid:    ) . Since α optimal solution (  .   ) follows α ( cid:   ) Hα ≤     ¯α ( cid:   ) H ¯α =     ( cid:   )   j αj ( cid:   ) ( cid:   )       α ( cid:   ) Hα ≤ α ( cid:   ) Hα .     implies either   desired property   satisfy ( cid:   ) αi =   .   α ( cid:   ) Hα =   , case ¯α optimal solution   α ( cid:   ) Hα ( cid:   ) =   , case optimal solutions view theorem one equivalently replace (  .   ) following simpliﬁed optimization problem two equality constraints min α s.t .     ( cid:   ) , j yiyjαiαj ( cid:    ) xi , xj ( cid:    )  ( cid:   ) i=   ( cid:   ) i=  αiyi =   αi =     ≤ αi ≤   νm . (  .  a ) (  .  b ) (  .  c ) (  .  d ) following theorems , state without proof , explain signif- icance ν connection ν-SVM soft margin formu- lation . Theorem  .  Suppose run ν-SVM kernel k data obtain ρ >   . ( ) ν upper bound fraction margin errors , points yi ( ( cid:    ) w , xi ( cid:    ) + bi ) < ρ .  .  Extensions     ( ii ) ν lower bound fraction support vectors , points yi ( ( cid:    ) w , xi ( cid:    ) + bi ) = ρ . ( iii ) Suppose data ( X , ) generated iid distribution p ( x , ) neither p ( x , = +  ) p ( x , = −  ) contain discrete components . Moreover , assume kernel k analytic non- constant . probability   , asympotically , ν equals fraction support vectors fraction margin errors . Theorem  .  (  .   ) leads decision function ρ >   , (  .  ) C =   ρ leads decision function .  . .  Squared Hinge Loss binary classiﬁcation , actual loss one would like minimize so-called  -  loss l ( w , x , ) = ( cid:   )   ( ( cid:    ) w , x ( cid:    ) + b ) ≥     otherwise . (  .   ) loss diﬃcult work non-convex ( see Figure  .  ) . loss ( ( cid:    ) w , x ( cid:    ) + b ) Fig .  .  .  -  loss non-convex intractable depicted red . hinge loss convex upper bound  -  loss shown blue . square hinge loss diﬀerentiable convex upper bound  -  loss depicted green . fact , shown ﬁnding optimal ( w , b ) pair minimizes  -  loss training dataset labeled points NP hard [ BDEL   ] . Therefore various proxy functions binary hinge loss (  .   ) discussed Section  . .  used . Another popular proxy square     hinge loss :   Linear Models l ( w , x , ) = max (   ,   − ( ( cid:    ) w , x ( cid:    ) + b ) )   . (  .   ) Besides proxy  -  loss , squared hinge loss , unlike hinge loss , also diﬀerentiable everywhere . sometimes makes opti- mization primal easier . like case hinge loss one derive dual regularized risk minimization problem show quadratic programming problem ( problem  .  ) .  . .  Ramp Loss ramp loss l ( w , x , ) = min (   − , max (   ,   − ( ( cid:    ) w , x ( cid:    ) + b ) ) ) (  .   ) parameterized ≤   another proxy  -  loss ( see Figure  .  ) . Although convex , expressed diﬀerence two convex functions lconc ( w , x , ) = max (   ,   − ( ( cid:    ) w , x ( cid:    ) + b ) ) lcave ( w , x , ) = max (   , − ( ( cid:    ) w , x ( cid:    ) + b ) ) . Therefore Convex-Concave procedure ( CCP ) discussed Section Fig .  .  . ramp loss depicted = − .  viewed sum convex function namely binary hinge loss ( left ) concave function min (   ,   − ( ( cid:    ) w , x ( cid:    ) + b ) ) ( right ) . Viewed alternatively , ramp loss written diﬀerence two convex functions .  . .  used solve resulting regularized risk minimization problem ramp loss . Towards end write J ( w ) = ( cid:    ) w ( cid:    )   +     ( cid:    ) C  i=  ( cid:    ) ( cid:    ) Jconc ( w )  ( cid:   ) lconc ( w , xi , yi ) −  ( cid:   ) i=  C  ( cid:    ) lcave ( w , xi , yi ) . (  .   ) ( cid:    ) ( cid:    ) Jcave ( w ) ( cid:    ) ( cid:    )  .  Support Vector Regression     Recall every iteration CCP replace Jcave ( w ) ﬁrst order Taylor approximation , computing requires ∂wJ ( w ) = C   ( cid:   ) i=  ∂wlcave ( w , xi , yi ) . (  .   ) turn computed ∂wlcave ( w , xi , yi ) = δiyixi δi = ( cid:   ) −  > ( ( cid:    ) w , x ( cid:    ) + b )   otherwise . (  .   ) Ignoring constant terms , iteration CCP algorithm involves solv- ing following minimization problem ( also see (  .    ) ) J ( w ) = ( cid:    ) w ( cid:    )   +     C   ( cid:   ) i=  lconc ( w , xi , yi ) − ( cid:   ) C   ( cid:   ) i=  ( cid:   ) δiyixi w . (  .   ) Let δ denote vector Rm components δi . Using notation (  .  ) write following dual optimization problem closely related standard SVM dual (  .  ) ( see problem  .  ) α ( cid:   ) Hα − α ( cid:   ) e   min   α s.t . α ( cid:   ) =   − C  δ ≤ αi ≤ C  ( e − δ ) . (  .  a ) (  .  b ) (  .  c ) fact , problem solved SMO solver minor modiﬁca- tions . Putting everything together yields Algorithm  .  . Algorithm  .  CCP Ramp Loss   : Initialize δ  α    : repeat   : Solve (  .   ) ﬁnd αt+  Compute δt+  using (  .   )   :   : δt+  = δt  .  Support Vector Regression opposed classiﬁcation labels yi binary valued , re- gression real valued . Given tolerance ( cid:   ) , aim ﬁnd       Linear Models loss ( cid:   ) − ( ( cid:    ) w , x ( cid:    ) + b ) Fig .  .  . ( cid:   ) insensitive loss . points lie within ( cid:   ) tube shaded gray incur zero loss points outside incur linear loss . hyperplane parameterized ( w , b ) |yi − ( ( cid:    ) w , xi ( cid:    ) + b ) | ≤ ( cid:   ) . (  .   ) words , want ﬁnd hyperplane training data lies within ( cid:   ) tube around hyperplane . may always able ﬁnd hyperplane , hence relax condition introducing slack variables ξ+ write corresponding primal problem ξ− min w , b , ξ+ , ξ− s.t . ( cid:    ) w ( cid:    )   +      ( cid:   ) ( ξ+ + ξ− ) C  i=  yi − ( ( cid:    ) w , xi ( cid:    ) + b ) ≤ ( cid:   ) + ξ+  ( ( cid:    ) w , xi ( cid:    ) + b ) − yi ≤ ( cid:   ) + ξ−  ≥   , ξ− ξ+ ≥   .   (  .  a ) (  .  b ) (  .  c ) (  .  d ) Lagrangian written introducing non-negative Lagrange mul- tipliers α+ , β+ β− : , α− L ( w , b , ξ+ , ξ− , α+ , α− , β+ , β− ) = ( cid:    ) w ( cid:    )   +     C   ( cid:   ) i=  ( ξ+ + ξ− ) −  ( cid:   ) ( β+ ξ+ + β− ξ− ) i=  + +  ( cid:   ) i=   ( cid:   ) i=  α+ ( yi − ( ( cid:    ) w , xi ( cid:    ) + b ) − ( cid:   ) − ξ+ ) α− ( ( ( cid:    ) w , xi ( cid:    ) + b ) − yi − ( cid:   ) − ξ− ) .  .  Support Vector Regression     Taking gradients respect primal variables setting   , obtain following conditions : w =  ( cid:   ) i=  ( α+ − α− ) xi  ( cid:   ) α+ =  ( cid:   ) α−  i=  α+ + β+ = α− + β− = i=  C  C  . (  .   ) (  .   ) (  .   ) (  .   ) Noting α { + , − }  Lagrangian yields dual , β { + , − }  ≥   substituting conditions min α+ , α−     ( cid:   ) , j ( α+ − α− ) ( α+ j − α− j ) ( cid:    ) xi , xj ( cid:    ) + ( cid:   )  ( cid:   ) i=  ( α+ + α− ) −  ( cid:   ) i=  yi ( α+ − α− ) s.t . α−   ( cid:   ) i=  α+ =   ≤ α+ ≤   ≤ α− ≤  ( cid:   ) i=  C  C  . (  .  a ) (  .  b ) (  .  c ) (  .  d ) quadratic programming problem one equality constraint , hence SMO like decomposition method derived ﬁnding optimal coeﬃcients α+ α− ( Problem  .  ) . consequence (  .   ) , analogous classiﬁcation case , one map data via feature map φ RKHS kernel k recover decision boundary f ( x ) = ( cid:    ) w , φ ( x ) ( cid:    ) + b via f ( x ) =  ( cid:   ) i=  ( α+ − α− ) ( cid:    ) φ ( x ) , φ ( x ) ( cid:    ) + b =  ( cid:   ) i=  ( α+ − α− ) k ( xi , x ) + b . (  .   ) Finally , KKT conditions ( cid:   ) C  ( cid:   ) − α+  ξ+ =   ( cid:   ) C  ( cid:   ) − α−  ξ− =   ( ( ( cid:    ) w , xi ( cid:    ) + b ) − yi − ( cid:   ) − ξ− ) =   α+ α− ( yi − ( ( cid:    ) w , xi ( cid:    ) + b ) − ( cid:   ) − ξ+ ) =   ,       Linear Models allow us draw many useful conclusions : • Whenever |yi − ( ( cid:    ) w , xi ( cid:    ) + b ) | < ( cid:   ) , implies ξ+ = α− =   . words , points lie inside ( cid:   ) tube around hyperplane ( cid:    ) w , x ( cid:    ) + b contribute solution thus leading sparse expansions terms α . • ( ( cid:    ) w , xi ( cid:    ) +b ) −yi > ( cid:   ) ξ− >   therefore α− = α+ = ξ− = C , α− • Finally , ( ( cid:    ) w , xi ( cid:    ) + b ) − yi = ( cid:   ) ξ− . =   . case yi − ( ( cid:    ) w , xi ( cid:    ) + b ) > ( cid:   ) symmetric >   , α+ =   . =     ≤ α− , =   . Similarly , yi − ( ( cid:    ) w , xi ( cid:    ) + b ) = ( cid:   ) obtain ≤ C , ξ− =   α− ≤ C = C =   . hand , ξ+ =   α+ yields ξ− =   , ξ+ ξ+ =   α+ ξ+ =   ,   ≤ α+ Note α+ α− never simultaneously non-zero .  . .  Incorporating General Loss Functions Using reasoning Section  . .  deduce (  .   ) loss function support vector regression given l ( w , x , ) = max (   , |y − ( cid:    ) w , x ( cid:    ) | − ( cid:   ) ) . (  .   ) turns support vector regression framework easily extended handle , general , convex loss functions ones found Table  .  . Diﬀerent losses diﬀerent properties hence lead diﬀerent estimators . instance , square loss leads penalized least squares ( LS ) regression , Laplace loss leads penalized least absolute deviations ( LAD ) estimator . Huber ’ loss hand combination penalized LS LAD estimators , pinball loss parameter τ ∈ [   ,   ] used estimate τ -quantiles . Setting τ =  .  pinball loss leads scaled version Laplace loss . deﬁne ξ = − ( cid:    ) w , x ( cid:    ) , easily veriﬁed losses written  l ( w , x , ) =   l+ ( ξ − ( cid:   ) ) l− ( −ξ − ( cid:   ) )    ξ > ( cid:   ) ξ < ( cid:   ) ξ ∈ [ − ( cid:   ) , ( cid:   ) ] . (  .   ) diﬀerent loss functions , support vector regression formu-  .  Support Vector Regression lation written uniﬁed fashion follows min w , b , ξ+ , ξ− s.t . ( cid:    ) w ( cid:    )   +     C   ( cid:   ) l+ ( ξ+ ) + l− ( ξ− ) i=  yi − ( ( cid:    ) w , xi ( cid:    ) + b ) ≤ ( cid:   ) + ξ+  ( ( cid:    ) w , xi ( cid:    ) + b ) − yi ≤ ( cid:   ) + ξ−  ξ+ ≥   , ξ− ≥   .   dual case given min α+ , α−     ( cid:   ) ( α+ − α− ) ( α+ j − α− j ) ( cid:    ) xi , xj ( cid:    ) , j C   ( cid:   ) i=  + ( ξ+ ) + − ( ξ− ) + ( cid:   )  ( cid:   ) i=  ( α+ + α− ) − −  ( cid:   ) α+ =  ( cid:   ) α−  s.t . i=  i=    ≤ α { + , − }    ≤ ξ { + , − }  ξ { + , − }  = inf ≤ C  ∂ξl { + , − } ( ξ { + , − }  ) ( cid:   ) ξ { + , − } | C  ∂ξl { + , − } ≥ α { + , − }  ( cid:   ) .     (  .  a ) (  .  b ) (  .  c ) (  .  d ) (  .  a )  ( cid:   ) i=  yi ( α+ − α− ) (  .  b ) (  .  c ) (  .  d ) (  .  e ) + ( ξ ) = l+ ( ξ ) − ξ∂ξl+ ( ξ ) − ( ξ ) = l− ( ξ ) − ξ∂ξl− ( ξ ) . show (  .   ) specialized pinball loss . Clearly , l+ ( ξ ) = τ ξ l− ( −ξ ) = ( τ −  ) ξ , hence l− ( ξ ) = (  −τ ) ξ . Therefore , + ( ξ ) = ( τ −  ) ξ − ξ ( τ −   ) =   . Similarly − ( ξ ) =   . Since ∂ξl+ ( ξ ) = τ ∂ξl− ( ξ ) = (   − τ ) ξ ≥   , follows bounds α { + , − } computed   ≤ α+ (   − τ ) . denote α = α+ − α− τ   ≤ α− ≤ C ≤ C Table  .  . Various loss functions used support vector regression . brevity denote − ( cid:    ) w , x ( cid:    ) ξ write loss l ( w , x , ) terms ξ . ( cid:   ) -insensitive loss max (   , |ξ| − ( cid:   ) ) Laplace loss Square loss Huber ’ robust loss Pinball loss |ξ|     |ξ|  ( cid:   )    σ ξ  |ξ| − σ   ( cid:   ) τ ξ ( τ −   ) ξ |ξ| ≤ σ otherwise ξ ≥   otherwise .       Linear Models observe ( cid:   ) =   pinball loss (  .   ) specializes follows : min α s.t .     ( cid:   ) , j αiαj ( cid:    ) xi , xj ( cid:    ) −  ( cid:   ) i=  yiαi  ( cid:   ) αi =   i=  C  ( τ −   ) ≤ αi ≤ C  τ . (  .  a ) (  .  b ) (  .  c ) Similar specializations (  .   ) loss functions Table  .  derived .  . .  Incorporating ν Trick One also incorporate ν trick support vector regression . primal problem obtained incorporating ν trick written ( cid:   ) ( cid:   ) min w , b , ξ+ , ξ− , ( cid:   )     ( cid:    ) w ( cid:    )   + ( cid:   ) + ( ξ+ + ξ− )   νm  ( cid:   ) i=  s.t . ( ( cid:    ) w , xi ( cid:    ) + b ) − yi ≤ ( cid:   ) + ξ+  yi − ( ( cid:    ) w , xi ( cid:    ) + b ) ≤ ( cid:   ) + ξ−  ≥   , ξ− ξ+ ≥   , ( cid:   ) ≥   . Proceeding obtain following simpliﬁed dual   (  .  a ) (  .  b ) (  .  c ) (  .  d ) min α+ , α−     ( cid:   ) ( α− − α+ ) ( α− j − α+ j ) ( cid:    ) xi , xj ( cid:    ) − , j  ( cid:   ) ( α− − α+ ) =   s.t . i=   ( cid:   ) ( α− + α+ ) =   i=    ≤ α+ ≤   ≤ α− ≤   νm   νm .  ( cid:   ) i=  yi ( α− − α+ ) (  .  a ) (  .  b ) (  .  c ) (  .  d ) (  .  e )  .  Novelty Detection large margin approach also adapted perform novelty detection quantile estimation . Novelty detection unsupervised task one  .  Novelty Detection     interested ﬂagging small fraction input X = { x  , . . . , xm } atypical novel . viewed special case quantile estimation task , interested estimating simple set C P r ( x ∈ C ) ≥ µ µ ∈ [   ,   ] . One way measure simplicity use volume set . Formally , |C| denotes volume set , quantile estimation task estimate arginf { |C| s.t . P r ( x ∈ C ) ≥ µ } . (  .   ) Given input data X one compute empirical density ˆp ( x ) = ( cid:   )      x ∈ X otherwise , estimate ( necessarily unique ) µ-quantiles . Unfortunately , estimates brittle generalize well unseen data . One possible way address issue restrict C simple subsets spheres half spaces . words , estimate simple sets contain µ fraction dataset . purposes , speciﬁcally work half-spaces deﬁned hyperplanes . half-spaces may seem rather restrictive remember kernel trick used map data high-dimensional space ; half-spaces mapped space correspond non-linear decision boundaries input space . Furthermore , instead explicitly identifying C learn indicator function C , , function f takes values −  inside C −  elsewhere .     ( cid:    ) w ( cid:    )   regularizer , problem estimating hyperplane large fraction points input data X lie one sides written : min w , ξ , ρ s.t . ( cid:    ) w ( cid:    )   +       νm  ( cid:   ) i=  ξi − ρ ( cid:    ) w , xi ( cid:    ) ≥ ρ − ξi ξi ≥   . (  .  a ) (  .  b ) (  .  c ) Clearly , want ρ large possible volume half- space ( cid:    ) w , x ( cid:    ) ≥ ρ minimized . Furthermore , ν ∈ [   ,   ] parameter analogous ν introduced ν-SVM earlier . Roughly speaking , denotes fraction input data ( cid:    ) w , xi ( cid:    ) ≤ ρ . alternative interpretation (  .   ) assume separating data set X origin ( See Figure  .  illustration ) . Therefore , method also widely known one-class SVM .       Linear Models Fig .  .  . novelty detection problem viewed ﬁnding large margin hyperplane separates ν fraction data points away origin . Lagrangian (  .   ) written introducing non-negative Lagrange multipliers αi , βi : L ( w , ξ , ρ , α , β ) = ( cid:    ) w ( cid:    )   +       νm  ( cid:   ) i=  ξi − ρ +  ( cid:   ) i=  αi ( ρ − ξi − ( cid:    ) w , xi ( cid:    ) ) −  ( cid:   ) i=  βiξi . taking gradients respect primal variables setting   obtain  ( cid:   ) αixi w = i=    νm − βi ≤   νm αi =  ( cid:   ) i=  αi =   . (  .   ) (  .   ) (  .   ) Noting αi , βi ≥   substituting conditions La- grangian yields dual min α     ( cid:   ) , j αiαj ( cid:    ) xi , xj ( cid:    ) s.t .   ≤ αi ≤   νm  ( cid:   ) i=  αi =   . (  .  a ) (  .  b ) (  .  c )  .  Margins Probability     easily solved straightforward modiﬁcation SMO algorithm ( see Section  . .  Problem  .  ) . Like previous sections , analysis KKT conditions shows   < α ( cid:    ) w , xi ( cid:    ) ≤ ρ ; points called support vectors . , replace ( cid:    ) xi , xj ( cid:    ) kernel k ( xi , xj ) transform half-spaces feature space non-linear shapes input space . following theorem explains signiﬁcance parameter ν . Theorem  .  Assume solution (  .   ) satisﬁes ρ ( cid:   ) =   , following statements hold : ( ) ν upper bound fraction support vectors , points ( cid:    ) w , xi ( cid:    ) ≤ ρ . ( ii ) Suppose data X generated independently distribution p ( x ) contain discrete components . Moreover , assume kernel k analytic non-constant . probability   , asympotically , ν equals fraction support vectors .  .  Margins Probability discuss connection probabilistic models linear classiﬁers . issues consistency , optimization , eﬃciency , etc .  .  Beyond Binary Classiﬁcation contrast binary classiﬁcation two possible ways label training sample , extensions discuss training sample may associated one k possible labels . Therefore , use decision function y∗ = argmax y∈ {   , ... , k } f ( x , ) f ( x , ) = ( cid:    ) φ ( x , ) , w ( cid:    ) . (  .   ) Recall joint feature map φ ( x , ) introduced section  . .  . One way interpret equation view f ( x , ) compatibility score instance x label ; assign label highest compatibility score x . number extensions binary hinge loss (  .   ) used estimate score function . cases objective function written min w J ( w ) : = λ   ( cid:    ) w ( cid:    )   +     ( cid:   ) i=  l ( w , xi , yi ) . (  .   )     ( cid:   )   Linear Models   ( cid:    ) w ( cid:    )   empirical λ scalar trades oﬀ regularizer   risk   i=  l ( w , xi , yi ) . Plugging diﬀerent loss functions yields classiﬁers  diﬀerent settings . Two strategies exist ﬁnding optimal w. like binary SVM case , one compute maximize dual (  .   ) . However , number dual variables becomes m|Y| , number training points |Y| denotes size label set . second strategy optimize (  .   ) directly . However , loss functions discuss non-smooth , therefore non-smooth optimization algorithms bundle methods ( section  . .  ) need used .  . .  Multiclass Classiﬁcation multiclass classiﬁcation training example labeled one k pos- sible labels , , = {   , . . . , k } . discuss two diﬀerent extensions binary hinge loss multiclass setting . easily veriﬁed setting = { ±  } φ ( x , ) =   φ ( x ) recovers binary hinge loss cases .  . . .  Additive Multiclass Hinge Loss natural generalization binary hinge loss penalize labels misclassiﬁed . loss written l ( w , x , ) = ( cid:   ) ( cid:   ) ( cid:   ) =y max ( cid:  )   ,   − ( ( cid:   ) φ ( x , ) − φ ( x , ( cid:   ) ) , w ( cid:   ) ) ( cid:  ) . (  .   )  . . .  Maximum Multiclass Hinge Loss Another variant (  .   ) penalizes maximally violating label : l ( w , x , ) : = max ( cid:   )   , max ( cid:   ) ( cid:   ) =y (   − ( cid:   ) φ ( x , ) − φ ( x , ( cid:   ) ) , w ( cid:   ) ) ( cid:   ) . (  .   ) Note (  .   ) (  .   ) zero whenever f ( x , ) = ( cid:    ) φ ( x , ) , w ( cid:    ) ≥   + max ( cid:   ) ( cid:   ) =y ( cid:   ) φ ( x , ( cid:   ) ) , w ( cid:   ) =   + max ( cid:   ) ( cid:   ) =y f ( x , ( cid:   ) ) . (  .   ) words , ensure adequate margin separation , case   , score true label f ( x , ) every label f ( x , ( cid:   ) ) . However , diﬀer way penalize violators , , la- bels ( cid:   ) ( cid:   ) = f ( x , ) ≤   + f ( x , ( cid:   ) ) . one case linearly penalize violators sum contributions case lin- early penalize maximum violator . fact , (  .   ) interpreted  .  Beyond Binary Classiﬁcation     log odds ratio exponential family . Towards end choose η log η =   rewrite (  .   ) : log p ( y|x , w ) maxy ( cid:   ) ( cid:   ) =y p ( ( cid:   ) |x , w ) ( cid:   ) = φ ( x , ) − max ( cid:   ) ( cid:   ) =y φ ( x , ( cid:   ) ) , w ( cid:   ) ≥   . Rearranging yields (  .   ) .  . .  Multilabel Classiﬁcation multilabel classiﬁcation one k possible labels assigned training example . like multiclass case two diﬀerent losses deﬁned .  . . .  Additive Multilabel Hinge Loss let Yx ⊆ denote labels assigned x , generalize hinge loss penalize labels ( cid:   ) /∈ Yx assigned higher score ∈ Yx , loss written l ( w , x , ) = ( cid:   ) max ( cid:  )   ,   − ( ( cid:   ) φ ( x , ) − φ ( x , ( cid:   ) ) , w ( cid:   ) ) ( cid:  ) . (  .   ) y∈Yx ( cid:   ) /∈Yx  . . .  Maximum Multilabel Hinge Loss Another variant penalizes maximum violating pair . case loss written ( cid:   ) ( cid:   ) ( cid:  )   − ( cid:  ) ( cid:   ) φ ( x , ) − φ ( x , ( cid:   ) ) , w ( cid:   ) ( cid:  ) ( cid:  ) . (  .   ) l ( w , x , ) = max   , max y∈Yx , ( cid:   ) /∈Yx One immediately verify specializing losses mul- ticlass case recovers (  .   ) (  .   ) respectively , binary case recovers (  .   ) . losses zero min y∈Yx f ( x , ) = min y∈Yx ( cid:    ) φ ( x , ) , w ( cid:    ) ≥   + max ( cid:   ) /∈Yx ( cid:   ) φ ( x , ( cid:   ) ) , w ( cid:   ) =   + max ( cid:   ) /∈Yx f ( x , ( cid:   ) ) . interpreted follows : losses ensure labels assigned x larger scores compared labels assigned x margin separation least   . Although loss functions compatible multiple labels , prediction function argmaxy f ( x , ) takes account label highest score . signiﬁcant drawback models , overcome using multiclass approach instead . Let |Y| size label set z ∈ R|Y| denote vector ±  entries . set       Linear Models zy = +  ∈ Yx zy = −  otherwise , use multiclass loss (  .   ) z . predict compute z∗ = argmaxz f ( x , z ) assign x labels corresponding components z∗ +  . Since z take  |Y| possible values , approach feasible |Y| large . tackle problems , reduce computational complexity assume labels correlations captured via |Y| × |Y| positive semi-deﬁnite matrix P , φ ( x , ) written φ ( x ) ⊗ P . ⊗ denotes Kronecker product . Furthermore , express vector w n × |Y| matrix W , n denotes dimension φ ( x ) . assumptions ( cid:    ) φ ( x ) ⊗ P ( z − z ( cid:   ) ) , w ( cid:    ) rewritten ( cid:    ) ( cid:   ) ( cid:   ) ( cid:    ) φ ( x ) ( cid:   ) W P , ( z − z ( cid:   ) ) = ( cid:   )  φ ( x ) ( cid:   ) W P ( zi − z ( cid:   ) ) ,  (  .   ) specializes ( cid:   ) ( cid:   ) l ( w , x , z ) : = max   ,   − ( cid:    ) ( cid:   )  min z ( cid:   ) ( cid:   ) =zi φ ( x ) ( cid:   ) W P ( cid:    )  ( cid:   ) ( cid:   ) ( zi − z ( cid:   ) ) . (  .   ) analogous specialization (  .   ) also derived wherein mini- mum replaced summation . Since minimum ( summation case may ) |Y| possible labels , computing loss tractable even set labels large .  . .  Ordinal Regression Ranking generalize discussion consider slightly general ranking problems . Denote set directed acyclic graphs N nodes . presence edge ( , j ) ∈ indicates preferred j . goal ﬁnd function f ( x , ) imposes total order {   , . . . , N } close agreement y. Speciﬁcally , estimation error given number subgraphs disagreement total order imposed f , additive version loss written l ( w , x , ) = ( cid:   ) G∈A ( ) max ( , j ) ∈G (   ,   − ( f ( x , ) − f ( x , j ) ) ) , (  .   ) ( ) denotes set possible subgraphs . maximum margin version , hand , given l ( w , x , ) = max G∈A ( ) max ( , j ) ∈G (   ,   − ( f ( x , ) − f ( x , j ) ) ) . (  .   )  .  Large Margin Classiﬁers Structure     words , test subgraph G ranking imposed G satisﬁed f . Selecting speciﬁc types directed acyclic graphs recovers multiclass multilabel settings ( problem  .  ) .  .  Large Margin Classiﬁers Structure  . .  Margin deﬁne margin pictures  . .  Penalized Margin diﬀerent types loss , rescaling  . .  Nonconvex Losses max - max loss  .  Applications  . .  Sequence Annotation  . .  Matching  . .  Ranking  . .  Shortest Path Planning  . .  Image Annotation  . .  Contingency Table Loss  .  Optimization  . .  Column Generation subdiﬀerentials  . .  Bundle Methods  . .  Overrelaxation Dual things exactly       Linear Models  .   CRFs vs Structured Large Margin Models  .  .  Loss Function  .  .  Dual Connections  .  .  Optimization Problems Problem  .  ( Deriving Margin {   } ) Show distance point xi hyperplane H = { x| ( cid:    ) w , x ( cid:    ) + b =   } given | ( cid:    ) w , xi ( cid:    ) + b |/ ( cid:    ) w ( cid:    ) . Problem  .  ( SVM without Bias {   } ) homogeneous hyperplane one passes origin , , H = { x| ( cid:    ) w , x ( cid:    ) =   } . (  .   ) devise soft margin classiﬁer uses homogeneous hyperplane decision boundary , corresponding primal optimization problem written follows : min w , ξ s.t . ( cid:    ) w ( cid:    )   + C  ( cid:   ) ξi     i=  yi ( cid:    ) w , xi ( cid:    ) ≥   − ξi ξi ≥   , (  .  a ) (  .  b ) (  .  c ) Derive dual (  .   ) contrast (  .  ) . changes SMO algorithm would make solve dual ? Problem  .  ( Deriving simpliﬁed ν-SVM dual {   } ) Lemma  .  used (  .   ) show constraint ( cid:   ) ( cid:   ) αi ≥   replaced αi =   . Show equivalent way arrive conclusion arguing constraint ρ ≥   redundant primal (  .   ) . Hint : Observe whenever ρ <   objective function always non-negative . hand , setting w = ξ = b = ρ =   yields objective function value   . Problem  .  ( Fenchel Lagrange Duals {   } ) derived La- grange dual (  .   ) Section  .  showed (  .  ) . Derive Fenchel dual (  .   ) relate (  .  ) . Hint : See theorem  . .  [ BL   ] .  .   CRFs vs Structured Large Margin Models     Problem  .  ( Dual square hinge loss {   } ) analog (  .  ) working square hinge loss following min w , b , ξ s.t . ( cid:    ) w ( cid:    )   +     C   ( cid:   ) i=  ξ   yi ( ( cid:    ) w , xi ( cid:    ) + b ) ≥   − ξi ξi ≥   , (  .  a ) (  .  b ) (  .  c ) Derive Lagrange dual optimization problem show Quadratic Programming problem . Problem  .  ( Dual ramp loss {   } ) Derive Lagrange dual (  .   ) show Quadratic Programming problem (  .   ) . Problem  .  ( SMO various SVM formulations {   } ) Derive SMO like decomposition algorithm solving dual following problems : • ν-SVM (  .   ) . • SV regression (  .   ) . • SV novelty detection (  .   ) . Problem  .  ( Novelty detection Balls {   } ) Section  .  as- sumed wanted estimate halfspace contains major frac- tion input data . alternative approach use balls , , estimate ball small radius feature space encloses majority input data . Write corresponding optimization problem dual . Show kernel translation invariant , , k ( x , x ( cid:   ) ) depends ( cid:    ) x − x ( cid:   ) ( cid:    ) optimization problem balls equivalent (  .   ) . Explain happens geometrically . Problem  .  ( Multiclass Multilabel loss Ranking Loss {   } ) Show multiclass ( resp . multilabel ) losses (  .   ) (  .   ) ( resp . (  .   ) (  .   ) ) derived special cases (  .   ) (  .   ) re- spectively . Problem  .   Invariances ( basic loss ) Problem  .   Polynomial transformations - SDP constraints Appendix   Linear Algebra Functional Analysis A .  Johnson Lindenstrauss Lemma Lemma  .  ( Johnson Lindenstrauss ) Let X set n points Rd represented n × matrix . Given ( cid:   ) , β >   let k ≥   +  β ( cid:   )  /  − ( cid:   )  /  log n (  .  ) positive integer . Construct × k random matrix R independent standard normal random variables , , Rij ∼ N (   ,   ) , let E =   √ k AR . (  .  ) Deﬁne f : Rd → Rk function maps rows rows E. probability least   − n−β , u , v ∈ X (   − ( cid:   ) ) ( cid:    ) u − v ( cid:    )   ≤ ( cid:    ) f ( u ) − f ( v ) ( cid:    )   ≤ (   + ( cid:   ) ) ( cid:    ) u − v ( cid:    )   . (  .  ) proof presentation large follows [ ? ] . ﬁrst show Lemma  .  arbitrary vector α ∈ Rd let qi denote i-th compo- nent f ( α ) . qi ∼ N (   , ( cid:    ) α ( cid:    )   /k ) hence ( cid:    ) ( cid:    ) f ( α ) ( cid:    )   ( cid:    ) E = k ( cid:   ) i=  E ( cid:  ) q   ( cid:  ) = ( cid:    ) α ( cid:    )   . (  .  ) words , expected length vectors preserved even em- bedding k dimensional space . Next show lengths embedded vectors tightly concentrated around mean . Lemma  .  ( cid:   ) >   unit vector α ∈ Rd ( cid:   ) ( cid:   ) ( cid:    ) f ( α ) ( cid:    )   >   + ( cid:   ) P r ( cid:   ) ( cid:   ) ( cid:    ) f ( α ) ( cid:    )   <   − ( cid:   ) P r ( cid:   ) < exp − ( cid:   ) < exp − k   k   ( cid:  ) ( cid:   )  /  − ( cid:   )  /  ( cid:  ) ( cid:  ) ( cid:   )  /  − ( cid:   )  /  ( cid:  ) ( cid:   ) ( cid:   ) . (  .  ) (  .  )           Linear Algebra Functional Analysis Corollary  .  choose k (  .  ) α ∈ Rd ( cid:   ) (   − ( cid:   ) ) ( cid:    ) α ( cid:    )   ≤ ( cid:    ) f ( α ) ( cid:    )   ≤ (   + ( cid:   ) ) ( cid:    ) α ( cid:    )   ( cid:   ) P r ≥   −   n +β . (  .  ) Proof Follows immediately Lemma  .  setting ( cid:   )   exp − k   ( cid:  ) ( cid:   )  /  − ( cid:   )  /  ( cid:  ) ( cid:   ) ≤   n +β , solving k . ( cid:  ) n ( cid:  ) pairs vectors u , v X , corresponding distances   ( cid:    ) u − v ( cid:    ) preserved within   ± ( cid:   ) factor shown lemma . Therefore , probability satisfying (  .  ) bounded ( cid:  ) n   n +β <    /nβ claimed Johnson Lindenstrauss Lemma . remains prove Lemma  .   .  . Proof ( Lemma  .  ) . Since qi =  √ j Rijαj linear combination stan- k dard normal random variables Rij follows qi normally distributed . compute mean note ( cid:  ) · ( cid:   ) E [ qi ] =   √ k ( cid:   ) j αj E [ Rij ] =   . Since Rij independent zero mean unit variance random variables , E [ RijRil ] =   j = l   otherwise . Using E ( cid:  ) q   ( cid:  ) = E   k    ( cid:   ) j=     Rijαj  =   k  ( cid:   )  ( cid:   ) j=  l=  αjαl E [ RijRil ] =   k  ( cid:   ) j=  α  j =   k ( cid:    ) α ( cid:    )   . Proof ( Lemma  .  ) . Clearly , λ ( cid:    ) ( cid:    ) ( cid:    ) f ( α ) ( cid:    )   >   + ( cid:   ) P r = P r ( cid:    ) exp ( cid:   ) λ ( cid:    ) f ( α ) ( cid:    )   ( cid:   ) ( cid:    ) > exp ( λ (   + ( cid:   ) ) ) . A .  Johnson Lindenstrauss Lemma     Using Markov ’ inequality ( P r [ X ≥ ] ≤ E [ X ] /a ) obtain ( cid:    ) ( cid:   ) λ ( cid:    ) f ( α ) ( cid:    )   ( cid:   ) ( cid:    ) > exp ( λ (   + ( cid:   ) ) ) ≤ exp P r = = = ( cid:    ) exp ( cid:   ) λ ( cid:    ) f ( α ) ( cid:    )   ( cid:   ) ( cid:    ) E ( cid:   ) ( cid:    ) ( cid:   ) ( cid:    ) ( cid:   ) k E E ( cid:    ) exp exp ( λ (   + ( cid:   ) ) ) λ ( cid:   ) k i=  q   exp ( λ (   + ( cid:   ) ) ) i=  exp ( cid:  ) λq   exp ( λ (   + ( cid:   ) ) ) ( cid:  ) ( cid:  ) k (   + ( cid:   ) ) ( cid:  ) ( cid:   ) E ( cid:  ) exp ( cid:  ) λq  exp ( cid:  ) λ  ( cid:  ) ( cid:    ) ( cid:   ) k . (  .  ) last equality qi ’ i.i.d . Since α unit vector , previous lemma qi ∼ N (   ,  /k ) . Therefore , kq  χ  random variable  moment generating function ( cid:   ) ( cid:   ) ( cid:   ) E ( cid:  ) exp ( cid:  ) λq   ( cid:  ) ( cid:  ) = E exp ( cid:   ) λ k kq     = . ( cid:    )   −  λ k Plugging (  .  ) ( cid:    ) exp ( cid:   ) λ ( cid:    ) f ( α ) ( cid:    )   ( cid:   ) ( cid:    ) > exp ( λ (   + ( cid:   ) ) ) P r  ≤  exp ( cid:  ) − λ ( cid:    ) k (   + ( cid:   ) ) ( cid:  )   −  λ k  k  . Setting λ = k ( cid:   )   (  + ( cid:   ) ) inequality simplifying ( cid:    ) ( cid:   ) λ ( cid:    ) f ( α ) ( cid:    )   ( cid:   ) ( cid:    ) > exp ( λ (   + ( cid:   ) ) ) exp P r ≤ ( exp ( − ( cid:   ) ) (   + ( cid:   ) ) ) k/  . Using inequality write log (   + ( cid:   ) ) < ( cid:   ) − ( cid:   )  /  + ( cid:   )  /  ( cid:    ) ( cid:   ) λ ( cid:    ) f ( α ) ( cid:    )   ( cid:   ) ( cid:    ) > exp ( λ (   + ( cid:   ) ) ) exp P r ( cid:   ) ≤ exp − k   ( cid:  ) ( cid:   )  /  − ( cid:   )  /  ( cid:  ) ( cid:   ) . proves (  .  ) . prove (  .  ) need repeat steps use inequality log (   − ( cid:   ) ) < − ( cid:   ) − ( cid:   )  /  . left exercise reader .       Linear Algebra Functional Analysis A .  Spectral Properties Matrices A . .  Basics A . .  Special Matrices unitary , hermitean , positive semideﬁnite A . .  Normal Forms Jacobi A .  Functional Analysis A . .  Norms Metrics vector space , norm , triangle inequality A . .  Banach Spaces normed vector space , evaluation functionals , examples , dual space A . .  Hilbert Spaces symmetric inner product A . .  Operators spectrum , norm , bounded , unbounded operators A .  Fourier Analysis A . .  Basics A . .  Operators Appendix   Conjugate Distributions         Binomial — Beta φ ( x ) = x   Conjugate Distributions eh ( nν , n ) = Γ ( nν +   ) Γ ( n (   − ν ) +   ) Γ ( n +   ) = B ( nν +   , n (   − ν ) +   ) traditional notation one represents conjugate p ( z ; α , β ) = Γ ( α + β ) Γ ( α ) Γ ( β ) zα−  (   − z ) β−  α = nν +   β = n (   − bν ) +   . Multinomial — Dirichlet φ ( x ) = ex eh ( nν , n ) = ( cid:   ) i=  Γ ( nνi +   ) Γ ( n + ) traditional notation one represents conjugate p ( z ; α ) = Γ ( ( cid:   ) ( cid:   ) i=  αi ) i=  Γ ( αi )  ( cid:   ) i=  zαi−   αi = nνi +   Poisson — Gamma φ ( x ) = x eh ( nν , n ) = n−nνΓ ( nν ) traditional notation one represents conjugate p ( z ; α ) = β−αΓ ( α ) zα− e−βx α = nν β = n . • Multinomial / Binomial • Gaussian • Laplace • Poisson • Dirichlet • Wishart • Student-t • Beta • Gamma Appendix   Loss Functions A .  Loss Functions multitude loss functions commonly used derive seemingly diﬀer- ent algorithms . often blurs similarities well subtle diﬀerences , often historic reasons : new loss typically accompa- nied least one publication dedicated . many cases , loss spelled explicitly either instead , given means con- strained optimization problem . case point papers introducing ( binary ) hinge loss [ BM   , CV   ] structured loss [ TGK   , TJHA   ] . Likewise , geometric description obscures underlying loss function , novelty detection [ SPST+   ] . section give expository yet unifying presentation many loss functions . Many well known , others , multivariate ranking , hazard regression , Poisson regression commonly used machine learning . Tables A .  A .  contain choice subset simple scalar vectorial losses . aim put multitude loss functions uniﬁed framework , show losses ( sub ) gradients computed eﬃciently use solver framework . Note losses , convex , continuously diﬀerentiable . situation give subgradient . may optimal , convergence rates algorithm depend element subdiﬀerential provide : cases ﬁrst order Taylor approximation lower bound tight point expansion . setion , little abuse notation , vi understood i-th component vector v v clearly element sequence set . A . .  Scalar Loss Functions well known [ Wah   ] convex optimization problem min ξ ξ subject ( cid:    ) w , x ( cid:    ) ≥   − ξ ξ ≥   (  .  )           Loss Functions  u l   v  r    e R e g r e     n    ( f −  ) ( cid:   )  ( f −  ) w h e r e  ( cid:   )    ( f −  )  c  l e    f  -   r g  n  u l   c l      x  ( cid:   ) Γ (  ,  ( cid:   ) ) ( f  ( cid:   ) − f  + ∆ (  ,  ( cid:   ) ) ) Γ (  ,  ( cid:   ) ) ( e  ∗ − e  ) [  J H      ]   f    x  u l   c l    [ C  L      ] l  g ( cid:   )  ( cid:   ) e x p ( f  ( cid:   ) ) − f  ( cid:    ) ( cid:   )  ( cid:   ) e  ( cid:   ) e x p ( f ( cid:   ) ) ( cid:    ) / ( cid:   )  ( cid:   ) e x p ( f ( cid:   ) ) − e  w h e r e  ∗    h e  r g   x  f  h e l    [ C      ]   f  -   r g  n  u l   c l    [  G K     ]   x  ( cid:   ) ( f  ( cid:   ) − f  + ∆ (  ,  ( cid:   ) ) ) e  ∗ − e  w h e r e  ∗    h e  r g   x  f  h e l    L     e r  v    v e V e c   r   l l    f u n c    n   n   h e  r  e r  v    v e  ,  e p e n   n g  n  h e v e c   r f : = W x  n   n  . Q u  n   l e r e g r e     n [ K  e     ]   x ( τ ( f −  ) , (   − τ ) (  − f ) ) τ  f f >   n  τ −     h e r w   e P      n r e g r e     n [ C r e     ] e x p ( f ) −  f e x p ( f ) −  ( cid:   ) -  n  e n     v e [ V G      ]   x (   , | f −  | − ( cid:   ) )    f | f −  | ≤ ( cid:   ) , e l  e   g n ( f −  ) H u b e r ’  r  b u   l    [   R +     ]    ( f −  )    f | f −  | ≤   , e l  e | f −  | −    f −   f | f −  | ≤   , e l  e   g n ( f −  ) L e     b   l u  e  e v      n | f −  | N  v e l   [  P   +     ] L e     e  n  q u  r e  [ W  l     ]    ( f −  )     x (   , ρ − f ) E x p  n e n    l [ C  L      ] e x p ( −  f ) L  g     c [ C       ] l  g (   + e x p ( −  f ) ) H  n g e [ B      ]  q u  r e  H  n g e [ K      ] L    ¯l ( f ,  )   x (   ,   −  f )      x (   ,   −  f )   f −    g n ( f −  )  e r  v    v e ¯l ( cid:   ) ( f ,  ) −  e x p ( −  f ) −  / (   + e x p ( −  f ) )    f f ≥ ρ  n  −     h e r w   e    f  f ≥    n  −    h e r w   e    f  f ≥    n  f −    h e r w   e  c  l  r l    f u n c    n   n   h e  r  e r  v    v e  ,  e p e n   n g  n f : = ( cid:    ) w , x ( cid:    ) ,  n   . A .  Loss Functions     takes value max (   ,   − ( cid:    ) w , x ( cid:    ) ) . latter convex function w x . Likewise , may rewrite ( cid:   ) -insensitive loss , Huber ’ robust loss , quantile regression loss , novelty detection loss terms loss functions rather constrained optimization problem . cases , ( cid:    ) w , x ( cid:    ) play key role insofar loss convex terms scalar quantity ( cid:    ) w , x ( cid:    ) . large number loss functions fall category , described Table A .  . Note functions type continuously diﬀerentiable . case adopt convention ∂x max ( f ( x ) , g ( x ) ) = ( cid:   ) ∂xf ( x ) ∂xg ( x ) f ( x ) ≥ g ( x ) otherwise . (  .  ) Since interested obtaining arbitrary element subd- iﬀerential convention consistent requirements . Let us discuss issue eﬃcient computation . scalar losses may write l ( x , , w ) = ¯l ( ( cid:    ) w , x ( cid:    ) , ) , described Table A .  . case simple application chain rule yields ∂wl ( x , , w ) = ¯l ( cid:   ) ( ( cid:    ) w , x ( cid:    ) , ) ·x . instance , squared loss ¯l ( ( cid:    ) w , x ( cid:    ) , ) =     ( ( cid:    ) w , x ( cid:    ) − )   ¯l ( cid:   ) ( ( cid:    ) w , x ( cid:    ) , ) = ( cid:    ) w , x ( cid:    ) − . Consequently , derivative empirical risk term given ∂wRemp ( w ) =     ( cid:   ) i=  ¯l ( cid:   ) ( ( cid:    ) w , xi ( cid:    ) , yi ) · xi . (  .  ) means want compute l ∂wl large number observations xi , represented matrix X , make use fast linear algebra routines pre-compute vectors f = Xw g ( cid:   ) X gi = ¯l ( cid:   ) ( fi , yi ) . (  .  ) possible loss functions listed Table A .  , many similar losses . advantage uniﬁed representation im- plementation individual loss done little time . computational infrastructure computing Xw g ( cid:   ) X shared . Eval- uating ¯l ( fi , yi ) ¯l ( cid:   ) ( fi , yi ) done ( ) time time-critical comparison remaining operations . Algorithm  .  describes details . important often neglected issue worth mentioning . Computing f requires us right multiply matrix X vector w computing g requires left multiplication X vector g ( cid:   ) . X stored row major format Xw computed rather eﬃciently g ( cid:   ) X       Loss Functions Algorithm  .  ScalarLoss ( w , X , )   : input : Weight vector w , feature matrix X , labels   : Compute f = Xw   : Compute r = ( cid:   )    : g ← g ( cid:   ) X   : return Risk r gradient g ¯l ( fi , yi ) g = ¯l ( cid:   ) ( f , ) expensive . particularly true X ﬁt main memory . Converse case X stored column major format . Similar problems encountered X sparse matrix stored either compressed row format compressed column format . A . .  Structured Loss recent years structured estimation gained substantial popularity machine learning [ TJHA   , TGK   , BHS+   ] . core relies two types convex loss functions : logistic loss : l ( x , , w ) = log ( cid:   ) ( cid:   ) ∈Y exp ( cid:  ) ( cid:   ) w , φ ( x , ( cid:   ) ) ( cid:   ) ( cid:  ) − ( cid:    ) w , φ ( x , ) ( cid:    ) , (  .  ) soft-margin loss : l ( x , , w ) = max ( cid:   ) ∈Y Γ ( , ( cid:   ) ) ( cid:   ) w , φ ( x , ( cid:   ) ) − φ ( x , ) ( cid:   ) + ∆ ( , ( cid:   ) ) . (  .  ) φ ( x , ) joint feature map , ∆ ( , ( cid:   ) ) ≥   describes cost mis- classifying ( cid:   ) , Γ ( , ( cid:   ) ) ≥   scaling term indicates much large margin property enforced . instance , [ TGK   ] choose Γ ( , ( cid:   ) ) =   . hand [ TJHA   ] suggest Γ ( , ( cid:   ) ) = ∆ ( , ( cid:   ) ) , reportedly yields better performance . Finally , [ McA   ] recently sug- gested generic functions Γ ( , ( cid:   ) ) . logistic loss also interpreted negative log-likelihood conditional exponential family model : p ( y|x ; w ) : = exp ( ( cid:    ) w , φ ( x , ) ( cid:    ) − g ( w|x ) ) , (  .  ) normalizing constant g ( w|x ) , often called log-partition func- tion , reads g ( w|x ) : = log ( cid:   ) ( cid:   ) ∈Y exp ( cid:  ) ( cid:   ) w , φ ( x , ( cid:   ) ) ( cid:   ) ( cid:  ) . (  .  ) A .  Loss Functions     consequence Hammersley-Cliﬀord theorem [ Jor   ] every expo- nential family distribution corresponds undirected graphical model . case implies labels factorize according undirected graphical model . large number problems addressed setting , amongst named entity tagging [ LMP   ] , sequence alignment [ TJHA   ] , segmentation [ RSS+   ] path planning [ RBZ   ] . clearly impossible give examples settings section , would brief summary ﬁeld justice . therefore refer reader edited volume [ BHS+   ] references therein . underlying graphical model tractable eﬃcient inference al- gorithms based dynamic programming used compute (  .  ) (  .  ) . discuss intractable graphical models Section A . . .  , turn attention derivatives structured losses . comes computing derivatives logistic loss , (  .  ) , ∂wl ( x , , w ) = ( cid:   ) ( cid:   ) φ ( x , ( cid:   ) ) exp ( cid:    ) w , φ ( x , ( cid:   ) ) ( cid:    ) ( cid:   ) ( cid:   ) exp ( cid:    ) w , φ ( x , ( cid:   ) ) ( cid:    ) − φ ( x , ) (  .  ) = Ey ( cid:   ) ∼p ( ( cid:   ) |x ) ( cid:  ) φ ( x , ( cid:   ) ) ( cid:  ) − φ ( x , ) . (  .   ) p ( y|x ) exponential family model (  .  ) . case (  .  ) denote ¯y ( x ) argmax RHS , ¯y ( x ) : = argmax Γ ( , ( cid:   ) ) ( cid:   ) w , φ ( x , ( cid:   ) ) − φ ( x , ) ( cid:   ) + ∆ ( , ( cid:   ) ) . (  .   ) ( cid:   ) allows us compute derivative l ( x , , w ) ∂wl ( x , , w ) = Γ ( , ¯y ( x ) ) [ φ ( x , ¯y ( x ) ) − φ ( x , ) ] . (  .   ) case loss maximized one distinct value ¯y ( x ) may average individual values , since convex combination terms lies subdiﬀerential . Note (  .  ) majorizes ∆ ( , y∗ ) , y∗ : = argmaxy ( cid:   ) ( cid:    ) w , φ ( x , ( cid:   ) ) ( cid:    ) [ TJHA   ] . seen via following series inequalities : ∆ ( , y∗ ) ≤ Γ ( , y∗ ) ( cid:    ) w , φ ( x , y∗ ) − φ ( x , ) ( cid:    ) + ∆ ( , y∗ ) ≤ l ( x , , w ) . ﬁrst inequality follows Γ ( , y∗ ) ≥   y∗ maximizes ( cid:    ) w , φ ( x , ( cid:   ) ) ( cid:    ) thus implying Γ ( , y∗ ) ( cid:    ) w , φ ( x , y∗ ) − φ ( x , ) ( cid:    ) ≥   . second inequal- ity follows deﬁnition loss . conclude section simple lemma heart several derivations [ Joa   ] . proof original paper far trivial , straightforward setting :       Loss Functions Lemma  .  Denote δ ( , ( cid:   ) ) loss let φ ( xi , yi ) feature map observations ( xi , yi )   ≤ ≤ m. Moreover , denote X , set patterns labels respectively . Finally let Φ ( X , ) : =  ( cid:   ) i=  φ ( xi , yi ) ∆ ( , ( cid:   ) ) : =  ( cid:   ) i=  δ ( yi , ( cid:   ) ) . (  .   ) following two losses equivalent :  ( cid:   ) i=  max ( cid:   ) ( cid:   ) w , φ ( xi , ( cid:   ) ) − φ ( xi , yi ) ( cid:   ) + δ ( yi , ( cid:   ) ) max ( cid:   ) ( cid:   ) w , Φ ( X , ( cid:   ) ) − Φ ( X , ) ( cid:   ) + ∆ ( , ( cid:   ) ) . immediately obvious , since feature map loss decompose , allows us perform maximization ( cid:   ) maximizing components . , showed aggregating data labels single feature map loss yields results identical minimizing sum individual losses . holds , particular , sample error loss [ Joa   ] . Also note equivalence hold whenever Γ ( , ( cid:   ) ) constant . A . . .  Intractable Models discuss cases computing l ( x , , w ) expensive . instance , intractable graphical models , computation ( cid:   ) computed eﬃciently . [ WJ   ] propose use convex majoriza- tion log-partition function cases . setting means instead dealing exp ( cid:    ) w , φ ( x , ) ( cid:    ) l ( x , , w ) = g ( w|x ) − ( cid:    ) w , φ ( x , ) ( cid:    ) g ( w|x ) : = log exp ( cid:    ) w , φ ( x , ) ( cid:    ) ( cid:   )  one uses easily computable convex upper bound g via sup µ∈MARG ( x ) ( cid:    ) w , µ ( cid:    ) + HGauss ( µ|x ) . (  .   ) (  .   ) MARG ( x ) outer bound conditional marginal polytope associated map φ ( x , ) . Moreover , HGauss ( µ|x ) upper bound entropy using Gaussian identical variance . reﬁned tree decompositions exist , . key beneﬁt approach solution µ optimization problem (  .   ) immediately used gradient upper bound . computationally rather eﬃcient . A .  Loss Functions     Likewise note [ TGK   ] use relaxations solving structured esti- mation problems form l ( x , , w ) = max ( cid:   ) Γ ( , ( cid:   ) ) ( cid:   ) w , φ ( x , ( cid:   ) ) − φ ( x , ) ( cid:   ) + ∆ ( , ( cid:   ) ) , (  .   ) enlarging domain maximization respect ( cid:   ) . instance , instead integer programming problem might relax setting linear program much cheaper solve . , , provides upper bound original loss function . summary , demonstrated convex relaxation strategies well applicable bundle methods . fact , results corresponding optimization procedures used directly optimization steps . A . .  Scalar Multivariate Performance Scores discuss series structured loss functions implemented eﬃciently . sake completeness , give concise rep- resentation previous work multivariate performance scores ranking methods . loss functions rely access ( cid:    ) w , x ( cid:    ) , computed eﬃciently using operations Section A . .  . A . . .  ROC Score Denote f = Xw vector function values training set . well known area ROC curve given AUC ( x , , w ) =   m+m− ( cid:   ) yi < yj ( ( cid:    ) w , xi ( cid:    ) < ( cid:    ) w , xj ( cid:    ) ) , (  .   ) m+ m− numbers positive negative observations respectively , ( · ) indicator function . Directly optimizing cost   − AUC ( x , , w ) diﬃcult continuous w. using max (   ,   + ( cid:    ) w , xi − xj ( cid:    ) ) surrogate loss function pairs ( , j ) yi < yj following convex multivariate empirical risk Remp ( w ) =   m+m− ( cid:   ) yi < yj max (   ,   + ( cid:    ) w , xi − xj ( cid:    ) ) =   m+m− ( cid:   ) yi < yj max (   ,   + fi − fj ) . (  .   ) Obviously , could compute Remp ( w ) derivative ( m  ) op- eration . However [ Joa   ] showed computed ( log ) time using sorting operation , describe . Denote c = f −     auxiliary variable let j indices       Loss Functions Algorithm  .  ROCScore ( X , , w )   : input : Feature matrix X , labels , weight vector w   : initialization : s− = m− s+ =   l =  m c = Xw −       : π ← {   , . . . , } sorted ascending order c   : =   yπi = −    :   :   :   : lπi ← s+ s− ← s− −   else lπi ← −s− s+ ← s+ +   end   :    : end    : Rescale l ← l/ ( m+m− ) compute r = ( cid:    ) l , c ( cid:    ) g = l ( cid:   ) X .    : return Risk r subgradient g yi = −  yj =   . follows ci − cj =   + fi − fj . eﬃcient algorithm due observation distinct terms ck , k =   , . . . , , diﬀerent frequency lk sign , appear (  .   ) . frequencies lk determined ﬁrst sorting c ascending order scanning labels according sorted order c keeping running statistics number s− negative labels yet encounter , number s+ positive labels encountered . visiting yk , know ck appears s+ ( s− ) times positive ( negative ) sign (  .   ) yk = −  ( yk =   ) . Algorithm  .  spells explicitly compute Remp ( w ) subgradient . A . . .  Ordinal Regression Essentially preference relationships need hold ordinal re- gression . diﬀerence yi need take binary values . Instead , may arbitrary number diﬀerent values yi ( e.g. ,   corresponding ’ strong reject ’    corresponding ’ strong accept ’ , comes ranking papers conference ) . , yi ∈ {   , . . . , n } rather yi ∈ { ±  } . goal ﬁnd w ( cid:    ) w , xi − xj ( cid:    ) <   whenever yi < yj . Whenever relationship satis- ﬁed , incur cost C ( yi , yj ) preferring xi xj . examples , C ( yi , yj ) could constant i.e. , C ( yi , yj ) =   [ Joa   ] linear i.e. , C ( yi , yj ) = yj − yi . Denote mi number xj yj = . case , ¯M = m  − ( cid:   ) n pairs ( yi , yj ) yi ( cid:   ) = yj ; implies = ¯M /  pairs ( yi , yj ) yi < yj . Normalizing total i=  m  A .  Loss Functions     number comparisons may write overall cost estimator    ( cid:   ) yi < yj C ( yi , yj ) ( ( cid:    ) w , xi ( cid:    ) > ( cid:    ) w , xj ( cid:    ) ) =     ( cid:   ) m  − n ( cid:   ) ( cid:   ) m   . (  .   )  Using convex majorization maximizing ROC score , obtain empirical risk form Remp ( w ) =    ( cid:   ) yi < yj C ( yi , yj ) max (   ,   + ( cid:    ) w , xi − xj ( cid:    ) ) (  .   ) goal ﬁnd eﬃcient algorithm obtaining number times individual losses nonzero compute value gradient Remp ( w ) . complication arises fact observations xi label yi may appear either side inequality depending whether yj < yi yj > yi . problem solved follows : sort f = Xw ascending order traverse keeping track many items lower value yj   apart terms value fi . way may compute count statistics eﬃciently . Algorithm  .  describes details , generalizing results [ Joa   ] . , runtime ( log ) , thus allowing eﬃcient computation . A . . .  Preference Relations general , loss may described means set preference relations j ( cid:   ) arbitrary pairs ( , j ) ∈ {   , . . . }   associated cost C ( , j ) incurred whenever ranked j . set preferences may may form partial total order domain observations . cases eﬃcient computations along lines Algorithm  .  exist . general , case need rely fact set P containing preferences suﬃciently small enumerated eﬃciently . risk given   |P | ( cid:   ) ( , j ) ∈P C ( , j ) ( ( cid:    ) w , xi ( cid:    ) > ( cid:    ) w , xj ( cid:    ) ) (  .   )       Loss Functions Algorithm  .  OrdinalRegression ( X , , w , C )   : input : Feature matrix X , labels , weight vector w , score matrix C   : initialization : l =  n ui = mi ∀i ∈ [ n ] r =   g =  m   : Compute f = Xw set c = [ f −     ] ∈ R m ( concatenate   , f +   vectors ) ) /  i=  m    : Compute = ( m  − ( cid:   ) n   : Rescale C ← C/M   : π ← {   , . . . ,  m } sorted ascending order c   : =    m j = πi mod   : πi ≤   :    :    :    :    :    :    :    :    :    :    :    : k =   yj −   r ← r − C ( k , yj ) ukcj gj ← gj − C ( k , yj ) uk end lyj ← lyj +   else k = yj +   n r ← r + C ( yj , k ) lkcj+m gj ← gj + C ( yj , k ) lk end uyj ← uyj −   end    :    : end    : g ← g ( cid:   ) X    : return : Risk r subgradient g , majorization argument allows us write convex upper bound Remp ( w ) = ∂wRemp ( w ) =   |P |   |P | ( cid:   ) ( , j ) ∈P ( cid:   ) ( , j ) ∈P C ( , j ) max (   ,   + ( cid:    ) w , xi ( cid:    ) − ( cid:    ) w , xj ( cid:    ) ) (  .   ) C ( , j ) ( cid:   )   xi − xj ( cid:    ) w , xj − xi ( cid:    ) ≥   otherwise (  .   ) implementation straightforward , given Algorithm  .  . A .  Loss Functions     Algorithm  .  Preference ( X , w , C , P )   : input : Feature matrix X , weight vector w , score matrix C , prefer- ence set P   : initialization : r =   g =  m   : Compute f = Xw   : ( , j ) ∈ P   : fj − fi <     :   : r ← r + C ( , j ) (   + fi − fj ) gi ← gi + C ( , j ) gj ← gj − C ( , j ) end   :   : end    : g ← g ( cid:   ) X    : return Risk r subgradient g A . . .  Ranking webpage document ranking often situation similar described Section A . . .  , however diﬀerence care objects xi ranked according scores yi moreover diﬀerent degrees importance placed diﬀerent documents . information retrieval literature full large number diﬀer- ent scoring functions . Examples criteria Normalized Discounted Cumulative Gain ( NDCG ) , Mean Reciprocal Rank ( MRR ) , Precision @ n , Expected Rank Utility ( ERU ) . used address issue evaluat- ing rankers , search engines recommender sytems [ Voo   , JK   , BHK   , BH   ] . instance , webpage ranking ﬁrst k retrieved docu- ments matter , since users unlikely look beyond ﬁrst k , say    , retrieved webpages internet search . [ LS   ] show scores optimized directly minimizing following loss : l ( X , , w ) = max π ( cid:   )  ( cid:   ) w , xπ ( ) − xi ci ( cid:   ) + ( cid:    ) − ( π ) , b ( ) ( cid:    ) . (  .   ) ci monotonically decreasing sequence , documents assumed arranged order decreasing relevance , π permutation , vectors b ( ) depend choice particular ranking measure , ( π ) denotes permutation according π. Pre-computing f = Xw may rewrite (  .   ) l ( f , ) = max π ( cid:    ) c ( cid:   ) f ( π ) − ( π ) ( cid:   ) b ( ) ( cid:    ) − c ( cid:   ) f + ( cid:   ) b ( ) (  .   )       Loss Functions Algorithm  .  Ranking ( X , , w )   : input : Feature matrix X , relevances , weight vector w   : Compute vectors b ( ) according ranking measure   : Compute f = Xw   : Compute elements matrix Cij = cifj − biaj   : π = LinearAssignment ( C )   : r = c ( cid:   ) ( f ( π ) − f ) + ( − ( π ) ) ( cid:   ) b   : g = c ( π−  ) − c g ← g ( cid:   ) X   : return Risk r subgradient g consequently derivative l ( X , , w ) respect w given ∂wl ( X , , w ) = ( c ( ¯π−  ) − c ) ( cid:   ) X ¯π = argmax π c ( cid:   ) f ( π ) − ( π ) ( cid:   ) b ( ) . (  .   ) π−  denotes inverse permutation , π◦π−  =   . Finding permutation maximizing c ( cid:   ) f ( π ) − ( π ) ( cid:   ) b ( ) linear assignment problem easily solved Hungarian Marriage algorithm , , Kuhn-Munkres algorithm . original papers [ Kuh   ] [ Mun   ] implied algorithm ( m  ) cost number terms . Later , [ Kar   ] suggested algorithm expected quadratic time size assignment problem ( ignor- ing log-factors ) . Finally , [ OL   ] propose linear time algorithm large problems . Since case number pages fairly small ( order        per query ) scaling behavior per query important . used existing implementation due [ JV   ] . Note also training sets consist collection ranking problems , , several ranking problems size        . means parallelization able distribute work onto cluster worksta- tions , able overcome issue rather costly computation per collection queries . Algorithm  .  spells steps detail . A . . .  Contingency Table Scores [ Joa   ] observed Fβ scores related quantities dependent con- tingency table also computed eﬃciently means structured es- timation . scores depend general number true false positives negatives alike . Algorithm  .  shows corresponding em- pirical risk subgradient computed eﬃciently . pre- vious losses , use convex majorization obtain tractable optimization problem . A .  Loss Functions     Given set labels estimate ( cid:   ) , numbers true positives ( T+ ) , true negatives ( T− ) , false positives ( F+ ) , false negatives ( F− ) determined according contingency table follows : >   <   ( cid:   ) >   ( cid:   ) <   T+ F− F+ T− sequel , denote m+ = T+ + F− m− = T− + F+ numbers positives negative labels , respectively . note Fβ score computed based contingency table [ Joa   ] Fβ ( T+ , T− ) = (   + β  ) T+ T+ + m− − T− + β m+ . (  .   ) want use ( cid:    ) w , xi ( cid:    ) estimate label observation xi , may use following structured loss “ directly ” optimize w.r.t . Fβ score [ Joa   ] : l ( X , , w ) = max ( cid:   ) ( cid:    ) ( ( cid:   ) − ) ( cid:   ) f + ∆ ( T+ , T− ) ( cid:    ) , (  .   ) f = Xw , ∆ ( T+ , T− ) : =   − Fβ ( T+ , T− ) , ( T+ , T− ) determined using ( cid:   ) . Since ∆ depend speciﬁc choice ( , ( cid:   ) ) rather sets disagree , l maximized follows : Enumerating possible m+m− contingency tables way given conﬁguration ( T+ , T− ) , T+ ( T− ) positive ( negative ) observations xi largest ( lowest ) value ( cid:    ) w , xi ( cid:    ) labeled positive ( negative ) . eﬀectively implemented nested loop hence run ( m  ) time . Algorithm  .  describes procedure details . A . .  Vector Loss Functions Next discuss “ vector ” loss functions , i.e. , functions w best de- scribed matrix ( denoted W ) loss depends W x . , feature vector x ∈ Rd , label ∈ Rk , weight matrix W ∈ Rd×k . also denote feature matrix X ∈ Rm×d matrix feature vectors xi , stack columns Wi W vector w . relevant cases multiclass classiﬁcation using exponential families model structured estimation , hierarchical mod- els , i.e. , ontologies , multivariate regression . Many cases summarized Table A .  .     Algorithm  .  Fβ ( X , , w )   Loss Functions   : input : Feature matrix X , labels , weight vector w   : Compute f = Xw   : π+ ← { : yi =   } sorted descending order f   : π− ← { : yi = −  } sorted ascending order f   : Let p  =   pi =   ( cid:   ) m+   : Let n  =   ni =   ( cid:   ) m−   : ( cid:   ) ← −y r ← −∞   : =   m+   : , =   , . . . , m+ , =   , . . . , m− k=i fπ+ k=i fπ− j =   m− k k rtmp = ∆ ( , j ) − pi + nj rtmp > r r ← rtmp T+ ← T− ← j    :    :    :    :    : end end π+     :    : end    : ( cid:   )    : ( cid:   )    : g ← ( ( cid:   ) − ) ( cid:   ) X    : return Risk r subgradient g ←   , =   , . . . , T+ ← −  , =   , . . . , T− π−  A . . .  Unstructured Setting simplest loss multivariate regression , l ( x , , W ) =   x ( cid:   ) W ) . case clear pre-computing XW subsequent calcu- lations loss gradient signiﬁcantly accelerated .   ( y−x ( cid:   ) W ) ( cid:   ) ( y− second class important losses given plain multiclass classiﬁcation problems , e.g. , recognizing digits postal code categorizing high-level document categories . case , φ ( x , ) best represented ey ⊗x ( using linear model ) . Clearly may view ( cid:    ) w , φ ( x , ) ( cid:    ) operation chooses column indexed xW , since labels correspond diﬀerent weight vector Wy . Formally set ( cid:    ) w , φ ( x , ) ( cid:    ) = [ xW ] . case , structured estimation losses rewritten l ( x , , W ) = max Γ ( , ( cid:   ) ) ( cid:   ) Wy ( cid:   ) − Wy , x ( cid:   ) + ∆ ( , ( cid:   ) ) ( cid:   ) ∂W l ( x , , W ) = Γ ( , y∗ ) ( ey∗ − ey ) ⊗ x . (  .   ) (  .   ) Γ ∆ deﬁned Section A . .  y∗ denotes value ( cid:   ) A .  Loss Functions     RHS (  .   ) maximized . means unstructured multiclass settings may simply compute xW . Since needs per- formed observations xi may take advantage fast linear algebra routines compute f = XW eﬃciency . Likewise note comput- ing gradient observations matrix-matrix multiplication , : denote G matrix rows gradients Γ ( yi , y∗ − eyi ) . ∂W Remp ( X , , W ) = G ( cid:   ) X . Note G sparse two nonzero entries per row , makes computation G ( cid:   ) X essentially expensive two matrix vector multiplications . Whenever many classes , may yield signiﬁcant computational gains . ) ( ey∗  Log-likelihood scores exponential families share similar expansions .  l ( x , , W ) = log ( cid:   ) ( cid:   ) exp ( cid:   ) w , φ ( x , ( cid:   ) ) ( cid:   ) − ( cid:    ) w , φ ( x , ) ( cid:    ) = log exp ( cid:   ) Wy ( cid:   ) , x ( cid:   ) − ( cid:    ) Wy , x ( cid:    ) ( cid:   ) ( cid:   ) ∂W l ( x , , W ) = ( cid:   ) ( cid:   ) ( ey ( cid:   ) ⊗ x ) exp ( cid:   ) Wy ( cid:   ) , x ( cid:   ) ( cid:   ) exp ( cid:   ) Wy ( cid:   ) , x ( cid:   ) ( cid:   ) − ey ⊗ x . (  .   ) (  .   ) main diﬀerence soft-margin setting gradients sparse number classes . means computation gradients slightly costly . A . . .  Ontologies Fig . A .  . Two ontologies . Left : binary hierarchy internal nodes {   , . . . ,   } labels {   , . . .    } . Right : generic directed acyclic graph internal nodes {   , . . . ,   ,    } labels {   , . . . ,    ,    , . . . ,    } . Note node   two parents , namely nodes     . Moreover , labels need found level tree : nodes       one level lower rest nodes . Assume labels want estimate found belong directed acyclic graph . instance , may gene-ontology graph       Loss Functions [ ABB+   ] patent hierarchy [ CH   ] , genealogy . cases hierarchy categories element x may belong . Figure A .  gives two examples directed acyclic graphs ( DAG ) . ﬁrst example binary tree , second contains nodes diﬀerent numbers children ( e.g. , node      ) , nodes diﬀerent levels children ( e.g. , nodes      ) , nodes one parent ( e.g. , node   ) . well known fundamental property trees many internal nodes leaf nodes . goal build classiﬁer able categorize observa- tions according leaf node belong ( leaf node assigned label ) . Denote k +   number nodes DAG including root node . case may design feature map φ ( ) ∈ Rk [ CH   ] associating every label vector describing path root node , ignoring root node . instance , ﬁrst DAG Figure A .  φ (   ) = (   ,   ,   ,   ,   ,   ,   ,   ,   ,   ,   ,   ,   ,   ) φ (    ) = (   ,   ,   ,   ,   ,   ,   ,   ,   ,   ,   ,   ,   ,   ) Whenever several paths admissible , right DAG Figure A .  average possible paths . example , φ (    ) = (  .  ,  .  ,   ,   ,   ,   ,   ,   ,   ,   ,   ,   ,   ,   ) φ (    ) = (   ,   ,   ,   ,   ,   ,   ,   ,   ,   ,   ,   ,   ,   ,   ) . Also note lengths paths need ( e.g. , reach    takes longer path reach    ) . Likewise , natural assume ∆ ( , ( cid:   ) ) , i.e. , cost mislabeling ( cid:   ) depend similarity path . words , likely cost placing x wrong sub-sub-category less getting main category object wrong . complete setting , note φ ( x , ) = φ ( ) ⊗ x cost computing labels k inner products , since value ( cid:    ) w , φ ( x , ) ( cid:    ) particular obtained sum contributions segments path . means values terms computed simple breadth ﬁrst traversal graph . , may make use vectorization approach , since may compute xW ∈ Rk obtain contributions segments DAG performing graph traversal . Since patterns xi may vectorize matters pre-computing XW . Also note φ ( ) −φ ( ( cid:   ) ) nonzero edges paths ( cid:   ) diﬀer . Hence change weights parts graph categorization diﬀers . Algorithm  .  describes subgradient loss computation soft-margin type loss function . A .  Loss Functions     Algorithm  .  Ontology ( X , , W )   : input : Feature matrix X ∈ Rm×d , labels , weight matrix W ∈ Rd×k   : initialization : G =   ∈ Rm×k r =     : Compute f = XW let fi = xiW   : =     :   : Let Di DAG edges annotated values fi Traverse Di ﬁnd node y∗ maximize sum fi values path plus ∆ ( yi , ( cid:   ) )   : Gi = φ ( y∗ ) − φ ( yi )   :   : end    : g = G ( cid:   ) X    : return Risk r subgradient g r ← r + zy∗ − zyi reasoning applies estimation using exponential fam- ilies model . diﬀerence need compute soft-max paths rather exclusively choosing best path ontol- ogy . , breadth-ﬁrst recursion suﬃces : leaves DAG associated probability p ( y|x ) . obtain Ey∼p ( y|x ) [ φ ( ) ] need perform bottom-up traversal DAG summing probability weights path . Wherever node one parent , distribute probability weight equally parents . Bibliography [ ABB+   ] M. Ashburner , C. . Ball , J . A. Blake , D. Botstein , H. Butler , J. . Cherry , A. P. Davis , K. Dolinski , S. S. Dwight , J. T. Eppig , M. A. Harris , D. P. Hill , L. Issel-Tarver , A. Kasarskis , S. Lewis , J. C. Matese , J. E. Richard- son , M. Ringwald , G. M. Rubin , G. Sherlock , Gene ontology : tool uniﬁcation biology . gene ontology consortium , Nat Genet    (      ) ,   –    . [ AGML   ] S. F. Altschul , W. Gish , E. W. Myers , D. J. Lipman , Basic local alignment search tool , Journal Molecular Biology     (      ) , .   ,    –     . [ BBL   ] O. Bousquet , S. Boucheron , G. Lugosi , Theory classiﬁcation : sur- vey recent advances , ESAIM : Probab . Stat .   (      ) ,    –     . [ BCR   ] C. Berg , J. P. R. Christensen , P. Ressel , Harmonic analysis semi- groups , Springer , New York ,      . [ BDEL   ] S. Ben-David , N. Eiron , P.M. Long , diﬃculty approximately maximizing agreements , J. Comput . System Sci .    (      ) , .   ,    –    . [ Bel   ] R. E. Bellman , Adaptive control processes , Princeton University Press , Princeton , NJ ,      . [ Bel   ] Alexandre Belloni , Introduction bundle methods , Tech . report , Operation Research Center , M.I.T. ,      . [ Ber   ] J. O. Berger , Statistical decision theory Bayesian analysis , Springer , New York ,      . [ BH   ] J. Basilico T. Hofmann , Unifying collaborative content-based ﬁlter- ing , Proc . Intl . Conf . Machine Learning ( New York , NY ) , ACM Press ,      , pp .   –   . [ BHK   ] J. S. Breese , D. Heckerman , C. Kardie , Empirical analysis predictive algorithms collaborative ﬁltering , Proceedings   th Conference Uncertainty Artiﬁcial Intelligence ,      , pp .   –   . [ BHS+   ] G. Bakir , T. Hofmann , B. Sch¨olkopf , A. Smola , B. Taskar , S. V. N . Vishwanathan , Predicting structured data , MIT Press , Cambridge , Mas- sachusetts ,      . [ Bil   ] Patrick Billingsley , Convergence probability measures , John Wiley Sons ,      . [ Bis   ] C. M. Bishop , Neural networks pattern recognition , Clarendon Press , Oxford ,      . [ BK   ] R. M. Bell Y. Koren , Lessons netﬂix prize challenge , SIGKDD Explorations   (      ) , .   ,   –   . [ BKL   ] A. Beygelzimer , S. Kakade , J. Langford , Cover trees nearest neigh- bor , International Conference Machine Learning ,      . [ BL   ] J. M. Borwein A. S. Lewis , Convex analysis nonlinear optimization : Theory examples , CMS books Mathematics , Canadian Mathematical Society ,      .           Bibliography [ BM   ] K. P. Bennett O. L. Mangasarian , Robust linear programming discrimi- nation two linearly inseparable sets , Optim . Methods Softw .   (      ) ,   –   . [ BNJ   ] D. Blei , A. Ng , M. Jordan , Latent Dirichlet allocation , Journal Ma- chine Learning Research   (      ) ,    –     . [ BT   ] D.P . Bertsekas J.N . Tsitsiklis , Introduction probability , Athena Sci- entiﬁc ,      . [ BV   ] S. Boyd L. Vandenberghe , Convex optimization , Cambridge University Press , Cambridge , England ,      . [ CDLS   ] R. Cowell , A. Dawid , S. Lauritzen , D. Spiegelhalter , Probabilistic networks expert sytems , Springer , New York ,      . [ CH   ] Lijuan Cai T. Hofmann , Hierarchical document categorization sup- port vector machines , Proceedings Thirteenth ACM conference Infor- mation knowledge management ( New York , NY , USA ) , ACM Press ,      , pp .   –   . [ Cra   ] H. Cram´er , Mathematical methods statistics , Princeton University Press ,      . [ Cre   ] N. A. C. Cressie , Statistics spatial data , John Wiley Sons , New York ,      . [ CS   ] K. Crammer . Singer , Ultraconservative online algorithms multi- class problems , Journal Machine Learning Research   (      ) ,    –    . [ CSS   ] M. Collins , R. E. Schapire , . Singer , Logistic regression , AdaBoost Bregman distances , Proc .   th Annu . Conference Comput . Learning Theory , Morgan Kaufmann , San Francisco ,      , pp .    –    . [ CV   ] Corinna Cortes V. Vapnik , Support vector networks , Machine Learning    (      ) , .   ,    –    . [ DG   ] S. Dasgupta A. Gupta , elementary proof theorem johnson lindenstrauss , Random Struct . Algorithms    (      ) , .   ,   –   . [ DG   ] J . Dean S. Ghemawat , MapReduce : simpliﬁed data processing large clusters , CACM    (      ) , .   ,    –    . [ DGL   ] L. Devroye , L. Gy¨orﬁ , G. Lugosi , probabilistic theory pattern recognition , Applications mathematics , vol .    , Springer , New York ,      . [ Fel   ] W. Feller , introduction probability theory applications ,   ed. , John Wiley Sons , New York ,      . [ FJ   ] A. Frieze M. Jerrum , analysis monte carlo algorithm esti- mating permanent , Combinatorica    (      ) , .   ,   –   . [ FS   ] Y. Freund R. E. Schapire , Large margin classiﬁcation using percep- tron algorithm , Machine Learning    (      ) , .   ,    –    . [ FT   ] L. Fahrmeir G. Tutz , Multivariate statistical modelling based gener- alized linear models , Springer ,      . [ GIM   ] A. Gionis , P. Indyk , R. Motwani , Similarity search high dimensions via hashing , Proceedings   th VLDB Conference ( Edinburgh , Scotland ) ( M. P. Atkinson , M. E. Orlowska , P. Valduriez , S. B. Zdonik , M. L. Brodie , eds . ) , Morgan Kaufmann ,      , pp .    –    . [ GS   ] T.L . Griﬃths M. Steyvers , Finding scientiﬁc topics , Proceedings National Academy Sciences     (      ) ,     –     . [ GW   ] P. Groeneboom J . A. Wellner , Information bounds nonparametric maximum likelihood estimation , DMV , vol .    , Springer ,      . [ Hal   ] P. Hall , bootstrap edgeworth expansions , Springer , New York ,      . [ Hay   ] S. Haykin , Neural networks : comprehensive foundation , Macmillan , New York ,      ,  nd edition . Bibliography     [ Heb   ] D. O. Hebb , organization behavior , John Wiley Sons , New York ,      . [ Hoe   ] W. Hoeﬀding , Probability inequalities sums bounded random variables , Journal American Statistical Association    (      ) ,   –   . [ HUL   ] J.B. Hiriart-Urruty C. Lemar´echal , Convex analysis minimization algorithms , II , vol .         , Springer-Verlag ,      . [ IM   ] P. Indyk R. Motawani , Approximate nearest neighbors : Towards remov- ing curse dimensionality , Proceedings   th Symposium Theory Computing ,      , pp .    –    . [ JK   ] K. Jarvelin J. Kekalainen , IR evaluation methods retrieving highly relevant documents , ACM Special Interest Group Information Retrieval ( SI- GIR ) , New York : ACM ,      , pp .   –   . [ Joa   ] T. Joachims , support vector method multivariate performance mea- sures , Proc . Intl . Conf . Machine Learning ( San Francisco , California ) , Morgan Kaufmann Publishers ,      , pp .    –    . [ Joa   ] , Training linear SVMs linear time , Proc . ACM Conf . Knowledge Discovery Data Mining ( KDD ) , ACM ,      . [ Jor   ] M. I. Jordan , introduction probabilistic graphical models , MIT Press ,      , Appear . [ JV   ] R. Jonker A. Volgenant , shortest augmenting path algorithm dense sparse linear assignment problems , Computing    (      ) ,    –    . [ Kar   ] R.M . Karp , algorithm solve × n assignment problem expected time ( mn log n ) , Networks    (      ) , .   ,    –    . [ KD   ] S. S. Keerthi D. DeCoste , modiﬁed ﬁnite Newton method fast solution large scale linear SVMs , J. Mach . Learn . Res .   (      ) ,    –    . [ Kel   ] J. E. Kelly , cutting-plane method solving convex programs , Journal Society Industrial Applied Mathematics   (      ) , .   ,    –    . [ Kiw   ] Krzysztof C. Kiwiel , Proximity control bundle methods convex non- diﬀerentiable minimization , Mathematical Programming    (      ) ,    –    . [ KM   ] Paul Komarek Andrew Moore , dynamic adaptation AD-trees eﬃcient machine learning large data sets , Proc . Intl . Conf . Machine Learn- ing , Morgan Kaufmann , San Francisco , CA ,      , pp .    –    . [ Koe   ] R. Koenker , Quantile regression , Cambridge University Press ,      . [ Kuh   ] H.W . Kuhn , Hungarian method assignment problem , Naval Re- search Logistics Quarterly   (      ) ,   –   . [ Lew   ] D. D. Lewis , Naive ( Bayes ) forty : independence assumption in- formation retrieval , Proceedings ECML-   ,   th European Conference Machine Learning ( Chemnitz , DE ) ( C. N´edellec C. Rouveirol , eds . ) , .      , Springer Verlag , Heidelberg , DE ,      , pp .  –   . [ LK   ] C. Leslie R. Kuang , Fast kernels inexact string matching , Proc . Annual Conf . Computational Learning Theory ,      . [ LMP   ] J. D. Laﬀerty , A. McCallum , F. Pereira , Conditional random ﬁelds : Probabilistic modeling segmenting labeling sequence data , Proceedings International Conference Machine Learning ( San Francisco , CA ) , vol .    , Morgan Kaufmann ,      , pp .    –    . [ LNN   ] Claude Lemar´echal , Arkadii Nemirovskii , Yurii Nesterov , New variants bundle methods , Mathematical Programming    (      ) ,    –    . [ LS   ] Q . Le A.J . Smola , Direct optimization ranking measures , J. Mach . Learn . Res . (      ) , submitted . [ LT   ] Z. Q. Luo P. Tseng , convergence coordinate descent method       Bibliography convex diﬀerentiable minimization , Journal Optimization Theory Applications    (      ) , .   ,  –   . [ Lue   ] D. G. Luenberger , Linear nonlinear programming , second ed. , Addison- Wesley , Reading , May      . [ Mar   ] M.E . Maron , Automatic indexing : experimental inquiry , Journal Association Computing Machinery   (      ) ,    –    . [ McA   ] David McAllester , Generalization bounds consistency structured labeling , Predicting Structured Data ( Cambridge , Massachusetts ) , MIT Press ,      . [ McD   ] C. McDiarmid , method bounded diﬀerences , Survey Combina- torics , Cambridge University Press ,      , pp .    –    . [ Mit   ] T. M. Mitchell , Machine learning , McGraw-Hill , New York ,      . [ MN   ] P. McCullagh J . A. Nelder , Generalized linear models , Chapman Hall , London ,      . [ MSR+   ] K.-R. M¨uller , A. J. Smola , G. R¨atsch , B. Sch¨olkopf , J. Kohlmorgen , V. Vapnik , Predicting time series support vector machines , Artiﬁcial Neu- ral Networks ICANN ’    ( Berlin ) ( W. Gerstner , A. Germond , M. Hasler , J.-D. Nicoud , eds . ) , Lecture Notes Comput . Sci. , vol .      , Springer-Verlag ,      , pp .    –     . [ Mun   ] J. Munkres , Algorithms assignment transportation problems , Journal SIAM   (      ) , .   ,   –   . [ MYA   ] N. Murata , S. Yoshizawa , S. Amari , Network information criterion — determining number hidden units artiﬁcial neural network models , IEEE Transactions Neural Networks   (      ) ,    –    . [ Nad   ] E. A. Nadaraya , nonparametric estimates density functions re- gression curves , Theory Probability Applications    (      ) ,    –    . [ NW   ] J. Nocedal S. J. Wright , Numerical optimization , Springer Series Operations Research , Springer ,      . [ OL   ] J.B. Orlin Y. Lee , Quickmatch : fast algorithm assignment problem , Working Paper     -   , Sloan School Management , Massachusetts Institute Technology , Cambridge , , March      . [ Pap   ] A. Papoulis , fourier integral applications , McGraw-Hill , New York ,      . [ Pla   ] J. Platt , Fast training support vector machines using sequential minimal optimization , Advances Kernel Methods — Support Vector Learning ( Cam- bridge , ) ( B. Sch¨olkopf , C. J. C. Burges , A. J. Smola , eds . ) , MIT Press ,      , pp .    –    . [ PTVF   ] W. H. Press , S. A. Teukolsky , W. T. Vetterling , B. P. Flannery , Numerical recipes c. art scientiﬁc computation , Cambridge University Press , Cambridge , UK ,      . [ Rao   ] C. R. Rao , Linear statistical inference applications , John Wiley Sons , New York ,      . [ RBZ   ] N. Ratliﬀ , J. Bagnell , M. Zinkevich , Maximum margin planning , Inter- national Conference Machine Learning , July      . [ Ros   ] F. Rosenblatt , perceptron : probabilistic model information storage organization brain , Psychological Review    (      ) , .   ,    –    . [ RPB   ] M. Richardson , A. Prakash , E. Brill , Beyond pagerank : machine learn- ing static ranking , Proceedings   th international conference World Wide Web , WWW ( L. Carr , D. De Roure , A. Iyengar , C.A . Goble , M. Dahlin , eds . ) , ACM ,      , pp .    –    . Bibliography     [ RSS+   ] G. R¨atsch , S. Sonnenburg , J. Srinivasan , H. Witte , K.-R. M¨uller , R. J . Sommer , B. Sch¨olkopf , Improving Caenorhabditis elegans genome an- notation using machine learning , PLoS Computational Biology   (      ) , .   , e   doi:  .    /journal.pcbi.        . [ Rud   ] W. Rudin , Functional analysis , McGraw-Hill , New York ,      . [ Sil   ] B. W. Silverman , Density estimation statistical data analysis , Mono- graphs statistics applied probability , Chapman Hall , London ,      . [ SPST+   ] B. Sch¨olkopf , J. Platt , J. Shawe-Taylor , A. J. Smola , R. C . Williamson , Estimating support high-dimensional distribution , Neu- ral Comput .    (      ) , .   ,     –     . [ SS   ] B. Sch¨olkopf A. Smola , Learning kernels , MIT Press , Cambridge , ,      . [ SW   ] G.R . Shorack J.A . Wellner , Empirical processes applications statistics , Wiley , New York ,      . [ SZ   ] Helga Schramm Jochem Zowe , version bundle idea minimiz- ing nonsmooth function : Conceptual idea , convergence analysis , numerical results , SIAM J. Optimization   (      ) ,    –    . [ TGK   ] B. Taskar , C. Guestrin , D. Koller , Max-margin Markov networks , Advances Neural Information Processing Systems    ( Cambridge , ) ( S. Thrun , L. Saul , B. Sch¨olkopf , eds . ) , MIT Press ,      , pp .   –   . [ TJHA   ] I. Tsochantaridis , T. Joachims , T. Hofmann , Y. Altun , Large margin methods structured interdependent output variables , J. Mach . Learn . Res .   (      ) ,     –     . [ Vap   ] V. Vapnik , Estimation dependences based empirical data , Springer , Berlin ,      . [ Vap   ] , nature statistical learning theory , Springer , New York ,      . , Statistical learning theory , John Wiley Sons , New York ,      . [ Vap   ] [ vdG   ] S. van de Geer , Empirical processes M-estimation , Cambridge University Press ,      . [ vdVW   ] A. W. van der Vaart J . A. Wellner , Weak convergence empirical processes , Springer ,      . [ VGS   ] V. Vapnik , S. Golowich , A. J. Smola , Support vector method func- tion approximation , regression estimation , signal processing , Advances Neural Information Processing Systems   ( Cambridge , ) ( M. C. Mozer , M. I. Jordan , T. Petsche , eds . ) , MIT Press ,      , pp .    –    . [ Voo   ] E. Voorhees , Overview TRECT      question answering track , TREC ,      . [ VS   ] S. V. N. Vishwanathan A. J. Smola , Fast kernels string tree matching , Kernel Methods Computational Biology ( Cambridge , ) ( B. Sch¨olkopf , K. Tsuda , J. P. Vert , eds . ) , MIT Press ,      , pp .    –    . [ VSV   ] S. V. N. Vishwanathan , A. J. Smola , R. Vidal , Binet-Cauchy kernels dynamical systems application analysis dynamic scenes , International Journal Computer Vision    (      ) , .   ,   –    . [ Wah   ] G. Wahba , Support vector machines , reproducing kernel Hilbert spaces randomized GACV , Tech . Report     , Department Statistics , University Wisconsin , Madison ,      . [ Wat   ] G. S. Watson , Smooth regression analysis , Sankhya    (      ) ,    –    . [ Wil   ] C. K. I. Williams , Prediction Gaussian processes : linear regression linear prediction beyond , Learning Inference Graphical Models ( M. I. Jordan , ed . ) , Kluwer Academic ,      , pp .    –    .       Bibliography [ WJ   ] M. J. Wainwright M. I. Jordan , Graphical models , exponential fami- lies , variational inference , Tech . Report     , UC Berkeley , Department Statistics , September      . 