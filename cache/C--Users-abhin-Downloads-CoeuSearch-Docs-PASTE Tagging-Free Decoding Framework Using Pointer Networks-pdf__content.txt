PASTE: A Tagging-Free Decoding Framework Using Pointer Networks for Aspect Sentiment Triplet Extraction Rajdeep Mukherjee∗, Tapas Nayak∗, Yash Butala, Sourangshu Bhattacharya and Pawan Goyal Department of Computer Science and Engineering, IIT Kharagpur, India {rajdeep1989, yashbutala}@iitkgp.ac.in, tnk02.05@gmail.com, {sourangshu, pawang}@cse.iitkgp.ac.in Abstract Aspect Sentiment Triplet Extraction (ASTE) deals with extracting opinion triplets, consist- ing of an opinion target or aspect, its associ- ated sentiment, and the corresponding opinion term/span explaining the rationale behind the sentiment. Existing research efforts are ma- jorly tagging-based. Among the methods tak- ing a sequence tagging approach, some fail to capture the strong interdependence between the three opinion factors, whereas others fall short of identifying triplets with overlapping aspect/opinion spans. A recent grid tagging approach on the other hand fails to capture the span-level semantics while predicting the sen- timent between an aspect-opinion pair. Differ- ent from these, we present a tagging-free so- lution for the task, while addressing the lim- itations of the existing works. We adapt an encoder-decoder architecture with a Pointer Network-based decoding framework that gen- erates an entire opinion triplet at each time step thereby making our solution end-to-end. Interactions between the aspects and opinions are effectively captured by the decoder by con- sidering their entire detected spans while pre- dicting their connecting sentiment. Extensive experiments on several benchmark datasets es- tablish the better efﬁcacy of our proposed ap- proach, especially in recall, and in predict- ing multiple and aspect/opinion-overlapped triplets from the same review sentence. We re- port our results both with and without BERT and also demonstrate the utility of domain- speciﬁc BERT post-training for the task. 1 Introduction Aspect-based Sentiment Analysis (ABSA) is a broad umbrella of several ﬁne-grained sentiment analysis tasks, and has been extensively studied since its humble beginning in SemEval 2014 (Pon- tiki et al., 2014a). Overall, the task revolves around ∗Equal contribution Sent 1: Triplets Sent 2: Triplets The ﬁlm was good , but could have been better . [Aspect ; Opinion ; Sentiment] (1) ﬁlm ; good ; positive (2) ﬁlm ; could have been better ; negative The weather was gloomy , but the food was tasty . (1) weather ; gloomy ; negative (2) food ; tasty ; positive Table 1: triplets (opinion triplets) present in review sentences. Examples of Aspect-Opinion-Sentiment automatically extracting the opinion targets or as- pects being discussed in review sentences, along with the sentiments expressed towards them. Early efforts on Aspect-level Sentiment Classiﬁcation (Tay et al., 2018; Li et al., 2018a; Xue and Li, 2018) focus on predicting the sentiment polarities for given aspects. However, in a real-world sce- nario, aspects may not be known a-priori. Works on End-to-End ABSA (Li et al., 2019; He et al., 2019; Chen and Qian, 2020) thus focus on extracting the aspects as well as the corresponding sentiments si- multaneously. These methods do not however cap- ture the reasons behind the expressed sentiments, which could otherwise provide valuable clues for more effective extraction of aspect-sentiment pairs. Consider the two examples shown in Table 1. For the ﬁrst sentence, the sentiment associated with the aspect ﬁlm, changes depending on the connect- ing opinion phrases; good suggesting a positive sentiment, and could have been better indicating a negative sentiment. Hence, simply extracting the pairs ﬁlm-positive, and ﬁlm-negative without additionally capturing the reasoning phrases may confuse the reader. For the second sentence, the opinion term gloomy has a higher probability of being associated with weather, than with food. We therefore observe that the three elements or opin- ion factors of an opinion triplet are strongly inter- dependent. In order to offer a complete picture of what is being discussed, how is the sentiment, and why is it so, (Peng et al., 2020) deﬁned the task of Aspect Sentiment Triplet Extraction (ASTE). Given an opinionated sentence, it deals with ex- tracting all three elements: the aspect term/span, the opinion term/span, and the connecting senti- ment in the form of opinion triplets as shown in Table 1. It is to be noted here that a given sentence might contain multiple triplets, which may further share aspect or opinion spans (For e.g., the two triplets for Sent. 1 in Table 1 share the aspect ﬁlm). An efﬁcient solution for the task must therefore be able to handle such challenging data points. Peng et al. (2020) propose a two-stage pipeline framework. In the ﬁrst stage, they extract aspect- sentiment pairs and opinion spans using two sepa- rate sequence-tagging tasks, the former leveraging a uniﬁed tagging scheme proposed by (Li et al., 2019), and the later based on BIEOS1 tagging scheme. In the second stage, they pair up the extracted aspect and opinion spans, and use an MLP-based classiﬁer to determine the validity of each generated triplet. Zhang et al. (2020) propose a multi-task framework to jointly detect aspects, opinions, and sentiment dependencies. Although they decouple the sentiment prediction task from aspect extraction, they use two separate sequence taggers (BIEOS-based) to detect the aspect and opinion spans in isolation before predicting the con- necting sentiment. Both these methods however break the interaction between aspects and opinions during the extraction process. While the former ad- ditionally suffers from error propagation problem, the latter, relying on word-level sentiment depen- dencies, cannot guarantee sentiment consistency over multi-word aspect/opinion spans. Xu et al. (2020b) propose a novel position-aware tagging scheme (extending BIEOS tags) to bet- ter capture the interactions among the three opin- ion factors. One of their model variants however cannot detect aspect-overlapped triplets, while the other cannot identify opinion-overlapped triplets. Hence, they need an ensemble of two variants to be trained for handling all cases. Wu et al. (2020) try to address this limitation by proposing a novel grid tagging scheme-based approach. However, they end up predicting the relationship between every possible word pair, irrespective of how they are syntactically connected, thereby impacting the span-level sentiment consistency guarantees. Different from all these tagging-based methods, 1BIOES is a commonly used tagging scheme for sequence labeling tasks, and denotes “begin, inside, outside, end and single” respectively. we propose to investigate the utility of a tagging- free scheme for the task. Our innovation lies in formulating ASTE as a structured prediction prob- lem. Taking motivation from similar sequence- to-sequence approaches proposed for joint entity- relation extraction (Nayak and Ng, 2020; Chen et al., 2021), semantic role labeling (Fei et al., 2021) etc., we propose PASTE, a Pointer Network- based encoder-decoder architecture for the task of ASTE. The pointer network effectively captures the aspect-opinion interdependence while detect- ing their respective spans. The decoder then learns to model the span-level interactions while predict- ing the connecting sentiment. An entire opinion triplet is thus decoded at every time step, thereby making our solution end-to-end. We note here how- ever, that the aspect and opinion spans can be of varying lengths, which makes the triplet decod- ing process challenging. For ensuring uniformity, we also propose a position-based representation scheme to be suitably exploited by our proposed architecture. Here, each opinion triplet is repre- sented as a 5-point tuple, consisting of the start and end positions of the aspect and opinion spans, and the sentiment (POS/NEG/NEU) expressed towards the aspect. To summarize our contributions: • We present an end-to-end tagging-free solution for the task of ASTE that addresses the limita- tions of previous tagging-based methods. Our proposed architecture, PASTE, not only exploits the aspect-opinion interdependence during the span detection process, but also models the span-level interactions for sentiment prediction, thereby truly capturing the inter-relatedness be- tween all three elements of an opinion triplet. • We propose a position-based scheme to uni- formly represent an opinion triplet, irrespective of varying lengths of aspect and opinion spans. • Extensive experiments on the ASTE-Data-V2 dataset (Xu et al., 2020b) establish the overall superiority of PASTE over strong state-of-the- art baselines, especially in predicting multiple and/or overlapping triplets. We also achieve sig- niﬁcant (15.6%) recall gains in the process. 2 Our Approach Given the task of ASTE, our objective is to jointly extract the three elements of an opinion triplet, i.e., the aspect span, its associated sentiment, and the Figure 1: Dependency Parse Tree for the example review sentence in Table 2 Sentence Target Triplets Overlapping Triplets Ambience was good , but the main course and service were disappointing . (0 0 2 2 POS) (6 7 11 11 NEG) (9 9 11 11 NEG) (9 9 11 11 NEG) (6 7 11 11 NEG) Table 2: Triplet representation for Pointer-network based decoding corresponding opinion span, while modeling their interdependence. Towards this goal, we ﬁrst intro- duce our triplet representation scheme, followed by our problem formulation. We then present our Pointer Network-based decoding framework, PASTE, and ﬁnally discuss a few model variants. Through exhaustive experiments, we investigate the utility of our approach and present a performance comparison with strong state-of-the-art baselines. 2.1 Triplet Representation In order to address the limitations of BIEOS tagging-based approaches and to facilitate joint ex- traction of all three elements of an opinion triplet, we represent each triplet as a 5-point tuple, consist- ing of the start and end positions of the aspect span, the start and end positions of the opinion span, and the sentiment (POS/NEG/NEU) expressed towards the aspect. This allows us to model the relative context between an aspect-opinion pair which is not possible if they were extracted in isolation. It further helps to jointly extract the sentiment asso- ciated with such a pair. An example sentence with triplets represented under the proposed scheme is shown in Table 2. As may be noted, such a scheme can easily represent triplets with overlapping aspect or opinion spans, possibly with varying lengths. 2.2 Problem Formulation , eap i ), (sop To formally deﬁne the ASTE task, given a re- view sentence s = {w1, w2, ..., wn} with n words, our goal is to extract a set of opinion triplets i ), sentii]}|T | T = {ti | ti = [(sap i=1, i where ti represents the ith triplet and |T | repre- sents the length of the triplet set. For the ith triplet, i and eap sap respectively denote the start and end positions of its constituent aspect span, sop and i eop respectively denote the start and end positions i of its constituent opinion span, and sentii repre- i , eop i sents the sentiment polarity associated between them. Here, sentii ∈ {P OS, N EU, N EU }, where P OS, N EG, and N EU respectively repre- sent the positive, negative, and neutral sentiments. 2.3 The PASTE Framework We now present PASTE, our Pointer network- based decoding framework for the task of Aspect Sentiment Triplet Extraction. Figure 2 gives an overview of our proposed architecture. 2.3.1 Sentence Encoder As previously motivated, the association between an aspect, an opinion, and their connecting senti- ment is highly contextual. This factor is more note- worthy in sentences containing multiple triplets with/without varying sentiment polarities and/or overlapping aspect/opinion spans. Long Short Memory Networks (or LSTMs) (Hochreiter and Schmidhuber, 1997) are known for their context modeling capabilities. Similar to (Nayak and Ng, 2020; Chen et al., 2021), we employ a Bi- directional LSTM (Bi-LSTM) to encode our input sentences. We use pre-trained word vectors of di- mension dw to obtain the word-level features. We then note from Figure 1 that aspect spans are often characterized by noun phrases, whereas opinion spans are often composed of adjective phrases. Re- ferring to the dependency tree in the same ﬁgure, the aspect and the opinion spans belonging to the same opinion triplet are often connected by the same head word. These observations motivate us to use both part-of-speech (POS) and dependency- based (DEP) features for each word. More speciﬁcally, we use two embedding layers, Epos ∈ R|POS| × dpos, and Edep ∈ R|DEP| × ddep to obtain the POS and DEP-features of dimensions dpos and ddep respectively, with |POS| and |DEP| representing the length of POS-tag and DEP-tag Figure 2: Model architecture of PASTE, a Pointer Network-based decoding framework for ASTE. sets over all input sentences. All three features are concatenated to obtain the input vector representa- tion xi ∈ Rdw+dpos+ddep corresponding to the ith word in the given sentence S = {w1, w2, ..., wn}. The vectors are passed through the Bi-LSTM to i ∈ Rdh. obain the contextualized representations hE Here, dh represents the hidden state dimension of the triplet generating LSTM decoder as detailed in the next section. Accordingly, the hidden state di- mension of both the forward and backward LSTM of the Bi-LSTM encoder are set to dh/2. For the BERT-based variant of our model, Bi- LSTM gets replaced by BERT (Devlin et al., 2019) as the sentence encoder. The pre-trained word vec- tors are accordingly replaced by BERT token em- beddings. We now append the POS and DEP fea- tures vectors to the 768-dim. token-level outputs from the ﬁnal layer of BERT. 2.3.2 Pointer Network-based Decoder Referring to Figure 2, opinion triplets are decoded using an LSTM-based Triplet Decoder, that takes into account the history of previously generated pairs/tuples of aspect and opinion spans, in order to avoid repetition. At each time step t, it generates a t ∈ Rdh that is used by the hidden representation hD two Bi-LSTM + FFN-based Pointer Networks to respectively predict the aspect and opinion spans, while exploiting their interdependence. The tuple representation tupt thus obtained is concatenated with hD t and passed through an FFN-based Senti- ment Classiﬁer to predict the connecting sentiment, thereby decoding an entire opinion triplet at the tth time step. We now elaborate each component of our proposed decoder framework in greater depth: Span Detection with Pointer Networks Our pointer network consists of a Bi-LSTM, with hidden dimension dp, followed by two feed- forward layers (FFN) on top to respectively predict the start and end locations of an entity span. We use two such pointer networks to produce a tuple of hidden vectors corresponding to the aspect and opinion spans of the triplet to be decoded at time step t. We concatenate hD t with each of the encoder hidden state vectors hE i and pass them as input to the ﬁrst Bi-LSTM. The output hidden state vector corresponding to the ith token of the sentence thus obtained is simultaneously fed to the two FFNs with sigmoid to generate a pair of scores in the range of 0 to 1. After repeating the process for all tokens, the normalized probabilities of the ith token to be the start and end positions of an aspect span (sp1 respectively) are obtained using softmax operations over the two sets of scores thus generated. Here p1 refers to the ﬁrst pointer net- work. Similar scores corresponding to the opinion span are obtained using the second pointer net- work, p2; difference being that apart from hD t , we also concatenate the output vectors from the ﬁrst Bi-LSTM with encoder hidden states hE i and pass them as input to the second Bi-LSTM. This helps us to model the interdependence between an aspect-opinion pair. These scores are used to ob- tain the hidden state representations apt ∈ R2dp and opt ∈ R2dp corresponding to the pair of aspect and opinion spans thus predicted at time step t. We request our readers to kindly refer to the appendix for more elaborate implementation details. i and ep1 i Here we introduce the term generation direc- tion which refers to the order in which we gener- ate the hidden representations for the two entities, i.e. aspect and opinion spans. This allows us to deﬁne two variants of our model. The variant discussed so far uses p1 to detect the aspect span before predicting the opinion span using p2, and is henceforth referred to as PASTE-AF (AF stands for aspect ﬁrst). Similarly, we obtain the second variant PASTE-OF (opinion ﬁrst) by reversing the generation direction. The other two components of our model remain the same for both the variants. Triplet Decoder and Attention Modeling The decoder consists of an LSTM with hidden dimension dh whose goal is to generate the se- quence of opinion triplets, T , as deﬁned in Section 2.2. Let tupt = apt (cid:107) opt ; tupt ∈ R4dp denote the tuple (aspect, opinion) representation obtained from the pointer networks at time step t. Then, tupprev = (cid:80) j<t tupj ; tup0 = (cid:126)0 ∈ R4dp repre- sents the cumulative information about all tuples predicted before the current time step. We obtain an attention-weighted context representation of the t ∈ Rdh) using input sentence at time step t (sE Bahdanau et al. (2015) Attention2. In order to pre- vent the decoder from generating the same tuple again, we pass tupprev as input to the LSTM along t ∈ Rdh, the hidden repre- with sE t sentation for predicting the triplet at time step t: to generate hD t = LSTM(sE hD t (cid:107) tupprev , hD t−1) Sentiment Classiﬁer Finally, we concatenate tupt, with hD t and pass it through a feed-forward network with soft- max to generate the normalized probabilities over {P OS, N EG, N EU } ∪ {N ON E}, thereby pre- dicting the sentiment label sentit for the current triplet. Interaction between the entire predicted spans of aspect and opinion is thus captured for sentiment identiﬁcation. Here P OS, N EG, N EU respectively represent the positive, negative, and neutral sentiments. N ON E is a dummy sentiment that acts as an implicit stopping criteria for the decoder. During training, once a triplet with sen- timent N ON E is predicted, we ignore all subse- quent predictions, and none of them contribute to the loss. Similarly, during inference, we ignore any triplet predicted with the N ON E sentiment. 2Please refer to the appendix for implementation details. 2.3.3 Training For training our model, we minimize the sum of negative log-likelihood loss for classifying the sen- timent and the four pointer locations corresponding to the aspect and opinion spans: L = − 1 M × J M (cid:88) J (cid:88) [log(sm ap,j · em ap,j) m=1 + log(sm j=1 op,j · em op,j) + log(senm j )] Here, m represents the mth training instance with M being the batch size, j represents the jth decod- ing time step with J being the length of the longest target sequence among all instances in the current batch. sp, ep; p ∈ {ap, op} and sen respectively represent the softmax scores corresponding to the true start and end positions of the aspect and opin- ion spans and their associated true sentiment label. , sop i , eop Inferring The Triplets , eap i 2.3.4 Let sap i ; i ∈ [1, n] represent the ob- i tained pointer probabilities for the ith token in the given sentence (of length n) to be the start and end positions of an aspect span and opinion span re- spectively. First, we choose the start (j) and end (k) positions of the aspect span with the constraint 1 ≤ j ≤ k ≤ n such that sap k is maximized. We then choose the start and end positions of the opinion span similarly such that they do not overlap with the aspect span. Thus, we obtain one set of four pointer probabilities. We repeat the process to obtain the second set, this time by choosing the opinion span before the aspect span. Finally, we choose the set (of aspect and opinion spans) that gives the higher product of the four probabilities. j × eap 3 Experiments 3.1 Datasets and Evaluation Metrics We conduct our experiments on the ASTE-Data- V2 dataset created by Xu et al. (2020b). It is de- rived from ASTE-Data-V1 (Peng et al., 2020) and presents a more challenging scenario with 27.68% of all sentences containing triplets with overlap- ping aspect or opinion spans. The dataset contains triplet-annotated sentences from two domains: lap- top and restaurant, corresponding to the original datasets released by the SemEval Challenge (Pon- tiki et al., 2014a,b,c). It is to be noted here that the opinion term annotations were originally de- rived from (Fan et al., 2019). 14Lap belongs to Dataset Train Dev Test # Pos. 817 169 364 14Lap # Neg. 517 141 116 # Neu. 126 36 63 # Pos. 1692 404 773 14Rest # Neg. 480 119 155 # Neu. 166 54 66 # Pos. 783 185 317 15Rest # Neg. 205 53 143 # Neu. 25 11 25 # Pos. 1015 252 407 16Rest # Neg. 329 76 78 # Neu. 50 11 29 Restaurant (All) # Neg. 1014 248 376 # Neu. 241 76 120 # Pos. 3490 841 1497 Table 3: ASTE-Data-V2 Statistics: # Triplets with various sentiment polarities Laptop Restaurant Single Multi MultiPol Overlap # Sent. Single Multi MultiPol Overlap # Sent. 2728 668 1140 4536 1447 347 608 2402 1281 321 532 2134 731 197 317 1245 906 219 328 1453 545 133 184 862 205 45 71 321 257 59 97 413 361 86 144 591 47 10 18 75 Dataset Train Dev Test Total Table 4: Statistics of Laptop and Restaurant datasets from ASTE-Data-V2: Single and Multi respectively represent # sentences with single and multiple triplets. MultiPol and Overlap are subsets of Multi. MultiPol representing # sentences containing at least two triplets with different sentiment polarities. Overlap represents # sentences with aspect/opinion overlapped triplets. # Sent. represents the total no. of sentences overall. the laptop domain and is henceforth referred to as the Laptop. 14Rest, 15Rest, and 16Rest belong to the restaurant domain. Each dataset comes with its pre-deﬁned split of training, development, and test sets. Similar to prior works, we report our results on the individual datasets. Additionally, we also conduct experiments on the combined restaurant dataset, henceforth referred to as the Restaurant. Tables 3 and 4 present the dataset statistics. We consider precision, recall, and micro-F1 as our evaluation metrics for the triplet extraction task. A predicted triplet is considered a true positive only if all three predicted elements exactly match with those of a ground-truth opinion triplet. 3.2 Experimental Setup For our non-BERT experiments, word embeddings are initialized (and kept trainable) using pre-trained 300-dim. Glove vectors (Pennington et al., 2014), and accordingly dw is set to 300. dpos and ddep are set to 50 each. dh is set to 300, and accordingly the hidden state dimensions of both the LSTMs (backward and forward) of the Bi-LSTM-based encoder are set to 150 each. dp is set to 300. For our BERT experiments, uncased version of pre-trained BERT-base (Devlin et al., 2019) is ﬁne-tuned to encode each sentence. All our model variants are trained end-to-end on Tesla P100-PCIE 16GB GPU with Adam optimizer (learning rate: 10−3, weight decay: 10−5). A dropout rate of 0.5 is applied on the embeddings to avoid overﬁtting3. We make our codes and datasets publicly available4. 3Please refer to the appendix for more details. 4https://github.com/rajdeep345/PASTE/ 3.3 Baselines • Wang et al. (2017) (CMLA) and Dai and Song (2019) (RINANTE) propose different methods to co-extract aspects and opinion terms from re- view sentences. Li et al. (2019) propose a uni- ﬁed tagging scheme-based method for extracting opinion target-sentiment pairs. Peng et al. (2020) modiﬁes these methods to jointly extract targets with sentiment, and opinion spans. It then applies an MLP-based classiﬁer to determine the validity of all possible generated triplets. These modiﬁed versions are referred to as CMLA+, RINANTE+, and Li-uniﬁed-R, respectively. • Peng et al. (2020) propose a BiLSTM+GCN- based approach to co-extract aspect-sentiment pairs, and opinion spans. They then use the same inference strategy as above to conﬁrm the cor- rectness of the generated triplets. • OTE-MTL (Zhang et al., 2020) uses a multi- task learning framework to jointly detect aspects, opinions, and sentiment dependencies. • JET (Xu et al., 2020b) is the ﬁrst end-to-end approach for the task of ASTE that leverages a novel position-aware tagging scheme. One of their variants, JETt, however cannot handle aspect-overlapped triplets. Similarly, JETo, can- not handle opinion-overlapped triplets. • GTS (Wu et al., 2020) models ASTE as a novel grid-tagging task. However, given that it predicts the sentiment relation between all possible word pairs, it uses a relaxed (majority-based) matching criteria to determine the ﬁnal triplets. Model CMLA+ RINANTE+ Li-uniﬁed-R (Peng et al., 2020) JETo (M = 4) JETo (M = 5) OTE-MTL GTS-BiLSTM w/o DE PASTE-AF PASTE-OF With BERT JETo (M = 4) JETo (M = 6) GTS-BERT PASTE-AF w/ BERT-PT PASTE-OF w/ BERT-PT P. 0.301 0.217 0.406 0.374 0.546 0.560 0.492 0.597 0.537 0.521 0.570 0.554 0.549 0.550 0.612 0.554 0.597 Laptop F1 0.332 0.201 0.423 0.429 0.429 0.433 0.451 0.439 0.510 0.500 R. 0.369 0.187 0.443 0.504 0.354 0.354 0.405 0.348 0.486 0.481 0.389 0.473 0.521 0.516 0.536 0.519 0.553 0.462 0.510 0.535 0.532 0.571 0.536 0.574 Dev F1 - - - - 0.457 0.458 0.458 0.465 0.496 0.482 0.475 0.488 0.579 0.514 0.576 0.503 0.547 P. - - - - 0.770 - 0.710 0.768 0.707 0.707 0.727 - 0.748 0.710 0.747 0.705 0.737 Restaurant R. - - - - 0.520 - 0.579 0.629 0.701 0.706 F1 - - - - 0.621 - 0.637 0.692 0.704 0.707 0.549 - 0.732 0.704 0.718 0.705 0.737 0.626 - 0.740 0.707 0.732 0.705 0.737 Dev F1 - - - - 0.641 - 0.729 0.748 0.741 0.740 0.645 - 0.767 0.744 0.759 0.744 0.759 Table 5: Comparative results on the Laptop (14Lap) and Restaurant datasets from ASTE-Data-V2. Bolded values represent the best F1 scores. Underlined scores are obtained with Post-trained BERT. Model P. CMLA+ 0.392 RINANTE+ 0.314 0.410 Li-uniﬁed-R 0.432 (Peng et al., 2020) 0.630 OTE-MTL JETo (M = 6) 0.615 GTS-BiLSTM w/o DE 0.686 0.624 PASTE-AF 0.634 PASTE-OF With BERT JETo (M = 6) GTS-BERT PASTE-AF w/ BERT-PT PASTE-OF w/ BERT-PT 0.706 0.674 0.648 0.667 0.667 0.687 14Rest F1 0.428 0.350 0.510 0.515 0.587 0.581 0.597 0.621 0.626 R. 0.471 0.394 0.674 0.637 0.551 0.551 0.528 0.618 0.619 0.559 0.673 0.638 0.665 0.608 0.638 0.624 0.674 0.643 0.666 0.636 0.661 Dev F1 - - - - 0.547 0.535 0.556 0.568 0.566 0.569 0.651 0.570 0.585 0.573 0.592 P. 0.346 0.299 0.447 0.481 0.579 0.644 0.654 0.548 0.548 0.645 0.637 0.583 0.617 0.585 0.636 15Rest F1 0.370 0.300 0.478 0.523 0.489 0.525 0.528 0.541 0.537 R. 0.398 0.301 0.514 0.575 0.427 0.443 0.443 0.534 0.526 0.520 0.551 0.567 0.608 0.565 0.598 0.575 0.591 0.575 0.613 0.575 0.616 Dev F1 - - - - 0.569 0.610 0.606 0.649 0.650 0.648 0.720 0.626 0.673 0.645 0.660 P. 0.413 0.257 0.373 0.470 0.603 0.709 0.686 0.622 0.623 0.704 0.654 0.655 0.661 0.619 0.680 16Rest F1 0.417 0.239 0.443 0.542 0.565 0.632 0.588 0.625 0.629 R. 0.421 0.223 0.545 0.642 0.534 0.570 0.515 0.628 0.636 0.584 0.680 0.644 0.698 0.667 0.677 0.638 0.667 0.650 0.679 0.642 0.678 Dev F1 - - - - 0.597 0.609 0.625 0.667 0.659 0.638 0.715 0.660 0.690 0.670 0.695 Table 6: Comparative results on the individual restaurant datasets from ASTE-Data-V2 3.4 Experimental Results While training our model variants, the best weights are selected based on F1 scores on the development set. We report our median scores over 5 runs of the experiment. Performance comparisons on the Lap- top (14Lap) and combined Restaurant datasets are reported in Table 5, whereas the same on individual restaurant datasets are reported in Table 6. Both the tables are divided into two sections; the for- mer comparing the results without BERT, and the latter comparing those with BERT. The scores for CMLA+, RINANTE+, Li-uniﬁed-R, and (Peng et al., 2020) are taken from Xu et al. (2020b). We replicate the results for OTE-MTL on ASTE-Data- V2 and report their average scores over 10 runs of the experiment. For JET, we compare with their best reported results on the individual datasets; i.e. JETo (M = 5) for 14Lap (w/o BERT), JETo (M = 6) for 14Lap (w/ BERT), and JETo (M = 6) for 14Rest, 15Rest, and 16Rest (both w/ and w/o BERT). How- ever, owing to resource constraints and known opti- mization issues with their codes, we could not repli- cate their results on the Restaurant dataset beyond M = 4 (for both w/ and w/o BERT). GTS uses dou- ble embeddings (Xu et al., 2018) (general Glove vectors + domain-speciﬁc embeddings trained with fastText). For fair comparison, we replicate their results without using the domain-speciﬁc embed- dings (DE). For both w/ and w/o BERT, we report their median scores over 5 runs of the experiment. We also report the F1 scores on the development set corresponding to the test set results. Model JETo (M = 4) OTE-MTL GTS-BiLSTM w/o DE PASTE-AF PASTE-OF With BERT JETo (M = 4) GTS-BERT PASTE-AF PASTE-OF Laptop Single Multi MultiPol Overlap 0.453 0.485 0.418 0.506 0.495 0.406 0.277 0.452 0.512 0.502 0.219 0.172 0.237 0.216 0.205 0.363 0.380 0.403 0.507 0.511 Restaurant Single Multi MultiPol Overlap 0.654 0.716 0.726 0.702 0.711 0.558 0.506 0.588 0.567 0.582 0.602 0.656 0.675 0.705 0.704 0.518 0.646 0.660 0.688 0.693 0.514 0.533 0.555 0.593 0.430 0.536 0.519 0.502 0.229 0.338 0.265 0.282 0.400 0.540 0.526 0.511 0.655 0.739 0.704 0.699 0.609 0.740 0.709 0.708 0.509 0.648 0.601 0.571 0.536 0.722 0.699 0.697 Table 7: Comparison of F1 scores on different splits of Laptop and Restaurant datasets from ASTE-Data-V2 From Table 5, both our variants, PASTE-AF and PASTE-OF, perform comparably as we sub- stantially outperform all the non-BERT baselines. On Laptop, we achieve 13.1% F1 gains over OTE- MTL, whereas on Restaurant, we obtain 2.2% F1 gains over GTS-BiLSTM. We draw similar conclu- sions from Table 6, except that we are narrowly outperformed by JETo (M = 6) on 16Rest. Our better performance may be attributed to our better Recall scores with around 15.6% recall gains (av- eraged across both our variants) over the respective strongest baselines (in terms of F1) on the Laptop and Restaurant datasets. Such an observation es- tablishes the better efﬁcacy of PASTE in modeling the interactions between the three opinion factors as we are able to identify more ground-truth triplets from the data, compared to our baselines. With BERT, we comfortably outperform JET on all the datasets. Although we narrowly beat GTS-BERT on Laptop, it outperforms us on all the restaurant datasets. This is owing to the fact that GTS-BERT obtains a substantial improvement in scores over GTS since its grid-tag prediction task and both the pre-training tasks of BERT are all dis- criminative in nature. We on the other hand, do not observe such huge jumps (F1 gains of 5.1%, 2.7%, 6.3%, and 3.3% on the Laptop, Rest14, Rest15, and Rest16 datasets respectively, noticeably more improvement on datasets with lesser training data; no gains on Restaurant) since BERT is known to be unsuitable for generative tasks. We envisage to improve our model by replacing BERT with BART (Lewis et al., 2020), a strong sequence-to-sequence pretrained model for NLG tasks. Finally, motivated by Xu et al. (2019, 2020a), we also demonstrate the utility of leveraging domain- speciﬁc language understanding for the task by reporting our results with BERT-PT (task-agnostic post-training of pre-trained BERT on domain- speciﬁc data) in both the tables. While we achieve substantial performance improvement, we do not use these scores to draw our conclusions in order to ensure fair comparison with the baselines. 4 Analysis & Discussion 4.1 Robustness Analysis In order to better understand the relative advan- tage of our proposed approach when compared to our baselines for the opinion triplet extraction task, and to further investigate the reason behind our better recall scores, in Table 7 we compare the F1 scores on various splits of the test sets as de- ﬁned in Table 4. We observe that with our core architecture (w/o BERT), PASTE consistently out- performs the baselines on both Laptop and Restau- rant datasets when it comes to handling sentences with multiple triplets, especially those with overlap- ping aspect/opinion spans. This establishes the fact that PASTE is better than previous tagging-based approaches in terms of modeling aspect-opinion span-level interdependence during the extraction process. This is an important observation con- sidering the industry-readiness (Mukherjee et al., 2021b) of our proposed approach since our model is robust towards challenging data instances. We however perform poorly when it comes to identi- fying triplets with varying sentiment polarities in the same sentence. This is understandable since we do not utilize any specialized sentiment mod- eling technique. In future, we propose to utilize word-level Valence, Arousal, Dominance scores (Mukherjee et al., 2021a) as additional features to better capture the sentiment of the opinion phrase. In this work, we propose a new perspective to solve ASTE by investigating the utility of a tagging- free scheme, as against all prior tagging-based methods. Hence, it becomes imperative to analyze how we perform in terms of identifying individual Dataset Laptop Restaurant Model JETo (M = 4) OTE-MTL GTS-BiLSTM w/o DE PASTE-AF PASTE-OF JETo (M = 4) OTE-MTL GTS-BiLSTM w/o DE PASTE-AF PASTE-OF Aspect R. 0.495 0.576 0.724 0.765 0.790 0.638 0.706 0.835 0.851 0.848 P. 0.801 0.812 0.725 0.792 0.801 0.871 0.905 0.791 0.837 0.836 F1 0.611 0.674 0.724 0.778 0.796 0.736 0.793 0.812 0.844 0.842 Opinion R. 0.528 0.584 0.684 0.704 0.719 0.666 0.718 0.837 0.852 0.854 P. 0.805 0.826 0.692 0.757 0.763 0.885 0.913 0.826 0.844 0.848 F1 0.638 0.684 0.688 0.730 0.740 0.760 0.804 0.832 0.848 0.851 Sentiment % Acc. 0.846 0.858 0.870 0.840 0.831 0.947 0.943 0.945 0.939 0.939 Table 8: Comparative results of aspect, opinion and sentiment prediction on Laptop and Restaurant datasets Dataset Laptop Restaurant Model PASTE-AF P. 0.537 - POS & DEP 0.530 0.505 w/ Random 0.707 - POS & DEP 0.708 0.686 w/ Random PASTE-OF R. 0.486 0.451 0.410 0.706 0.702 0.627 F1 % F1 ↓ 0.510 0.488 0.453 0.707 0.705 0.655 - 4.3% 11.2% - 0.3% 7.4% Table 9: Ablation Results elements of an opinion triplet. Table 8 presents such a comparison. It is encouraging to note that we substantially outperform our baselines on both aspect and opinion span detection sub-tasks. How- ever, as highlighted before, we are outperformed when it comes to sentiment detection. 4.2 Ablation Study: Since our Decoder learns to decode the sequence of triplets from left to right without repetition, while training our models we sort the target triplets in the same order as generation direction; i.e. for train- ing PASTE-AF/PASTE-OF, the target triplets are sorted in ascending order of aspect/opinion start positions. As an ablation, we sort the triplets ran- domly while training the models and report our ob- tained scores in Table 9. An average drop of 9.3% in F1 scores for both our model variants establish the importance of sorting the triplets for training our models. When experimenting without the POS and DEP features, we further observe an average drop of 2.3% in F1 scores, thereby demonstrating their utility for the ASTE task. When experiment- ing with BERT, although these features helped on the Laptop and Rest15 datasets, overall we did not observe any signiﬁcant improvement. 5 Related Works ABSA is a collection of several ﬁne-grained sen- timent analysis tasks, such as Aspect Extraction (Li et al., 2018b, 2020), Aspect-level Sentiment Classiﬁcation (Li et al., 2018a; Xue and Li, 2018), Aspect-oriented Opinion Extraction (Fan et al., 2019), E2E-ABSA (Li et al., 2019; He et al., 2019), and Aspect-Opinion Co-Extraction (Wang et al., 2017; Dai and Song, 2019). However, none of these works offer a complete picture of the aspects being discussed. Towards this end, Peng et al. (2020) recently coined the task of Aspect Senti- ment Triplet Extraction (ASTE), and proposed a 2-stage pipeline solution. More recent end-to-end approaches such as OTE-MTL(Zhang et al., 2020), and GTS (Wu et al., 2020) fail to guarantee senti- ment consistency over multi-word aspect/opinion spans, since they depend on word-pair dependen- cies. JET (Xu et al., 2020b) on the other hand re- quires two different models to be trained to detect aspect-overlapped and opinion-overlapped triplets. Different from all these tagging-based methods, we propose a tagging-free solution for the ASTE task. 6 Conclusion We investigate the utility of a tagging-free scheme for the task of Aspect Sentiment Triplet Extrac- tion using a Pointer network-based decoding frame- work. Addressing the limitations of previous tagging-based methods, our proposed architecture, PASTE, not only exploits the aspect-opinion inter- dependence during the span detection process, but also models the span-level interactions for senti- ment prediction, thereby truly capturing the inter- relatedness between all three elements of an opin- ion triplet. We demonstrate the better efﬁcacy of PASTE, especially in recall, and in predicting mul- tiple and/or overlapping triplets, when experiment- ing on the ASTE-Data-V2 dataset. Acknowledgements This research is supported by IMPRINT-2, Science and Engineering Research Board (SERB), India. References Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben- gio. 2015. Neural machine translation by jointly In 3rd Inter- learning to align and translate. national Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings. Yubo Chen, Yunqi Zhang, Changran Hu, and Yongfeng Huang. 2021. Jointly extracting explicit and implicit relational triples with reasoning pattern enhanced bi- In Proceedings of the 2021 nary pointer network. Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5694–5703, Online. Association for Computational Linguistics. Zhuang Chen and Tieyun Qian. 2020. Relation-aware collaborative learning for uniﬁed aspect-based sen- timent analysis. In Proceedings of the 58th Annual Meeting of the Association for Computational Lin- guistics, pages 3685–3694, Online. Association for Computational Linguistics. Hongliang Dai and Yangqiu Song. 2019. Neural as- pect and opinion term extraction with mined rules as weak supervision. In Proceedings of the 57th An- nual Meeting of the Association for Computational Linguistics, pages 5268–5277, Florence, Italy. Asso- ciation for Computational Linguistics. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- In Proceedings of the 2019 Conference standing. of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota. Associ- ation for Computational Linguistics. Zhifang Fan, Zhen Wu, Xin-Yu Dai, Shujian Huang, and Jiajun Chen. 2019. Target-oriented opinion words extraction with target-fused neural sequence In Proceedings of the 2019 Conference labeling. of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2509–2518, Minneapolis, Minnesota. Associ- ation for Computational Linguistics. Hao Fei, Fei Li, Bobo Li, and Donghong Ji. 2021. Encoder-decoder based uniﬁed semantic role label- ing with label-aware syntax. volume 35, pages 12794–12802. Ruidan He, Wee Sun Lee, Hwee Tou Ng, and Daniel Dahlmeier. 2019. An interactive multi-task learn- ing network for end-to-end aspect-based sentiment analysis. In Proceedings of the 57th Annual Meet- ing of the Association for Computational Linguis- tics, pages 504–515, Florence, Italy. Association for Computational Linguistics. Sepp Hochreiter and Jürgen Schmidhuber. 1997. Neural Comput., Long short-term memory. 9(8):1735–1780. Diederik P. Kingma and Jimmy Ba. 2015. Adam: A In 3rd Inter- method for stochastic optimization. national Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings. Mike Lewis, Yinhan Liu, Naman Goyal, Mar- jan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising sequence-to-sequence pre- training for natural language generation, translation, and comprehension. In Proceedings of the 58th An- nual Meeting of the Association for Computational Linguistics, pages 7871–7880, Online. Association for Computational Linguistics. Kun Li, Chengbo Chen, Xiaojun Quan, Qing Ling, and Yan Song. 2020. Conditional augmentation for aspect term extraction via masked sequence-to- sequence generation. In Proceedings of the 58th An- nual Meeting of the Association for Computational Linguistics, pages 7056–7066, Online. Association for Computational Linguistics. Xin Li, Lidong Bing, Wai Lam, and Bei Shi. 2018a. Transformation networks for target-oriented senti- ment classiﬁcation. In Proceedings of the 56th An- nual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 946– 956, Melbourne, Australia. Association for Compu- tational Linguistics. Xin Li, Lidong Bing, Piji Li, and Wai Lam. 2019. A uniﬁed model for opinion target extraction and target sentiment prediction. In The Thirty-Third AAAI Con- ference on Artiﬁcial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artiﬁcial In- telligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artiﬁcial Intelligence, EAAI 2019, Honolulu, Hawaii, USA, January 27 - February 1, 2019, pages 6714–6721. AAAI Press. Xin Li, Lidong Bing, Piji Li, Wai Lam, and Zhimou Yang. 2018b. Aspect term extraction with history attention and selective transformation. In Proceed- ings of the Twenty-Seventh International Joint Con- ference on Artiﬁcial Intelligence, IJCAI 2018, July 13-19, 2018, Stockholm, Sweden, pages 4194–4200. ijcai.org. Rajdeep Mukherjee, Atharva Naik, Sriyash Poddar, So- ham Dasgupta, and Niloy Ganguly. 2021a. Under- standing the role of affect dimensions in detecting emotions from tweets: A multi-task approach. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Infor- mation Retrieval, page 2303–2307, New York, NY, USA. Association for Computing Machinery. Rajdeep Mukherjee, Shreyas Shetty, Subrata Chat- topadhyay, Subhadeep Maji, Samik Datta, and Pawan Goyal. 2021b. Reproducibility, replicabil- ity and beyond: Assessing production readiness of aspect based sentiment analysis in the wild. In Advances in Information Retrieval, pages 92–106, Cham. Springer International Publishing. In for co-extraction of aspect and opinion terms. Proceedings of the Thirty-First AAAI Conference on Artiﬁcial Intelligence, February 4-9, 2017, San Fran- cisco, California, USA, pages 3316–3322. AAAI Press. Tapas Nayak and Hwee Tou Ng. 2020. Effective mod- eling of encoder-decoder architecture for joint entity and relation extraction. In AAAI. Haiyun Peng, Lu Xu, Lidong Bing, Fei Huang, Wei Lu, and Luo Si. 2020. Knowing what, how and why: A near complete solution for aspect-based sentiment analysis. In AAAI. Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. GloVe: Global vectors for word representation. In Proceedings of the 2014 Confer- ence on Empirical Methods in Natural Language Processing (EMNLP), pages 1532–1543, Doha, Qatar. Association for Computational Linguistics. Maria Pontiki, Dimitris Galanis, John Pavlopoulos, Harris Papageorgiou, Ion Androutsopoulos, and Suresh Manandhar. 2014a. SemEval-2014 task 4: Aspect based sentiment analysis. In Proceedings of the 8th International Workshop on Semantic Evalua- tion (SemEval 2014), pages 27–35, Dublin, Ireland. Association for Computational Linguistics. Zhen Wu, Chengcan Ying, Fei Zhao, Zhifang Fan, Xinyu Dai, and Rui Xia. 2020. Grid tagging scheme for aspect-oriented ﬁne-grained opinion extraction. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2576–2585, On- line. Association for Computational Linguistics. Hu Xu, Bing Liu, Lei Shu, and Philip Yu. 2019. BERT post-training for review reading comprehension and aspect-based sentiment analysis. In Proceedings of the 2019 Conference of the North American Chap- ter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2324–2335, Minneapolis, Minnesota. Association for Computational Linguis- tics. Hu Xu, Bing Liu, Lei Shu, and Philip Yu. 2020a. DomBERT: Domain-oriented language model for aspect-based sentiment analysis. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1725–1731, Online. Association for Computational Linguistics. Maria Pontiki, Dimitris Galanis, John Pavlopoulos, Harris Papageorgiou, Ion Androutsopoulos, and Suresh Manandhar. 2014b. SemEval-2014 task 4: Aspect based sentiment analysis. In Proceedings of the 8th International Workshop on Semantic Evalua- tion (SemEval 2014), pages 27–35, Dublin, Ireland. Association for Computational Linguistics. Hu Xu, Bing Liu, Lei Shu, and Philip S. Yu. 2018. Dou- ble embeddings and CNN-based sequence labeling for aspect extraction. In Proceedings of the 56th An- nual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 592– 598, Melbourne, Australia. Association for Compu- tational Linguistics. Lu Xu, Hao Li, Wei Lu, and Lidong Bing. 2020b. Position-aware tagging for aspect sentiment triplet extraction. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Process- ing (EMNLP), pages 2339–2349, Online. Associa- tion for Computational Linguistics. Wei Xue and Tao Li. 2018. Aspect based sentiment analysis with gated convolutional networks. In Pro- ceedings of the 56th Annual Meeting of the Associa- tion for Computational Linguistics (Volume 1: Long Papers), pages 2514–2523, Melbourne, Australia. Association for Computational Linguistics. Chen Zhang, Qiuchi Li, Dawei Song, and Benyou Wang. 2020. A multi-task learning framework for opinion triplet extraction. In Findings of the Associ- ation for Computational Linguistics: EMNLP 2020, pages 819–828, Online. Association for Computa- tional Linguistics. Maria Pontiki, Dimitris Galanis, John Pavlopoulos, Harris Papageorgiou, Ion Androutsopoulos, and Suresh Manandhar. 2014c. SemEval-2014 task 4: Aspect based sentiment analysis. In Proceedings of the 8th International Workshop on Semantic Evalua- tion (SemEval 2014), pages 27–35, Dublin, Ireland. Association for Computational Linguistics. Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: A simple way to prevent neural net- J. Mach. Learn. Res., works from overﬁtting. 15(1):1929–1958. Yi Tay, Luu Anh Tuan, and Siu Cheung Hui. 2018. Learning to attend via word-aspect associative fu- In Pro- sion for aspect-based sentiment analysis. ceedings of the Thirty-Second AAAI Conference on Artiﬁcial Intelligence, (AAAI-18), the 30th innova- tive Applications of Artiﬁcial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Ad- vances in Artiﬁcial Intelligence (EAAI-18), New Or- leans, Louisiana, USA, February 2-7, 2018, pages 5956–5963. AAAI Press. Wenya Wang, Sinno Jialin Pan, Daniel Dahlmeier, and Xiaokui Xiao. 2017. Coupled multi-layer attentions A Appendix A.1 Pointer Network-based Decoder Referring to Figure 2, opinion triplets are decoded using an LSTM-based Triplet Decoder, that takes into account the history of previously generated pairs/tuples of aspect and opinion spans, in order to avoid repetition. At each time step t, it generates a t ∈ Rdh that is used by the hidden representation hD two Bi-LSTM + FFN-based Pointer Networks to respectively predict the aspect and opinion spans, while exploiting their interdependence. The tuple representation tupt thus obtained is concatenated with hD t and passed through an FFN-based Senti- ment Classiﬁer to predict the connecting sentiment, thereby decoding an entire opinion triplet at the tth time step. We now elaborate each component of our proposed decoder framework in greater depth. A.1.1 Span Detection with Pointer Networks Our pointer network consists of a Bi-LSTM, with hidden dimension dp, followed by two feed- forward layers (FFN) on top to respectively predict the start and end locations of an entity span. We use two such pointer networks to produce a tuple of hidden vectors corresponding to the aspect and opinion spans of the triplet to be decoded at time step t. We concatenate hD t with each of the encoder hidden state vectors hE i and pass them as input to the ﬁrst Bi-LSTM. The output hidden state vector corresponding to the ith token of the sentence thus obtained is simultaneously fed to the two FFNs with sigmoid to generate a pair of scores ˜sp1 i and ˜ep1 i in the range of 0 to 1 as follows: ˜sp1 i = Wp1 s hp1 i + bp1 s , ˜ep1 i = Wp1 e hp1 i + bp1 e e ∈ Rdp×1, bp1 s ∈ Rdp×1, Wp1 Here, Wp1 s , and bp1 e are respectively the weights and bias parame- ters of the two FFNs for the ﬁrst pointer network (p1). After repeating the process for all tokens in the sentence, the normalized probabilities of the ith token to be the start and end positions of an aspect span (sp1 respectively) are obtained using softmax operations over the two sets of scores thus generated (by the two FFNs) as follows: i and ep1 i Sp1 = softmax(˜Sp1), Ep1 = softmax(˜Ep1) Similar equations are used for the second pointer network (p2) to generate the normalized probabil- ities, sp2 , for the ith token to be the start and end positions of an opinion span respectively; i and ep2 i difference being that apart from concatenating hD t , we also concatenate the output vectors hp1 from i the ﬁrst Bi-LSTM with encoder hidden states hE i and pass them as input to the second Bi-LSTM. The vector representations for the aspect and opinion spans at time step t are obtained as follows: apt = opt = n (cid:88) i=1 n (cid:88) i=1 i hp1 sp1 i (cid:107) i hp2 sp2 i (cid:107) n (cid:88) i=1 n (cid:88) i=1 i hp1 ep1 i ; apt ∈ R2dp i hp2 ep2 i ; opt ∈ R2dp Here we introduce the term generation direc- tion which refers to the order in which we gener- ate the hidden representations for the two entities, i.e. aspect and opinion spans. This allows us to deﬁne two variants of our model. The variant discussed so far uses p1 to detect the aspect span before predicting the opinion span using p2, and is henceforth referred to as PASTE-AF (AF stands for aspect ﬁrst). Similarly, we obtain the second variant PASTE-OF (opinion ﬁrst) by reversing the generation direction. The other two components of our model remain the same for both the variants. A.2 Attention Modeling We use Badhanau Attention (Bahdanau et al., 2015) to obtain the context representation of the input sentence (sE t ∈ Rdh) at time step t as follows: ˜tupprev = Wtup tupprev + btup t = WuhE ui i t = W˜q ˜tupprev + b˜q ; ˜ai ˜qi t−1 + bq ; ai t = WqhD qi t = v˜a tanh(˜qi t = va tanh(qi t + ui t) t + ui t) ˜αt = softmax(˜at) ; αt = softmax(at) sE t = n (cid:88) i=1 ˜αi + αi 2 hE i Here, W˜q, Wq, Wu ∈ Rdh×dh, v˜a, va ∈ Rdh are learnable attention parameters, and b˜q, bq ∈ Rdh are bias vectors. First, we obtain ˜tupprev from tupprev using a linear embedding layer, with Wtup ∈ R4dp×dh and btup as its weights and bias parameters. We then use both ˜tupprev and hD t−1 separately to obtain two attentive context vectors, ˜qt and qt respectively. These are then concate- nated along with tupprev to deﬁne the current con- text of our LSTM-based decoder. The correspond- ing normalized attention scores, ˜αt and αt, are averaged to obtain the attention-weighted sentence representation at decoding time step t. A.3 Experimental Setup For our non-BERT experiments, word embeddings are initialized (and kept trainable) using pre-trained 300-dim. Glove vectors (Pennington et al., 2014), and accordingly dw is set to 300. The dimensions of POS and DEP embeddings, i.e. dpos and ddep are set to 50 each. The decoder (LSTM) hidden dimension dh is set to 300, and accordingly the hidden state dimensions of both backward and for- ward LSTMs of the Bi-LSTM-based encoder are set to 150 each. We set the hidden dimension dp of the Bi-LSTMs in pointer networks to 300. For our BERT experiments, uncased version of pre-trained BERT-base (Devlin et al., 2019) is ﬁne-tuned to encode each sentence. All our model variants are trained end-to-end with Adam optimizer (Kingma and Ba, 2015) with 10−3 as the learning rate, and 10−5 as weight de- cay. Dropout (0.5) (Srivastava et al., 2014) is ap- plied on embeddings to avoid overﬁtting. Our non- BERT model variants are trained for 100 epochs with a batch size of 10. Our BERT-based variants are trained for 30 epochs with a batch size of 16. Model selected according to the best F1 score on the development data is used to evaluate on the test data. We run each model ﬁve times and report the median scores. All our experiments are run on Tesla P100-PCIE 16GB GPU. 