VideoBERT : Joint Model Video Language Representation Learning Chen Sun , Austin Myers , Carl Vondrick , Kevin Murphy , Cordelia Schmid Google Research         p e      ] V C .  c [   v           .         : v  X r  Figure   : VideoBERT text-to-video generation future forecasting . ( ) Given recipe text divided sentences , = y  : , generate sequence video tokens x = x  : computing x∗ = arg maxk p ( xt = k|y ) using VideoBERT . ( ) Given video token , show top three future tokens forecasted VideoBERT different time scales . case , VideoBERT predicts bowl ﬂour cocoa powder may baked oven , may become brownie cupcake . visualize video tokens using images training set closest centroids feature space . Abstract   . Introduction Self-supervised learning become increasingly impor- tant leverage abundance unlabeled data avail- able platforms like YouTube . Whereas existing approaches learn low-level representations , propose joint visual-linguistic model learn high-level features without explicit supervision . particular , inspired recent success language modeling , build upon BERT model learn bidirectional joint distributions sequences visual linguistic tokens , derived vector quantization video data off-the-shelf speech recognition outputs , respectively . use VideoBERT nu- merous tasks , including action classiﬁcation video cap- tioning . show applied directly open- vocabulary classiﬁcation , conﬁrm large amounts training data cross-modal information critical performance . Furthermore , outperform state-of-the- art video captioning , quantitative results verify model learns high-level semantic features . Deep learning beneﬁt lot labeled data [    ] , hard acquire scale . Consequently lot recent interest “ self supervised learning ” , train model various “ proxy tasks ” , hope result discovery features representa- tions used downstream tasks . wide variety proxy tasks proposed image video domains . However , methods focus low level features ( e.g. , textures ) short temporal scales ( e.g. , motion patterns last second less ) . in- terested discovering high-level semantic features correspond actions events unfold longer time scales ( e.g . minutes ) , since representations would useful various video understanding tasks . paper , exploit key insight human language evolved words describe high-level objects events , thus provides natural source “ self ” supervision . particular , present simple way model relationship visual domain   Season steak salt pepper.Carefully place steak pan.Flip steak side.Now let rest enjoy delicious steak . output videoinputvideooutputvideofuturesVideoBERTVideoBERTinput text      Figure   : Additional text-to-video generation future forecasting examples VideoBERT , see Figure   details . linguistic domain combining three off-the-shelf meth- ods : automatic speech recognition ( ASR ) system con- vert speech text ; vector quantization ( VQ ) applied low-level spatio-temporal visual features derived pre- trained video classﬁcation models ; recently pro- posed BERT model [   ] learning joint distributions sequences discrete tokens . precisely , approach apply BERT learn model form p ( x , ) , x sequence “ visual words ” , sequence spoken words . Given joint model , easily tackle variety interesting tasks . example , perform text-to-video predic- tion , used automatically illustrate set instructions ( recipe ) , shown top examples Figure     . also perform traditional video-to-text task dense video captioning [    ] shown Figure   . Section  .  , show approach video captioning signiﬁcantly outperforms previous state-of-the-art [    ] YouCook II dataset [    ] . also use model “ unimodal ” fashion . example , implied marginal distribution p ( x ) lan- guage model visual words , use long- range forecasting . illustrated bottom examples Figure     . course , uncertainty future , model generate plausible guesses much higher level abstraction deep generative models video , based VAEs GANs ( see e.g. , [   ,   ,    ,    ] ) , tend predict small changes low level aspects scene , location pose small number objects . summary , main contribution paper simple way learn high level video representations capture semantically meaningful temporally long-range structure . remainder paper describes con- tribution detail . particular , Section   brieﬂy reviews related work ; Section   describes adapt recent progress natural language modeling video domain ; Section   presents results activity recognition video captioning tasks ; Section   concludes .   . Related Work Supervised learning . successful ap- proaches video representation learning leveraged large labeled datasets ( e.g. , [   ,    ,    ,   ] ) train convolu- tional neural networks video classiﬁcation . However , expensive collect labeled data , cor- responding label vocabularies often small ca- pable representing nuances many kinds actions ( e.g. , “ sipping ” slightly different “ drinking ” slightly different “ gulping ” ) . addition , ap- proaches designed representing short video clips , typically seconds long . main difference work focus long-term evolution events video , use manually provided labels . Unsupervised learning . Recently , variety ap- proaches learning density models video proposed . use single static stochastic variable , “ decoded ” sequence using RNN , either using VAE-style loss [    ,    ] GAN-style loss [    ,    ] . recent work uses temporal stochastic vari- ables , e.g. , SV P model [   ] SVGLP model [   ] . also various GAN-based approaches , SAVP approach [    ] MoCoGAN approach [    ] . differ work use BERT model , without explicit stochastic latent variables , ap- plied visual tokens derived video . Thus model generative model pixels , gen- erative model features derived pixels , approach used work ( e.g. , [    ] ) . Self-supervised learning . avoid difﬁculties learning joint model p ( x  : ) , become popular learn conditional models form p ( xt+  : |x  : ) , partition signal two blocks , gray output videoinputvideooutputvideofuturesVideoBERTVideoBERTinput textCut cabbage pieces.Put cabbage wok stir fry.Add soy sauce ... keep stir frying.Put plate dish ready served . scale color , previous frame next frame ( e.g. , [    ] ) , try predict one ( see e.g. , [    ] overview ) . approach similar , except use quantized visual words instead pixels . Furthermore , al- though learn set conditional distributions , model proper joint generative model , explained Section   . Cross-modal learning . multi-modal nature video also extensive source supervision learn- ing video representations , paper builds . Since videos contain synchronized audio visual signals , two modalities supervise learn strong self-supervised video representations [   ,    ,    ] .  work , use speech ( provided ASR ) rather low- level sounds source cross-modal supervision . Natural language models . build upon recent progress NLP community , large-scale lan- guage models ELMO [    ] BERT [   ] shown state-of-the-art results various NLP tasks , word level ( e.g. , POS tagging ) sentence level ( e.g. , semantic classiﬁcation ) . BERT model extended pre-train multi-lingual data [    ] . paper builds BERT model capture structure linguistic visual domains . Image video captioning . much re- cent work image captioning ( see e.g. , [    ,   ,    ] ) , model form p ( y|x ) , manually pro- vided caption x image . also work video captioning , using either manually provided temporal segmentation estimated segmentations ( see e.g. , [    ,    ] ) . use joint p ( x , ) model apply video captioning , achieve state-of-the-art results , discuss Section  .  . Instructional videos . Various papers ( e.g. , [    ,   ,    ,    ,    ] ) trained models analyse instructional videos , cooking . differ work use manual labeling , learn large-scale genera- tive model words ( discretized ) visual signals .   . Models section , brieﬂy summarize BERT model , describe extend jointly model video language data .  .  . BERT model BERT [   ] proposes learn language representations  using “ masked language model ” training objective . detail , let x = { x  , . . . , xL } set discrete to- kens , xl ∈ X . deﬁne joint probability distribution set follows : p ( x|θ ) =   Z ( θ ) L ( cid:   ) l=  φl ( x|θ ) ∝ exp ( cid:   ) log φl ( x|θ ) ( cid:   ) L ( cid:   ) l=  φl ( x ) l ’ th potential function , parameters θ , Z partition function . model permutation invariant . order capture order information , “ tag ” word position sentence . BERT model learns embed- ding word tokens , well tags , sums embedding vectors get continuous representation token . log potential ( energy ) functions location deﬁned log φl ( x|θ ) = xT l fθ ( x\l ) xl one-hot vector l ’ th token ( tag ) ,  x\l = ( x  , . . . , xl−  , MASK , xl+  , . . . , xL ) function f ( x\l ) multi-layer bidirectional trans- former model [    ] takes L × D  tensor , contain- ing D -dimensional embedding vectors corresponding x\l , returns L × D  tensor , D  size output transformer node . See [   ] details . model trained approximately maximize pseudo log-likelihood L ( θ ) = Ex∼D L ( cid:   ) l=  log p ( xl|x\l ; θ ) practice , stochastically optimize logloss ( computed softmax predicted f function ) sampling locations well training sentences . BERT extended model two sentences con- catenating together . However , often interested simply modeling extended sequence , rather relationships two sentences ( e.g. , pair consecutive randomly selected sentences ) . BERT accomplishes prepending every sequence spe- cial classiﬁcation token , [ CLS ] , joining sentences special separator token , [ SEP ] . ﬁnal hidden state corresponding [ CLS ] token used aggregate sequence representation predict label classiﬁcation tasks , may otherwise ignored . addition differentiating sentences [ SEP ] token , BERT also optionally tags token sentence comes . corresponding joint model written p ( x , , c ) , x ﬁrst sentence , second , c = {   ,   } label indicating whether sentences separate consecutive source document . consistency original paper , also add [ SEP ] token end sequence , even though strictly needed . , typical masked-out training sentence pair may look like : [ CLS ] let ’ make traditional [ MASK ] cuisine [ SEP ] orange corre- chicken [ MASK ] sauce [ SEP ] . sponding class label case would c =   , indicating x consecutive .  Figure   : Illustration VideoBERT context video text masked token prediction , cloze , task . task also allows training text-only video-only data , VideoBERT furthermore trained using linguistic-visual alignment classiﬁcation objective ( shown , see text details ) .  .  . VideoBERT model extend BERT video , way may still leverage pretrained language models scalable im- plementations inference learning , decided make minimal changes , transform raw visual data discrete sequence tokens . end , propose generate sequence “ visual words ” applying hi- erarchical vector quantization features derived video using pretrained model . See Section  .  details . Besides simplicity , approach encourages model focus high level semantics longer-range temporal dynamics video . contrast existing self-supervised approaches video representation learning , learn low-level properties local textures motions , discussed Section   . combine linguistic sentence ( derived video using ASR ) visual sentence generate data : [ CLS ] orange chicken [ MASK ] sauce [ > ] v   [ MASK ] v   v   [ SEP ] , v   v   visual tokens , [ > ] special token in- troduce combine text video sentences . See Figure   illustration . cloze task extends naturally sequences linguistic visual tokens , applying next sentence pre- diction task , used BERT , less straightforward . propose linguistic-visual alignment task , use ﬁnal hidden state [ CLS ] token predict whether linguistic sentence temporally aligned visual sen- tence . Note noisy indicator semantic relat- edness , since even instructional videos , speaker may referring something visually present . combat , ﬁrst randomly concatenate neighbor- ing sentences single long sentence , allow model learn semantic correspondence even two well aligned temporally . Second , since pace state transitions even action vary greatly be- tween different videos , randomly pick subsampling rate     steps video tokens . helps model robust variations video speeds , also allows model capture temporal dynamics greater time horizons learn longer-term state transi- tions . leave investigation ways combining video text future work . Overall , three training regimes corresponding different input data modalities : text-only , video-only video-text . text-only video-only , standard mask-completion objectives used training model . text-video , use linguistic-visual alignment clas- siﬁcation objective described . overall training objective weighted sum individual objectives . text objective forces VideoBERT well language modeling ; video objective forces learn “ language model video ” , used learning dynam- ics forecasting ; text-video objective forces learn correspondence two domains . trained model , use va- riety downstream tasks , work quantita- tively evaluate two applications . ﬁrst application , treat probabilistic model , ask predict im- pute symbols MASKed . illustrate Section  .  , perform “ zero-shot ” classiﬁ- cation . second application , extract predicted representation ( derived internal activations model ) [ CLS ] token , use dense vector representation entire input . combined features derived input used downstream supervised learning task . demonstrate Section  .  , perform video captioning .   . Experiments Analysis section describe experimental setup , show quantitative qualitative results . inPlacethe [ > ] thepansteak [ SEP ] [ CLS ] EinEPlaceEtheE [ > ] EtheEpanE [ MASK ] E [ SEP ] E [ CLS ] E [ MASK ] Ev ( ) Ev ( ) Ev ( ) Ev ( ) inPlacethe [ > ] thepan [ SEP ] [ CLS ] [ MASK ] [ MASK ] T T T T T T T T  T T  T  T  T  T VideoBERT  .  . Dataset Deep learning models , language vision do- mains , consistently demonstrated dramatic gains performance increasingly large datasets . example , “ large ” BERT model ( use ) pretrained concatenation BooksCorpus (    M words ) English Wikipedia (  ,   M words ) . Therefore , would like train VideoBERT comparably large-scale video dataset . Since inter- ested connection language vision , would like ﬁnd videos spoken words likely refer visual content . Intuitively , often case instructional videos , focus cooking videos speciﬁcally , since well studied domain existing annotated datasets available evaluation . Unfor- tunately , datasets relatively small , turn YouTube collect large-scale video dataset training . extract set publicly available cooking videos YouTube using YouTube video annotation sys- tem retrieve videos topics related “ cooking ” “ recipe ” . also ﬁlter videos duration , removing videos longer    minutes , resulting set    K videos . total duration dataset   ,    hours , roughly     days . reference , two or- ders magnitude larger next largest cooking video dataset , YouCook II , consists  K videos to- tal duration     hours [    ] . obtain text videos , utilize YouTube ’ au- tomatic speech recognition ( ASR ) toolkit provided YouTube Data API [   ] retrieve timestamped speech in- formation . API returns word sequences pre- dicted language type . Among    K videos ,    K ASR retrieved API ,    K predicted English . experiments , use videos video-only objective , use text English ASR VideoBERT ’ text-only video- text objectives . evaluate VideoBERT YouCook II dataset [    ] , contains      YouTube videos averaging  .   min- utes duration , total     hours . videos manually annotated segmentation boundaries captions . average  .  segments per video ,  .  words per caption . use provided dataset split ,      videos training     validation . avoid po- tential bias pretraining , also remove videos appear YouCook II pretraining set .  .  . Video Language Preprocessing input video , sample frames    fps , create clips   -frame (  .  seconds ) non-overlapping windows video .   -frame clip , apply pretrained video ConvNet extract features . work , use S D [    ] adds separable temporal convolutions Inception network [    ] backbone . take feature activations ﬁnal linear classiﬁer apply  D average pooling obtain     -dimension feature vector . pretrain S D network Kinet- ics [   ] dataset , covers wide spectrum actions YouTube videos , serves generic representa- tion individual clip . tokenize visual features using hierarchical k- means . adjust number hierarchy levels number clusters per level k visually inspecting co- herence representativeness clusters . set =   k =    , yields     =       clusters total . Figure   illustrates result “ vector quantization ” process . ASR word sequence , break stream words sentences adding punctuation using off-the-shelf LSTM-based language model . sen- tence , follow standard text preprocessing steps BERT [   ] tokenize text WordPieces [    ] . use vocabulary provided authors BERT , contains   ,    tokens . Unlike language naturally broken sen- tences , unclear break videos semantically coherent segments . use simple heuristic address problem : ASR sentence available , as- sociated starting ending timestamps , treat video tokens fall time period segment . ASR available , simply treat    tokens segment .  .  . Model Pre-training initialize BERT weights text pre-trained checkpoint . Speciﬁcally , use BERTLARGE model re- leased authors [   ] , using backbone archi- tecture :    layers Transformer blocks , block      hidden units    self-attention heads . add support video tokens appending   ,    entries word embedding lookup table new “ visual words ” . initialize entries S D features corresponding cluster centroids . input embeddings frozen pretraining . model training process largely follows setup BERT : use   Cloud TPUs Pod conﬁguration total batch size     , train model  .  million iterations , roughly   epochs . use Adam optimizer initial learning rate  e-  , linear decay learning rate schedule . training process takes around   days .  .  . Zero-shot action classiﬁcation pretrained , VideoBERT model used “ zero-shot ” classiﬁcation novel datasets , YouCook II ( “ zero-shot ” mean model Figure   : Examples video sentence pairs pretraining videos . quantize video segment token , represent corresponding visual centroid . row , show original frames ( left ) visual centroids ( right ) . see tokenization process preserves semantic information rather low-level visual appearance . trained YouCook II data label ontol- ogy used YouCook II ) . precisely , want com- pute p ( y|x ) x sequence visual tokens , sequence words . Since model trained predict sentences , deﬁne ﬁxed sentence , “ let show [ MASK ] [ MASK ] , ” ex- tract verb noun labels tokens predicted ﬁrst second masked slots , respectively . See Figure   qualitative results . quantitative evaluation , use YouCook II dataset . [    ] , authors collected ground truth bound- ing boxes    common objects validation set YouCook II . However , ground truth la- bels actions , many common objects labeled . , collect action object labels , derived ground truth captions , address shortcoming . run off-the-shelf part-of-speech tagger ground truth captions retrieve     common nouns    common verbs , use derive ground truth labels . VideoBERT ’ word piece vocabulary gives power effectively perform open-vocabulary clas- siﬁcation , thus likely make semantically cor- rect predictions exactly match limited ground truth . , report top-  top-  classiﬁca- tion accuracy metrics , latter intended miti- gate issue , leave sophisticated evaluation techniques future work . Lastly , one verb noun associated video clip , deem prediction correct matches . report performance validation set YouCook II . Table   shows top-  top-  accuracies VideoBERT ablations . verify VideoBERT actually makes use video inputs , ﬁrst remove video inputs VideoBERT , use language Figure   : Using VideoBERT predict nouns verbs given video clip . See text details . video clip ﬁrst converted video tokens ( two shown example ) , visualized using centroids . Method S D [    ] BERT ( language prior ) VideoBERT ( language prior ) VideoBERT ( cross modal ) Supervision verb top-  ( % ) verb top-  ( % ) object top-  ( % ) object top-  ( % ) yes      .   .   .   .    .   .   .    .    .   .   .    .    .   .    .    .  Table   : Action classiﬁcation performance YouCook II dataset . See text details . Method Data size verb top-  ( % ) verb top-  ( % ) object top-  ( % ) object top-  ( % ) VideoBERT VideoBERT VideoBERT VideoBERT   K   K    K    K  .   .   .   .    .    .    .    .   .   .    .    .    .    .    .    .  Table   : Action classiﬁcation performance YouCook II dataset function pre-training data size . model p ( ) perform prediction . also use lan- guage prior text-only BERT model , ﬁne-tuned cooking videos . see VideoBERT signiﬁcantly outperforms baselines . expected , language prior VideoBERT adapted cooking sen- tences , better vanilla BERT model . compare fully supervised classiﬁer trained using training split YouCook II . use pre-computed S D features ( inputs VideoBERT ) , applying average pooling time , followed linear classiﬁer . Table   shows results . see , supervised framework outperforms VideoBERT top-  verb accuracy , surprising given VideoBERT effectively open vocabulary . ( See Fig- ure   illustration ambiguity action la- bels . ) However , top-  accuracy metric reveals VideoBERT achieves comparable performance fully supervised S D baseline , without using supervision YouCook II , indicating model able per- form competitively “ zero-shot ” setting .  .  . Beneﬁts large training sets also studied impact size pretrain- ing dataset . experiment , take random subsets   K ,   K    K videos pretraining set , pretrain VideoBERT using setup , number epochs . Table   shows perfor- mance . see accuracy grows monotonically amount data increases , showing signs satura- tion . indicates VideoBERT may beneﬁt even larger pretraining datasets .  .  . Transfer learning captioning demonstrate effectiveness VideoBERT used feature extractor . extract features given video inputs , use simple ﬁll-in-the-blank task , appending video tokens template sentence “ let ’ [ MASK ] [ MASK ] [ MASK ] , [ MASK ] [ MASK ] . ” extract fea- tures video tokens masked text tokens , take average concatenate two together , used supervised model downstream task . evaluate extracted features video captioning , following setup [    ] , ground truth video segmentations used train supervised model map- ping video segments captions . use model , namely transformer encoder-decoder , replace inputs encoder features derived VideoBERT described . also concatenate VideoBERT features average-pooled S D features ; baseline , also consider using S D features without VideoBERT . set number Transformer block lay- ers   , hidden unit size     , Dropout probability  .  . use  -fold cross validation training split set hyper-parameters , report performance validation set . train model   K iterations batch size     . use Adam optimizer VideoBERT pre-training , set initial learning rate  e-  linear decay schedule . Table   shows results . follow standard prac- tice machine translation compute BLEU ME- TEOR scores micro-averaged corpus level , also re- port ROUGE-L [    ] CIDEr [    ] scores . base- line method [    ] , recompute metrics using predictions provided authors . see VideoBERT consistently outperforms S D baseline , es- pecially CIDEr . also see cross-modal pre- training outperforms video-only version . Furthermore , concatenating features VideoBERT S D , model achieves best performance across metrics  . Figure   shows qualitative results . note predicted word sequence rarely exactly equal ground truth , explains metrics Table   ( measure n-gram overlap ) low absolute value . However , semantically results seem reasonable .  The metrics used [    ] macro-averaged video level may suffer undesirable sparsity artifacts . Using provided evaluation code , VideoBERT + S D B @    .   , METEOR   .   . Method BLEU-  BLEU-  METEOR ROUGE-L CIDEr Zhou et al . [    ] S D [    ] VideoBERT ( video ) VideoBERT VideoBERT + S D  .    .    .    .    .    .    .    .    .    .     .    .     .     .     .     .     .     .     .     .    .    .    .    .    .   Table   : Video captioning performance YouCook II . follow setup [    ] report captioning performance validation set , given ground truth video segments . Higher numbers better . Figure   : Examples generated captions VideoBERT S D baseline . last example , VideoBERT fails exploit full temporal context , since misses paper towel frame .   . Discussion conclusion paper adapts powerful BERT model learn joint visual-linguistic representation video . exper- imental results demonstrate able learn high- level semantic representations , outperform state- of-the-art video captioning YouCook II dataset . also show model used directly open- vocabulary classiﬁcation , performance grows monotonically size training set . work ﬁrst step direction learning joint representations . many applications , includ- ing cooking , important use spatially ﬁne-grained vi- sual representations , instead working frame clip level , distinguish individual objects attributes . envision either using pretrained object detection semantic segmentation models , using unsu- pervised techniques broader coverage . also want explicitly model visual patterns multiple temporal scales , instead current approach , skips frames builds single vocabulary . Beyond improving model , plan assess ap- proach video understanding tasks , do- mains besides cooking . ( example , may use re- cently released COIN dataset manually labeled instruc- tional videos [    ] . ) believe future prospects large scale representation learning video language look quite promising . Acknowledgements . would like thank Jack Hessel , Bo Pang , Radu Soricut , Baris Sumengen , Zhenhai Zhu , BERT team sharing amazing tools greatly fa- cilitated experiments ; Justin Gilmer , Abhishek Kumar , David Ross , Rahul Sukthankar helpful discussions . Chen would like thank Y. M. inspiration . References [   ] YouTube Data API . https : //developers.google . com/youtube/v /docs/captions .   [   ] Jean-Baptiste Alayrac , Piotr Bojanowski , Nishant Agrawal , Josef Sivic , Ivan Laptev , Simon Lacoste-Julien . Unsu- pervised learning narrated instruction videos . CVPR ,      .   [   ] Yusuf Aytar , Carl Vondrick , Antonio Torralba . Sound- net : Learning sound representations unlabeled video . NeurIPS ,      .   [   ] Mohammad Babaeizadeh , Chelsea Finn , Dumitru Erhan , Roy H Campbell , Sergey Levine . Stochastic variational video prediction . ICLR ,      .   [   ] Emily Denton Rob Fergus . Stochastic video generation learned prior . ICML ,      .   [   ] Jacob Devlin , Ming-Wei Chang , Kenton Lee , Kristina BERT : Pre-training deep bidirectional arXiv preprint Toutanova . transformers language understanding . arXiv:    .      ,      .   ,   ,   [   ] Chunhui Gu , Chen Sun , David Ross , Carl Vondrick , Car- oline Pantofaru , Yeqing Li , Sudheendra Vijayanarasimhan , George Toderici , Susanna Ricco , Rahul Sukthankar , et al . AVA : video dataset spatio-temporally localized atomic visual actions . CVPR ,      .   [   ] Andrej Karpathy Li Fei-Fei . Deep visual-semantic align- ments generating image descriptions . CVPR ,      .   [   ] Kay , Joao Carreira , Karen Simonyan , Brian Zhang , Chloe Hillier , Sudheendra Vijayanarasimhan , Fabio Viola , Tim Green , Trevor Back , Paul Natsev , et al . kinetics hu- man action video dataset . arXiv preprint arXiv:    .      ,      .   ,   [    ] Ranjay Krishna , Kenji Hata , Frederic Ren , Li Fei-Fei , Juan Carlos Niebles . Dense-Captioning events videos . ICCV ,      .   ,   [    ] Girish Kulkarni , Visruth Premraj , Sagnik Dhar , Siming Li , Yejin Choi , Alexander C Berg , Tamara L Berg . Baby talk : Understanding generating image descriptions . CVPR ,      .   [    ] Guillaume Lample Alexis Conneau . Cross-lingual lan- guage model pretraining . arXiv preprint arXiv:    .      ,      .   [    ] Alex X Lee , Richard Zhang , Frederik Ebert , Pieter Abbeel , Chelsea Finn , Sergey Levine . Stochastic adversarial video prediction . arXiv:    .      ,      .   [    ] Chin-Yew Lin . Rouge : package automatic evaluation summaries . Text Summarization Branches ,      .   [    ] Jiasen Lu , Jianwei Yang , Dhruv Batra , Devi Parikh . Neural baby talk . CVPR ,      .   [    ] Jonathan Malmaud , Jonathan Huang , Vivek Rathod , Nick Johnston , Andrew Rabinovich , Kevin Murphy . ’ cookin ’ ? interpreting cooking videos using text , speech vision . NAACL , Mar .      .   [    ] Michael Mathieu , Camille Couprie , Yann LeCun . Deep multi-scale video prediction beyond mean square error . ICLR ,      .   [    ] Ishan Misra , C Lawrence Zitnick , Martial Hebert . Shuf- ﬂe learn : unsupervised learning using temporal order veriﬁcation . ECCV ,      .   [    ] Mathew Monfort , Alex Andonian , Bolei Zhou , Kandan Ra- makrishnan , Sarah Adel Bargal , Yan Yan , Lisa Brown , Quanfu Fan , Dan Gutfreund , Carl Vondrick , et al . Moments time dataset : one million videos event understanding . TPAMI ,      .   [    ] Andrew Owens , Phillip Isola , Josh McDermott , Antonio Tor- ralba , Edward H Adelson , William Freeman . Visually indicated sounds . CVPR ,      .   [    ] Andrew Owens , Jiajun Wu , Josh H McDermott , William Freeman , Antonio Torralba . Ambient sound provides supervision visual learning . ECCV ,      .   [    ] Matthew E Peters , Mark Neumann , Mohit Iyyer , Matt Gard- ner , Christopher Clark , Kenton Lee , Luke Zettlemoyer . Deep contextualized word representations . NAACL ,      .   [    ] Marc Aurelio Ranzato Alex Graves . Deep unsupervised learning . NIPS Tutorial ,      .   [    ] Chen Sun , Abhinav Shrivastava , Saurabh Singh , Abhi- nav Gupta . Revisiting unreasonable effectiveness data deep learning era . ICCV ,      .   [    ] Christian Szegedy , Wei Liu , Yangqing Jia , Pierre Sermanet , Scott E. Reed , Dragomir Anguelov , Dumitru Erhan , Vincent Vanhoucke , Andrew Rabinovich . Going deeper convolutions . arXiv preprint arXiv:    .     ,      .   [    ] Yansong Tang , Dajun Ding , Yongming Rao , Yu Zheng , Danyang Zhang , Lili Zhao , Jiwen Lu , Jie Zhou . COIN : large-scale dataset comprehensive instructional video analysis . CVPR ,      .   [    ] Sergey Tulyakov , Ming-Yu Liu , Xiaodong Yang , Jan Kautz . MoCoGAN : Decomposing motion content video generation . CVPR ,      .   [    ] Ashish Vaswani , Noam Shazeer , Niki Parmar , Jakob Uszko- reit , Llion Jones , Aidan N Gomez , Lukasz Kaiser , Illia Polosukhin . Attention need . NIPS ,      .   [    ] Ramakrishna Vedantam , C Lawrence Zitnick , Devi Parikh . Cider : Consensus-based image description evalua- tion . CVPR ,      .   [    ] Carl Vondrick , Hamed Pirsiavash , Antonio Torralba . An-  ticipating visual representations unlabeled video . CVPR ,      .   [    ] Carl Vondrick , Hamed Pirsiavash , Antonio Torralba . Generating videos scene dynamics . NeurIPS ,      .   [    ] Jacob Walker , Carl Doersch , Abhinav Gupta , Martial Hebert . uncertain future : Forecasting static images using variational autoencoders . ECCV ,      .   [    ] Yonghui Wu , Mike Schuster , Zhifeng Chen , Quoc V Le , Mohammad Norouzi , Wolfgang Macherey , Maxim Krikun , Yuan Cao , Qin Gao , Klaus Macherey , et al . Google ’ neural machine translation system : Bridging gap be- arXiv preprint tween human machine translation . arXiv:    .      ,      .   [    ] Saining Xie , Chen Sun , Jonathan Huang , Zhuowen Tu , Kevin Murphy . Rethinking spatiotemporal feature learning video understanding . ECCV ,      .   ,   ,   [    ] Tianfan Xue , Jiajun Wu , Katherine Bouman , Bill Free- man . Visual dynamics : Probabilistic future frame synthesis via cross convolutional networks . NIPS ,      .   [    ] Hang Zhao , Zhicheng Yan , Heng Wang , Lorenzo Torresani , Antonio Torralba . Slac : sparsely labeled dataset arXiv preprint action classiﬁcation localization . arXiv:    .      ,      .   [    ] Luowei Zhou , Nathan Louis , Jason J Corso . Weakly- supervised video object grounding text loss weight- ing object interaction . BMVC ,      .   [    ] Luowei Zhou , Chenliang Xu , Jason J Corso . Towards automatic learning procedures web instructional videos . AAAI ,      .   ,   ,   [    ] Luowei Zhou , Yingbo Zhou , Jason J. Corso , Richard Socher , Caiming Xiong . End-to-end dense video captioning masked transformer . CVPR ,      .   ,   ,   ,   Figure A  : Visualizations video text prediction . example , show key frames original video ( top left ) associated ASR outputs ( top right ) , show centroid images video tokens ( bottom left ) top predicted verbs nouns VideoBERT ( bottom right ) . Note ASR outputs used predict verbs nouns . Figure A  : Visualizations video video prediction . Given input video token , show top   predicted video tokens   steps away future . visualize video token centroids . Figure A  : Visualizations text video prediction . particular , make small changes input text , compare generated video tokens vary . show top   retrieved video tokens text query . 